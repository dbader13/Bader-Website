<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | David A. Bader</title>
    <link>http://localhost:1313/blog/</link>
      <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 07 Oct 2025 11:32:11 -0500</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/sharing.jpg</url>
      <title>Posts</title>
      <link>http://localhost:1313/blog/</link>
    </image>
    
    <item>
      <title>41% of Schools Report AI Cyber Incidents</title>
      <link>http://localhost:1313/blog/20251007-technewsworld/</link>
      <pubDate>Tue, 07 Oct 2025 11:32:11 -0500</pubDate>
      <guid>http://localhost:1313/blog/20251007-technewsworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20251007-technewsworld/students-in-computer-lab_hu_5232e3fd4086a2d8.webp 400w,
               /blog/20251007-technewsworld/students-in-computer-lab_hu_e9163e2f158875b.webp 760w,
               /blog/20251007-technewsworld/students-in-computer-lab_hu_1ee5ecfa753a2aa6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20251007-technewsworld/students-in-computer-lab_hu_5232e3fd4086a2d8.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Some 41% of schools in the United States and the United Kingdom have experienced AI-related cyber incidents, ranging from phishing campaigns to harmful student-generated content, according to a recently released study from a systems identity and access management firm.&lt;/p&gt;
&lt;p&gt;Among the 41% of schools experiencing an AI-related cyber incident, 11% reported that the incident caused a disruption, while 30% noted that the incident was contained quickly, according to a survey of 1,460 education administrators in the U.S. and U.K. conducted by TrendCandy for &lt;a href=&#34;https://www.keepersecurity.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Keeper Security&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Most institutions (82%) said they feel at least “somewhat prepared” to handle AI-related cybersecurity threats, though that number falls to 32% for those who feel “very prepared,” the survey noted. This confidence, tempered by caution, shows that schools are aware of risks, but sizable gaps remain in overall preparedness, and uncertainty persists about the effectiveness of existing safeguards.&lt;/p&gt;
&lt;p&gt;“Our research found that while almost every education leader is concerned about AI-related threats, only one in four feels confident in identifying them,” said Keeper Security Cybersecurity Evangelist Anne Cutler.&lt;/p&gt;
&lt;p&gt;“The challenge is not a lack of awareness, but the difficulty of knowing when AI crosses the line from helpful to harmful,” she told TechNewsWorld. “The same tools that help a student brainstorm an essay can also be misused to create a convincing phishing message or even a deepfake of a classmate. Without visibility, schools struggle to separate legitimate use from activity that introduces risk.”&lt;/p&gt;
&lt;h2 id=&#34;ai-cyber-incidents-underreported&#34;&gt;AI Cyber Incidents Underreported?&lt;/h2&gt;
&lt;p&gt;The finding that 41% of schools have already experienced an AI-related cyber incident is surprisingly high, though perhaps not unexpected given the rapid and largely uncontrolled proliferation of AI tools in educational settings, observed &lt;a href=&#34;https://people.njit.edu/profile/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology (NJIT), in Newark, N.J.&lt;/p&gt;
&lt;p&gt;“This number is concerning because it suggests that nearly half of our educational institutions are dealing with security challenges before they’ve had the opportunity to establish proper safeguards,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“Schools have historically been vulnerable targets for cyberattacks due to limited cybersecurity budgets and IT staffing, and the introduction of AI tools — many of which students and faculty adopt independently without institutional vetting — has dramatically expanded the attack surface.&lt;/p&gt;
&lt;p&gt;“What’s particularly troubling is that this 41% likely represents only the incidents that schools have detected and reported, meaning the actual number could be considerably higher,” he said.&lt;/p&gt;
&lt;p&gt;James McQuiggan, CISO advisor at &lt;a href=&#34;https://www.knowbe4.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KnowBe4&lt;/a&gt;, a security awareness training provider in Clearwater, Fla., agreed. “Based on how quickly schools are trying to adopt AI tools and most likely without any strong cybersecurity hygiene practices, this number could be conservative,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“Unfortunately,” he continued, “many schools do not have the necessary resources and governance to manage AI securely and safely for their students, which increases the risk of data exposure and misuse.”&lt;/p&gt;
&lt;p&gt;“The number is not surprising when you consider those incidents include phishing emails,” added Paul Bischoff, consumer privacy advocate at &lt;a href=&#34;https://www.comparitech.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Comparitech&lt;/a&gt;, a reviews, advice, and information website for consumer security products.&lt;/p&gt;
&lt;p&gt;“Many phishing campaigns are not run by native English speakers,” he told TechNewsWorld. “The AI helps them craft more convincing phishing emails with fewer mistakes.”&lt;/p&gt;
&lt;p&gt;According to the 2025 Verizon Data Breach Investigations Report, phishing accounts for 77% of breaches in the education sector, making it the most common attack on that sector.&lt;/p&gt;
&lt;h2 id=&#34;ai-adoption-spreads-across-schools&#34;&gt;AI Adoption Spreads Across Schools&lt;/h2&gt;
&lt;p&gt;The study also found that AI is now a common part of classrooms and faculty offices. Eighty-six percent of institutions permit the use of AI tools by students, while only 2% have banned them outright. Among faculty, it added, adoption is even higher at 91%.&lt;/p&gt;
&lt;p&gt;It reported that students are primarily using AI for supportive and exploratory tasks. The most common uses are research (62%), brainstorming (60%), and language assistance (49%). Creative projects (45%) and revision support (40%) follow, while more sensitive tasks, such as coding (30%) and completing assignments (27%), are more tightly controlled.&lt;/p&gt;
&lt;p&gt;“While the report shows that 86% of schools allow student use of AI and 91% of faculty use it, the reality is that schools have largely lost the ability to meaningfully prohibit AI use even if they wanted to,” argued NJIT’s Bader. “AI tools are freely available on personal devices, and students are accessing them outside school networks regardless of institutional policies.”&lt;/p&gt;
&lt;p&gt;“The more productive question isn’t whether to allow AI, but how to integrate it responsibly,” he said. “Schools face a choice not about whether AI will be used, but whether they’ll take a leadership role in shaping that use through education, ethical frameworks, and appropriate guardrails.”&lt;/p&gt;
&lt;p&gt;“Attempting to ban AI would be both futile and counterproductive,” he added. “It would simply push usage underground while denying students the digital literacy skills they’ll need in an AI-augmented world.”&lt;/p&gt;
&lt;p&gt;Sam Whitaker, vice president of social impact and strategic initiatives at &lt;a href=&#34;https://www.studyfetch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StudyFetch&lt;/a&gt;, an AI-powered platform that specializes in transforming course materials into interactive study tools for students and educators, in Los Angeles, warned that there’s a difference between “AI use” and “responsible AI use.”&lt;/p&gt;
&lt;p&gt;“Unrestricted use of productivity tools like ChatGPT or AI learning platforms that aren’t built for learning is not only dangerous, it’s potentially catastrophic for students’ long-term creativity and critical thinking,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“Schools have not only a choice but a responsibility to provide responsible solutions that are truly built for learning and not cheating,” he added.&lt;/p&gt;
&lt;h2 id=&#34;school-policies-lag-behind-ai-use&#34;&gt;School Policies Lag Behind AI Use&lt;/h2&gt;
&lt;p&gt;While schools and universities are building frameworks to govern AI use, the study noted, implementation is uneven. Policy development is still playing catch-up to practice, it explained, with just over half having detailed policies (51%) or informal guidance (53%) in place, while less than 60% deploy AI detection tools and student education programs.&lt;/p&gt;
&lt;p&gt;With more than 40% already impacted, it continued, the fact that only a third (34%) have dedicated budgets and just 37% have incident response plans indicates a concerning gap in preparedness.&lt;/p&gt;
&lt;p&gt;Keeper’s Cutler pointed out that relying on informal guidelines rather than formal policies leaves both students and faculty uncertain about how AI can safely be used to enhance learning and where it could create unintended risks.&lt;/p&gt;
&lt;p&gt;“What we found is that the absence of policy is less about reluctance and more about being in catch-up mode,” she said. “Schools are embracing AI use, but governance hasn’t kept pace.”&lt;/p&gt;
&lt;p&gt;“Policies provide a necessary framework that balances innovation with accountability,” she explained. “That means setting expectations for how AI can support learning, ensuring sensitive information such as student records or intellectual property cannot be shared with external platforms, and mandating transparency about when and how AI is used in coursework or research. Taken together, these steps preserve academic integrity and protect sensitive data.”&lt;/p&gt;
&lt;p&gt;While policies governing AI are necessary, they shouldn’t be formulated with a cookie-cutter approach. “It is important to remember that a one-size-fits-all approach most likely won’t work here,” warned Elyse J. Thulin, a research assistant professor at the University of Michigan’s Institute for Firearm Injury Prevention. “A baseline of guidance is important, but different organizations will need to tailor their approaches based on their needs and infrastructure.”&lt;/p&gt;
&lt;p&gt;“With any new technology comes potential for harm and wrongful use, as well as the benefits of advancement,” she added. “This does not necessarily mean the technology is harmful or wrong. We just need to work to identify ways to prevent it from being used improperly.”&lt;/p&gt;
&lt;p&gt;“AI is included in that and is developing at an extremely rapid pace, so continued support for research in this area is absolutely critical,” she continued. “The more we can study and identify these use patterns, the better we establish evidence-based strategies and solutions to ultimately protect students and make school environments safer for everyone.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technewsworld.com/story/41-of-schools-report-ai-cyber-incidents-179949.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technewsworld.com/story/41-of-schools-report-ai-cyber-incidents-179949.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nvidia Analysts See Up To Five Times Return On $100 Billion OpenAI Deal. Is Nvidia A Buy Now?</title>
      <link>http://localhost:1313/blog/20250924-investors/</link>
      <pubDate>Wed, 24 Sep 2025 10:20:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250924-investors/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Vidya Ramakrishnan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nvidia stock (&lt;a href=&#34;https://research.investors.com/quote.aspx?symbol=NVDA&amp;amp;_gl=1*1qonkel*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVDA&lt;/a&gt;) touched an all-time high in intraday trade Monday after the company said it would invest up to $100 billion in ChatGPT developer OpenAI. Nvidia shares fell to test their &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/what-is-the-50-day-moving-average-when-to-buy-or-sell-growth-stocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;50-day moving average&lt;/a&gt; on Wednesday. Is Nvidia stock a &lt;a href=&#34;https://www.investors.com/research/nvidia-stock-nvda-buy-or-sell-what-investors-need-to-know-jensen-huang/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;buy or sell now&lt;/a&gt;?&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.investors.com/news/technology/nvidia-stock-openai-strategic-partnership/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Under the strategic partnership&lt;/a&gt;, OpenAI will deploy at least 10 gigawatts of AI data centers running Nvidia processors. The move shows that Nvidia is moving beyond selling artificial intelligence hardware to becoming &amp;ldquo;equity partners in the AI economy they are enabling,&amp;rdquo; &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology, told &lt;a href=&#34;https://www.marketwatch.com/story/why-nvidia-is-pouring-100-billion-into-openai-f2cb6983?mod=mw_quote_news_topstories&amp;amp;_gl=1*16uovj6*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MarketWatch&lt;/a&gt;. Nvidia recently announced a $5 billion stake in Intel (&lt;a href=&#34;https://research.investors.com/quote.aspx?symbol=INTC&amp;amp;_gl=1*16uovj6*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;INTC&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The Nvidia/OpenAI deal could raise antitrust concerns, Andre Barlow, an antitrust lawyer with Doyle, Barlow &amp;amp; Mazard, said, according to a &lt;a href=&#34;https://www.reuters.com/legal/litigation/nvidias-100-billion-openai-play-raises-big-antitrust-issues-2025-09-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reuters report&lt;/a&gt;, indicating the partnership may capture the Department of Justice&amp;rsquo;s attention.&lt;/p&gt;
&lt;p&gt;Though details of the Nvidia investment are not clear, some analysts appear unfazed. Evercore ISI &lt;a href=&#34;https://www.marketwatch.com/story/nvidias-openai-deal-adds-to-a-brewing-concern-but-will-that-actually-hurt-the-stock-3bc3a015?mod=mw_quote_news_topstories&amp;amp;_gl=1*1175bh*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;raised its Nvidia&amp;rsquo;s sales esimates&lt;/a&gt; by $5.5 billion in the second half of 2026 and boosted its price target to 225 from 214. Bank of America analysts see the OpenAI investment returning three to five times the initial investment.&lt;/p&gt;
&lt;p&gt;But Stacy Rasgon at Bernstein raised &amp;ldquo;circular concerns&amp;rdquo; with Nvidia investing in startups that then go on to buy its graphic processing units. But the analyst noted that demand remained healthy.&lt;/p&gt;
&lt;h2 id=&#34;us-china-talks&#34;&gt;U.S.-China Talks&lt;/h2&gt;
&lt;p&gt;Nvidia stock suffered a slight loss last week after some news about China regulators.&lt;/p&gt;
&lt;p&gt;On Wednesday, China&amp;rsquo;s Cyberspace Administration ordered major tech companies to stop buying artificial intelligence chips made by Nvidia, The Financial Times reported.&lt;/p&gt;
&lt;p&gt;Earlier, China&amp;rsquo;s State Administration for Market Regulation was reported to be &lt;a href=&#34;https://www.investors.com/news/technology/nvidia-stock-china-pressures-us-chipmakers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;investigating whether Nvidia breached&lt;/a&gt; the country&amp;rsquo;s antitrust rules by acquiring Mellanox Technology in 2020.&lt;/p&gt;
&lt;p&gt;Nvidia said it had &lt;a href=&#34;https://nvidianews.nvidia.com/news/nvidia-receives-approval-to-proceed-with-mellanox-acquisition-from-chinas-antitrust-authority&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;received approval &amp;ldquo;from all necessary authorities&amp;rdquo;&lt;/a&gt; to move forward with its planned acquisition of the network technology group.&lt;/p&gt;
&lt;p&gt;On Friday, President Donald Trump and President Xi Jinping of China had a phone conversation and agreed to more talks in October, &lt;a href=&#34;https://www.barrons.com/articles/xi-trump-call-tiktok-trade-93808200?mod=article_inline&amp;amp;_gl=1*1tasctu*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Barron&amp;rsquo;s&lt;/a&gt; reported.&lt;/p&gt;
&lt;h2 id=&#34;nvidia-stock-second-quarter-results&#34;&gt;Nvidia Stock: Second-Quarter Results&lt;/h2&gt;
&lt;p&gt;Nvidia reported &lt;a href=&#34;https://www.investors.com/news/technology/nvidia-stock-nvidia-earnings-live-coverage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fiscal second-quarter earnings&lt;/a&gt; of $1.05 per share, topping the estimate of $1.01 per share. Sales of $46.74 billion beat views of $46.05 billion. The company reported zero sales from its China H20 chip and did not include H20 sales to China in its fiscal third-quarter revenue outlook of $54 billion.&lt;/p&gt;
&lt;p&gt;Wall Street was expecting $53.43 billion. The company also announced a $60 billion buyback.&lt;/p&gt;
&lt;h2 id=&#34;analyst-estimates-ahead-of-earnings&#34;&gt;Analyst Estimates Ahead Of Earnings&lt;/h2&gt;
&lt;p&gt;Ahead of Nvidia&amp;rsquo;s second-quarter earnings report, William Blair analyst Sebastien Naji gave an outperform rating with a &lt;a href=&#34;https://www.barrons.com/articles/nvidia-stock-price-earnings-e9acd4fa?mod=search_headline&amp;amp;_gl=1*1tasctu*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;price target of $205&lt;/a&gt;. The analyst expected zero revenue from China for the second quarter, but had anticipated China would boost Nvidia&amp;rsquo;s outlook.&lt;/p&gt;
&lt;p&gt;Susquehanna analyst Christopher Rolland raised his price target for Nvidia &lt;a href=&#34;https://www.barrons.com/articles/nvidia-stock-price-ai-bubble-b4dd712d?mod=mw_quote_news&amp;amp;adobe_mc=MCMID%3D66489638613658054300386610654558475837%7CMCORGID%3DCB68E4BA55144CAA0A4C98A5%2540AdobeOrg%7CTS%3D1755783578&amp;amp;_gl=1*112z0rn*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;to $210 from $180&lt;/a&gt; while maintaining a positive rating. However, in his earnings preview note, the analyst was cautious about Nvidia&amp;rsquo;s revenue from its H20 chips.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theinformation.com/articles/nvidia-orders-halt-h20-production-china-directive-purchases&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Information&lt;/a&gt; recently reported that Nvidia had ordered component makers of its China chip to stop production. This comes after the Cyberspace Administration of China had asked Nvidia in July to explain if its chips could be tracked or shut down remotely.&lt;/p&gt;
&lt;p&gt;Chief Executive Jensen Huang &lt;a href=&#34;https://www.nytimes.com/2025/08/22/business/jensen-huang-nvidia-china.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said the chips&lt;/a&gt; do not have such capabilities. Separately, the AI giant may also be working on a new chip for China that may be more powerful than the H20, &lt;a href=&#34;https://www.reuters.com/world/china/nvidia-working-new-ai-chip-china-that-outperforms-h20-sources-say-2025-08-19/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reuters&lt;/a&gt; reported.&lt;/p&gt;
&lt;h2 id=&#34;nvidias-china-revenue-deal&#34;&gt;Nvidia&amp;rsquo;s China Revenue Deal&lt;/h2&gt;
&lt;p&gt;In August, Nvidia struck a deal that secured a license to sell its H20 AI chips in China, but, in exchange, the company will &lt;a href=&#34;https://www.investors.com/news/technology/nvidia-amd-give-15-china-chip-revenue-u-s-government/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;give 15% of its revenue from chip sales in China to the U.S. government.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nvidia&amp;rsquo;s revenue deal could invite scrutiny, according to Doug Jacobson, an international trade attorney at Jacobson Burton Kelley, &lt;a href=&#34;https://www.barrons.com/articles/nvidia-h20-china-chips-illegal-9ef82914?mod=article_inline&amp;amp;_gl=1*7vytgs*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Barron&amp;rsquo;s&lt;/a&gt; reported. &amp;ldquo;We&amp;rsquo;re far beyond uncharted waters. We&amp;rsquo;re in an uncharted universe,&amp;rdquo; Jacobson said, noting the State Department is able to charge export-license fees related to defense technology, but the fees are not based on earned revenue.&lt;/p&gt;
&lt;p&gt;A fee arrangement such as Nvidia&amp;rsquo;s may run counter to a statute at the Bureau of Industry and Security, an agency that manages export controls, Aiysha Hussain, a former BIS senior adviser, told Barron&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;Nvidia shares touched an all-time high of 184.48 in August after trade talks between the U.S. and China resumed. Treasury Secretary Scott Bessent said there were &amp;ldquo;the makings of a deal&amp;rdquo; with China.&lt;/p&gt;
&lt;p&gt;Nvidia placed an order for 300,000 H20 chips with Taiwan Semiconductor, adding to an existing inventory of 600,000-700,000 chips, &lt;a href=&#34;https://finance.yahoo.com/news/exclusive-nvidia-orders-300-000-030423386.html?fr=yhssrp_catchall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reuters reported&lt;/a&gt; in late July, citing unnamed sources. In 2024, Nvidia sold around 1 million H20 chips, the report said, citing research firm SemiAnalysis.&lt;/p&gt;
&lt;h2 id=&#34;a-4-trillion-market-cap&#34;&gt;A $4 Trillion Market Cap&lt;/h2&gt;
&lt;p&gt;Shares of Nvidia jumped 17% in June, guiding the chipmaker to outpace Microsoft in market capitalization. The valuation race between the two tech titans had been close ever since Nvidia announced its first-quarter results on May 28.&lt;/p&gt;
&lt;p&gt;In July, Nvidia became the first company to hit a $4 trillion market cap, overtaking tech titans Apple and Microsoft.&lt;/p&gt;
&lt;p&gt;In terms of its 12-month price performance, Nvidia has outperformed 87% of &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/relative-strength-rating-stock-chart-analysis-helps-pick-outstanding-growth-stocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;all other stocks&lt;/a&gt; in Investor&amp;rsquo;s Business Daily&amp;rsquo;s database.&lt;/p&gt;
&lt;p&gt;Funds own 41% of Nvidia&amp;rsquo;s outstanding shares, according to &lt;a href=&#34;https://get.investors.com/marketsurge/?artProdLink=MarketSurge&amp;amp;_gl=1*8wgrw*_gcl_au*MjEyNjc0NzA1MC4xNzU4ODA5OTIx*_ga*NDA2NDI2OTUyLjE3NTg4MDk5MjI.*_ga_K2H7B9JRSS*czE3NTg4MDk5MjEkbzEkZzEkdDE3NTg4MTAxMzYkajQ1JGwwJGgzOTM1NTcyOTk.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBD MarketSurge&lt;/a&gt;. Going by its &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/stocks-funds-are-buying/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accumulation/Distribution Rating of C&lt;/a&gt;, it appears that funds are not currently accumulating the stock. The rating measures price and &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/stock-chart-analysis-study-volume-in-bases/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;volume&lt;/a&gt; action over the last 13 weeks.&lt;/p&gt;
&lt;p&gt;The AI chip behemoth has a top-level &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/growth-stocks-and-how-to-analyze-earnings-growth-using-ibd-eps-rating/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Earnings Per Share Rating&lt;/a&gt; of 99. Further, the stock&amp;rsquo;s all-around strength, or &lt;a href=&#34;https://www.investors.com/how-to-invest/investors-corner/how-to-research-growth-stocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Composite Rating&lt;/a&gt;, sits at 99.&lt;/p&gt;
&lt;h2 id=&#34;is-nvidia-stock-a-buy&#34;&gt;Is Nvidia Stock A Buy?&lt;/h2&gt;
&lt;p&gt;Looking at &lt;a href=&#34;https://www.investors.com/how-to-invest/how-to-read-stock-charts-understanding-technical-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chart signals&lt;/a&gt; and &lt;a href=&#34;https://www.investors.com/how-to-invest/how-to-buy-stocks-using-stock-lists-stock-ratings-stock-screener/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;technical measures&lt;/a&gt; can help investors assess &lt;a href=&#34;https://www.investors.com/how-to-invest/how-to-buy-stocks-using-stock-lists-stock-ratings-stock-screener/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;whether Nvidia stock is a buy or sell now&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nvidia remains in a flat base and offers an entry at 184.48.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.investors.com/research/nvidia-nvda-stock-china-ai-chip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.investors.com/research/nvidia-nvda-stock-china-ai-chip/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Nvidia&#39;s $100 billion investment in OpenAI signals a major transformation</title>
      <link>http://localhost:1313/blog/20250922-marketwatch/</link>
      <pubDate>Mon, 22 Sep 2025 09:38:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250922-marketwatch/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Britney Ngugen&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&amp;lsquo;Nvidia isn&amp;rsquo;t just selling hardware anymore,&amp;rsquo; one expert says. The company is increasingly moving to be an equity partner with other AI companies.&lt;/p&gt;
&lt;p&gt;Nvidia CEO Jensen Huang said on Monday that the company plans to deploy 10 gigawatts of power to support OpenAI.&lt;/p&gt;
&lt;p&gt;Nvidia Corp. is preparing to deploy millions of its graphics processing units in artificial-intelligence data centers as part of its latest multibillion-dollar investment, signaling a further transformation in how the company does business and approaches its leadership role in the AI wave.&lt;/p&gt;
&lt;p&gt;The chip maker said Monday that it intends to support OpenAI&amp;rsquo;s next-generation AI infrastructure for training and running upcoming models by providing at least 10 gigawatts of its systems. That translates to millions of its chips, according to the company.&lt;/p&gt;
&lt;p&gt;Nvidia (NVDA) will invest up to $100 billion in the AI startup as its systems are rolled out, including for data-center and power capacity, it said. The investment will be made progressively following the deployment of each gigawatt. The details of what it is calling a strategic partnership have yet to be finalized, Nvidia said, but the companies are targeting the first phase to come online during the second half of next year using the chip maker&amp;rsquo;s upcoming Rubin AI platform.&lt;/p&gt;
&lt;p&gt;OpenAI Chief Executive Sam Altman has been vocal about OpenAI&amp;rsquo;s progress being &amp;ldquo;slowed by the lack of adequate infrastructure,&amp;rdquo; Greg Halter, director of research at Carnegie Investment Counsel, told MarketWatch. At the same time, there have been concerns about a potential AI bubble as more investments are made in the space.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Nvidia&amp;rsquo;s investment signals that the need for AI infrastructure is real and still expanding,&amp;rdquo; Halter said in emailed comments.&lt;/p&gt;
&lt;p&gt;The OpenAI announcement comes just days after Nvidia said it would invest $5 billion in Intel Corp. (INTC) while striking a partnership with that company as well.&lt;/p&gt;
&lt;p&gt;See more: Intel&amp;rsquo;s stock is soaring. Here&amp;rsquo;s why Nvidia is investing $5 billion in the chip maker.&lt;/p&gt;
&lt;p&gt;The timing of the latest deal suggests Nvidia is &amp;ldquo;diversifying both geographically (supply chain) and strategically (across the AI value chain). This looks more like infrastructure building than bubble behavior,&amp;rdquo; &lt;strong&gt;David Bader&lt;/strong&gt;, the director of the Institute for Data Science at the New Jersey Institute of Technology, said in emailed comments to MarketWatch.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This partnership represents the maturation of AI infrastructure as a legitimate asset class,&amp;rdquo; he added. &amp;ldquo;We&amp;rsquo;re witnessing the formation of vertical integration in the AI stack - from chips to training to deployment. Nvidia isn&amp;rsquo;t just selling hardware anymore; they&amp;rsquo;re becoming equity partners in the AI economy they&amp;rsquo;re enabling.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Under the partnership, OpenAI is using Nvidia as its &amp;ldquo;preferred strategic compute and networking partner for its AI factory growth plans.&amp;rdquo; Both companies will &amp;ldquo;co-optimize their roadmaps&amp;rdquo; for OpenAI&amp;rsquo;s AI software and Nvidia&amp;rsquo;s software and hardware.&lt;/p&gt;
&lt;p&gt;Demand for computing power &amp;ldquo;is going through the roof&amp;rdquo; for OpenAI&amp;rsquo;s ChatGPT, Nvidia Chief Executive Jensen Huang said during a Monday appearance on CNBC&amp;rsquo;s &amp;ldquo;Halftime Report.&amp;rdquo; &amp;ldquo;This partnership is about building an AI infrastructure that enables AI to go from the labs into the world.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Altman also appeared on CNBC, according to a transcript that was shared with MarketWatch, and said without the infrastructure being outlined in the deal, OpenAI wouldn&amp;rsquo;t be able to provide its services.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We can&amp;rsquo;t keep making better models,&amp;rdquo; Altman said. &amp;ldquo;And now that we really see what&amp;rsquo;s on the near-term horizon of how good the models are getting, the new use cases that are being enabled, what people want to do, this is like the fuel that we need to drive improvement, to drive better models, to drive revenue, everything.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Altman also addressed OpenAI&amp;rsquo;s partner Microsoft (MSFT), and said that it and Nvidia are &amp;ldquo;passive investors,&amp;rdquo; while the startup&amp;rsquo;s nonprofit and board remain in control of the company.&lt;/p&gt;
&lt;p&gt;Shares of Nvidia rose 3.9% on Monday following the announcement, closing at a new all-time high of $183.61, according to Dow Jones Market Data.&lt;/p&gt;
&lt;p&gt;The news has been good for picks-and-shovels companies too, Halter pointed out, noting positive moves for Vertiv Holdings Co. (VRT) which gained 5.8%, and GE Vernova Inc. (GEV) which rose 3.2%.&lt;/p&gt;
&lt;p&gt;Despite expectations on Wall Street for Nvidia to &amp;ldquo;invest downward into AI factories,&amp;rdquo; David Wagner, head of equity at Aptus Capital Advisors, told MarketWatch the &amp;ldquo;announcement is much earlier than many would have expected.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Now, Wagner said that investors will be waiting to see how the investment will impact OpenAI&amp;rsquo;s relationship with Broadcom Inc. (AVGO), which it is working with on custom chips. Broadcom&amp;rsquo;s stock slipped 1.6% on Monday.&lt;/p&gt;
&lt;p&gt;-Britney Nguyen&lt;/p&gt;
&lt;p&gt;This content was created by MarketWatch, which is operated by Dow Jones &amp;amp; Co. MarketWatch is published independently from Dow Jones Newswires and The Wall Street Journal.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.marketwatch.com/story/why-nvidia-is-pouring-100-billion-into-openai-f2cb6983&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.marketwatch.com/story/why-nvidia-is-pouring-100-billion-into-openai-f2cb6983&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.morningstar.com/news/marketwatch/20250922112/why-nvidias-100-billion-investment-in-openai-signals-a-major-transformation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.morningstar.com/news/marketwatch/20250922112/why-nvidias-100-billion-investment-in-openai-signals-a-major-transformation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-us/money/markets/why-nvidia-s-100-billion-investment-in-openai-signals-a-major-transformation/ar-AA1N4Dja&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-us/money/markets/why-nvidia-s-100-billion-investment-in-openai-signals-a-major-transformation/ar-AA1N4Dja&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NVIDIA to Invest $5 Billion in Intel, Partner on AI Infrastructure and Products</title>
      <link>http://localhost:1313/blog/20250918-techstrong/</link>
      <pubDate>Thu, 18 Sep 2025 11:02:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250918-techstrong/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jon Swartz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250918-techstrong/NVIDIA_hu_ae0d629ab23453e3.webp 400w,
               /blog/20250918-techstrong/NVIDIA_hu_107551bd54f1938.webp 760w,
               /blog/20250918-techstrong/NVIDIA_hu_7c9f032b3ec2dda7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250918-techstrong/NVIDIA_hu_ae0d629ab23453e3.webp&#34;
               width=&#34;760&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;NVIDIA Corp. said Thursday it will invest $5 billion in Intel Corp. and partner with the struggling chipmaker, a move that ties the industry’s dominant player to one of its long-time rivals. The companies said they will collaborate on custom data centers that power artificial intelligence (AI) infrastructure, as well as products for personal computers.&lt;/p&gt;
&lt;p&gt;As part of the agreement announced Thursday, Intel will design custom data center CPUs to be paired with NVIDIA’s AI GPUs. The two chips will be linked using NVIDIA’s proprietary technology, enabling faster communication between them than previously possible.&lt;/p&gt;
&lt;p&gt;In the consumer market, NVIDIA will supply Intel with a custom graphics chip to be paired with Intel’s PC processors using the same high-speed connections, a combination that could give Intel an advantage over competitors like AMD Inc.&lt;/p&gt;
&lt;p&gt;NVIDIA is purchasing Intel common stock at $23.28 per share, subject to regulatory approval. The announcement sent Intel shares soaring 30% in premarket trading Thursday.&lt;/p&gt;
&lt;p&gt;“This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem — a fusion of two world-class platforms,” NVIDIA CEO Jensen Huang said in a statement. “Together, we will expand our ecosystems and lay the foundation for the next era of computing.”&lt;/p&gt;
&lt;p&gt;The pact, on the heels of the U.S. government taking a roughly 10% stake in the company, will make NVIDIA one of Intel’s largest shareholders, with about 4%. Though the pact calls for Intel and NVIDIA to co-develop chips for PCs and data centers, it does not extend to Intel’s contract manufacturing unit, or foundry, producing chips for NVIDIA.&lt;/p&gt;
&lt;p&gt;Analysts say Intel’s foundry business will likely need to secure a major customer such as NVIDIA, Apple Inc., Qualcomm Inc., or Broadcom Inc. if it is to remain viable.&lt;/p&gt;
&lt;p&gt;A close working relationship between NVIDIA and Intel could eventually lead to NVIDIA using Intel manufacturing for its chips and packaging capabilities, assuming Intel can get their top-of-the-line processes to work as they plan, longtime analyst Jack Gold said. Any relationship with NVIDIA at this point, while not explicitly talking about the foundry services, should be seen as a possible extension of the partnership in the future, Gold added.&lt;/p&gt;
&lt;p&gt;The partnership could pose a challenge to Taiwan Semiconductor Manufacturing Co., which currently produces NVIDIA’s flagship processors. That work could eventually shift to Intel if the collaboration deepens. AMD, a rival to Intel in supplying data center chips, also faces potential setbacks as NVIDIA throws its weight behind Intel.&lt;/p&gt;
&lt;p&gt;In a post on X, Futurum Group CEO Daniel Newman gave his thoughts: “I said over and over again… If the US government backs $INTC, everything will fall into place for it. It’s happening…”&lt;/p&gt;
&lt;p&gt;“For NVIDIA, this is about securing additional advanced manufacturing capacity and fostering a viable, U.S.-based alternative to TSMC,” &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at New Jersey Institute of Technology, said in an email. “While most of NVIDIA’s advanced GPUs today are manufactured by TSMC in Taiwan, that geographic concentration creates exposure to political and security risks. Intel, despite its difficulties in execution and delays in process technology, still represents a unique and irreplaceable U.S. semiconductor fabrication capability. This partnership could give NVIDIA both influence and priority access to Intel Foundry Services, while providing Intel with capital and validation at a time when it is racing to re-establish technological leadership.”&lt;/p&gt;
&lt;p&gt;The timing is impossible to ignore in light of China’s ban on purchasing NVIDIA’s most advanced chips, which aims to blunt U.S. leadership in AI hardware, according to Bader.&lt;/p&gt;
&lt;p&gt;“This move not only curtails a lucrative market for NVIDIA but also accelerates the urgency for diversifying supply chains and reinforcing domestic semiconductor independence,” he said. “It underscores how central these chips have become — not just for commercial applications like generative AI and high-performance computing, but for national competitiveness and security.”&lt;/p&gt;
&lt;p&gt;Added Mario Morales, group vice president, enabling technologies and semiconductors, at IDC: “The announcement today highlights the ongoing AI race between the US and China. While the US currently leads in technology, China is rapidly advancing with its own AI initiatives. The collaboration between Nvidia and Intel is seen as a strategic move to maintain US leadership in AI technology infrastructure and endpoints.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://techstrong.ai/articles/nvidia-to-invest-5-billion-in-intel-work-together-on-ai-infrastructure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://techstrong.ai/articles/nvidia-to-invest-5-billion-in-intel-work-together-on-ai-infrastructure/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nvidia Will Contribute $5 Billion to Intel to Build Custom x86 Chip</title>
      <link>http://localhost:1313/blog/20250918-techrepublic/</link>
      <pubDate>Thu, 18 Sep 2025 10:43:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250918-techrepublic/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Megan Crouse&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-source-nvidiaintel&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Source: Nvidia/Intel&#34; srcset=&#34;
               /blog/20250918-techrepublic/intel-and-nvidia-sept-25-770x421_hu_a6553937cfce885b.webp 400w,
               /blog/20250918-techrepublic/intel-and-nvidia-sept-25-770x421_hu_1e25403c77e5a7df.webp 760w,
               /blog/20250918-techrepublic/intel-and-nvidia-sept-25-770x421_hu_c940401cea486204.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250918-techrepublic/intel-and-nvidia-sept-25-770x421_hu_a6553937cfce885b.webp&#34;
               width=&#34;760&#34;
               height=&#34;416&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Source: Nvidia/Intel
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Nvidia will invest $5 billion in its competitor Intel to further intertwine Nvidia’s AI-boosting chips and Intel’s x86 architecture and CPUs. The companies will work together on several generations of data center and PC infrastructure, both companies announced on Thursday.&lt;/p&gt;
&lt;p&gt;“This historic collaboration tightly couples Nvidia’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem — a fusion of two world-class platforms,” said Nvidia CEO Jensen Huang in a &lt;a href=&#34;https://nvidianews.nvidia.com/news/nvidia-and-intel-to-develop-ai-infrastructure-and-personal-computing-products&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;intel-will-make-x86-socs-designed-to-connect-to-nvidia-chiplets&#34;&gt;Intel will make x86 SOCs designed to connect to Nvidia chiplets&lt;/h2&gt;
&lt;p&gt;Intel’s x86 architecture is at the heart of many CPUs. Now, Intel will build custom x86 system-on-chips designs specifically to enable easy integration with Nvidia RTX GPU chiplets. The SOCs will be placed inside PCs data centers running Nvidia’s high-end CPUs and GPUs to accelerate AI processes. Intel will also build NVIDIA-custom x86 CPUs for data centers. The Nvidia and Intel architectures will be connected using Nvidia’s NVLink interconnect.&lt;/p&gt;
&lt;p&gt;Nvidia will invest $5 billion in Intel’s common stock at a purchase price of $23.28 per share, subject to regulatory approval. That’s about &lt;a href=&#34;https://www.axios.com/2025/09/18/ai-nvidia-intel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7% less&lt;/a&gt; than Intel’s closing stock price on Wednesday.&lt;/p&gt;
&lt;h2 id=&#34;how-this-intel-nvidia-deal-could-benefit-both-companies&#34;&gt;How this Intel-Nvidia deal could benefit both companies&lt;/h2&gt;
&lt;p&gt;For Nvidia, acquiring a stake in Intel could give Nvidia strategic influence over the AI infrastructure tech stack. Nvidia has reached into nearly every aspect of generative AI inference, training, and acceleration, and earlier became the most valuable company as GPUs proved critical for advanced generative AI.&lt;/p&gt;
&lt;p&gt;For Intel, the capital strengthens its balance sheet. Intel has received injections of cash from other entities recently. Japanese holding company SoftBank contributed &lt;a href=&#34;https://www.techrepublic.com/article/news-intel-softbank-investment/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$2 billion&lt;/a&gt; to Intel in August. The U.S. government took &lt;a href=&#34;https://www.techrepublic.com/article/news-trump-intel-deal-chip-production/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a roughly 10% stake&lt;/a&gt; in Intel later in August, in an unusual deal that saw the government reallocate existing grant programs.&lt;/p&gt;
&lt;p&gt;​​“Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia’s AI and accelerated computing leadership to enable new breakthroughs for the industry,” said Lip-Bu Tan, chief executive officer of Intel, in the press release.&lt;/p&gt;
&lt;p&gt;Intel is not faring well against major competitor Arm. In early 2025, Arm announced it would &lt;a href=&#34;https://www.techrepublic.com/article/arm-cpu-qualcomm-nvidia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;manufacture its own server CPUs&lt;/a&gt;, with manufacturing handled by &lt;a href=&#34;https://www.techrepublic.com/article/news-intel-tsmc-joint-venture/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;foundries such as TSMC&lt;/a&gt;, while continuing to license its semiconductor designs to other companies.&lt;/p&gt;
&lt;p&gt;Intel’s manufacturing side faces stiff competition. Many potential customers, such as Apple, have chosen Taiwan Semiconductor Manufacturing Company instead. Nvidia and Intel have not announced any foundry or manufacturing commitment as part of the deal.&lt;/p&gt;
&lt;p&gt;At a press conference on Thursday, Jensen and Tan steered questions about the Intel foundry back to the product announcements. Jensen noted that Nvidia will become a customer for Intel’s x86.&lt;/p&gt;
&lt;p&gt;“In the future, we will buy x86 CPUs from Intel and fuse it with our rack scale system,” he said.&lt;/p&gt;
&lt;p&gt;At the same time, “we’re both very successful customers of TSMC’s,” Jensen said.&lt;/p&gt;
&lt;p&gt;The deal had been in progress for over a year, Huang and Tan said, meaning the plan was put into place well before Nvidia’s current &lt;a href=&#34;https://www.techrepublic.com/article/news-nvidia-china-ban/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;geopolitical woes&lt;/a&gt;. The deal doesn’t seem to have been struck due to any strategy related to sales in China or the significant role President Donald Trump’s administration plays in the tech industry, either. However, the move might prove prescient in the long run.&lt;/p&gt;
&lt;p&gt;“This investment essentially gives NVIDIA three critical advantages,” said &lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor and Director of the Institute for Data Science at New Jersey Institute of Technology, in an email to TechRepublic. “First, priority access to Intel’s foundry capacity when they need it most; second, influence over Intel’s roadmap and manufacturing priorities; and third, a viable U.S.-based alternative that reduces their Taiwan exposure. It’s not about Intel being the best option—it’s about Intel being the only credible domestic option.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.techrepublic.com/article/news-nvidia-intel-deal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.techrepublic.com/article/news-nvidia-intel-deal/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CISA tool aims to boost security for software onboarding</title>
      <link>http://localhost:1313/blog/20250911-reversinglabs/</link>
      <pubDate>Thu, 11 Sep 2025 09:19:13 +0200</pubDate>
      <guid>http://localhost:1313/blog/20250911-reversinglabs/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250911-reversinglabs/onboarding-software-cisa-1400x732_hu_989b496fcb18797d.webp 400w,
               /blog/20250911-reversinglabs/onboarding-software-cisa-1400x732_hu_2c10b88aa6574bc0.webp 760w,
               /blog/20250911-reversinglabs/onboarding-software-cisa-1400x732_hu_522e43c68d6c99d6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250911-reversinglabs/onboarding-software-cisa-1400x732_hu_989b496fcb18797d.webp&#34;
               width=&#34;760&#34;
               height=&#34;397&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The U.S. Cybersecurity and Infrastructure Security Agency (CISA) has released a new web-based tool that it says will beef up cybersecurity practices throughout the software procurement lifecycle.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.cisa.gov/software-acquisition-guide/tool&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;free, interactive tool&lt;/a&gt; is based on CISA’s “&lt;a href=&#34;https://www.insidegovernmentcontracts.com/2024/08/new-guides-released-relating-to-secure-software-development-requirements/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Software Acquisition Guide&lt;/a&gt; for Government Enterprise Consumers: Software Assurance in the Cyber-Supply Chain Risk Management (C-SCRM) Lifecycle,” which addresses the cybersecurity risks associated with the acquisition and use of software developed by third parties.&lt;/p&gt;
&lt;p&gt;“Whether evaluating a single product or managing a complex acquisition, the Web Tool empowers users to make informed, risk-aware decisions that align with federal cybersecurity guidance and best practices,” the agency &lt;a href=&#34;https://www.cisa.gov/news-events/news/cisa-unveils-tool-boost-procurement-software-supply-chain-security&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said in a statement&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CISA said the tool supports the principles of both &lt;a href=&#34;https://www.reversinglabs.com/blog/secure-by-design-and-secure-by-default-why-we-need-both-for-appsec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Secure by Design and Secure by Default&lt;/a&gt; by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;breaking the Software Acquisition Guide into manageable, adaptive sections based on user input;&lt;/li&gt;
&lt;li&gt;helping users focus on the most relevant questions for their acquisition context;&lt;/li&gt;
&lt;li&gt;enabling exportable summaries that can be shared with CISOs, CIOs, and other key decision makers; and&lt;/li&gt;
&lt;li&gt;supporting stronger due diligence and more secure outcomes across procurement efforts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“This tool demonstrates CISA’s commitment to offering practical, free solutions for smarter, more secure software procurement,” &lt;a href=&#34;https://www.linkedin.com/in/marcimccarthy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Marci McCarthy&lt;/a&gt;, CISA’s director of public affairs, said in a statement. “Transforming the Software Acquisition Guide into an interactive format simplifies integrating cybersecurity into every step of procurement.”&lt;/p&gt;
&lt;p&gt;Here’s what you need to know about the new CISA tool — and where it falls short &lt;a href=&#34;https://www.reversinglabs.com/blog/what-is-tpsrm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;on third-party software risk managementon TPSRM&lt;/a&gt; while securely onboarding software.&lt;/p&gt;
&lt;h2 id=&#34;procurement-security-shifted-left&#34;&gt;Procurement security shifted left&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/rmastrog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rosario Mastrogiacomo&lt;/a&gt;, chief strategy officer at Sphere Technology Solutions, praised the CISA tool as “a smart step forward in operationalizing Secure by Design principles.” By translating complex procurement requirements into digestible, actionable guidance, organizations can bake security into procurement from the start — not as an afterthought, he said.&lt;/p&gt;
&lt;p&gt;Melody (MJ) Kaufmann, an author and instructor at O’Reilly Media, said that, given that  procurement teams aren’t always security experts, suppliers often struggle to highlight the right details.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;This tool bridges that gap by giving both sides a clear, practical framework to evaluate and demonstrate secure practices, strengthening cybersecurity from the start of the procurement process.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/melodyjkaufmann/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Melody (MJ) Kaufmann&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology (NJIT), said the tool strengthens cybersecurity practices by introducing standardized security assessments at the procurement stage rather than treating security as an afterthought.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;It creates a consistent framework for evaluating suppliers’ security postures, development practices, and vulnerability management processes. This shifts security considerations upstream in the procurement cycle, where they can actually influence purchasing decisions.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/dbader13/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The tool also establishes baseline security requirements that suppliers must demonstrate, effectively raising the minimum security standards across the software ecosystem, Bader said.&lt;/p&gt;
&lt;p&gt;Jason Soroko, a senior fellow at Sectigo, said the tool can harden every phase of the buying journey by turning abstract guidance into concrete prompts that drive evidence, accountability, and traceability.&lt;/p&gt;
&lt;p&gt;Soroko said the new tool eases early intake and market research with structured framing of risk, RFPs with vetted requirements, consistent scoring, enforceable security commitments, and onboarding with controls to achieve measurable service levels.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The dynamic flow reduces noise, keeps nontechnical buyers focused on what matters, and normalizes requests for SBOM and VEX, build provenance and signing, vulnerability disclosure practices, patch timelines, logging and incident reporting, encryption and key management, access governance, and independent assurance such as SOC 2 or ISO 27001.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/jason-soroko-19b41920/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jason Soroko&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The exportable summary produced by the CISA tool can become an auditable artifact that supports approvals, risk acceptance, and continuous monitoring, which helps teams answer regulators and boards with confidence while also lowering supplier fatigue through a common language, Soroko said.&lt;/p&gt;
&lt;h2 id=&#34;cumbersome-manual--and-not-ml-friendly&#34;&gt;Cumbersome, manual — and not ML-friendly&lt;/h2&gt;
&lt;p&gt;Jeff Williams, co-founder and CTO of Contrast Security, is less upbeat. He said the tool is a questionnaire that produces a PDF that can be passed on to organizations using a vendor’s software program. “The process is cumbersome, evidence-free, and completely manual,” he said.&lt;/p&gt;
&lt;p&gt;Imagine trying to manage the thousands of PDF documents you would need for every piece of software you use, Williams said. “If you’re going to try to help people, at least make the information machine-readable. Fortunately, the OWASP CycloneDX project has exactly the right way to do this in their &lt;a href=&#34;https://cyclonedx.org/guides/OWASP_CycloneDX-Authoritative-Guide-to-Attestations-en.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;attestations project&lt;/a&gt;.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;I seriously doubt that this [CISA] tool helps anything. It’s an Excel spreadsheet in web clothing, except worse because it’s not machine-readable.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/planetlevel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jeff Williams&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Williams said the guide on which the tool is based is a grab bag of 393 requirements that range from obvious to obscure. Given the track record for large requirements documents issued by government agencies, he said, he doesn’t have a lot of confidence that this will change things. “I don’t really see how this is different from [CISA’s] last effort building an attestation form that nobody used. Except that this is a different, more comprehensive standard that will make the process even more burdensome.”&lt;/p&gt;
&lt;p&gt;Worse, Williams said, CISA hasn’t even followed its own guidance. “This tool is written in PHP using Drupal, a language and platform that both have a truly abysmal track record for security,” he noted. “I found an OWASP Top Ten level vulnerability almost immediately and will be disclosing it to CISA ASAP.”&lt;/p&gt;
&lt;h2 id=&#34;supply-chain-resiliency-is-key&#34;&gt;Supply chain resiliency is key&lt;/h2&gt;
&lt;p&gt;Shane Barney, CISO at Keeper Security, does see value in CISA’s tool, noting that it translates complex cybersecurity requirements into actionable insights for decision makers at every level. By tailoring outputs to the needs of the user, it makes security part of the conversation from the very beginning, he said. “That accessibility helps close gaps adversaries often exploit in third-party ecosystems, ensuring that security considerations are top of mind in procurement decisions,” Barney said.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Supply chain threats remain one of the most pressing concerns in today’s digital ecosystem. No organization operates in isolation, and each vendor introduces both value and potential vulnerability.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/shane-barney-69026528/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shane Barney&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Barney said the key to resilience is applying the same rigorous, security-first standards to suppliers as you do to your own environment. “CISA’s tool makes this process more accessible by embedding consistent criteria into procurement workflows. It enhances visibility and accountability, helping organizations identify partners that not only meet operational needs, but also strengthen the overall security of the supply chain.”&lt;/p&gt;
&lt;p&gt;Sphere Technology’s Mastrogiacomo said the new tool is exactly what’s needed to bridge the gap between cybersecurity policy and execution. “As threats to the software supply chain grow more sophisticated, tools that guide procurement teams through complex risk management decisions will be essential,” he said.&lt;/p&gt;
&lt;h2 id=&#34;tpsrm-must-evolve-beyond-questionnaires&#34;&gt;TPSRM must evolve beyond questionnaires&lt;/h2&gt;
&lt;p&gt;The tool’s long-term success hinges on whether it becomes the de facto industry standard rather than just another optional framework, NJIT’s Bader said. “CISA’s credibility and the government’s procurement power provide strong incentives for supplier participation, but the private sector will need to embrace similar standards for maximum impact,” he said.&lt;/p&gt;
&lt;p&gt;Bader said the tool must evolve to address emerging threats like AI-powered attacks and supply chain compromises. “Its effectiveness will ultimately be measured not just by adoption rates, but by whether it demonstrably reduces successful supply chain attacks over time,” he said.&lt;/p&gt;
&lt;p&gt;Patrick Enderby, senior product marketing manager at ReversingLabs (RL), said that as &lt;a href=&#34;https://www.reversinglabs.com/blog/lessons-learned-from-3cxs-software-supply-chain-compromise&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software supply chain attacks grow more sophisticated&lt;/a&gt;, manual, questionnaire-driven, and vendors’ blind-trust approaches can only take organizations so far. Security teams today are managing hundreds, sometimes thousands, of software requests per year.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Producing PDFs, aggregating SBOMs, and validating vendor attestations manually introduces delays, inconsistent results, and ironically a greater risk exposure.&amp;rdquo;&lt;br&gt;
&amp;ndash; &lt;a href=&#34;https://www.linkedin.com/in/penderby/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patrick Enderby&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;CISA’s effort signals growing federal pressure for stronger software supply chain security, but meeting these expectations demands automation, evidence-based validation, and scalable workflows, Enderby said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reversinglabs.com/blog/cisa-tool-software-onboarding-tpsrm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.reversinglabs.com/blog/cisa-tool-software-onboarding-tpsrm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harnessing the Power of LLMs for Software Performance Engineering</title>
      <link>http://localhost:1313/blog/20250828-fastcode/</link>
      <pubDate>Thu, 28 Aug 2025 10:24:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250828-fastcode/</guid>
      <description>&lt;p&gt;&lt;em&gt;By David A. Bader&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;someone&lt;/a&gt; who has spent decades optimizing parallel algorithms and wrestling with the complexities of high-performance computing, I&amp;rsquo;m constantly amazed by how the landscape of performance engineering continues to evolve. Today, I want to share an exciting development in our Fastcode initiative: leveraging Large Language Models (LLMs) as sophisticated analysis tools for software performance optimization.&lt;/p&gt;
&lt;h2 id=&#34;the-multi-llm-approach-why-diversity-matters&#34;&gt;The Multi-LLM Approach: Why Diversity Matters&lt;/h2&gt;
&lt;p&gt;In our recent work, we&amp;rsquo;ve been experimenting with an ensemble approach using multiple LLMs — Gemini, Claude, ChatGPT, Grok, Deepseek, and GitHub Copilot —to analyze existing codebases and research literature. This isn&amp;rsquo;t just about having multiple opinions; it&amp;rsquo;s about exploiting the unique strengths each model brings to performance analysis.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250828-fastcode/img_hu_9a61a2acb910d93f.webp 400w,
               /blog/20250828-fastcode/img_hu_daa0468346aaf29d.webp 760w,
               /blog/20250828-fastcode/img_hu_c0417c737d6cc28c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250828-fastcode/img_hu_9a61a2acb910d93f.webp&#34;
               width=&#34;760&#34;
               height=&#34;190&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Each LLM has been trained on different datasets and has distinct architectural biases. Gemini excels at mathematical reasoning that&amp;rsquo;s crucial for algorithmic complexity analysis. Claude demonstrates remarkable capability in understanding research papers and extracting actionable insights. ChatGPT offers broad programming knowledge across languages and paradigms. Meanwhile, Copilot&amp;rsquo;s tight integration with development workflows provides practical, immediately implementable suggestions.&lt;/p&gt;
&lt;h2 id=&#34;from-papers-to-performance-ingesting-research-at-scale&#34;&gt;From Papers to Performance: Ingesting Research at Scale&lt;/h2&gt;
&lt;p&gt;One of the most powerful applications we&amp;rsquo;ve discovered is feeding academic papers directly into these models alongside our source code. The traditional process of manually surveying literature, identifying relevant optimizations, and adapting them to specific codebases is tremendously time-consuming. By having LLMs digest both our implementation and the latest research simultaneously, we&amp;rsquo;re uncovering optimization opportunities that might have taken months to identify manually.&lt;/p&gt;
&lt;p&gt;For instance, when analyzing our triangle counting graph algorithms, we provided the models with both our C code implementation and recent papers on optimizing triangle counting algorithms. The LLMs not only identified specific bottlenecks in our memory access patterns but also suggested concrete modifications based on cutting-edge research we hadn&amp;rsquo;t yet incorporated.&lt;/p&gt;
&lt;h2 id=&#34;the-recommendation-engine-pattern-recognition-at-scale&#34;&gt;The Recommendation Engine: Pattern Recognition at Scale&lt;/h2&gt;
&lt;p&gt;What&amp;rsquo;s particularly fascinating is how these models excel at pattern recognition across disparate optimization domains. Traditional profiling tools tell us where performance bottlenecks exist, but LLMs help us understand why they exist and how to address them systematically.&lt;/p&gt;
&lt;p&gt;The models consistently identify several categories of optimizations: Memory Access Patterns: LLMs excel at recognizing cache-unfriendly access patterns and suggesting data structure reorganizations. They&amp;rsquo;ve recommended everything from array-of-structures to structure-of-arrays transformations to more sophisticated blocked algorithms.&lt;/p&gt;
&lt;p&gt;Algorithmic Alternatives: By cross-referencing our implementations with their vast knowledge of algorithms literature, the models suggest alternative approaches we might not have considered. Sometimes a simple change from a recursive to iterative implementation, or switching from depth-first to breadth-first traversal, yields significant performance gains.&lt;/p&gt;
&lt;p&gt;Parallelization Opportunities: Perhaps most relevant to our work, LLMs are surprisingly adept at identifying parallelizable sections of code and suggesting appropriate parallel programming models—whether that&amp;rsquo;s OpenMP for shared memory, MPI for distributed systems, or GPU kernels for massively parallel workloads.&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-limitations-keeping-humans-in-the-loop&#34;&gt;Challenges and Limitations: Keeping Humans in the Loop&lt;/h2&gt;
&lt;p&gt;Of course, this approach isn&amp;rsquo;t without its challenges. LLMs can sometimes suggest optimizations that are theoretically sound but practically infeasible given specific hardware constraints or real-world data characteristics. They might recommend vectorization strategies that don&amp;rsquo;t account for irregular memory access patterns in graph algorithms, or suggest parallel decompositions that introduce more synchronization overhead than performance benefit.&lt;/p&gt;
&lt;p&gt;This is why human expertise remains crucial. The LLMs serve as sophisticated recommendation engines, but the final decisions require deep understanding of the target architecture, the specific problem domain, and the trade-offs involved in each optimization choice.&lt;/p&gt;
&lt;h2 id=&#34;validation-and-iteration-the-scientific-method-applied&#34;&gt;Validation and Iteration: The Scientific Method Applied&lt;/h2&gt;
&lt;p&gt;We&amp;rsquo;ve developed a systematic approach to validating LLM recommendations. Each suggested optimization goes through rigorous benchmarking across multiple datasets and hardware configurations. We&amp;rsquo;ve found that approximately two-thirds of recommendations lead to measurable performance improvements, which is remarkably high for automated suggestions.&lt;/p&gt;
&lt;p&gt;More importantly, even the recommendations that don&amp;rsquo;t immediately pan out often provide valuable insights that inform subsequent optimization efforts. The models sometimes identify performance anti-patterns we hadn&amp;rsquo;t recognized, leading to broader improvements in our coding practices.&lt;/p&gt;
&lt;h2 id=&#34;looking-forward-the-future-of-ai-assisted-performance-engineering&#34;&gt;Looking Forward: The Future of AI-Assisted Performance Engineering&lt;/h2&gt;
&lt;p&gt;As these models continue to evolve, I envision even more sophisticated applications. Imagine LLMs that can not only suggest optimizations but also predict their performance impact across different architectures, or models that can automatically generate optimized implementations for new hardware targets by learning from existing optimization patterns.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re also exploring the potential for LLMs to assist in performance modeling itself—using natural language descriptions of algorithms to generate analytical performance models or even simulation code for complex systems.&lt;/p&gt;
&lt;h2 id=&#34;conclusion-amplifying-human-expertise&#34;&gt;Conclusion: Amplifying Human Expertise&lt;/h2&gt;
&lt;p&gt;The integration of LLMs into our Fastcode workflow isn&amp;rsquo;t about replacing human expertise—it&amp;rsquo;s about amplifying it. These models excel at the pattern recognition and literature synthesis that often consume significant time in the optimization process, freeing us to focus on the creative problem-solving and domain-specific insights that only human experts can provide.&lt;/p&gt;
&lt;p&gt;As we continue to push the boundaries of computational performance, tools like these will become increasingly essential. The future of software performance engineering lies not in choosing between human intuition and machine analysis, but in combining them effectively to tackle the ever-growing complexity of modern computing systems.&lt;/p&gt;
&lt;p&gt;The Fastcode initiative has always been about aggressive optimization through innovative approaches. Incorporating LLMs into our performance engineering toolkit represents the natural evolution of this philosophy—leveraging every available tool to squeeze maximum performance from our computational resources.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://fastcode.substack.com/p/harnessing-the-power-of-llms-for&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fastcode.substack.com/p/harnessing-the-power-of-llms-for&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Unwraps Zero-Cost Plan To Improve Nation’s Cybersecurity</title>
      <link>http://localhost:1313/blog/20250826-technewsworld/</link>
      <pubDate>Tue, 26 Aug 2025 11:10:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250826-technewsworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250826-technewsworld/US-government-cybersecurity_hu_900086bbc215dc1c.webp 400w,
               /blog/20250826-technewsworld/US-government-cybersecurity_hu_7a2cf72109995bdb.webp 760w,
               /blog/20250826-technewsworld/US-government-cybersecurity_hu_c20f96cb8a328d23.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250826-technewsworld/US-government-cybersecurity_hu_900086bbc215dc1c.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A plan to bolster the nation’s cybersecurity that will cost virtually nothing was unveiled Tuesday by the &lt;a href=&#34;https://isalliance.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Internet Security Alliance&lt;/a&gt; (ISA).&lt;/p&gt;
&lt;p&gt;In a 21-page document, the alliance offers five recommendations that it maintains “will cost the federal government virtually nothing” and, as a bonus, save private industry billions.&lt;/p&gt;
&lt;p&gt;The document titled “A Zero Cost Path to American Cybersecurity” outlines initiatives that operationalize the Trump administration’s philosophy of government.&lt;/p&gt;
&lt;p&gt;“These are pragmatic programs that can be implemented quickly,” the document noted. “They will generate significant material improvements in our nation’s cybersecurity almost immediately.&lt;/p&gt;
&lt;p&gt;“These steps will also put our nation’s intermediate and long-term security on a measurably effective and economically sustainable path that will enable us to address newly growing threats of systemic failure,” it continued.&lt;/p&gt;
&lt;p&gt;“With White House backing,” it added, “these initiatives can transform cybersecurity from a compliance burden into a competitive advantage — and secure both the nation’s digital future and the president’s legacy as the leader who turned deregulation into a national security triumph.”&lt;/p&gt;
&lt;h2 id=&#34;cutting-duplicate-rules&#34;&gt;Cutting Duplicate Rules&lt;/h2&gt;
&lt;p&gt;One recommendation from the alliance is that the federal Office of Management and Budget (OMB) should utilize its existing authority to eliminate duplicative cybersecurity regulations.&lt;/p&gt;
&lt;p&gt;“The scope of duplicative cybersecurity regulations is staggering,” declared &lt;a href=&#34;https://people.njit.edu/profile/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology (NJIT) in Newark, N.J.&lt;/p&gt;
&lt;p&gt;“A recent GAO analysis found that among just four major federal agencies, 49% to 79% of cybersecurity requirement parameters were in direct conflict with each other,” he told TechNewsWorld. “The most glaring example is incident reporting, where we currently have 45 different cyber incident reporting requirements spread across 22 federal agencies, each with their own forms and websites.”&lt;/p&gt;
&lt;p&gt;“This regulatory chaos is severely undermining our cybersecurity capabilities,” he continued. “Large financial institutions report their cyber teams now spend more than 70% of their time on regulatory compliance rather than actually improving security. Some companies are spending nearly half their entire cybersecurity budget just filling out duplicative compliance reports.”&lt;/p&gt;
&lt;p&gt;“When cybersecurity professionals are buried in paperwork instead of defending networks, we’re essentially doing the attackers’ work for them,” he said. “Eliminating these duplicative requirements would immediately free up billions of dollars in resources that could be redirected toward actual threat detection, incident response, and security improvements.”&lt;/p&gt;
&lt;p&gt;The problem is not only that duplicative regulations waste time for anyone involved with them, but they are often different and sometimes at cross-purposes with each other, added Roger Grimes, a defense evangelist at &lt;a href=&#34;https://www.knowbe4.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KnowBe4&lt;/a&gt; a security awareness training provider, in Clearwater, Fla.&lt;/p&gt;
&lt;p&gt;“NIST [National Institute of Standards and Technology] recommends not having a minimum password size, removing the need to change it on a periodic basis and removing the complexity requirement,” he told TechNewsWorld. “Most other cybersecurity guides, including many governmental regulations, require the exact opposite.”&lt;/p&gt;
&lt;p&gt;There may also be a significant hurdle in implementing the ISA’s recommendation through the OMB. “The OMB’s role is to coordinate regulations and look to see if they’re inconsistent or duplicative or whatever,” said Berin Szóka, president of &lt;a href=&#34;https://techfreedom.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TechFreedom&lt;/a&gt;, a technology advocacy group, in Washington, D.C. “They can’t repeal regulations as this document claims.”&lt;/p&gt;
&lt;p&gt;“It’s up to the agencies to repeal rules,” he told TechNewsWorld. “Then the question that you get into is whether the agencies can short-circuit the normal rulemaking process. The Trump administration has claimed that agencies don’t have to go through normal rulemaking if they think that a rule is unlawful. That’s just not true. That’s not how the Administrative Procedure Act works.”&lt;/p&gt;
&lt;h2 id=&#34;cyber-rule-cost-benefit-analysis&#34;&gt;Cyber Rule Cost-Benefit Analysis&lt;/h2&gt;
&lt;p&gt;The ISA is also recommending that a cost-benefit analysis be required for all cybersecurity regulations. “Despite spending trillions of dollars on cybersecurity regulatory compliance, no study has ever documented that the cybersecurity regulations actually enhance security,” it contended.&lt;/p&gt;
&lt;p&gt;However, Heath Renfrow, CISO and co-founder of &lt;a href=&#34;https://fenix24/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fenix24&lt;/a&gt;, a cyber disaster recovery firm in Chattanooga, Tenn., pointed out that while on the surface, a cost-benefit analysis seems like an economic safeguard, in cybersecurity, the cost side is tangible while the benefit side is probabilistic.&lt;/p&gt;
&lt;p&gt;“How do you quantify the avoided cost of a ransomware event that didn’t occur because of MFA adoption?” he asked. “Traditional cost-benefit frameworks, like OMB Circular A-4, break down because cyberattacks are low-frequency, high-impact events with cascading second-order effects.”&lt;/p&gt;
&lt;p&gt;“That said, forcing agencies to at least articulate assumptions, model scenarios, and test proportionality would raise the quality of regulatory drafting,” he told TechNewsWorld. “The danger is in weaponization. Companies may argue that unless you can prove a breach will cost X amount of money, the regulation isn’t worth it. The balance is ensuring analysis informs regulation, without paralyzing it.”&lt;/p&gt;
&lt;p&gt;NJIT’s Bader agreed that the risk of using cost-benefit analysis to avoid necessary regulations is real and concerning. “Many cybersecurity benefits are preventative and systemic, making them difficult to capture in traditional economic models,” he said. “The cascading effects we’ve seen from supply chain attacks like SolarWinds demonstrate that the true cost of cyber incidents often far exceeds initial estimates.”&lt;/p&gt;
&lt;p&gt;“However,” he continued, “when properly implemented with methodologies that account for these uncertainties, cost-benefit analysis could actually improve cybersecurity by ensuring resources go to the most impactful security measures. The key is developing models sophisticated enough to handle the unique characteristics of cyber risk, rather than applying standard regulatory cost-benefit frameworks that weren’t designed for this domain.”&lt;/p&gt;
&lt;h2 id=&#34;overhaul-cybersecurity-information-act&#34;&gt;Overhaul Cybersecurity Information Act&lt;/h2&gt;
&lt;p&gt;Another recommendation by the ISA is that the 2015 Cybersecurity Information Sharing Act should be reauthorized and modernized.&lt;/p&gt;
&lt;p&gt;The Cybersecurity Information Sharing Act (CISA 2015) — the legal foundation for public-private cyber collaboration — will expire on September 30, 2025, unless reauthorized, the ISA explained. “Allowing it to lapse would severely limit the government’s ability to share threat intelligence with industry, undermining national security,” it maintained.&lt;/p&gt;
&lt;p&gt;Bader argued that CISA needs urgent modernization because it was written for a threat landscape that no longer exists. “The 2015 law was crafted before we understood AI-enabled attacks, sophisticated supply chain compromises, or the unique vulnerabilities of cloud infrastructure,” he said. The definitions of what constitutes shareable cybersecurity information are too narrow for today’s threat environment.”&lt;/p&gt;
&lt;p&gt;“With the advent of AI and the ability to build malicious code a lot faster than we’ve ever seen before, traditional ways of sharing information can’t keep up with the threat,” added Matt Stern, chief security officer at &lt;a href=&#34;https://www.hypori.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hypori&lt;/a&gt;, a mobile infrastructure security firm, in Reston, Va.&lt;/p&gt;
&lt;p&gt;“If we’re going to even get parity with the threat,” he told TechNewsWorld. “We have to be able to modernize the regulation so that it can deal with getting threat information into the hands of the people that need it in a much more realistic and quicker fashion.”&lt;/p&gt;
&lt;p&gt;Fenix24’s Renfrow added that private sector participation in sharing is still weak under the existing act. “Modernization should include liability safe harbors for companies that share indicators of compromise in good faith and reciprocal obligations for the government to return actionable intel in real time,” he said.&lt;/p&gt;
&lt;h2 id=&#34;solving-cyber-workforce-shortage&#34;&gt;Solving Cyber Workforce Shortage&lt;/h2&gt;
&lt;p&gt;The ISA is also recommending that a cost-effective cybersecurity workforce be created for the government, largely through the PIVOTT Act currently before Congress.&lt;/p&gt;
&lt;p&gt;Under PIVOTT (Providing Individuals Various Opportunities for Technical Training), students can enroll in existing cybersecurity programs offered by colleges, community colleges, and certificate programs. The federal government would pay for their tuition. In return, the students would be required to perform a specified amount of government service.&lt;/p&gt;
&lt;p&gt;PIVOTT’s target is to enroll up to 10,000 students a year eventually. “At that rate, PIVOTT would solve the federal government’s cybersecurity workforce gap (35,000) in less than 4 years,” the ISA noted.&lt;/p&gt;
&lt;p&gt;“The PIVOTT Act’s apprenticeship and rotation model is promising because it treats cyber talent like a renewable resource,” Renfrow said. “You move skilled practitioners across agencies rather than each agency trying to grow its own siloed pipeline.”&lt;/p&gt;
&lt;p&gt;“Four years is optimistic,” he added, “but without structural change like PIVOTT, the problem will persist indefinitely.”&lt;/p&gt;
&lt;p&gt;While finding the PIVOTT concept a good one, Ida Byrd-Hill, CEO and founder of &lt;a href=&#34;https://autoworkz.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automation Workz&lt;/a&gt;, a reskilling and diversity consulting firm in Detroit, Mich., argued that more money should be directed at the workforce development system in the Department of Labor. “Most people do not realize that just getting technology training at a university, college, or community college is not sufficient if you don’t have a certification,” she told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“The problem could have long been solved if the government supported learn and earn programs,” she added. “The government has not acquiesced to do that yet. They have to step up. It’s not just about scholarships and training.”&lt;/p&gt;
&lt;h2 id=&#34;building-a-national-cybersecurity-dashboard&#34;&gt;Building a National Cybersecurity Dashboard&lt;/h2&gt;
&lt;p&gt;A fifth recommendation by ISA is to establish a national macroeconomic cybersecurity dashboard.&lt;/p&gt;
&lt;p&gt;The ISA explained that the federal government is spending tens of billions of dollars every year on an extensive range of cybersecurity projects. “Yet without a sophisticated model, policymakers are blind to the full economic cost of cyber threats, the ROI of defenses, the usefulness of alternative methods such as incentive programs rather than regulation, the systemic impacts of major incidents, and the most cost-effective ways to eliminate, mitigate, or transfer risk,” it noted.&lt;/p&gt;
&lt;p&gt;It advised the National Cyber Director to collaborate with federal government agencies to promote a more sophisticated cyber risk assessment methodology based on the proven NACD-ISA framework.&lt;/p&gt;
&lt;p&gt;“We desperately need a national cybersecurity dashboard because our current approach to cyber risk assessment is fundamentally broken,” Bader said. “Right now, we have dozens of agencies doing their own cyber risk assessments with no coordination or common methodology. It’s like trying to understand the health of the U.S. economy by looking at 22 different, incompatible financial reports.”&lt;/p&gt;
&lt;p&gt;“The NACD-ISA framework that would underpin this dashboard has been independently validated by MIT and &lt;a href=&#34;https://www.pwc.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PwC&lt;/a&gt; research,” he continued. “Organizations using these principles have 85% fewer cyber incidents and significantly better risk management outcomes. This isn’t theoretical — it’s a proven approach that works at the enterprise level.”&lt;/p&gt;
&lt;p&gt;“Think of it as a Cyber Dow Jones Index — not predicting daily movements but measuring the structural health of the economy under cyber stress,” Renfrow added. “Without that visibility, policymakers are steering blind in a domain where adversaries are already treating cyber as macroeconomic warfare.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technewsworld.com/story/alliance-unwraps-zero-cost-plan-to-improve-nations-cybersecurity-179892.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technewsworld.com/story/alliance-unwraps-zero-cost-plan-to-improve-nations-cybersecurity-179892.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI breakthroughs spur race for superintelligence</title>
      <link>http://localhost:1313/blog/20250819-economictimes/</link>
      <pubDate>Tue, 19 Aug 2025 09:05:12 +0200</pubDate>
      <guid>http://localhost:1313/blog/20250819-economictimes/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://m.economictimes.com/etreporter/author-swathi-moorthy-479257817.cms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Swathi Moorthy&lt;/a&gt;&lt;/em&gt;, ETtech&lt;/p&gt;
&lt;p&gt;Almost three years after the launch of &lt;a href=&#34;https://economictimes.indiatimes.com/tech/technology/everything-you-need-to-know-about-openai/slideshow/97276114.cms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI’s GPT3.5&lt;/a&gt; in November 2022, the techno optimists would say that the world of artificial intelligence is closer to superintelligence than ever before.&lt;/p&gt;
&lt;p&gt;From the beginning of this year, the world has seen one breakthrough launch of GenAI foundation models after another, starting with &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/ettech-explainer-what-is-deepseek-chinas-competitor-to-openai/articleshow/117577579.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSeek-R1 in January&lt;/a&gt; to &lt;a href=&#34;https://economictimes.indiatimes.com/topic/openai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&lt;/a&gt;’s &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/openais-chat-gpt-5-all-you-need-to-know/articleshow/123184361.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-5 early&lt;/a&gt; this month.&lt;/p&gt;
&lt;p&gt;OpenAI cofounder Sam Altman said in his June blog that we are closer to building “digital superintelligence”. But others are not as convinced.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of the institute of data science, New Jersey Institute of Technology, recently said that “superintelligence, properly defined, would represent systems that exceed human performance across virtually all cognitive domains. We’re nowhere near that threshold.”&lt;/p&gt;
&lt;h2 id=&#34;milestones&#34;&gt;Milestones&lt;/h2&gt;
&lt;h2 id=&#34;international-math-olympiad-july-2025&#34;&gt;International Math Olympiad, July 2025&lt;/h2&gt;
&lt;p&gt;An experimental mode of OpenAI and &lt;a href=&#34;https://economictimes.indiatimes.com/topic/google&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt; DeepMind’s Gemini DeepThink reached a significant milestone when they achieved &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/google-and-openais-ai-models-win-milestone-gold-at-global-math-competition/articleshow/122825546.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gold in the International Mathematical Olympiad&lt;/a&gt; by scoring 35 points out of 42 in July.&lt;/p&gt;
&lt;p&gt;Multiple people in the industry pointed out that this was an impressive feat for AI models. Gary Marcus, an American psychologist, in his post said, “Awfully impressive” and added that “a gold medal in the IMO is an accomplishment that even very successful mathematicians and scientists may well highlight on their CVs all their lives.”&lt;/p&gt;
&lt;p&gt;To be sure, it is unclear how this will help with mathematical research in the future..&lt;/p&gt;
&lt;h2 id=&#34;coding&#34;&gt;Coding&lt;/h2&gt;
&lt;p&gt;So far this has been the year of coding agents.&lt;/p&gt;
&lt;p&gt;From OpenAI to Gemini and everyone in between are focusing on upping their coding performance. While &lt;a href=&#34;https://economictimes.indiatimes.com/topic/claude&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude&lt;/a&gt; is one of the favoured &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/anthropic-releases-claude-opus-4-1-amid-rival-chatgpts-advancements/articleshow/123143392.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;models for coding&lt;/a&gt;, other platforms are catching up. In May, Google launched coding agent Jules.&lt;/p&gt;
&lt;p&gt;In July, Google paid $2.4 billion in licensing fees to coding platform Windsurf and hired founder Varun Mohan and other key personnel in the firm. OpenAI rolled out Codex in May and said that its latest model GPT-5 shows strong performance for coding. This puts these platforms in direct competition with startups such as Cursor and Lovable.&lt;/p&gt;
&lt;p&gt;Also Read: &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/operation-coding-war-thanks-ai/articleshow/121420810.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Operation coding war, thanks AI&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;2025-citius-fortius-altius&#34;&gt;2025: CITIUS, FORTIUS, ALTIUS&lt;/h2&gt;
&lt;h2 id=&#34;deepseek-january&#34;&gt;DeepSeek, January&lt;/h2&gt;
&lt;p&gt;When the Chinese open source model DeepSeek-R1 was launched in January 2025, to say they took the world by storm would be an understatement.&lt;/p&gt;
&lt;p&gt;Dubbed “DeepSeek moment”, it marked a significant technological breakthrough in training large language models. Unlike its Western counterparts, DeepSeek used reinforcement learning, a machine language technique, to train the system to make decisions through trial and error, with optimal results being rewarded.&lt;/p&gt;
&lt;p&gt;This helped the Chinese LLM maker to reduce the cost dramatically and at the same time performance was on par with the likes of OpenAI for reasoning.&lt;/p&gt;
&lt;h2 id=&#34;gemini-25-march&#34;&gt;Gemini 2.5, March&lt;/h2&gt;
&lt;p&gt;Search giant Google Search giant Google &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/google-rolls-out-experimental-version-of-gemini-2-5-pro-for-free-users/articleshow/119789671.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launched the Gemini 2.5 Pro&lt;/a&gt; Experimental model, its most advanced model yet, in March.&lt;/p&gt;
&lt;p&gt;These are reasoning models, as Google calls it thinking models, which shows strong reasoning and coding capabilities, and ranks high when it comes to coding, mathematics and science.&lt;/p&gt;
&lt;p&gt;On August 1, the company provided access to &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/award-winning-variant-of-geminis-ai-model-is-live-confirms-ceo-sundar-pichai/articleshow/123046462.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gemini 2.5 Deep Think&lt;/a&gt; for Ultra subscribers. According to its blog, Deep Think “is capable of thinking longer and generating multiple parallel streams of thought simultaneously” similar to how a human brain processes complex tasks.&lt;/p&gt;
&lt;h2 id=&#34;claude-37-and-4-sonnet-february-and-may&#34;&gt;Claude 3.7 and 4 Sonnet, February and May&lt;/h2&gt;
&lt;p&gt;Anthropic &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/anthropic-launches-advanced-ai-hybrid-reasoning-model/articleshow/118544367.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launched Claude 3.7 Sonnet&lt;/a&gt;, its most intelligent model and its first hybrid reasoning model, in February. The company said that they did a few things different for its reasoning model compared to its competition.&lt;/p&gt;
&lt;p&gt;According to the company, the latest model was designed to work for both instant responses and deep thinking, “Just as humans use a single brain for both quick responses and deep reflection.” Users will also have some control over how long they want the model to “deep think”.&lt;/p&gt;
&lt;p&gt;In addition, Anthropic said that they had optimised the model less for math and computer science competition problems and focused more on tasks the real businesses used LLMs for.&lt;/p&gt;
&lt;p&gt;In May, &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/anthropic-rolls-out-claude-4-family-of-ai-agents/articleshow/121347800.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anthropic launched Claude 4 Sonnet&lt;/a&gt;, which it said is a significant upgrade to 3.7 in terms of extended reasoning, and newer capabilities such as following instructions more accurately and running tools in parallel.&lt;/p&gt;
&lt;p&gt;Also Read: &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/ettech-explainer-how-is-sonnet-3-7-hybrid-reasoning-model-different-from-rest-of-ai-pack/articleshow/118563166.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ETtech Explainer: Anthropic’s new Claude ‘hybrid model’ moves global AI goalposts&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;grok-3-and-4-february-and-july&#34;&gt;Grok 3 and 4, February and July&lt;/h2&gt;
&lt;p&gt;After launching the reasoning model &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/grok-3-ai-launch-all-you-need-to-know/articleshow/118328908.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grok 3 in February&lt;/a&gt;, Elon Musk’s AI company xAI &lt;a href=&#34;https://economictimes.indiatimes.com/news/international/us/elon-musk-unveils-grok-4-a-day-after-post-on-hitler-and-antisemitic-responses-sparked-outrage/articleshow/122358675.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launched Grok 4&lt;/a&gt; in July. The company trained the latest model on 200,000 GPU clusters to run reinforcement learning, it said in a post.&lt;/p&gt;
&lt;p&gt;“This was made possible with innovations throughout the stack, including new infrastructure and algorithmic work that increased the compute efficiency of our training by 6x.” It launched Grok Heavy, which allows the model to consider multiple hypotheses through parallel test-time computing. .&lt;/p&gt;
&lt;h2 id=&#34;openai-o3-and-gpt-5-april-and-august&#34;&gt;OpenAI O3 and GPT-5, April and August&lt;/h2&gt;
&lt;p&gt;OpenAI &lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/openai-introduces-o3-o4-mini-reasoning-models/articleshow/120371270.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launched reasoning models, o3 and o4&lt;/a&gt;-mini, its reasoning models that were “trained to think longer before responding” in April. For the first time, these models can use every tool in GPT such as search and image generation.&lt;/p&gt;
&lt;p&gt;Following this, after much speculation, OpenAI launched the &lt;a href=&#34;https://economictimes.indiatimes.com/tech/technology/openai-makes-gpt-5-friendly-again-after-users-complain-of-cold-responses/articleshow/123356472.cms?from=mdr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;much-anticipated GPT-5&lt;/a&gt; early this month. The firm said GPT-5 is a unified system, which is efficient, capable of deeper reasoning, and has a router which chooses models based on the nature of conversation.&lt;/p&gt;
&lt;p&gt;Designed for handling real-world tasks, it hallucinates, follows instructions better and shows strong performance across coding, writing and health.&lt;/p&gt;
&lt;h2 id=&#34;key-misses&#34;&gt;Key misses&lt;/h2&gt;
&lt;p&gt;While 2025 was a breakthrough year in terms of the advanced models from tech firms, a few have seen significant setback. Notably, &lt;a href=&#34;https://economictimes.indiatimes.com/topic/meta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta&lt;/a&gt;’s open source model &lt;a href=&#34;https://economictimes.indiatimes.com/topic/llama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;llama&#34;&gt;Llama&lt;/h2&gt;
&lt;p&gt;After Meta saw significant success with Llama, the latest launch had hit snags. According to a report from the &lt;a href=&#34;https://economictimes.indiatimes.com/topic/wall-street-journal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wall Street Journal&lt;/a&gt; in May, Meta is delaying the launch of Llama 4 as the company struggles to improve the capabilities of Llama 4 model Behemoth.&lt;/p&gt;
&lt;p&gt;The model was expected to be released in April and now the launch has reportedly been pushed to fall or later. A July report from the New York Times revealed that Meta might abandon Llama and go for development of closed-source models under the Superintelligence team headed by Alexandr Wang.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://economictimes.indiatimes.com/tech/artificial-intelligence/ai-breakthroughs-spur-race-for-superintelligence/articleshow/123370199.cms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://economictimes.indiatimes.com/tech/artificial-intelligence/ai-breakthroughs-spur-race-for-superintelligence/articleshow/123370199.cms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-in/money/news/ai-breakthroughs-spur-race-for-superintelligence/ar-AA1KLbtg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-in/money/news/ai-breakthroughs-spur-race-for-superintelligence/ar-AA1KLbtg&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‘It’s missing something’: AGI, superintelligence and a race for the future</title>
      <link>http://localhost:1313/blog/20250809-guardian/</link>
      <pubDate>Sat, 09 Aug 2025 18:13:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250809-guardian/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Dan Milmo and Dara Kerr&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-openai-called-its-new-gpt-5-model-which-will-power-chatgpt-a-significant-step-on-the-path-to-artificial-general-intelligence-photograph-dado-ruvićreuters&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;OpenAI called its new GPT-5 model, which will power ChatGPT, a ‘significant step on the path to artificial general intelligence’. Photograph: Dado Ruvić/Reuters&#34; srcset=&#34;
               /blog/20250809-guardian/3326_hu_3c5c1e1835a925c.webp 400w,
               /blog/20250809-guardian/3326_hu_18934035e2d9e3e.webp 760w,
               /blog/20250809-guardian/3326_hu_2f86816e72b4a68e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250809-guardian/3326_hu_3c5c1e1835a925c.webp&#34;
               width=&#34;620&#34;
               height=&#34;496&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      OpenAI called its new GPT-5 model, which will power ChatGPT, a ‘significant step on the path to artificial general intelligence’. Photograph: Dado Ruvić/Reuters
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;A significant step forward but &lt;a href=&#34;https://www.theguardian.com/technology/2025/aug/07/openai-chatgpt-upgrade-big-step-forward-human-jobs-gpt-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;not a leap over the finish line&lt;/a&gt;. That was how Sam Altman, chief executive of OpenAI, described the latest upgrade to ChatGPT this week.&lt;/p&gt;
&lt;p&gt;The race Altman was referring to was artificial general intelligence (AGI), a theoretical state of AI where, by OpenAI’s definition, a highly autonomous system is able to do a human’s job.&lt;/p&gt;
&lt;p&gt;Describing the new GPT-5 model, which will power &lt;a href=&#34;https://www.theguardian.com/technology/chatgpt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt;, as a “significant step on the path to AGI”, he nonetheless added a hefty caveat.&lt;/p&gt;
&lt;p&gt;“[It is] missing something quite important, many things quite important,” said Altman, such as the model’s inability to “continuously learn” even after its launch. In other words, these systems are impressive but they have yet to crack the autonomy that would allow them to do a full-time job.&lt;/p&gt;
&lt;p&gt;OpenAI’s competitors, also flush with billions of dollars to lavish on the same goal, are straining for the tape too. Last month, Mark Zuckerberg, chief executive of Facebook parent &lt;a href=&#34;https://www.theguardian.com/technology/meta&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meta&lt;/a&gt;, said development of superintelligence – another theoretical state of AI where a system far exceeds human cognitive abilities – is “now in sight”.&lt;/p&gt;
&lt;p&gt;Google’s AI unit on Tuesday outlined its next step to AGI by announcing an &lt;a href=&#34;https://www.theguardian.com/technology/2025/aug/05/google-step-artificial-general-intelligence-deepmind-agi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unreleased model&lt;/a&gt; that trains AIs to interact with a convincing simulation of the real world, while Anthropic, another company making significant advances, announced an upgrade to its Claude Opus 4 model.&lt;/p&gt;
&lt;p&gt;So where does this leave the race to AGI and superintelligence?&lt;/p&gt;
&lt;p&gt;Benedict Evans, a tech analyst, says the race towards a theoretical state of AI is taking place against a backdrop of scientific uncertainty – despite the intellectual and financial investment in the quest.&lt;/p&gt;
&lt;p&gt;Describing AGI as a “thought experiment as much as it is a technology”, he says: “We don’t really have a theoretical model of why generative AI models work so well and what would have to happen for them to get to this state of AGI.”&lt;/p&gt;
&lt;p&gt;He adds: “It’s like saying ‘we’re building the Apollo programme but we don’t actually know how gravity works or how far away the moon is, or how a rocket works, but if we keep on making the rocket bigger maybe we’ll get there’.&lt;/p&gt;
&lt;p&gt;“To use the term of the moment, it’s very vibes-based. All of these AI scientists are really just telling us what their personal vibes are on whether we’ll reach this theoretical state – but they don’t know. And that’s what sensible experts say too.”&lt;/p&gt;
&lt;p&gt;However, Aaron Rosenberg, a partner at venture capital firm Radical Ventures – whose investments include leading AI firm Cohere – and former head of strategy and operations at Google’s AI unit DeepMind, says a more limited definition of AGI could be achieved around the end of the decade.&lt;/p&gt;
&lt;p&gt;“If you define AGI more narrowly as at least 80th percentile human-level performance in 80% of economically relevant digital tasks, then I think that’s within reach in the next five years,” he says.&lt;/p&gt;
&lt;p&gt;Matt Murphy, a partner at VC firm Menlo Ventures, says the definition of AGI is a “moving target”.&lt;/p&gt;
&lt;p&gt;He adds: “I’d say the race will continue to play out for years to come and that definition will keep evolving and the bar being raised.”&lt;/p&gt;
&lt;p&gt;Even without AGI, the generative AI systems in circulation are making money. The New York Times &lt;a href=&#34;https://www.nytimes.com/2025/08/01/business/dealbook/openai-ai-mega-funding-deal.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reported this month&lt;/a&gt; that OpenAI’s annual recurring revenue has reached $13bn (£10bn), up from $10bn earlier in the summer, and could pass $20bn by the year end. Meanwhile, OpenAI is reportedly in talks about a sale of shares held by current and former employees that would value it at about $500bn, &lt;a href=&#34;https://www.theguardian.com/technology/2025/aug/06/openai-chatgpt-talks-share-sale-price-more-than-musk-spacex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;exceeding the price tag for Elon Musk’s SpaceX&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some experts view statements about superintelligent systems as creating unrealistic expectations, while distracting from more immediate concerns such as making sure that systems being deployed now are reliable, transparent and free of bias.&lt;/p&gt;
&lt;p&gt;“The rush to claim ‘superintelligence’ among the major tech companies reflects more about competitive positioning than actual technical breakthroughs,” says &lt;strong&gt;David Bader&lt;/strong&gt;, director of the institute for data science at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;“We need to distinguish between genuine advances and marketing narratives designed to attract talent and investment. From a technical standpoint, we’re seeing impressive improvements in specific capabilities – better reasoning, more sophisticated planning, enhanced multimodal understanding.&lt;/p&gt;
&lt;p&gt;“But superintelligence, properly defined, would represent systems that exceed human performance across virtually all cognitive domains. We’re nowhere near that threshold.”&lt;/p&gt;
&lt;p&gt;Nonetheless, the major US tech firms will keep trying to build systems that match or exceed human intelligence at most tasks. Google’s parent Alphabet, Meta, Microsoft and Amazon alone will spend nearly $400bn this year on AI, &lt;a href=&#34;https://www.wsj.com/tech/ai/tech-ai-spending-company-valuations-7b92104b?mod=tech_trendingnow_article_pos2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;according to the Wall Street Journal&lt;/a&gt;, comfortably more than &lt;a href=&#34;https://eda.europa.eu/news-and-events/news/2024/12/04/eu-defence-spending-hits-new-records-in-2023-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EU members’ defence spend&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Rosenberg acknowledges he is a former Google DeepMind employee but says the company has big advantages in data, hardware, infrastructure and an array of products to hone the technology, from search to maps and YouTube. But advantages can be slim.&lt;/p&gt;
&lt;p&gt;“On the frontier, as soon as an innovation emerges, everyone else is quick to adopt it. It’s hard to gain a huge gap right now,” he says.&lt;/p&gt;
&lt;p&gt;It is also a global race, or rather a contest, that includes China. &lt;a href=&#34;https://www.theguardian.com/technology/deepseek&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSeek&lt;/a&gt; came from nowhere this year to announce the DeepSeek R1 model, boasting of “powerful and intriguing reasoning behaviours” comparable with OpenAI’s best work.&lt;/p&gt;
&lt;p&gt;Major companies looking to integrate AI into their operations have taken note. Saudi Aramco, the world’s largest oil company, uses DeepSeek’s AI technology in its main datacentre and said it was “really making a big difference” to its IT systems and was making the company more efficient.&lt;/p&gt;
&lt;p&gt;According to Artificial Analysis, a company that ranks AI models, six of the top 20 on its leaderboard – which ranks models according to a range of metrics including intelligence, price and speed – are Chinese. The six models are developed by DeepSeek, Zhipu AI, Alibaba and MiniMax. On the leaderboard for video generation models, six of the top 10 – including the current leader, ByteDance’s Seedance – are also Chinese.&lt;/p&gt;
&lt;p&gt;Microsoft’s president, Brad Smith, whose company has barred use of DeepSeek, told a US senate hearing in May that getting your AI model adopted globally was a key factor in determining which country wins the AI race.&lt;/p&gt;
&lt;p&gt;“The number one factor that will define whether the US or China wins this race is whose technology is most broadly adopted in the rest of the world,” he said, adding that the lesson from Huawei and 5G was that whoever establishes leadership in a market is “difficult to supplant”.&lt;/p&gt;
&lt;p&gt;It means that, arguments over the feasibility of superintelligent systems aside, vast amounts of money and talent are being poured into this race in the world’s two largest economies – and tech firms will keep running.&lt;/p&gt;
&lt;p&gt;“If you look back five years ago to 2020 it was almost blasphemous to say AGI was on the horizon. It was crazy to say that. Now it seems increasingly consensus to say we are on that path,” says Rosenberg.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theguardian.com/technology/2025/aug/09/its-missing-something-agi-superintelligence-and-a-race-for-the-future&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.theguardian.com/technology/2025/aug/09/its-missing-something-agi-superintelligence-and-a-race-for-the-future&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-gb/money/technology/it-s-missing-something-agi-superintelligence-and-a-race-for-the-future/ar-AA1KcYxc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-gb/money/technology/it-s-missing-something-agi-superintelligence-and-a-race-for-the-future/ar-AA1KcYxc&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slaughterhouse GPT-5</title>
      <link>http://localhost:1313/blog/20250807-puck/</link>
      <pubDate>Thu, 07 Aug 2025 10:21:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250807-puck/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Ian Krietzberg&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-in-the-demo-sam-altman-hinted-that-openai-is-discovering-new-paradigms-but-its-unclear-what-those-are-or-whether-they-are-in-some-way-present-in-gpt-5-photo-justin-sullivangetty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;In the demo, Sam Altman hinted that OpenAI is “discovering new paradigms,” but it’s unclear what those are, or whether they are in some way present in GPT-5. *Photo: Justin Sullivan/Getty Images*&#34; srcset=&#34;
               /blog/20250807-puck/GettyImages-2218344193-scaled-e1754599600977-1088x612_hu_8e536853096411a1.webp 400w,
               /blog/20250807-puck/GettyImages-2218344193-scaled-e1754599600977-1088x612_hu_f8b83e47b15b02c9.webp 760w,
               /blog/20250807-puck/GettyImages-2218344193-scaled-e1754599600977-1088x612_hu_d3a427a5850b0ffa.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250807-puck/GettyImages-2218344193-scaled-e1754599600977-1088x612_hu_8e536853096411a1.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      In the demo, Sam Altman hinted that OpenAI is “discovering new paradigms,” but it’s unclear what those are, or whether they are in some way present in GPT-5. &lt;em&gt;Photo: Justin Sullivan/Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For all the feverish excitement surrounding today’s release of GPT-5, the latest and most advanced OpenAI model seems to represent more of an incremental update than a paradigm-shifting breakthrough. At a press briefing, C.E.O. &lt;strong&gt;Sam Altman&lt;/strong&gt; called GPT-5 a “major upgrade” that serves as a “significant step along the path to A.G.I.,” or artificial general intelligence, an entirely hypothetical technology. At another point he referred to it as “very A.G.I.-like.” Obviously, we’re not quite there—and it’s not clear that we ever will be.&lt;/p&gt;
&lt;p&gt;Nevertheless, GPT-5, which comes in pro, standard, “mini,” and “nano” flavors, is a pretty impressive upgrade—&lt;a href=&#34;https://www.oneusefulthing.org/p/gpt-5-it-just-does-stuff?triedRedirect=true&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;particularly to the folks&lt;/a&gt; who got to test it out before the release. The system offers better coding and software development capabilities than previous models—it scored a 74.9 percent on SWE-bench, the popular software engineering benchmark—which could make it more competitive with Anthropic models. According to Altman, GPT-5 can “instantaneously create an entire piece of computer software … on demand.” In a demo at the briefing, an OpenAI researcher created a seemingly functional web app with a two-paragraph prompt in less than a minute. It’s also on par with the competition on a series of other benchmarks, although it’s not a knockout; on ARC-AGI-2, for instance, &lt;strong&gt;Elon Musk&lt;/strong&gt;’s Grok 4 maintains &lt;a href=&#34;https://x.com/fchollet/status/1953511631054680085&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a significant lead&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But what most distinguishes GPT-5 from past iterations is its two-model architecture, which combines a “high-throughput model” to handle ordinary questions with a “reasoning” model to answer thornier ones. A “real-time router” decides which of the two models to use based on each prompt. “This idea that we can use more compute, higher-quality data, better environments, whatever, to make smarter and smarter models, we see orders of magnitude more gains in front of us,” Altman said, with the addendum, of course, that he’ll have to invest in the compute side “at an eye-watering rate” to get there (surely good news for Nvidia, whose stock hit a new 52-week high today).&lt;/p&gt;
&lt;p&gt;GPT-5 also hallucinates less than previous OpenAI models, at least according to the company’s own internal evaluations. The improvements point to the possibility of new or hybrid architectures (or lots of engineering scaffolding), which OpenAI didn’t clarify when I asked about it. In the demo, Altman hinted that OpenAI is “discovering new paradigms,” but it’s unclear what those are, or whether they are in some way present in GPT-5. Meanwhile, the model’s context window—which refers to the amount of text, in tokens, that a model can “remember” at one time—is 400,000 tokens (in the A.P.I.), significantly less than the million-token-strong context window of GPT-4.1.&lt;/p&gt;
&lt;p&gt;Perhaps most notably for the average user, OpenAI says that GPT-5 just “feels more human”—although what that means is anyone’s guess. “In which ways is it more human than it was before?” asked Dr. &lt;strong&gt;David Bader&lt;/strong&gt;, the director of the Institute for Data Science at the New Jersey Institute of Technology, when I asked for his first impression. “Does it talk in one’s vernacular? Does it make more mistakes, since to err is human?” As even Altman conceded, we’re not quite through the looking glass yet.&lt;/p&gt;
&lt;h2 id=&#34;baby-steps&#34;&gt;Baby Steps&lt;/h2&gt;
&lt;p&gt;The road to GPT-5 has been longer than originally anticipated. Many thought it would &lt;a href=&#34;https://arstechnica.com/information-technology/2024/03/openais-gpt-5-may-launch-this-summer-upgrading-chatgpt-along-the-way/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launch&lt;/a&gt; last year, about a year after Altman debuted GPT-4 in March 2023. Instead, OpenAI released a series of incremental updates and new models: 4o and 4o-mini, o1, o3, and GPT-4.5 (which was &lt;a href=&#34;https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;later scrapped&lt;/a&gt;) and 4.1, none of which were apparently worthy of the lofty GPT-5 title.&lt;/p&gt;
&lt;p&gt;In fact, a downshift in the pace of innovation from giant leaps to incremental gains has become a defining feature of the A.I. industry. That’s not to say that many of these new products aren’t impressive. It’s just getting harder to achieve something that could be considered legitimately groundbreaking. In the meantime, increased competition between A.I. companies has pushed them to release smaller upgrades—on an accelerated time frame, no less—in order to stay one step ahead of their rivals. Earlier this week, Anthropic released &lt;a href=&#34;https://x.com/AnthropicAI/status/1952768432027431127&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Claude Opus 4.1&lt;/a&gt;, which is supposedly better at coding than Opus 4 (funny timing, OpenAI), and Google debuted &lt;a href=&#34;https://x.com/agrimgupta92/status/1952735042029105392&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind’s Genie 3&lt;/a&gt;, a “world model” with enhanced memory. OpenAI also released its long-awaited &lt;a href=&#34;https://openai.com/index/introducing-gpt-oss/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-oss series&lt;/a&gt;, a handful of smallish, partially open models.&lt;/p&gt;
&lt;p&gt;According to Dr. &lt;strong&gt;John Licato&lt;/strong&gt;, the director of the Advancing Machine and Human Reasoning Lab at the University of South Florida, this is just how science works. “There’s this kind of popular misconception of scientific advancement where it’s a lone genius in a room who creates something and it emerges from their mind fully baked,” he told me. “That’s not really how it works. Everybody keeps building off of each other, and they make a minor change, then when you’ve had enough of those minor changes, you call it a major release. This is no different.”&lt;/p&gt;
&lt;p&gt;Even at this incremental pace, Licato and Bader explained, genuine model improvements are becoming more difficult to squeeze out. Several &lt;a href=&#34;https://www.reuters.com/business/retail-consumer/openais-long-awaited-gpt-5-model-nears-release-2025-08-06/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reports&lt;/a&gt; have detailed OpenAI’s hard journey to GPT-5, and asserted that the leap between GPT-4 and GPT-5 is significantly smaller than the leap between GPT-3 and GPT-4. “The improvements are slowing down,” Bader told me. “They’re not as dramatic as those first couple of releases.”&lt;/p&gt;
&lt;p&gt;Bader wasn’t really surprised by anything in the GPT-5 release, and called the coding improvements “low-hanging fruit”—especially since Anthropic’s Claude has generally been considered a much better coding assistant than anything OpenAI has been able to produce thus far. And when it comes to OpenAI’s claim that GPT-5 will feature less user deception, less hallucination, and instantaneous software generation, he’ll believe it when he sees it. “I tend to discount statements like that as potentially more marketing- or publicity-related,” Bader said, noting that prerelease testing rarely captures the full breadth of a model’s performance; it takes millions of real-world users to find their actual limits. (Also, as with pretty much every A.I. model release since 2022, non-peer-reviewed demos always look fantastic—but the reality tends to be a little less rosy.)&lt;/p&gt;
&lt;p&gt;For Licato, who was expecting to see a significant hardware integration, or leveled-up video processing, a model that’s merely “better” probably won’t meet most people’s expectations. “It’s not a disappointment in the sense that it won’t actually be better,” he told me. “But a disappointment relative to what people are expecting, given that it’s the big version number.” The model, he later added, shouldn’t even be called GPT-5. “This is GPT-4.2. Absolutely nothing groundbreaking here, and honestly, even the presenters seemed underprepared,” he said. “They let Google and Anthropic dictate their release schedule, and it shows.”&lt;/p&gt;
&lt;p&gt;Indeed, many of the most impactful advances going forward will likely revolve around more prosaic concerns: product-market fit, user interface, customer experience, etcetera. (GPT-5 allows users to select a personality and choose different colors for their chats.) As Licato told me, for the big players, it’s all about “that attention-economy win.”&lt;/p&gt;
&lt;p&gt;Bader agreed that “we need to distinguish between genuine advances and marketing narratives designed to attract talent and investment.” Technically, he noted, we’ve seen plenty of impressive model advancements over the last few years, including enhanced planning, reasoning, and multimodal understanding. OpenAI, in particular, has pioneered key innovations in chain-of-thought reasoning, reinforcement learning, and improved tokenization strategies, but it’s not a significant step toward A.G.I., or manna from heaven for some of the &lt;a href=&#34;https://firstmonday.org/ojs/index.php/fm/article/view/13636&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A.I. pseudo-religions&lt;/a&gt; that have permeated across Silicon Valley.&lt;/p&gt;
&lt;p&gt;“When companies suggest they’re on the verge of achieving artificial general intelligence, it creates unrealistic expectations and potentially diverts attention from more immediate A.I. challenges around bias, reliability, and transparency,” Bader said. He added that we should recognize OpenAI’s technical, incremental advancements as “engineering achievements within the current paradigm—not evidence of an imminent leap to A.G.I.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://puck.news/gpt5s-underwhelming-launch-and-the-limits-of-ai-hype/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://puck.news/gpt5s-underwhelming-launch-and-the-limits-of-ai-hype/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research from NJIT and Yale Leads to Trio of Computing Innovations</title>
      <link>http://localhost:1313/blog/20250729-njit/</link>
      <pubDate>Tue, 29 Jul 2025 10:04:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250729-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-ai-image-of-data-patterns-by-adobe-firefly&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;AI image of data patterns by Adobe Firefly&#34; srcset=&#34;
               /blog/20250729-njit/Firefly_Start%20over.%20Show%20an%20example%20of%20a%20graph%20neural%20network.%20962626_hu_d4b6564a4b32b8d9.webp 400w,
               /blog/20250729-njit/Firefly_Start%20over.%20Show%20an%20example%20of%20a%20graph%20neural%20network.%20962626_hu_f0128ff4c080b28d.webp 760w,
               /blog/20250729-njit/Firefly_Start%20over.%20Show%20an%20example%20of%20a%20graph%20neural%20network.%20962626_hu_1999ba532f1ded5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250729-njit/Firefly_Start%20over.%20Show%20an%20example%20of%20a%20graph%20neural%20network.%20962626_hu_d4b6564a4b32b8d9.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      AI image of data patterns by Adobe Firefly
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;New research from New Jersey Institute of Technology and Yale University, intended to help identify obscure patterns in overwhelmingly large and convoluted data, is producing novel side effects that advance the state of very old and very new technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NJIT Distinguished Professor David Bader&lt;/strong&gt; said the inspiration for a joint $540,000 National Science Foundation grant, “Scalable Algorithmic and Software Foundations for Subgraph Counting and Enumeration,&amp;quot; is that existing pattern-finding science is struggling to keep pace with modern data sets.&lt;/p&gt;
&lt;p&gt;Applications such as anomaly detection in computer networks for cybersecurity purposes, protein interactions with human cells for evolving treatments and trending topic amplification in social media all begin when algorithms explore subgraphs, which can be as small as just a few interconnected data points.&lt;/p&gt;
&lt;p&gt;“Subgraph analysis has been a core part of my research for years,” Bader explained. “But what&amp;rsquo;s driving urgency now is the explosion in dynamic graph sizes — we&amp;rsquo;re seeing networks with billions of edges changing hundreds of millions of times per day. Traditional algorithms simply can&amp;rsquo;t keep up with this scale and dynamism.”&lt;/p&gt;
&lt;p&gt;Bader, along with Yale Assistant Professor Quanquan Liu and their team of students and researchers, are developing new algorithms that are provably efficient at counting and listing subgraphs. The team is also making new programming frameworks and they’re looking to perform real-world validation — currently the research is only theoretically optimized. Another goal is to make their software work well on local servers and cloud networks, which are different technical challenges. Liu is focused on the algorithm work, while Bader leads the systems research.&lt;/p&gt;
&lt;h2 id=&#34;challenges-and-side-effects&#34;&gt;Challenges and side effects&lt;/h2&gt;
&lt;p&gt;It’s not officially decided, but the new frameworks will likely integrate with Arkouda and Arachne. Arkouda is an open-source data science library for the Python programming language, and Arachne is an Arkouda extension led by Bader which adds features for working with large data.&lt;/p&gt;
&lt;p&gt;“Integration into Arachne is definitely the right approach. The major pro is that Arachne already has the distributed computing infrastructure … plus a Python API that data scientists are comfortable with. We&amp;rsquo;d be building on a proven foundation rather than starting from scratch. The challenge is that Arachne currently doesn&amp;rsquo;t handle dynamic graphs — it&amp;rsquo;s designed for static analysis. We need to carefully architect the batch-dynamic capabilities without breaking existing functionality. But this is much more tractable than building an entirely new system.”&lt;/p&gt;
&lt;p&gt;There are two novel side effects of this subgraph research. One is in batch processing, which is the concept that software will ingest all of the data before acting upon it. That’s been common since the dawn of industrial computing in the 1950s. Batch techniques are being implemented to make the new subgraph research possible, because the data being studied are too dynamic for software to examine one step at a time. The research team developed a new way to take a virtual snapshot of the data and work from that information, since real-world full graphs change too quickly to be analyzed in real-time.&lt;/p&gt;
&lt;p&gt;The other side effect is on the artificial intelligence frontier — Bader said faster subgraph counting could speed up AI applications that use a common research technique in data science called graph neural networks.&lt;/p&gt;
&lt;p&gt;“What excites me most is that we&amp;rsquo;re not just making algorithms faster — we&amp;rsquo;re making breakthrough analysis possible. When you can analyze billion-edge networks that change millions of times per day, you unlock insights that were previously impossible to capture.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/research-njit-and-yale-leads-trio-computing-innovations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/research-njit-and-yale-leads-trio-computing-innovations&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Selected Among &#39;35 Legends in the Class of 2025&#39; by HPCwire</title>
      <link>http://localhost:1313/blog/20250716-njit/</link>
      <pubDate>Wed, 16 Jul 2025 11:36:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250716-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250716-njit/35%20Legends%202025%20Badge_0_hu_db09de5a573e4a98.webp 400w,
               /blog/20250716-njit/35%20Legends%202025%20Badge_0_hu_f074143a29bf3dc0.webp 760w,
               /blog/20250716-njit/35%20Legends%202025%20Badge_0_hu_b958dbbdc0162088.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250716-njit/35%20Legends%202025%20Badge_0_hu_db09de5a573e4a98.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; in the Ying Wu College of Computing’s Department of Data Science has been recognized among an elite roster of High-Performance Computing (HPC) pioneers in the &lt;a href=&#34;https://www.hpcwire.com/35-hpc-legends/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPCwire 35 Legends Class of 2025&lt;/a&gt; list. The honorees are selected annually based on contributions to the HPC community over the past 35 years that have been instrumental in improving the quality of life on our planet through technology.&lt;/p&gt;
&lt;p&gt;“HPC has the potential to be the foundation for solving the defining challenges of the 21st century,” he said in a recent HPCwire interview.&lt;/p&gt;
&lt;p&gt;He credits his advancements to hands-on experimentation using several Commodore Amiga 1000 personal computers that had been “collecting dust in a closet” to build his first parallel computer as an undergraduate at Lehigh University in the late 1980’s.&lt;/p&gt;
&lt;p&gt;Bader’s radical (at the time) vision as a Ph.D. candidate at the University of Maryland to construct supercomputers from commodity components while strategically incorporating high-performance technologies where they mattered most would ultimately lead him to develop the first Linux supercomputer. The innovation used commodity off-the-shelf components at a time when most supercomputers depended on proprietary operating systems and specialized hardware. He then went on to develop Roadrunner, the first Linux supercomputer for open use by the national science and engineering community via the NSF’s (National Science Foundation) National Technology Grid.&lt;/p&gt;
&lt;p&gt;“The breakthrough came when I realized that the key wasn’t choosing between cost and performance but rather optimizing the integration of different technologies to maximize capability per dollar,” he said.&lt;/p&gt;
&lt;p&gt;By demonstrating that a Linux-based system could achieve production supercomputing performance while improving accessibility and reducing cost, Bader was instrumental in establishing what is now the predominant architecture for all major supercomputing worldwide.&lt;/p&gt;
&lt;p&gt;In 2021, Hyperion Research estimated that the total economic value of Linux supercomputing pioneered by his work has exceeded $100 trillion over the past 25 years.&lt;/p&gt;
&lt;p&gt;Bader is currently director for the Institute for Data Science at NJIT, directing work that encompasses research centers in AI, big data, medical informatics, quantum computing, and cybersecurity.&lt;/p&gt;
&lt;p&gt;He is a Fellow of IEEE, AAAS, SIAM, and ACM, and was awarded the IEEE Sidney Fernbach Award in 2021. His alma mater inducted him into the University of Maryland’s Innovation Hall of Fame the following year.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/david-bader-selected-among-35-legends-class-2025-hpcwire&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/david-bader-selected-among-35-legends-class-2025-hpcwire&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The HPCwire 35 Legends of 2025</title>
      <link>http://localhost:1313/blog/20250709-hpcwire/</link>
      <pubDate>Wed, 09 Jul 2025 18:25:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250709-hpcwire/</guid>
      <description>&lt;p&gt;The High Performance Computing (HPC) market is distinguished by the close collaboration among scientific researchers, end users, and technology vendors, all working in concert to advance the field. Our Legends are the pioneering researchers, visionary inventors, and influential executives who have translated innovative concepts into both technological breakthroughs and commercial successes.&lt;/p&gt;
&lt;p&gt;Thirty-five honorees are announced each year, selected by HPCwire editors and advisors based on their contributions to the HPC community over the past 35 years, and celebrated for the different ways they have helped move HPC forward. The HPCwire 35 Legends Class of 2024 can be found &lt;a href=&#34;https://www.hpcwire.com/35-hpc-legends-class-of-2024/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As you read through this list of HPC Legends, you’ll immediately notice their impressive stature. This year’s list does not, in any manner, prioritize by ranking; every single one of these legends are amazing, and we’re equally honored to have them in our community.&lt;/p&gt;
&lt;h2 id=&#34;hpcwire-35-legends-class-of-2025&#34;&gt;HPCwire 35 Legends Class of 2025:&lt;/h2&gt;
&lt;p&gt;Horst Simon&lt;br&gt;
HPC Benchmark Pioneer and Exascale Visionary&lt;/p&gt;
&lt;p&gt;Ralph Roskies&lt;br&gt;
Co-founder of PSC&lt;/p&gt;
&lt;p&gt;Michael Levine&lt;br&gt;
Co-founder of PSC&lt;/p&gt;
&lt;p&gt;Vinton Cerf&lt;br&gt;
One of the Fathers of the Internet&lt;/p&gt;
&lt;p&gt;Lisa Su&lt;br&gt;
Leader of AMD’s Exascale Era&lt;/p&gt;
&lt;p&gt;Thomas Lippert&lt;br&gt;
Architect of Europe’s First Exascale System&lt;/p&gt;
&lt;p&gt;Dieter Kranzlmüller&lt;br&gt;
Germany’s Supercomputing Leader&lt;/p&gt;
&lt;p&gt;Satoshi Sekiguchi&lt;br&gt;
Architect of Japan’s AI Supercomputing&lt;/p&gt;
&lt;p&gt;Susan Gregurick&lt;br&gt;
Architect of NIH HPC/Data Strategy&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250709-hpcwire/hpcw-35Legend2025-DAVID-BADER-X-1280x720_hu_34486d619a9da5ac.webp 400w,
               /blog/20250709-hpcwire/hpcw-35Legend2025-DAVID-BADER-X-1280x720_hu_31baba0988d70c02.webp 760w,
               /blog/20250709-hpcwire/hpcw-35Legend2025-DAVID-BADER-X-1280x720_hu_47144c62f560105d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250709-hpcwire/hpcw-35Legend2025-DAVID-BADER-X-1280x720_hu_34486d619a9da5ac.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/35-hpc-legends-david-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;
Linux-Cluster Trailblazer&lt;/p&gt;
&lt;p&gt;Wu Feng&lt;br&gt;
Co-Founder of Green500&lt;/p&gt;
&lt;p&gt;Ian Buck&lt;br&gt;
Inventor of CUDA&lt;/p&gt;
&lt;p&gt;Richard Tapia&lt;br&gt;
Early Champion of Diversity in HPC&lt;/p&gt;
&lt;p&gt;Mateo Valero Cortés&lt;br&gt;
Spain’s Vanguard of European HPC&lt;/p&gt;
&lt;p&gt;Lori Diachin&lt;br&gt;
Leader of the Exascale Computing Project&lt;/p&gt;
&lt;p&gt;Jysoo Lee&lt;br&gt;
Supercomputing Strategist for Korea and Saudi Arabia&lt;/p&gt;
&lt;p&gt;Bob Borchers&lt;br&gt;
Architect of U.S. NSF Supercomputing Initiatives&lt;/p&gt;
&lt;p&gt;Beverly Clayton&lt;br&gt;
The First Woman to Direct an HPC Center (PSC)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/35-hpc-legends/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/35-hpc-legends/&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;david-bader&#34;&gt;David Bader&lt;/h1&gt;
&lt;h2 id=&#34;linux-cluster-trailblazer&#34;&gt;Linux-Cluster Trailblazer&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt;’s passion for HPC began early and was deeply rooted in hands-on experimentation. As an undergraduate at Lehigh University in the late 1980s, he built his first parallel computer using several Commodore Amiga 1000 personal computers that had been collecting dust in a closet, he told &lt;em&gt;HPCwire&lt;/em&gt;. A year later, he designed parallel algorithms on a 128-processor nCUBE hypercube donated by Bell Labs.&lt;/p&gt;
&lt;p&gt;“These early experiences taught me a fundamental lesson: building powerful parallel machines requires simultaneous development of scalable, high-performance algorithms and robust system services,” he said.&lt;/p&gt;
&lt;p&gt;During his PhD studies at the University of Maryland, Bader had a radical (at the time) vision: that supercomputers could be constructed from commodity components while strategically incorporating high-performance technologies where they mattered most. Bader’s background in electrical engineering, computer engineering, and algorithm design provided an ideal foundation for understanding both the theoretical and practical aspects of this challenge.&lt;/p&gt;
&lt;p&gt;This mindset led to one of Bader’s most significant contributions to HPC. In 1998 at the University of New Mexico, he developed the first Linux supercomputer using commodity off-the-shelf components at a time when most supercomputers used proprietary operating systems and specialized hardware. After a successful prototype design, he led the development of RoadRunner, the first Linux supercomputer for open use by the national science and engineering community via the NSF’s National Technology Grid. RoadRunner’s revolutionary three-network architecture and balanced design philosophy combined commodity processors with high-performance interconnects and comprehensive system management capabilities.&lt;/p&gt;
&lt;p&gt;“The breakthrough came when I realized that the key wasn’t choosing between cost and performance but rather optimizing the integration of different technologies to maximize capability per dollar,” he told HPCwire.&lt;/p&gt;
&lt;p&gt;By demonstrating that a Linux-based system could achieve production supercomputing performance while being more accessible and cost-effective, Bader helped establish what is now the predominant architecture for all major supercomputers worldwide. The Computer History Museum recognizes this work on its Timeline of Computer History, and in 2021, Hyperion Research estimated that the total economic value of Linux supercomputing pioneered by this work has exceeded $100 trillion over the past 25 years.&lt;/p&gt;
&lt;p&gt;Bader has spent the years since widening HPC’s reach. At Georgia Tech he founded the School of Computational Science and Engineering. He is now a Distinguished Professor and Director of the Institute for Data Science at the New Jersey Institute of Technology, where he founded the Department of Data Science. His responsibilities include leading cutting-edge research at the intersection of HPC and real-world applications, directing an institute that encompasses research centers in AI, big data, medical informatics, quantum computing, and cybersecurity, and developing innovative academic programs including pioneering BS and PhD. programs in data science.&lt;/p&gt;
&lt;p&gt;Bader was co-founder of the Graph500 List for benchmarking “Big Data” computing platforms, which has influenced the HPC community to look beyond LINPACK as the sole performance ranking metric and consider the irregular, data-intensive workloads that are increasingly important in modern computing.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of IEEE, AAAS, SIAM, and ACM. He was awarded the IEEE Sidney Fernbach Award in 2021 and was inducted into the University of Maryland’s Innovation Hall of Fame the following year. This year, he was inducted into the Mimms Museum of Technology and Art Hall of Fame and received the Heatherington Award for Technological Innovation.&lt;/p&gt;
&lt;p&gt;Looking to the horizon of the discipline he helped shape, Bader concludes, “Having witnessed the transformation of HPC from proprietary systems to the Linux-dominated landscape we have today, I’m now focused on ensuring that future HPC systems are not just faster, but more accessible and applicable to the challenges that matter most to humanity. Whether it’s protecting our digital infrastructure, accelerating scientific discovery, or enabling new forms of artificial intelligence, HPC has the potential to be the foundation for solving the defining challenges of the 21st century.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/35-hpc-legends-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/35-hpc-legends-david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meta Positioning WhatsApp To Be a Super App</title>
      <link>http://localhost:1313/blog/20250709-technewsworld/</link>
      <pubDate>Wed, 09 Jul 2025 18:08:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250709-technewsworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250709-technewsworld/WhatsApp-icon_hu_e95ab71e41c0c8e7.webp 400w,
               /blog/20250709-technewsworld/WhatsApp-icon_hu_91c40e6d21c9dbcd.webp 760w,
               /blog/20250709-technewsworld/WhatsApp-icon_hu_2c7dd94f9b53114e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250709-technewsworld/WhatsApp-icon_hu_e95ab71e41c0c8e7.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Without much fanfare, Meta has been quietly enhancing the capabilities of its WhatsApp messaging software, which could transform it into a super app.&lt;/p&gt;
&lt;p&gt;While super apps have gained traction in Asia, they haven’t caught on in the West. Apps like WeChat in China, Grab in Singapore, Gojek in Indonesia, and Paytm in India offer users a bundle of services in a single app — such as messaging, payments, social media, shopping, booking, food delivery, and ride-hailing services.&lt;/p&gt;
&lt;p&gt;“Rather than replicate WeChat’s model in full, Meta appears to be abstracting the behaviors that matter most,” Paul Armstrong, the founder of the TBD Group, a technology consulting firm, wrote Tuesday in City A.M., a London-based business newspaper.&lt;/p&gt;
&lt;p&gt;“China’s WeChat integrates messaging, payments, e-commerce, social media, and even government services into a single environment,” he wrote. “WhatsApp is not built to host that degree of functionality, nor would most Western regulatory environments allow it.”&lt;/p&gt;
&lt;p&gt;“Meta is instead layering in lightweight versions of those capabilities,” he continued. “Each integration is designed to be contextually relevant, low-friction, and invisible when not needed.”&lt;/p&gt;
&lt;p&gt;“The result is not a Western WeChat clone,” he noted, “but a modular system with a similar behavioral footprint, transactional, sticky, and increasingly agent-mediated.”&lt;/p&gt;
&lt;h2 id=&#34;app-store-barriers-limit-super-app-growth&#34;&gt;App Store Barriers Limit Super App Growth&lt;/h2&gt;
&lt;p&gt;Ross Rubin, the principal analyst at &lt;a href=&#34;https://www.reticleresearch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reticle Research&lt;/a&gt;, a consumer technology advisory firm in New York City, noted that the idea of a super app coming to the United States has been around for some time.&lt;/p&gt;
&lt;p&gt;“It’s been challenging, in part, because unlike in China, where there’s a fragmented app store landscape, here you have two major players, both of whom have their own entrants in many of these categories, making it a bit more challenging to launch such an app,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;For example, if a super app wanted to offer ride sharing natively, it would lock horns with the likes of Uber. “That’s hard because you have to basically get users off the Uber app and onto the super app,” explained Malik Ahmed Khan, a technology equity analyst at &lt;a href=&#34;https://www.morningstar.com/products/research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Morningstar Research Services&lt;/a&gt; in Chicago.&lt;/p&gt;
&lt;p&gt;“The super app either has to have its own ride service or partner with Uber,” he told TechNewsWorld. “But why would Uber want to give up its users and book through another app when it can maintain them on its own app and be in charge of that customer account?”&lt;/p&gt;
&lt;p&gt;Adam Landis, head of growth at &lt;a href=&#34;https://www.branch.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Branch&lt;/a&gt;, a mobile analytics software company in Mountain View, Calif., agreed that app stores can be a barrier to the rise of a super app.&lt;/p&gt;
&lt;p&gt;“In Asia, super apps are deeply integrated into daily life,” he told TechNewsWorld. “In the U.S., Apple’s restrictive App Store policies — limiting payments, third-party app distribution, and ecosystem layering — have stifled similar development. But Apple’s loosening grip may open the door for true super app adoption.”&lt;/p&gt;
&lt;p&gt;“AI is reshaping digital commerce,” he added. “By building a self-contained commercial ecosystem, Meta can harness behavioral data and transactional intent to drive the next evolution of AI-powered commerce.”&lt;/p&gt;
&lt;p&gt;“AI is the accelerant,” he continued. “Platforms like OpenAI, with persistent context and multi-service interfaces, could become super apps in disguise, handling discovery, decision-making, and transactions autonomously.”&lt;/p&gt;
&lt;h2 id=&#34;whom-do-you-trust&#34;&gt;Whom Do You Trust?&lt;/h2&gt;
&lt;p&gt;Khan pointed out another challenge facing a Meta super app. “If Meta had all these different services integrated into one app, there could be some resistance from a data privacy perspective,” he said. “People might ask, ‘Do I want Meta to know when I’m ordering an Uber or know where I am going?&amp;rsquo;”&lt;/p&gt;
&lt;p&gt;“Consumers like things to be easy, so if an app comes along that reduces the friction to make payments, it may be attractive,” added &lt;a href=&#34;https://www.cs.umd.edu/~golbeck/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jennifer Golbeck&lt;/a&gt;, a professor at the University of Maryland’s College of Information Studies. “At the same time, consumers are rightfully concerned about privacy and security. Do they trust this company to take care of their credit card and banking information? Will they be charged for something unexpectedly? Is there a chance for fraud?”&lt;/p&gt;
&lt;p&gt;Golbeck argued that for new super apps to overtake existing mobile payment options, they will need to offer something new or more convenient. “If I were interacting with people on X or in WhatsApp often enough to make payments there, I may be inclined to use their payment method a lot, and then to use it in other contexts as well,” she told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“The real question is if there is enough demand for either app,” she continued. “Meta tried to launch WhatsApp payments in India without much success. They did face some regulatory hurdles, but once those were removed, they did not make much headway in gaining market share over established systems like Google Pay.”&lt;/p&gt;
&lt;p&gt;“I think whether Meta or X can create real demand for their payment system, given the current state of mobile payments, is the real question,” she said.&lt;/p&gt;
&lt;p&gt;There might be a demand for a WhatsApp super app in developing markets where bandwidth and app storage are more limited, but in Western markets, resistance is real, added Chris Sorensen, CEO of &lt;a href=&#34;https://www.phoneburner.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PhoneBurner&lt;/a&gt;, a power dialer and CRM solutions company, in Laguna Beach, Calif. “People are much more privacy conscious and wary of giving one company too much control,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“It is also important to note that super apps require broad integrations and behavior shifts that certainly won’t happen overnight,” he said.&lt;/p&gt;
&lt;h2 id=&#34;metas-data-strategy-with-whatsapp&#34;&gt;Meta’s Data Strategy With WhatsApp&lt;/h2&gt;
&lt;p&gt;The consumer demand question is fascinating because it varies significantly by market maturity, noted &lt;a href=&#34;https://people.njit.edu/profile/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology, in Newark, N.J. “In emerging markets, super apps solve real infrastructure problems — fragmented payment systems, limited internet access, multiple service providers,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“In mature markets like the U.S., the value proposition is less clear,” he continued. “Western consumers already have specialized apps that work well. The resistance often comes down to trust and privacy concerns, which are amplified when you’re asking users to consolidate their digital lives into a single platform controlled by one company.”&lt;/p&gt;
&lt;p&gt;“From a technical standpoint, Meta is absolutely positioning WhatsApp to become a super app,” he added.&lt;/p&gt;
&lt;p&gt;“The integration of business services, AI-powered agents, and the gradual introduction of payment systems all point to a platform consolidation strategy. What’s particularly interesting from a data science perspective is how Meta is leveraging its AI capabilities — specifically the Llama models — to create contextual experiences within conversations. This isn’t just feature addition. It’s algorithmic orchestration of user needs.”&lt;/p&gt;
&lt;p&gt;“Meta’s motivation is fundamentally about data and control,” he said. “As a data scientist, I can tell you that fragmented user experiences create fragmented datasets. By consolidating interactions within WhatsApp, Meta gains unprecedented visibility into user behavior patterns across the entire customer journey — from discovery to purchase to support. This creates tremendous competitive advantages in AI development, targeted advertising, and predictive analytics.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technewsworld.com/story/meta-positioning-whatsapp-to-be-a-super-app-179818.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technewsworld.com/story/meta-positioning-whatsapp-to-be-a-super-app-179818.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MIT Study Finds ChatGPT Can Harm Critical Thinking Over Time</title>
      <link>http://localhost:1313/blog/20250624-technewsworld/</link>
      <pubDate>Tue, 24 Jun 2025 08:59:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250624-technewsworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250624-technewsworld/students-in-classroom_hu_728ce1c1912c2ced.webp 400w,
               /blog/20250624-technewsworld/students-in-classroom_hu_a15121b2c9ecb058.webp 760w,
               /blog/20250624-technewsworld/students-in-classroom_hu_b9a1014aa0c8e82b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250624-technewsworld/students-in-classroom_hu_728ce1c1912c2ced.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A recent &lt;a href=&#34;https://arxiv.org/pdf/2506.08872v1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;study&lt;/a&gt; by the Media Lab at MIT found that prolonged use of ChatGPT, a large language model (LLM) chatbot, can have a harmful impact on the cognitive abilities of its users.&lt;/p&gt;
&lt;p&gt;Over four months, LLM users consistently underperformed at neural, linguistic, and behavioral levels, noted the report, whose main author was research scientist &lt;a href=&#34;https://www.media.mit.edu/people/nkosmyna/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nataliya Kos’myna&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These results raise concerns about the long-term educational implications of LLM reliance and underscore the need for deeper inquiry into AI’s role in learning, it added.&lt;/p&gt;
&lt;p&gt;“What really motivated me to put it out now before waiting for a full peer review is that I am afraid in six to eight months, there will be some policymaker who decides, ‘let’s do GPT kindergarten,&amp;rsquo;” Kos’myna told Time magazine. “I think that would be absolutely bad and detrimental. Developing brains are at the highest risk.”&lt;/p&gt;
&lt;p&gt;For the research, 54 subjects, aged 18 to 39, were divided into three groups to write several SAT essays. One group could use ChatGPT; the second, Google search; and the third, no tools at all. An EEG was used to measure the participants’ brain activity across 32 regions of the brain. Of the three groups, the ChatGPT users had the lowest brain engagement.&lt;/p&gt;


















&lt;figure  id=&#34;figure-eeg-analysis-from-the-mit-study-reveals-lower-neural-connectivity-in-participants-who-use-chatgpt-compared-to-those-who-use-search-engines-or-no-tools-image-credit-kosmyna-et-al-mit-media-lab-used-under-cc-by-nc-sa-40httpscreativecommonsorglicensesby-nc-sa40&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;EEG analysis from the MIT study reveals lower neural connectivity in participants who use ChatGPT compared to those who use search engines or no tools. (Image Credit: Kos’myna et al., MIT Media Lab, used under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/))&#34; srcset=&#34;
               /blog/20250624-technewsworld/mit-chatgpt-eeg-study-alpha-band_hu_2df5fe20a9fd54d6.webp 400w,
               /blog/20250624-technewsworld/mit-chatgpt-eeg-study-alpha-band_hu_977c6db1f8616056.webp 760w,
               /blog/20250624-technewsworld/mit-chatgpt-eeg-study-alpha-band_hu_760eb9aeaf0c979.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250624-technewsworld/mit-chatgpt-eeg-study-alpha-band_hu_2df5fe20a9fd54d6.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      EEG analysis from the MIT study reveals lower neural connectivity in participants who use ChatGPT compared to those who use search engines or no tools. (Image Credit: Kos’myna et al., MIT Media Lab, used under &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC-SA 4.0&lt;/a&gt;)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The researchers also found that over the course of several months, ChatGPT users became increasingly less diligent in their work, often just cutting and pasting whatever the chatbot fed them.&lt;/p&gt;
&lt;h2 id=&#34;study-raises-red-flag-on-ai-use&#34;&gt;Study Raises Red Flag on AI Use&lt;/h2&gt;
&lt;p&gt;“I’m super excited that they did this,” said Karen Kovacs North, a clinical professor of communication at the &lt;a href=&#34;https://annenberg.usc.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annenberg&lt;/a&gt; School for Communication and Journalism at the University of Southern California.&lt;/p&gt;
&lt;p&gt;“People are very excited about AI and about the potential of AI,” she told TechNewsWorld. “But the question that some people have always had is, what does it do to critical thinking if people rely on it for problem solving or to think through issues?”&lt;/p&gt;
&lt;p&gt;“MIT decided to take the bull by the horns and actually look at the critical thinking issue and found what a lot of people fear, which is that relying on AI might interfere with the development of critical thinking,” she continued. “They rightfully point out, this should be a cautionary tale.”&lt;/p&gt;
&lt;p&gt;While acknowledging that the study raises a red flag, Mark N. Vena, president and principal analyst at &lt;a href=&#34;https://www.smarttechresearch.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SmartTech Research&lt;/a&gt; in Las Vegas, points out that it relies on a small sample — 54 participants, with only 18 in the follow-up. “Its early release underscores urgency — but peer review is pending,” he told TechNewsWorld. “It highlights potential weakening in memory and critical thinking with heavy AI use, yet stops short of proving long-term harm.”&lt;/p&gt;
&lt;p&gt;“The study’s reveal is provocative,” he added, “but limited in scope and duration. It’s more of an early warning than a conclusive finding. Further research, especially long-term studies across age groups and use cases, is critical before drawing sweeping conclusions about AI’s impact on cognition.”&lt;/p&gt;
&lt;p&gt;One such use case might be the legal profession. “The concern around cognitive atrophy applies to all AI users — but the legal field offers a particularly sharp example,” said Boaz Ashkenazy, co-founder and CEO of &lt;a href=&#34;https://augmentedailabs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Augmented AI Labs&lt;/a&gt;, a builder and manager of generative AI business solutions, with offices in Seattle and New York City.&lt;/p&gt;
&lt;p&gt;“When young lawyers rely on AI to generate arguments or summarize case law without first developing their own reasoning, they risk bypassing the deep, foundational learning that comes from doing the work manually,” he told TechNewsWorld. “The same pattern can show up in other professions, where over-reliance on AI risks dulling human judgment, problem-solving, and strategic thinking.”&lt;/p&gt;
&lt;h2 id=&#34;confirming-suspicions&#34;&gt;Confirming Suspicions&lt;/h2&gt;
&lt;p&gt;“As a data scientist who has spent decades analyzing complex systems and their emergent behaviors, I find these MIT findings particularly significant because they represent the first neurological evidence of what many of us in the field have suspected — that over-reliance on AI systems may fundamentally alter human cognitive processes,” observed &lt;a href=&#34;https://people.njit.edu/profile/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology, in Newark, N.J.&lt;/p&gt;
&lt;p&gt;While conceding that the study is preliminary and limited in scope, he pointed out that the EEG data showing reduced neural connectivity in ChatGPT users aligns with computational theories about cognitive load distribution.&lt;/p&gt;
&lt;p&gt;“What concerns me most is the progressive nature of the decline over just a few sessions, suggesting that cognitive offloading to AI may create a feedback loop where users become increasingly dependent on external processing power at the expense of developing their own analytical capabilities,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“I’ve observed this myself,” confessed Rob Enderle, president and principal analyst with the &lt;a href=&#34;https://www.enderlegroup.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Enderle Group&lt;/a&gt;, an advisory services firm, in Bend, Ore. “The more you depend on AI, the less you do yourself, leading to skill degradation, if you don’t work to improve your prompting skills instead.”&lt;/p&gt;
&lt;p&gt;“So this means we need to be very careful with AI or we could significantly degrade our own unique capabilities,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;“This study offers empirical evidence of what we already knew,” added Ryan Trattner, CTO and co-founder of &lt;a href=&#34;https://www.studyfetch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StudyFetch&lt;/a&gt;, an AI educational platform in Los Angeles.&lt;/p&gt;
&lt;p&gt;“ChatGPT is not a learning tool,” he told TechNewsWorld. “Quite the opposite. It is a hindrance to true learning, and unrestricted use is killing critical thinking skills and students’ creativity.”&lt;/p&gt;
&lt;p&gt;“ChatGPT is the best — and worst — group project partner in history,” he declared. “It does all the work and never complains, but the group member who never shows up for meetings also doesn’t learn anything. Students are using ChatGPT as an AI co-worker to do all of the work and have no ownership of the result.”&lt;/p&gt;
&lt;h2 id=&#34;pattern-of-cognitive-decline&#34;&gt;Pattern of Cognitive Decline&lt;/h2&gt;
&lt;p&gt;John Bambenek, president of &lt;a href=&#34;https://www.bambenekconsulting.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bambenek Consulting&lt;/a&gt;, a cybersecurity and threat intelligence consulting firm, based in Schaumburg, Ill., and author of “Lies, Damn Lies, and AI,” noted that the MIT research reveals a similar pattern to other information innovations.&lt;/p&gt;
&lt;p&gt;For instance, social media has caused its own cognitive difficulties. “The invention of the internet and blogging also led to warnings ‘don’t believe everything you read on the internet,” he told TechNewsWorld. “ChatGPT, in particular, superficially allows people to query a huge body of knowledge and get credible-looking answers.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pedroespinoza.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pedro David Espinoza&lt;/a&gt;, a TED speaker, entrepreneur, AI investor, and author, agreed that some technology developments have contributed to general cognitive decline. “AI is accelerating this cognitive decline on steroids. Exponentially. Sadly,” he told TechNewsWorld.&lt;/p&gt;
&lt;p&gt;Bambenek added that GenAI can be most harmful in education because it has always been a challenge to get students to think critically and understand concepts, rather than merely reciting information.&lt;/p&gt;
&lt;p&gt;“The real byproduct of a student writing a paper is the growth of understanding in the subject that the exercise produces,” he said. “Getting GenAI to write the paper for you — which is increasingly getting harder to detect — means the true product of understanding isn’t achieved, only the superficial product, a paper.”&lt;/p&gt;
&lt;p&gt;“The biggest problem,” he continued, “is that absent requiring test taking to occur in strict proctored environments or going back to oral exams, there is little to nothing that can be done to stop cheating.”&lt;/p&gt;
&lt;p&gt;“There’s no question that AI is going to be used more and more for mind-numbing tasks like rote essays,” added &lt;a href=&#34;https://camd.northeastern.edu/people/dan-kennedy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Kennedy&lt;/a&gt;, a professor of journalism at Northeastern University in Boston. “I’m more worried about students and others who will use it for higher-level creative tasks, producing work that is dull and unimpressive but that appears to fulfill the requirements of the assignment.”&lt;/p&gt;
&lt;p&gt;“You can’t learn to write except by writing,” he told TechNewsWorld. “To the extent that AI takes away from that, then yes, we ought to be very worried about its use in education and everywhere else.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technewsworld.com/story/mit-study-finds-chatgpt-can-harm-critical-thinking-over-time-179801.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technewsworld.com/story/mit-study-finds-chatgpt-can-harm-critical-thinking-over-time-179801.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>In The Colleges</title>
      <link>http://localhost:1313/blog/20250615-taubetapi/</link>
      <pubDate>Sun, 15 Jun 2025 15:26:54 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250615-taubetapi/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250615-taubetapi/20250615-TheBent_hu_f568f61e1e48d3a0.webp 400w,
               /blog/20250615-taubetapi/20250615-TheBent_hu_8bd6258506db01ff.webp 760w,
               /blog/20250615-taubetapi/20250615-TheBent_hu_3ccff0428a3b7866.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250615-taubetapi/20250615-TheBent_hu_f568f61e1e48d3a0.webp&#34;
               width=&#34;760&#34;
               height=&#34;648&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Ph.D.&lt;br&gt;
&lt;em&gt;Pennsylvania Alpha &amp;lsquo;90&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;David was inducted into the Mimms Museum of Technology &amp;amp; Art Hall of Fame and received the Heatherington Award for Technological Innovation. A distinguished professor at NJIT, he revolutionized computing by designing the first commodity-based supercomputer, which established Linux-based systems and his work led to creating the first Linux supercomputer for open scientific use.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM’s New Quantum Roadmap Brings the Bitcoin Threat Closer</title>
      <link>http://localhost:1313/blog/20250614-decrypt/</link>
      <pubDate>Sat, 14 Jun 2025 10:59:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250614-decrypt/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://decrypt.co/author/jason&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jason Nelson&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250614-decrypt/Bitcoin-decrypt-style-09-gID_7_hu_8d44eb21f79c8cb6.webp 400w,
               /blog/20250614-decrypt/Bitcoin-decrypt-style-09-gID_7_hu_2c747a2e3c295e1c.webp 760w,
               /blog/20250614-decrypt/Bitcoin-decrypt-style-09-gID_7_hu_fb9adb7050696377.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250614-decrypt/Bitcoin-decrypt-style-09-gID_7_hu_8d44eb21f79c8cb6.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Quantum computers weren’t expected to pose a threat to &lt;a href=&#34;https://decrypt.co/resources/what-is-bitcoin-four-minute-instant-guide-explainer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bitcoin&lt;/a&gt;’s security anytime soon. But IBM has launched a &lt;a href=&#34;https://www.ibm.com/quantum/blog/large-scale-ftqc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt; that could expedite the timeline: the world’s first fault-tolerant quantum computer, set to debut by 2029.&lt;/p&gt;
&lt;p&gt;Despite their ability to calculate in multiple directions simultaneously, current-generation quantum computers have high &lt;a href=&#34;https://www.riverlane.com/quantum-error-correction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;error rates&lt;/a&gt;. Without fault tolerance, and the ability to detect and correct errors as they happen, quantum computers can’t run complex algorithms that would be needed to crack &lt;a href=&#34;https://decrypt.co/resources/blockchain-basics-what-is-blockchain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blockchains&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The system, named IBM &lt;a href=&#34;https://newsroom.ibm.com/2025-06-10-IBM-Sets-the-Course-to-Build-Worlds-First-Large-Scale,-Fault-Tolerant-Quantum-Computer-at-New-IBM-Quantum-Data-Center&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Starling&lt;/a&gt;, is being designed to execute 100 million quantum operations using 200 error-corrected qubits. It will be housed at IBM’s quantum data center in Poughkeepsie, New York, and is part of the company’s ongoing roadmap for scalable &lt;a href=&#34;https://decrypt.co/resources/from-the-quantum-realm-to-reality-a-beginner-guide-to-the-computer-of-the-future&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantum computing&lt;/a&gt;, which extends through 2033.&lt;/p&gt;
&lt;p&gt;“Recent revisions to that roadmap project a path to 2033 and beyond, and so far, we have successfully delivered on each of our milestones,” IBM said in a statement. “Based on that past success, we feel confident in our continued progress.”&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/zrZHPil0BTA?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;IBM’s approach to fault tolerance centers on error correction. Quantum systems are highly sensitive to noise and &lt;a href=&#34;https://research.ibm.com/publications/decoherence-from-classically-undetectable-sources-standard-quantum-limit-for-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;decoherence&lt;/a&gt;, environmental disturbances that can disrupt &lt;a href=&#34;https://www.ibm.com/think/topics/qubit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;qubits&lt;/a&gt; almost immediately. The company’s solution uses &lt;a href=&#34;https://www.ibm.com/quantum/blog/nature-qldpc-error-correction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bivariate Bicycle&lt;/a&gt; codes, a type of quantum low-density parity-check (LDPC) code that it claims reduces the number of physical qubits needed by up to 90% compared to earlier methods.&lt;/p&gt;
&lt;p&gt;Starling will also feature a real-time error correction decoder capable of running on field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs), enabling immediate response to errors before they escalate.IBM’s approach to fault tolerance centers on error correction. Quantum systems are highly sensitive to noise and decoherence, environmental disturbances that can disrupt qubits almost immediately. The company’s solution uses Bivariate Bicycle codes, a type of quantum low-density parity-check (LDPC) code that it claims reduces the number of physical qubits needed by up to 90% compared to earlier methods.&lt;/p&gt;
&lt;p&gt;Starling will also feature a real-time error correction decoder capable of running on field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs), enabling immediate response to errors before they escalate.&lt;/p&gt;
&lt;p&gt;“A huge effort is devoted to quantum error correction and mitigation, and the new processor’s connectivity is especially promising for implementing quantum error-correcting codes more efficiently,” the technical director of the IBM Quantum Innovation Center at USC, Rosa Di Felice, told &lt;em&gt;Decrypt&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;“This new processor could help simplify the complex calculations needed to understand how molecules and materials behave,” Di Felice said. “That could lead to breakthroughs in areas like preventing rust, improving chemical reactions, and designing new medicines.”&lt;/p&gt;
&lt;p&gt;To understand how IBM plans to achieve its goal, here’s a look at the company’s updated quantum computing roadmap.&lt;/p&gt;
&lt;h2 id=&#34;the-starling-roadmap&#34;&gt;The Starling roadmap&lt;/h2&gt;
&lt;h3 id=&#34;2025&#34;&gt;2025&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Launch of the 120-qubit IBM Nighthawk processor with 16x greater circuit depth capability.&lt;/li&gt;
&lt;li&gt;Qiskit software enhancements include dynamic circuits and integration with high-performance computing (HPC) environments.&lt;/li&gt;
&lt;li&gt;Introduction of modular fault-tolerant quantum computing architecture.&lt;/li&gt;
&lt;li&gt;IBM Quantum Loon is designed to test architecture components for the qLDPC code, including &amp;ldquo;C-couplers&amp;rdquo; that connect qubits over longer distances within the same chip.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2026&#34;&gt;2026&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IBM targets the first quantum advantage demonstrations.&lt;/li&gt;
&lt;li&gt;Expansion of error mitigation and utility mapping tools to support complex quantum workloads ahead of full fault tolerance.&lt;/li&gt;
&lt;li&gt;IBM Quantum Kookaburra, expected to be released in 2026, will be IBM&amp;rsquo;s first modular processor designed to store and process encoded information. It will combine quantum memory with logic operations—the basic building block for scaling fault-tolerant systems beyond a single chip.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2027&#34;&gt;2027&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Scaling to 1,080 qubits through chip-to-chip couplers.&lt;/li&gt;
&lt;li&gt;IBM Quantum Cockatoo, expected in 2027, will entangle two Kookaburra modules using &amp;ldquo;L-couplers.&amp;rdquo; This architecture will link quantum chips together, much like nodes in a larger system, thereby avoiding the need to build impractically large chips.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;20282029&#34;&gt;2028–2029&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Prototype of a fault-tolerant quantum computer (Starling) expected by 2028, with full deployment targeted for 2029.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-it-matters&#34;&gt;Why it matters&lt;/h2&gt;
&lt;p&gt;Earlier this week, Strategy co-founder Michael Saylor &lt;a href=&#34;https://www.ibm.com/quantum/blog/nature-qldpc-error-correction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;downplayed&lt;/a&gt; the threat of quantum computers, calling them a bigger risk to banks and governments than to Bitcoin.&lt;/p&gt;
&lt;p&gt;“They will hack your banking system, your Google account, your Microsoft account, and every other asset you have much sooner, because they&amp;rsquo;re an order of magnitude weaker,&amp;quot; he said at the time.&lt;/p&gt;
&lt;p&gt;Experts, such as &lt;strong&gt;Professor David Bader&lt;/strong&gt; of the New Jersey Institute of Technology, view fault tolerance as the linchpin of practical quantum computing—and potentially a threat to current cryptographic systems.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Fault tolerance is really about making these quantum computers less fragile and less error-prone,” he said. “That is a key technology needed to scale up from beyond a handful of qubits to what we think we&amp;rsquo;ll need for real applications, which may be on the order of tens of thousands to millions of qubits.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader acknowledged the fear that one of these applications could compromise cryptographic algorithms that secure cryptocurrencies like Bitcoin, and emphasized the importance of blockchain developers moving toward quantum-resistant encryption.&lt;/p&gt;
&lt;p&gt;“A powerful quantum computer capable of running Shor&amp;rsquo;s algorithm is still years away,” he said. “Blockchains won’t suddenly break in 2029—but it’s worth watching.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://decrypt.co/325183/ibm-quantum-roadmap-brings-blockchain-threat-closer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://decrypt.co/325183/ibm-quantum-roadmap-brings-blockchain-threat-closer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cryptonews.net/news/bitcoin/31104248/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cryptonews.net/news/bitcoin/31104248/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ainvest.com/news/ibm-unveils-2029-quantum-roadmap-threatening-bitcoin-security-2506/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ainvest.com/news/ibm-unveils-2029-quantum-roadmap-threatening-bitcoin-security-2506/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ainvest.com/news/ibm-starling-quantum-computer-reduce-error-rates-90-2029-2506/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ainvest.com/news/ibm-starling-quantum-computer-reduce-error-rates-90-2029-2506/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://followin.io/en/feed/18343502&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://followin.io/en/feed/18343502&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eblockmedia.com/news/articleView.html?idxno=21706&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eblockmedia.com/news/articleView.html?idxno=21706&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://finway.com.ua/en/ibm-introduce-quantum-computer-quantum/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://finway.com.ua/en/ibm-introduce-quantum-computer-quantum/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 password habits that put you at risk</title>
      <link>http://localhost:1313/blog/20250613-theweek/</link>
      <pubDate>Fri, 13 Jun 2025 10:49:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250613-theweek/</guid>
      <description>

















&lt;figure  id=&#34;figure-more-than-half-of-people-admitted-that-they-use-familiar-names-in-their-passwords-image-credit-userba011d64_201--getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&amp;#39;More than half of people admitted that they use familiar names in their passwords&amp;#39; (Image credit: Userba011d64_201 / Getty Images)&#34; srcset=&#34;
               /blog/20250613-theweek/duuHtzkuJap9bdt73eRbP8-1024-80_hu_478cbd6fd58ee1c2.webp 400w,
               /blog/20250613-theweek/duuHtzkuJap9bdt73eRbP8-1024-80_hu_1626a5be19e92ba5.webp 760w,
               /blog/20250613-theweek/duuHtzkuJap9bdt73eRbP8-1024-80_hu_86559bc4e41dc35c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250613-theweek/duuHtzkuJap9bdt73eRbP8-1024-80_hu_478cbd6fd58ee1c2.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;lsquo;More than half of people admitted that they use familiar names in their passwords&amp;rsquo; (Image credit: Userba011d64_201 / Getty Images)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By David Faris&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Anyone who has gritted their way through a mandatory employee cybersecurity training understands that hackers are gunning for our passwords, which have become the Holy Grails of dark web schemers. A compromised password can give criminals access to everything from your credit card number to your Social Security information, and the fallout can be an enormous hassle. Yet most people are too busy to spend much time thinking about password management or are operating on well-intentioned but extremely dated advice. What can individuals do to stay ahead of the next phishing operation?&lt;/p&gt;
&lt;h2 id=&#34;dont-use-iterations-of-an-existing-password&#34;&gt;Don&amp;rsquo;t use iterations of an existing password&lt;/h2&gt;
&lt;p&gt;While tempting, using &amp;ldquo;variations of old passwords&amp;rdquo; is a strategy that &amp;ldquo;might offer convenience but can also make passwords predictable to attackers,&amp;rdquo; said &lt;a href=&#34;https://www.forbes.com/advisor/business/software/american-password-habits/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Forbes&lt;/a&gt;. The temptation to do this is much higher if your organization compels you to regularly update your password because &amp;ldquo;when forced to change one, the chances are that the new password will be similar to the old one,&amp;rdquo; said the U.K.&amp;rsquo;s &lt;a href=&#34;https://www.ncsc.gov.uk/blog-post/problems-forcing-regular-password-expiry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Cyber Security Centre&lt;/a&gt;. That&amp;rsquo;s why forced password expiration is a &amp;ldquo;dying concept,&amp;rdquo; said the &lt;a href=&#34;https://www.sans.org/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SANS Institute&lt;/a&gt;. Nevertheless, if you must update, and your password is &lt;a href=&#34;https://theweek.com/cartoons/taco-donald-trump-birthday-parade-editorial-cartoons&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TacoTuesday2025*&lt;/a&gt;, it would be best not to change it to TacoTuesday2026* next year.&lt;/p&gt;
&lt;h2 id=&#34;dont-use-the-same-password-across-multiple-accounts&#34;&gt;Don&amp;rsquo;t use the same password across multiple accounts&lt;/h2&gt;
&lt;p&gt;So many bad password habits arise from the difficulty of managing so many accounts, and 78% of respondents in a &lt;a href=&#34;https://www.forbes.com/advisor/business/software/american-password-habits/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2024 survey&lt;/a&gt; admitted to recycling passwords across multiple accounts or domains. If you use the same password across a number of domains, you are leaving yourself open to coordinated attacks. Having obtained your skeleton password, hackers will &amp;ldquo;launch credential-stuffing attacks&amp;rdquo; by &amp;ldquo;using those logins to access other accounts,&amp;rdquo; said &lt;a href=&#34;https://go.redirectingat.com/?id=92X1679923&amp;amp;xcust=theweekus_us_1330499560868812462&amp;amp;xs=1&amp;amp;url=https%3A%2F%2Fwww.dashlane.com%2Fblog%2F4-habits-organization-risk&amp;amp;sref=https%3A%2F%2Ftheweek.com%2Fculture-life%2Fpersonal-technology%2Fpassword-habits-to-avoid-hackers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dashlane&lt;/a&gt;. And because &amp;ldquo;most online accounts assign your email address as a username, it doesn&amp;rsquo;t take Mr. Robot to crack that code,&amp;rdquo; said &lt;a href=&#34;https://www.pcmag.com/news/stop-using-the-same-password-on-multiple-sites-no-really&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PC Mag&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;dont-use-personal-details-in-your-passwords&#34;&gt;Don&amp;rsquo;t use personal details in your passwords&lt;/h2&gt;
&lt;p&gt;Another extremely common practice that experts caution against is using &amp;ldquo;your personal details such as your birthday, hometown or pet&amp;rsquo;s name,&amp;rdquo; said the &lt;a href=&#34;https://www.cyber.gc.ca/en/guidance/best-practices-passphrases-and-passwords-itsap30032&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Canadian Centre for Cyber Security&lt;/a&gt;. While tying your passwords to easily accessible life experiences, milestones and individual data obviously makes it easier for you to remember, the problem is that using such details increases your risk because they &amp;ldquo;can be found by a quick search on social networking sites,&amp;rdquo; said the &lt;a href=&#34;https://www.dhs.gov/archive/news/2013/05/08/protecting-your-personal-information-secure-passwords&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Homeland Security&lt;/a&gt;. &amp;ldquo;More than half of people admitted that they use familiar names in their passwords,&amp;rdquo; including a child&amp;rsquo;s name, a street name or a parent&amp;rsquo;s name, said &lt;a href=&#34;https://www.security.org/resources/online-password-strategies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Security.org&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;dont-give-your-passwords-to-other-people&#34;&gt;Don&amp;rsquo;t give your passwords to other people&lt;/h2&gt;
&lt;p&gt;It may seem like a good deed, a way to save money and an act of protest against the proliferation of streaming services to give your Netflix information to a friend in exchange for their &lt;a href=&#34;https://theweek.com/feature/briefing/1022621/what-the-heck-is-max-your-questions-about-the-new-ish-streaming-service&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max&lt;/a&gt; login, but sharing passwords is a major security risk. Because &amp;ldquo;nearly a third of respondents reported using the same password for all their streaming accounts,&amp;rdquo; this risks &amp;ldquo;moochers sharing passwords with other moochers without the account holder&amp;rsquo;s knowledge or consent,&amp;rdquo; said &lt;a href=&#34;https://www.pcmag.com/news/sharing-your-streaming-passwords-is-more-dangerous-than-you-think&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PC Mag&lt;/a&gt;. This also provides another way for hackers and phishers to gain access to your passwords and your vital information. This even includes password sharing with a spouse or domestic partner. &amp;ldquo;Your own security might be excellent,&amp;rdquo; said Wired, &amp;ldquo;but if you&amp;rsquo;ve shared your credentials, you&amp;rsquo;re at the mercy of the weakest link.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;dont-use-short-or-simple-passwords&#34;&gt;Don&amp;rsquo;t use short or simple passwords&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;Something simple, short and predictable&amp;rdquo; is a &amp;ldquo;terrible password,&amp;rdquo; said &lt;a href=&#34;https://www.weforum.org/stories/2024/07/popular-passwords-cybercrime-digital-safety/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The World Economic Forum&lt;/a&gt;. For example, the password &amp;ldquo;123456&amp;rdquo; has been &amp;ldquo;used over 4.5 million times&amp;rdquo; and &amp;ldquo;takes less than a second for hackers to crack.&amp;rdquo; That&amp;rsquo;s an example of how a &amp;ldquo;simple or short password such as a word or name, a sequence of numbers, or combination of these, can be easily guessed by malicious attackers,&amp;rdquo; said cybersecurity expert &lt;a href=&#34;https://davidbader.net/post/20230303-cbs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;. Unsurprisingly, &amp;ldquo;as character length increases, the total amount of compromised passwords decreases,&amp;rdquo; said Specops Software. Yet only 20% of respondents in a &lt;a href=&#34;https://www.statista.com/statistics/1305713/average-character-length-of-a-password-us/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021 survey&lt;/a&gt; reported using passwords longer than 12 characters.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://theweek.com/culture-life/personal-technology/password-habits-to-avoid-hackers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://theweek.com/culture-life/personal-technology/password-habits-to-avoid-hackers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eight YWCC Research Projects Funded Through AI@NJIT Initiative</title>
      <link>http://localhost:1313/blog/20250527-njit/</link>
      <pubDate>Tue, 27 May 2025 10:21:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250527-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250527-njit/Gemini_Generated_Image_9ytx2y9ytx2y9ytx_hu_a563a402cc0e8af4.webp 400w,
               /blog/20250527-njit/Gemini_Generated_Image_9ytx2y9ytx2y9ytx_hu_877adb433d6dfee8.webp 760w,
               /blog/20250527-njit/Gemini_Generated_Image_9ytx2y9ytx2y9ytx_hu_add58a54f27aa488.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250527-njit/Gemini_Generated_Image_9ytx2y9ytx2y9ytx_hu_a563a402cc0e8af4.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Seven faculty members in the Ying Wu College of Computing (YWCC) have received grant support to fund eight research projects as part of the Grace Hopper Artificial Intelligence Institute at NJIT (GHRI). The Institute was launched through a $6 million investment by an anonymous donor and matching funds in collaboration with the university’s &lt;a href=&#34;https://news.njit.edu/njit-devotes-over-10-million-new-funds-push-artificial-intelligence?utm_source=chatgpt.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$10 million AI@NJIT initiative&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Funded proposals focus on any aspect of AI, including but not limited to machine learning, natural language processing, robotics, AI ethics, and applications of AI across various disciplines.&lt;/p&gt;
&lt;p&gt;“The Grace Hopper AI Research Institute exemplifies our dedication to innovation and interdisciplinary collaboration, positioning our university as a leader in AI research and application,” said John Pelesko, provost and senior executive vice president, in a &lt;a href=&#34;https://news.njit.edu/njits-grace-hopper-ai-research-institute-launches-1-million-research-proposals&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;February article announcing the initial launch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;YWCC Dean Jamie Payton added, &amp;ldquo;The seven faculty members chosen to represent GHRI, the college and the university are leaders in their respective fields, pioneering AI research and discovery with wide-ranging impact—including in safety and security, health care, and even space weather, which has effects on Earth&amp;rsquo;s atmosphere and our communication infrastructure.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Associate Professor Hai Phan (Data Science)&lt;/strong&gt; has received AI@NJIT funding for two research proposals:&lt;/p&gt;
&lt;p&gt;&lt;u&gt;FunSec: Certified Functional and Secure Code Generation with Large Language Models:&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The project aims to develop a novel AI assistant for software developers that generates &lt;strong&gt;Fun&lt;/strong&gt;ctional &amp;amp; &lt;strong&gt;Sec&lt;/strong&gt;ure code snippets together with a rigorous probabilistic guarantee with a confidence interval a for the functionality and security of the generated small snippets. The research will mitigate significant security vulnerabilities in CLLM-generated (Code LLMs) code, which pose severe risks, such as data breaches, unauthorized access to sensitive information, identity theft, financial loss, malware infection, disruption of operations, etc., when deployed in real-world software systems.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Automating Chip Design with AI: The Journey from Zero to Silicon:&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;This project aims to develop the first comprehensive LLM-based ecosystem, enabling an effective pipeline for automating AI accelerator generation and verification, ensuring a seamless transition from high-level specifications to low-level implementations considering budgetary restraints (e.g., power, latency, area). This research is well-aligned with the CHIPS Act to strengthen domestic semiconductor manufacturing, design and research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinguished Professor David Bader (Data Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;AI-Enhanced Graph Pattern Matching and Cluster Analysis for Neural Circuit Discovery:&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;This project advances fundamental neuroscience research to understand the complexity of brain networks and their functional organization. The research will develop novel AI-enhanced algorithms and systems that can recognize both functionally similar neural patterns and significant network clusters through multimetric analysis, directly supporting neuroscientists in their quest to understand the brain’s connectome. The innovation utilizes the research team’s established expertise in parallel graph algorithms and large-scale dating processing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Professor Chengjun Liu (Computer Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Toward Increased Rosacea Awareness Among Population Using Advanced AI: Explainable AI Automatic Rosacea Diagnosis and Region of Interest Detection:&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The project utilizes deep learning and explainable statistical approaches to increase rosacea awareness, which afflicts approximately 16 million Americans according to the National Rosacea Society. The results will better assist physician diagnosis on the disease using interpretable automatic detection methods, with three-fold contributions. The first can automatically distinguish patients suffering from rosacea from people who are clean of this disease with a significantly higher accuracy than other methods in unseen test data, including both classical deep learning and statistical methods. The second addresses the interpretability issue by measuring the similarity between the test sample and the means of two classes, namely the rosacea class versus the normal class, which allows medical professionals and patients to understand and trust the results. The final contribution will not only help increase awareness of rosacea in the general population but will also remind patients who suffer from the disease of possible early treatments, as rosacea is more treatable in its early stages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant Professor Akshay Rangamani (Data Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Does it Add Up? Characterizing Mathematical Skills in Language Models&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;This project aims to characterize the parameter geometry in language models trained on mathematical problems. This characterization will help the research team understand a) when language models go beyond pattern matching to learn generalizable representations, b) how language models learn hierarchical skills, and c) how model capabilities grow with model size. While the project will focus on the controlled domain of arithmetic and algebra, the team will aim to generalize their findings to more general language models. Beyond the intellectual merit, this project will also train undergraduate students to perform foundational AI research, use the latest AI models, and allow them to interact with the broader research community through conferences and workshops.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Professor Jason Wang (Computer Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Prediction of Extreme Events in Space Weather Using Generative Artificial Intelligence and Multimodal Machine Learning Techniques:&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;Research will entail development of an AI toolbox for multimodal space weather (SWx) forecasting. The toolbox will integrate data spanning three solar cycles, enabling a comprehensive analysis, interpretation and prediction of solar transient events and their precursors. A key innovation of the project is the creation of novel datasets using generative AI techniques. The researchers envision that these enhanced datasets will provide new insights into the dynamics of solar active regions and their impact on space weather. The results will significantly enhance the ability to forecast extreme SWx events, including solar flares, coronal mass ejections and solar energetic particle events, and mitigate their effects on technological systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant Professor Lijing Wang (Data Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Large Language Model-Driven AI Platform for Next-Generation Surgical Planning and Navigation&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The project will develop a next-generation AI-based platform for automated surgical planning and navigation through incorporation of a large language model with state-of-the-art machine learning algorithms in image processing. It will further prepare surgery-critical information in 3D augmented reality (AR) space for intuitive visualization. The platform has the potential to help neurosurgeons protect functional eloquent regions of the brain, control surgical risk and improve patient outcomes. Expected results will fully automate the surgical preparation process and release the power of surgical teams. The success of this project will improve the value proposition for next-level surgical innovation and result in patentable technologies for NJIT.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant Professor Mengjia Xu (Data Science)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Predicting Solar Active Regions with the Aid of Artificial Intelligence&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;The proposed project will leverage advanced sequence models to bridge significant knowledge gaps in solar active regions (AR) prediction and evolution, offering a transformative approach to forecasting space weather while addressing the need for extended prediction windows to meet the operational requirements of modern space missions. The research has two objectives: 1) Train two different deep learning models on extended solar AR emergence datasets. 2) Develop an open-source machine learning dataset visualization tool for AR emergent precursors. The expected outcomes will aid with the safety and success of NASA’s space-based operations by mitigating solar flares, coronal mass ejections and solar energetic particles. Such events can potentially downgrade radio communications, incapacitate satellites, expose airline passengers to elevated radiation levels and endanger life in outer space.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/eight-ywcc-research-projects-funded-through-ainjit-initiative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/eight-ywcc-research-projects-funded-through-ainjit-initiative&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Roswell, City Lifestyle: BYTE25</title>
      <link>http://localhost:1313/blog/20250501-roswell/</link>
      <pubDate>Sat, 24 May 2025 10:00:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250501-roswell/</guid>
      <description>

















&lt;figure  id=&#34;figure-rena-youngblood-presented-dr-david-bader-with-the-hetherington-award-for-technological-innovation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Rena Youngblood presented Dr. David Bader with the Hetherington Award for Technological Innovation&#34; srcset=&#34;
               /blog/20250501-roswell/1748055859896_hu_545c753d9c8df74b.webp 400w,
               /blog/20250501-roswell/1748055859896_hu_8dca128e9960ac6b.webp 760w,
               /blog/20250501-roswell/1748055859896_hu_cec246bb5b61db5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250501-roswell/1748055859896_hu_545c753d9c8df74b.webp&#34;
               width=&#34;760&#34;
               height=&#34;485&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Rena Youngblood presented Dr. David Bader with the Hetherington Award for Technological Innovation
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;a-night-of-innovation-recognition-and-transformation&#34;&gt;A Night of Innovation, Recognition, and Transformation&lt;/h2&gt;
&lt;p&gt;The Computer Museum of America has entered a bold new era, officially rebranding as the Mimms Museum of Technology and Art. To announce this came with BYTE25, the museum&amp;rsquo;s signature fundraiser, which celebrated innovation and legacy, with a Hall of Fame induction, a new exhibit reveal, and a night of technology-driven experiences.&lt;/p&gt;
&lt;p&gt;BYTE25 recognized three trailblazers whose contributions shaped the digital world. &lt;strong&gt;Dr. David Bader&lt;/strong&gt; revolutionized high-performance computing with his work on Linux-based supercomputers. Dan Bricklin, hailed as the &amp;ldquo;Father of the Spreadsheet,&amp;rdquo; changed business forever with VisiCalc. And John Yates, a leader in technology law, helped build Atlanta&amp;rsquo;s thriving tech ecosystem. Each inductee took the stage to reflect on their achievements and the future of innovation.&lt;/p&gt;
&lt;p&gt;With its rebranding as the Mimms Museum of Technology and Art, the Institution signaled its expansion beyond computing history to explore the intersection of technology and creativity. Founder Lonnie Mimms shared his vision for the museum&amp;rsquo;s next chapter, ensuring its role as a hub for education and inspiration.&lt;/p&gt;
&lt;p&gt;Guests at BYTE25 received an exclusive first look at groundbreaking artifacts and the unveiling of the stunning Salvador Dali exhibit, featuring signed prints by the legendary artist. The event also showcased a cutting-edge technology exhibit with interactive displays and rare artifacts, reinforcing the museum&amp;rsquo;s commitment to preserving history while embracing the future.&lt;/p&gt;
&lt;p&gt;Television personality, actor, and author Tom Clark brought energy and humor as the evening&amp;rsquo;s emcee and auctioneer. Attendees enjoyed &amp;ldquo;byte&amp;rdquo;-sized delicacies, signature cocktails, a spirited auction, live music by The Real Acoustic Soul, dancing, networking and a chance to mingle with tech industry leaders.&lt;/p&gt;
&lt;p&gt;BYTE25 wasn&amp;rsquo;t just a night of reflection—it was a launchpad for the future, proving that the story of technology is still being written, one byte at a time.&lt;/p&gt;
&lt;p&gt;For more information, visit: mimmsmuseum.org | Visit at: 5000 Commerce Parkway, Roswell&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Article by Tran Bui, Photography by Luis Contreras&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://citylifestyle.com/roswellga/issues/2025-05&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://citylifestyle.com/roswellga/issues/2025-05&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Strives to Make Gemini a ‘World Model’ at I/O Conference</title>
      <link>http://localhost:1313/blog/20250520-techstrong/</link>
      <pubDate>Tue, 20 May 2025 09:42:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250520-techstrong/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jon Swartz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250520-techstrong/google_hu_bffdaac1b7d5d3fa.webp 400w,
               /blog/20250520-techstrong/google_hu_3da97cbc28dec8dd.webp 760w,
               /blog/20250520-techstrong/google_hu_845318290aa54eb8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250520-techstrong/google_hu_bffdaac1b7d5d3fa.webp&#34;
               width=&#34;760&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;MOUNTAIN VIEW, Calif. — The theme was simple at &lt;a href=&#34;https://www.youtube.com/watch?v=o8NiE3XMPrM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google I/O 2025&lt;/a&gt; on Tuesday: AI, AI, and — in case you missed it — more AI.&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) is seeping into every pore of Google’s products. During a two-hour product marathon, Alphabet Inc.’s parent company opened a fire hose of news around Gemini and multimodal AI system &lt;a href=&#34;https://techcrunch.com/2024/12/12/google-wants-to-sell-those-project-astra-ar-glasses-some-day-but-it-wont-be-today/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Astra&lt;/a&gt;. Case in point: A new tab in Google search, called AI Mode, lets customers in the U.S. post longer queries (up to 10 at a time) and follow-up questions. A custom image generation model trained specifically for fashion offers visual try-ons through search.&lt;/p&gt;
&lt;p&gt;When the dust had settled at the outdoors Shoreline Amphitheatre, industry experts concluded Google got a AI makeover and Gemini had been made into an AI operating system.&lt;/p&gt;
&lt;p&gt;“I learned that today is the start of Gemini season,” Alphabet CEO Sundar Pichai joked during his keynote speech at Shoreline Amphitheater here Tuesday morning. “Not really sure what the big deal is. Every day is Gemini season here at Google.”&lt;/p&gt;
&lt;p&gt;Gemini Ultra (U.S. only), Google said, promises the “highest level of access” to Google’s AI-powered apps and services, for about $250 a month. It includes Google’s Veo 3 video-generating AI model (now available), a new Flow video editing app, and a powerful AI capability called Gemini 2.5 Pro Deep Think mode, which hasn’t launched.&lt;/p&gt;
&lt;p&gt;Gemini 2.5 Pro Deep Think, an enhanced reasoning mode, was teased. Though Google didn’t provide details, the thinking is that Deep Think could be similar, if not exceed, OpenAI’s o1-pro and upcoming o3-pro models, which likely use an engine to search for and synthesize the best solution to a problem.&lt;/p&gt;
&lt;p&gt;Google’s goal is to make Gemini a “world model” that DeepMind CEO Demis Hassabis said is capable of making “plans and imagine new experiences by simulating aspects of the world just like the brain does.”&lt;/p&gt;
&lt;p&gt;“A universal AI system will perform everyday tasks for us,” Hassabis said on stage. “It will take care of mundane admin and surface delightful new recommendations, making us more productive and enriching our lives.”&lt;/p&gt;
&lt;p&gt;Among the fusillade of news:&lt;/p&gt;
&lt;p&gt;— Google entered the glasses fray: Under its Android XR project, it said it is partnering with Xreal on its first spectacles to run an augmented reality version of its operating system. Project Astra, developed within Google DeepMind to showcase nearly real-time, multimodal AI capabilities, will be showcased in glasses via partners Samsung and Warby Parker. Google, however, did not share a launch date.&lt;/p&gt;
&lt;p&gt;— Google showed something called “Project Mariner” that adds AI agentic functionality to the Gemini app for things like searching for an apartment.&lt;/p&gt;
&lt;p&gt;— Google Beam, an AI-first video communication platform, uses multiple cameras to capture different angles, rendering 2-D video streams into 3-D visuals. Live speech translation in Google Meet allows English-to-Spanish (and vice versa) in a demo highlighting a vacation rental scenario.&lt;/p&gt;
&lt;p&gt;— Veo 3 and the forthcoming Imagen 4 AI image generator will be used to supercharge Flow, Google’s AI-powered video tool geared for filmmakers.&lt;/p&gt;
&lt;p&gt;— Google Cloud said enhancements to its Agent2Agent interoperability protocol allow agents across systems and platforms to communicate. The service is gaining adoption through partners such as Microsoft and PayPal, according to Google.&lt;/p&gt;
&lt;p&gt;— Google AI Studio can help users code just by inputting text directions via Jules, a new asynchronous coding agent that is in public beta.&lt;/p&gt;
&lt;p&gt;Google’s new asynchronous coding agent, &lt;a href=&#34;https://jules.google/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jules&lt;/a&gt;, set to compete with OpenAI’s Codex and Microsoft’s Copilot, could upend the entire tech space, says &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of the Institute for Data Science at New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;“With Google’s Jules now publicly available in beta, OpenAI’s recently launched Codex, and Microsoft’s enhanced GitHub Copilot, we’re witnessing a fundamental shift from passive code suggestion to autonomous task execution,” Bader said in an email. “These tools aren’t just completing code anymore — they’re planning multi-step solutions, spawning virtual environments, and managing entire development workflows with minimal supervision.”&lt;/p&gt;
&lt;p&gt;The two-day summit here arguably highlighted the biggest week in AI news this year, with announcements from Google, Microsoft Corp., Dell Inc., IBM Corp.’s Red Hat Inc. and SAP, among others.&lt;/p&gt;
&lt;p&gt;Obvious parallels are with Microsoft, which on Monday &lt;a href=&#34;https://techstrong.ai/features/microsoft-commits-to-building-open-agentic-ai-ecosystem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unleashed tools and platforms&lt;/a&gt; that collectively propel an open AI ecosystem, called &lt;a href=&#34;https://blogs.microsoft.com/blog/2025/05/19/microsoft-build-2025-the-age-of-ai-agents-and-building-the-open-agentic-web/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agentic Web&lt;/a&gt;, based on the model context protocol (MCP). Microsoft and Google simultaneously hosted their flagship developer conferences with a focus on courting developers to build AI apps/use cases on their platforms.&lt;/p&gt;
&lt;p&gt;“To this point Google is not just watching AI from a treadmill…but instead in our opinion has been one of the most innovative and helped close the gap on AI over the past year after starting way behind Microsoft and OpenAI,” Wedbush Securities analyst Dan Ives said in a note to investors Tuesday before Google I/O kicked off. “We expect Sundar to address the future of Google’s AI strategy in his keynote later today and unveil innovations on AI coming to the Google and Gemini platforms for developers.”&lt;/p&gt;
&lt;p&gt;Next up: Apple Inc. and its WWDC developers conference next month in Cupertino, Calif., where it will reportedly open its AI models to third-party apps for the first time, according to a Bloomberg report.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://techstrong.ai/agentic-ai/google-strives-to-make-gemini-a-world-model-at-i-o-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://techstrong.ai/agentic-ai/google-strives-to-make-gemini-a-world-model-at-i-o-conference/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rooted in Research, Ready to Lead: NJIT&#39;s Master&#39;s and Doctoral Degree Class of 2025</title>
      <link>http://localhost:1313/blog/20250519-njit/</link>
      <pubDate>Mon, 19 May 2025 14:40:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250519-njit/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250519-njit/20250519-PhDCeremony-1079-2_hu_15272c889a999422.webp 400w,
               /blog/20250519-njit/20250519-PhDCeremony-1079-2_hu_8c6bd3dcea9062e5.webp 760w,
               /blog/20250519-njit/20250519-PhDCeremony-1079-2_hu_1571e4916a255b18.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-njit/20250519-PhDCeremony-1079-2_hu_15272c889a999422.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey Institute of Technology served up a full day of fanfare as its advanced degree graduates walked across the stage in the Bloom Wellness and Events Center in three commencement ceremonies. The 2025 master’s and doctoral class exemplifies the institution’s commitment to research and scholarship across a diverse range of disciplines.&lt;/p&gt;
&lt;p&gt;Provost and Senior Vice President for Academic Affairs John Pelesko called the ceremonies to order. Pelesko, himself a 1997 alumnus of NJIT as the very first Ph.D. graduate in mathematical sciences, implored students to both soak in this moment as they will look back on it for many years to come, and look forward to the future as the only limits are one’s own ambition and passion.&lt;/p&gt;
&lt;p&gt;President Teik C. Lim encouraged graduates to be open to what comes and make a positive impact as leaders, building on the progress made by NJIT graduates before them.&lt;/p&gt;
&lt;p&gt;“Embrace opportunities and use your talents to pursue goals that will lift others and improve our world. That is how you lead a life of consequence and value. Also understand that you are building upon a foundation that was created by previous generations of Highlanders, and that you are contributing to the legacy of our great university,” said Lim.&lt;/p&gt;
&lt;p&gt;NJIT’s master’s and Ph.D. graduates enter rarified air, noted NJIT Board of Trustees Chair Robert Cohen, as this class of Highlanders join just 14% of the world’s population with an advanced degree. “What you have accomplished is extraordinary and deserves a round of applause … Continue to make us proud.”&lt;/p&gt;
&lt;h2 id=&#34;masters-ceremony-for-ywcc&#34;&gt;Master’s Ceremony for YWCC&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/euksgo1fmRg?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;em&gt;Caption: Watch the 9 a.m. commencement ceremony recognizing master’s students from Ying Wu College&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;YWCC Dean Jamie Payton lauded the graduates’ accomplishments and their unique position of being able to affect change in the rapidly moving computing fields such as AI and machine learning.&lt;/p&gt;
&lt;p&gt;“You graduate at a moment when the pace of technological change is accelerating, and so is the need for computing professionals who lead with insight, integrity and impact,” said Payton. “I have every confidence that you are ready to not only meet this moment, but shape what comes next.”&lt;/p&gt;
&lt;p&gt;Payton introduced the ceremony’s keynote speaker, Maria Vasquez Karim, a distinguished 2005 computer engineering and applied mathematics graduate, Albert Dorman Honors Scholar and an EOP student. Karim’s homecoming is highlighted by her generous gift to fund an endowed scholarship that will support future NJIT students.&lt;/p&gt;
&lt;p&gt;Reflecting on her own journey, she described the emotional full-circle moment of returning to campus two decades after earning her degrees. She credited her early foundation — shaped by her family’s immigration from Ecuador and the sacrifices of her parents and grandparents — as the roots of her perseverance and success.&lt;/p&gt;


















&lt;figure  id=&#34;figure-maria-vasquez-karim-05-youtube-engineering-program-manager-at-google&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Maria Vasquez Karim &amp;#39;05, YouTube Engineering Program Manager at Google&#34; srcset=&#34;
               /blog/20250519-njit/2025-YWCC-Commencement-5273_hu_26c013410921c6f9.webp 400w,
               /blog/20250519-njit/2025-YWCC-Commencement-5273_hu_2260492407e60cc.webp 760w,
               /blog/20250519-njit/2025-YWCC-Commencement-5273_hu_491753472039cd93.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-njit/2025-YWCC-Commencement-5273_hu_26c013410921c6f9.webp&#34;
               width=&#34;690&#34;
               height=&#34;460&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Maria Vasquez Karim &amp;lsquo;05, YouTube Engineering Program Manager at Google
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Karim spoke candidly about becoming a mother while still an undergraduate, calling her son her “North Star” and a constant source of motivation. She balanced parenthood with a demanding academic load and ultimately earned a full scholarship to Columbia University’s master’s program in electrical engineering. “There were countless nights fueled by instant coffee and a desperate determination, moments of doubt that whispered louder than reason,” she told the graduates. Today, her son is a software engineer, a fact she shared with pride, underscoring the generational impact of perseverance.&lt;/p&gt;
&lt;p&gt;Her career, which includes roles at IBM, Intel, British Telecom and most recently YouTube, exemplifies the power of adaptability and the lifelong value of an NJIT education. She encouraged graduates to view failures as stepping stones and to lean into emerging technologies like generative AI, calling them “a super amplifier for every dream you hold.” At YouTube, she helped lead key projects — both technical and cultural. She established the New Platform Machine Learning Evaluation Team to improve the platform&amp;rsquo;s performance for billions of users, and she is the co-chair lead of the Familia Employee Resource Group where she champions Latinx voice by fostering engagement and cultivating community.&lt;/p&gt;
&lt;p&gt;Karim closed by offering guidance on thriving in the ever-evolving tech world: embrace continuous learning, seek purpose beyond code, cultivate mentorship and community, prioritize well-being and above all, be bold. “Your NJIT education has instilled in you the courage to tackle the complex and the grit to overcome any obstacle,” she said. With passion and authenticity, she urged the graduates to lead with integrity and to use their skills for meaningful, lasting impact.&lt;/p&gt;
&lt;h2 id=&#34;masters-ceremony-for-nce-hcad-hcsla-and-mtsm&#34;&gt;Master’s Ceremony for NCE, HCAD, HCSLA and MTSM&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/f8QdSStAwyc?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;em&gt;Caption: Watch the 1 p.m. commencement ceremony recognizing master’s students from Newark College of Engineering, Hillier College of Architecture and Design, Jordan Hu College of Science and Liberal Arts and Martin Tuchman School of Management&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://commencement.njit.edu/commencement-address-speakers#:~:text=Kenneth%20Colao&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alumnus Kenneth Colao &amp;lsquo;77&lt;/a&gt; delivered a deeply personal address, drawing from decades of experience as an engineer, entrepreneur, and president and CEO of CNY Group. Acknowledging NJIT’s enduring role in his life, Colao reflected on the university as the place where his professional journey began. He credited NJIT with providing a transformative education and emphasized the importance of gratitude and resilience, thanking university leadership, faculty, family and friends for supporting graduates on their path.&lt;/p&gt;


















&lt;figure  id=&#34;figure-kenneth-colao-77-president-and-ceo-of-cny-group&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Kenneth Colao &amp;#39;77, president and CEO of CNY Group&#34; srcset=&#34;
               /blog/20250519-njit/2025-MastersALL-Commencement-6334_hu_c00034b5684a2ea0.webp 400w,
               /blog/20250519-njit/2025-MastersALL-Commencement-6334_hu_49f7d058745a2040.webp 760w,
               /blog/20250519-njit/2025-MastersALL-Commencement-6334_hu_bf88d80db793dc8b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-njit/2025-MastersALL-Commencement-6334_hu_c00034b5684a2ea0.webp&#34;
               width=&#34;690&#34;
               height=&#34;460&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Kenneth Colao &amp;lsquo;77, president and CEO of CNY Group
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In recounting his own trajectory, Colao shared how he leveraged early jobs and modest beginnings into a remarkable career full of both soaring highs and destitution. He urged graduates to act with purpose and listen to their instincts — guiding principles that shaped his own path through both opportunity and adversity. “Life is hard and will rarely follow what the textbook states nor what others may have prescribed as best for you,” Colao said. “Which is why it is so critical that you have a vision and purpose you can always come back to as your guiding light.”&lt;/p&gt;
&lt;p&gt;Colao’s message was rooted in perseverance. He described his lowest moment — personally guaranteeing $83 million in debt after 9/11 forced huge losses in his first company — and the extraordinary decision to rebuild without declaring bankruptcy. He stressed the value of maintaining one’s reputation and integrity, citing how he paid off debts over 16 years and retained the loyalty of his team throughout the crisis. His mantra: “Never ever give up,” became a rallying point for the graduates.&lt;/p&gt;
&lt;p&gt;Colao called on the graduates to lead with compassion and build “good karma” in business and in life. Colao reminded students that success is about more than achievement — it&amp;rsquo;s about the relationships, trust and moral compass that guide their journey. “I’m humbled by what life offers when you ‘lean in.’ The education you have received, knowledge and leadership skills you’ve gained, hold the key to turning a thought into success.”&lt;/p&gt;
&lt;h2 id=&#34;hooding-and-commencement-ceremony-for-doctoral-candidates&#34;&gt;Hooding and Commencement Ceremony for Doctoral Candidates&lt;/h2&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/yXjVh2Q_-2c?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;em&gt;Caption: Watch the 4 p.m. commencement ceremony recognizing doctoral candidates across all of NJIT’s colleges&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Ph.D. ceremony featured student speaker Oliver Alvarado Rodriguez, who received a doctorate in computer science under &lt;strong&gt;Professor David Bader&lt;/strong&gt;. His work has been instrumental in advancing graph analytics, particularly though the Arachne framework. After graduation, he will be joining Hewlett Packard Enterprise as a research software engineer, contributing to the Chapel Programming Language team.&lt;/p&gt;


















&lt;figure  id=&#34;figure-phd-student-speaker-oliver-alvarado-rodriguez&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Ph.D. student speaker Oliver Alvarado Rodriguez&#34; srcset=&#34;
               /blog/20250519-njit/20250519-PhDCeremony-0522_hu_7c5d9bff685e2fbd.webp 400w,
               /blog/20250519-njit/20250519-PhDCeremony-0522_hu_d6967c963112b8f.webp 760w,
               /blog/20250519-njit/20250519-PhDCeremony-0522_hu_bc0dfbbc8d9b6cc6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-njit/20250519-PhDCeremony-0522_hu_7c5d9bff685e2fbd.webp&#34;
               width=&#34;690&#34;
               height=&#34;460&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Ph.D. student speaker Oliver Alvarado Rodriguez
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;From childhood dreams of becoming an astronaut to confronting his stutter and lifelong anxiety, Rodriguez shared how imagination and resilience helped him push forward through moments of doubt and burnout. “There were times I seriously considered quitting — not fleeting doubts, but real actions,” he admitted, recalling a moment when he nearly sent a resignation email to his advisor. His honesty shed light on the emotional weight many doctoral students carry behind the scenes.&lt;/p&gt;
&lt;p&gt;Rodriguez emphasized that earning a Ph.D. is not only an academic achievement, but also a personal triumph. He highlighted the invisible work of emotional regulation, self-discovery and balancing life’s challenges while advancing in science. “Success doesn’t mean being perfect … It means showing up again and again even when you’re unsure, even when you’re afraid.”&lt;/p&gt;
&lt;p&gt;Two-time alumnus Anita LaSalle delivered the evening’s keynote address. She is a former program director at the Division for Computer and Network Systems at the National Science Foundation and a retired computer science professor. At the NSF she helped create the Innovation Corps, an entrepreneurial training program that facilitates the transformation of invention to impact.&lt;/p&gt;
&lt;p&gt;In her candid and compelling address, LaSalle acknowledged the challenges facing science and higher education in today’s political climate. Referencing recent freezes and withdrawals of federal research funding, she warned of the ripple effects on institutions, industries and individual researchers and underscored the urgency of the moment.&lt;/p&gt;


















&lt;figure  id=&#34;figure-lasalle-holds-a-bs-and-ms-in-mechanical-engineering-from-njit-and-a-phd-from-stevens-institute-of-technology-with-a-concentration-in-computer-engineering&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;LaSalle holds a B.S. and M.S. in Mechanical Engineering from NJIT and a Ph.D. from Stevens Institute of Technology, with a concentration in computer engineering.&#34; srcset=&#34;
               /blog/20250519-njit/20250519-PhDCeremony-0650_0_hu_74acf702669a901f.webp 400w,
               /blog/20250519-njit/20250519-PhDCeremony-0650_0_hu_e20a8a4f6255f05f.webp 760w,
               /blog/20250519-njit/20250519-PhDCeremony-0650_0_hu_d2bdc926bb540e3d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-njit/20250519-PhDCeremony-0650_0_hu_74acf702669a901f.webp&#34;
               width=&#34;690&#34;
               height=&#34;460&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      LaSalle holds a B.S. and M.S. in Mechanical Engineering from NJIT and a Ph.D. from Stevens Institute of Technology, with a concentration in computer engineering.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;LaSalle also reminded the audience of how much America has gained from investing in science — from everyday technologies like GPS and MRIs to life-saving medical research. “And all of this research has produced payoffs for us — for every $1 invested in research there is a $5 gain.”&lt;/p&gt;
&lt;p&gt;Yet despite these concerns, she pointed to her own family’s story — her father, an immigrant who built a life and business during the Great Depression — as proof of what’s possible in this country. “In my gut, I still feel there is hope for you and for our country,” she said. She emphasized that NJIT has prepared the Class of 2025 to meet any challenge, just as it had prepared her. She urged graduates to thank those who helped them reach this milestone and reminded them that the enduring promise of America still exists — if we protect it.&lt;/p&gt;
&lt;h3 id=&#34;doctoral-awards&#34;&gt;Doctoral Awards&lt;/h3&gt;
&lt;p&gt;Computer engineer Mahmoud Khaled Ahmed Nazzal was awarded the Hashimoto Prize for his distinguished research in electrical and computer engineering. The Hashimoto Prize is part of an endowment that recognizes the generosity and vision of Dr. Kazuo Hashimoto, who is known for more than 1,000 patents and applications related to the invention of the telephone answering machine and other devices in electronics and telecommunications.&lt;/p&gt;
&lt;p&gt;Nazzal is researching new approaches to make AI systems more secure and trustworthy by linking two leading AI types: graph neural networks (GNNs) and large language models (LLMs). GNNs excel at understanding connections, while LLMs are masters of language. By combining their strengths, Nazzal&amp;rsquo;s research tackles security weaknesses in both. His work demonstrates new ways to find vulnerabilities in AI systems, guide language models to write safer computer code, strengthen adversarial defense and even deepfake detection. This integration offers a promising path toward building more reliable and secure AI for various applications. Nazzal will be joining the Department of Computer Science at Old Dominion University as a tenure-track faculty member.&lt;/p&gt;
&lt;p&gt;Environmental engineer Jianan Gao was awarded the Outstanding Ph.D. Dissertation award by Sotirios Ziavras, vice provost for graduate studies and dean of graduate faculty. Gao is developing innovative water filters that use electricity to clean water in a more sustainable way. Unlike traditional filters that just remove pollutants, these electrified membranes can potentially transform waste into valuable resources with little to no chemical additives. Gao’s research focuses on making these electric filters more powerful, efficient and cost-effective, with the technology offering both a promising solution for tackling global water pollution, and recovering useful materials from wastewater.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/rooted-research-ready-lead-njits-masters-and-doctoral-degree-class-2025&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/rooted-research-ready-lead-njits-masters-and-doctoral-degree-class-2025&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Billionaire Emerges As Key Nvidia Partner, With A New Lab Based In NJ</title>
      <link>http://localhost:1313/blog/20250519-forbes/</link>
      <pubDate>Mon, 19 May 2025 09:21:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250519-forbes/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.forbes.com/sites/elizabethmacbride/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elizabeth MacBride&lt;/a&gt;&lt;/em&gt;, Senior Contributor.  Business Journalist&lt;/p&gt;


















&lt;figure  id=&#34;figure-thai-lee-speaks-at-the-recent-opening-of-the-ai-labs-in-new-jersey-shi&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Thai Lee speaks at the recent opening of the AI Labs in New Jersey. *SHI*&#34; srcset=&#34;
               /blog/20250519-forbes/960x0_hu_eb9cb267c95d876.webp 400w,
               /blog/20250519-forbes/960x0_hu_8bf2e529f50f7e01.webp 760w,
               /blog/20250519-forbes/960x0_hu_242fde23ed756469.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250519-forbes/960x0_hu_eb9cb267c95d876.webp&#34;
               width=&#34;760&#34;
               height=&#34;638&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Thai Lee speaks at the recent opening of the AI Labs in New Jersey. &lt;em&gt;SHI&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Nvidia is opening another distribution strategy for its GPU capacity, by establishing a network of IT consulting firms as partners to test and sell AI solutions. The partners use new Spark machines and sell Nvidia infrastructure to enterprises.&lt;/p&gt;
&lt;p&gt;One of the first partners to emerge is SHI, one of America’s largest women-owned businesses. SHI, which has more than $14B in revenue, recently opened an AI and Cyber Labs in Piscataway, N.J., equipped with the latest Nvidia technology. SHI also trained its staff to help companies build and test AI applications.&lt;/p&gt;
&lt;p&gt;Working with trained partners – humans – makes sense since given the complexity of enterprise AI deployments. “We believe we’re developing capabilities that are going to be very useful,” said SHI CEO Thai Lee in an interview. Her company is privately held, meaning she’s one of the richest women in America.&lt;/p&gt;
&lt;p&gt;“Many organizations that I talk to are afraid their business models today are not going to keep them alive.”&lt;/p&gt;
&lt;p&gt;Nvidia has established a DGX SuperPOD Specialization Partner program, which signals they’re building a network of specialized partners who can deploy and manage their high-performance AI infrastructure, said &lt;strong&gt;David A. Bader&lt;/strong&gt;, distinguished professor in Data Science at the New Jersey Institute of Technology, by email. ePlus, another IT services provider, has also become certified as a partner. The DGX SuperPOD is what Nvidia calls its AI supercomputer, accessed through the Cloud.&lt;/p&gt;
&lt;p&gt;Nvidia is facing more competition from Chinese models for AI development, and needs to move quickly to cement its edge in enterprise AI.&lt;/p&gt;
&lt;p&gt;“The work being done in SHI’s AI &amp;amp; Cyber Labs showcases the accelerating shift toward AI-driven innovation across industries,” said Craig Weinstein, vice president of the America’s Partner Organization, Nvidia, in an e-mailed statement. From AI agents helping people develop business strategies, to AI-assisted fraud detection, organizations are moving beyond experimentation to tangible results with SHI’s labs, transforming how they operate and compete in the AI era.”&lt;/p&gt;
&lt;p&gt;Many enterprise AI projects are stumbling on cost and data problems, according to outside researchers. The move to become a Nvidia partner makes sense for Somerset-based SHI, which is expanding into AI-as-a-service. The new lab, opened to fanfare in April, will give teams from corporations access to the infrastructure needed to develop AI applications. They’ll pay to rent time on Nvidia Sparks – new desktop AI development devices – at the Lab, along with receiving the advice of the SHI team based there.&lt;/p&gt;
&lt;p&gt;Lee said companies will need to adapt to the next wave of AI, including agentic AI and physical AI – meaning, the capacity of robots, autonomous vehicles and the like.&lt;/p&gt;
&lt;p&gt;Currently, companies that want to work on AI applications have to contract to engage with GPUs, which were housed in data centers requiring massive energy and cooling. Different models are now emerging for AI application development. The Spark, which is being sold as a personal AI computer, costs about $4,000, and contains Nvidia GPUs.&lt;/p&gt;
&lt;p&gt;“These systems, powered by the Grace Blackwell platform, represent a democratization of AI computing power that was previously only available in data centers,” said Bader. “For researchers, data scientists, and organizations exploring AI applications, having local access to this level of computing power through SHI&amp;rsquo;s facility provides tremendous advantages. It enables experimentation with large language models and complex AI systems without the latency, cost, or data sovereignty concerns that can come with cloud-based approaches.”&lt;/p&gt;
&lt;h2 id=&#34;the-difficulty-of-enterprise-ai&#34;&gt;The Difficulty of Enterprise AI&lt;/h2&gt;
&lt;p&gt;The idea behind the Lab is that people will rent time there and with the assistance of more than 100 trained SHI staff, develop or test applications. Jack Hogan, vice president of advanced growth technologies at SHI, gave an example of how this has played out in practice.&lt;/p&gt;
&lt;p&gt;“A large pharmaceutical company came to us, that is literally spending billions of dollars a year in AI infrastructure hosted in the could and on premises,” he said. “They had a specific use case to ensure that the eight-figure purchase they were about to make was going to work.”&lt;/p&gt;
&lt;p&gt;SHI charged them for a six-week engagement, with a price tag in “the low six figures.”&lt;/p&gt;
&lt;p&gt;“They ultimately proved to themselves that their data wasn&amp;rsquo;t ready,” he said. “They prevented themselves from making a substantial mistake in just going out and buying this equipment without knowing that it was going to work.”&lt;/p&gt;
&lt;p&gt;SHI is also working with Nvidia on creating synthetic data for companies, so that even if they don’t share their data, they can validate their use cases in the Lab.&lt;/p&gt;
&lt;p&gt;Many enterprise-level AI projects are failing, according to research by Gartner. “Cost is one of the greatest near-term threats to AI and generative AI (GenAI) success. Gartner estimates that more than half of organizations who are embarking on AI initiatives beyond “everyday AI” are abandoning their efforts due to unexpected cost overruns,” according to a Gartner Research Note, titled Toolkit: Customer Calculation of AI and GenAI Multiyear Costs by Use Case, issued in March 2025.&lt;/p&gt;
&lt;p&gt;About 30% of AI projects that make it past proof of concept will be abandoned due to poor data quality, inadequate risk controls, escalating costs or unclear business value, &lt;a href=&#34;https://www.gartner.com/en/documents/5845247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the research firm also predicted&lt;/a&gt;. However, at the same time, the successful AI projects will make a difference, it said reporting that through 2026, GenAI will reduce manually intensive data management costs up to 20% each year while enabling four times as many new use cases.&lt;/p&gt;
&lt;h2 id=&#34;more-labs-to-come&#34;&gt;More Labs To Come?&lt;/h2&gt;
&lt;p&gt;Nvidia, with a market capitalization of $2.6T, is one of the world’s most valuable companies. By teaming up with it, SHI is further emerging as one of the most significant IT service companies in the United States,&lt;/p&gt;
&lt;p&gt;Lee said SHI may open more labs across the country, working with Nvidia or others. “The age of the GPU is here and it is going to completely transform the way that the computing world is been operating,” said Lee.&lt;/p&gt;
&lt;p&gt;She also gave an interview on her leadership style, &lt;a href=&#34;https://www.forbes.com/sites/elizabethmacbride/2025/05/19/the-ai-future-worries-thai-lee-and-its-coming-faster-than-you-think/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.forbes.com/sites/elizabethmacbride/2025/05/19/one-of-americas-biggest-entrepreneurs-emerges-as-a-key-nvidia-partner/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.forbes.com/sites/elizabethmacbride/2025/05/19/one-of-americas-biggest-entrepreneurs-emerges-as-a-key-nvidia-partner/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Contribute to State AI Task Force, NJIT Magazine, Spring 2025</title>
      <link>http://localhost:1313/blog/20250515-njit-stateai/</link>
      <pubDate>Thu, 15 May 2025 14:40:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250515-njit-stateai/</guid>
      <description>&lt;p&gt;Distinguished Professors &lt;strong&gt;David Bader&lt;/strong&gt; and Guiling “Grace” Wang,
along with Professors Cristian Borcea and Vincent Oria in the Ying Wu
College of Computing (YWCC), served on Governor Phil Murphy’s AI Task
Force. Comprised of industry leaders, academic experts, consumer
advocates and government innovators, the group worked to establish
programs, trainings and tools that will advance New Jersey’s
leadership in AI.&lt;/p&gt;
&lt;p&gt;Recommendations by the task force were published in “The Report to the
Governor on Artificial Intelligence,” which aims to create economic
opportunities for residents and businesses, encourage ethical use of
AI technologies, promote equitable outcomes, support public and
private workforces and improve government services and citizen
experience.&lt;/p&gt;
&lt;p&gt;The YWCC faculty members each contributed to primary working groups in
four areas: security, safety and privacy considerations for AI use
cases; workforce training, jobs of the future and training public
professionals; AI, equity and literacy; and making New Jersey a hub
for AI innovation.&lt;/p&gt;
&lt;p&gt;Recommendations included ways to expand opportunities for AI education
and literacy, promote a strong workforce and AI talent pipeline,
address biases and discrimination, foster a collaborative AI
innovation ecosystem and economy across the state, and bolster the
state’s use of GenAI to support policy outcomes and improve the
resident experience.&lt;/p&gt;
&lt;p&gt;In addition to his work with the state, Bader serves on other boards,
including on the Computing Research Association. In addition, he is a
scientific advisory board member for the Flatiron Institute, Simons
Foundation; an advisory board member for ARLIS at the University of
Maryland and steering committee chair for the Seed Fund at the
Northeast Big Data Innovation Hub.&lt;/p&gt;
&lt;p&gt;Wang, the director of NJIT’s Center for Artificial Research, serves as
a technology expert for a state committee aimed at helping judges and
lawyers understand AI’s role in the legal system. The New Jersey State
Court Artificial Intelligence Committee is examining potential
policies and practices in numerous areas, and Wang is the sole
academic appointed to the committee of 31 members. Wang also has been
named to serve as an AI subject matter expert for the Department of
Homeland Security’s Science Advice and Guidance for Emergencies
Program, where she may be called upon by the DHS Chief Scientist to
provide counsel during national emergencies.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://magazine.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://magazine.njit.edu/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Q&amp;A with David Bader, NJIT Magazine, Spring 2025</title>
      <link>http://localhost:1313/blog/20250515-njit-qa/</link>
      <pubDate>Thu, 15 May 2025 14:40:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250515-njit-qa/</guid>
      <description>&lt;h2 id=&#34;qa-with-david-bader-distinguished-professor-data-science&#34;&gt;Q&amp;amp;A with &lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor, Data Science&lt;/h2&gt;
&lt;h2 id=&#34;ai-and-society&#34;&gt;AI and Society&lt;/h2&gt;
&lt;p&gt;A conversation with Evan Koblentz and David Bader&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;A lot of people who are information workers are afraid that AI will
make their careers obsolete. Technological progress can’t be stopped,
so how should people adapt?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: In the face of technological progress, particularly with the rapid
advancement of AI, it’s understandable that information workers may
feel apprehensive about the future of their careers. However, rather
than viewing AI as a harbinger of obsolescence, it’s crucial to see it
as a catalyst for evolution and innovation in our work practices. The
key to adapting is in embracing these technologies, learning to work
alongside them, and leveraging their capabilities to enhance our own
skill sets and productivity. The first step in this adaptation process
is to cultivate a mindset of lifelong learning. As AI and other
technologies continue to evolve, so too must our skills and
knowledge. This means staying informed about new technologies, seeking
out educational opportunities, and being open to acquiring new
competencies that complement the capabilities of AI.&lt;/p&gt;
&lt;p&gt;For instance, developing skills in data literacy, AI ethics and
understanding the principles of machine learning can make workers more
versatile and valuable in an AI-integrated workplace. Additionally,
it’s important to focus on the uniquely human skills that AI cannot
replicate, such as creativity, emotional intelligence and critical
thinking. By honing these abilities, workers can ensure they remain
irreplaceable components of the workforce, capable of tasks that
require a human touch, from complex decision-making to empathetic
interactions with customers or clients.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;Other than creative prompt-making, what should non-programmers
learn now about AI?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: For non-programmers looking to delve deeper into AI, understanding
the ethical implications and societal impacts of AI is paramount. It’s
important to be aware of how AI decisions are made, the potential
biases in AI systems, and the ethical considerations of AI
use. Additionally, developing data literacy is crucial, as it enables
individuals to evaluate AI outputs and understand the importance of
data quality and biases. A basic grasp of AI and machine learning
concepts, even without programming skills, can demystify AI
technologies and reveal their potential applications. Staying informed
about AI advancements across various sectors can also inspire
innovative ideas and foster interdisciplinary collaborations. By
focusing on these areas, non-programmers can contribute meaningfully
to the AI conversation and its future direction.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;There’s a popular sci-fi plot where the computers get so smart that
people lose control. The new class of user-friendly AI is certainly
making people excited but also nervous. Should we be afraid?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: The emergence of user-friendly AI technologies has indeed brought
this conversation into the mainstream, highlighting the balance we
must strike between harnessing the benefits of AI and addressing valid
concerns about its implications. It’s critical to recognize that the
AI technologies we’re creating today are built with numerous
safeguards, are subject to ethical guidelines, and operate within
evolving regulatory environments.  These measures are designed to
ensure AI systems augment human abilities and decision-making, rather
than supplanting or undermining human control.&lt;/p&gt;
&lt;p&gt;While it’s natural to harbor concerns about the rapid progression of
AI, allowing fear to dominate the discourse would be a disservice to
the potential benefits these technologies can offer. Instead, this
moment calls for proactive engagement with AI, an investment in
understanding its inner workings, limitations and the ethical dilemmas
it presents. By advocating for responsible AI development, emphasizing
education and promoting transparency, we can foster an environment
where AI serves as a tool for societal advancement. This approach
ensures that we remain at the helm of AI’s trajectory, steering it
towards outcomes that uplift humanity rather than scenarios that fuel
dystopian fears.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;You and your peers at the Institute for Data Science (IDS) are
known for researching the building blocks and tools that help make AI
infrastructure possible.  Specifically, what would you say are IDS’
most important contributions so far?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: The Institute for Data Science at NJIT has made groundbreaking
contributions to graph analytics and high-performance computing
through the development of Arachne, a sophisticated and open-source
framework for processing massivescale graphs. At its foundation,
Arachne implements a hybrid edge list and adjacency structure that
revolutionizes how large-scale graphs are processed.  This
architectural innovation represents a significant leap forward in
handling the complexities of large-scale network analysis.&lt;/p&gt;
&lt;p&gt;Beyond its technical architecture Arachne has demonstrated remarkable
versatility in real-world applications. In cybersecurity, it enables
rapid detection of emerging threat patterns through its pattern
matching capabilities. The framework’s ability to track community
structure and identify anomalies has proven valuable for social
network analysis, while its high-performance processing has enhanced
financial fraud detection systems. These advances in graph processing
highlight IDS’ broader contributions to high-performance computing.&lt;/p&gt;
&lt;p&gt;The significance of these contributions extends beyond their immediate
applications. As artificial intelligence systems increasingly rely on
graph-based representations for processing complex relationships, the
optimizations and algorithms developed at IDS, particularly through
Arachne, have become fundamental to making these systems more
practical and scalable. Their work continues to bridge the gap between
theoretical computer science and practical applications, enabling the
next generation of AI infrastructure through innovative approaches to
graph processing and high-performance computing.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;Some of your own research focuses on democratizing supercomputing
power.  Can that help lead to another approach for AI equal access?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: The concept of democratizing supercomputing offers intriguing
possibilities for expanding AI access.  When we consider how
democratized supercomputing could influence AI development, several
key pathways emerge. Fundamentally, the process of making
high-performance computing more accessible to diverse researchers and
institutions, rather than concentrating it among elite organizations,
could reshape how AI capabilities develop and spread.&lt;/p&gt;
&lt;p&gt;As supercomputing becomes more widely available, smaller organizations
and independent researchers gain the ability to train and run AI
models without massive capital investments in dedicated hardware. This
democratization creates opportunities for innovation from previously
excluded participants in the AI development landscape.&lt;/p&gt;
&lt;p&gt;However, the path to democratized AI through supercomputing faces
several significant challenges. Computing power, while crucial,
represents just one element in the complex ecosystem of AI
development. Equal consideration must be given to data access,
technical expertise and algorithmic innovation.  The environmental
impact of distributed supercomputing systems requires careful
assessment, particularly regarding energy consumption. Additionally,
any distributed computing approach to AI development must address
robust security and privacy protections.&lt;/p&gt;
&lt;p&gt;This intersection of democratized supercomputing and AI access
highlights broader questions about how we can make artificial
intelligence technology more equitable and accessible while
maintaining necessary safeguards and standards.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;How does supercomputer democratization impact the overall work of
the Institute for Data Science?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: The rise of user-friendly artificial intelligence systems like
ChatGPT marks a pivotal moment in our pursuit to democratize data
science and supercomputing. For my work, this evolution serves as both
a tool and a testament to the power of making complex computational
capabilities accessible to a broader audience. It enriches the palette
of methodologies and technologies at our disposal, enabling us to
tackle more ambitious projects with greater efficiency and
creativity. By integrating these AI systems into our research and
educational programs, we’re not just enhancing our ability to process
and analyze data, we’re also empowering students and researchers with
the means to innovate and explore new horizons in data science without
being hindered by the technical complexities that once acted as
barriers. For the Institute for Data Science, the impact of such AI
systems is transformative. They serve as a bridge between advanced
computational technologies and a diverse range of disciplinary
domains, facilitating interdisciplinary research and collaboration.&lt;/p&gt;
&lt;p&gt;Q: &lt;strong&gt;A new model from China, called DeepSeek, seems to be as good as
Western models but has far lower costs and technology
requirements. How did they do it, and what can Western companies learn
from this?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A: DeepSeek’s emergence represents a significant challenge to
established thinking about AI development. While Western companies
have typically pursued AI advancement through massive computational
resources and extensive funding, DeepSeek has demonstrated that
remarkable results can be achieved through more efficient methods and
careful engineering.  The company’s approach centers on targeted
reinforcement learning focused specifically on reasoning tasks, rather
than the broader supervised learning methods common in Western models.
They’ve developed an innovative rulebased reward system that actually
outperforms traditional neural reward models, while using
significantly fewer resources. Perhaps most impressively, they’ve
managed to compress advanced capabilities into relatively small
models, achieving with 1.5 billion parameters what others do with far
larger models.&lt;/p&gt;
&lt;p&gt;The financial implications are striking.  DeepSeek developed their R1
model for less than $6 million, a fraction of the hundreds of millions
typically spent by Western competitors. They’ve translated this cost
efficiency into their pricing model, offering services at $0.55 for
input and $2.19 for output per million tokens, substantially
undercutting market rates while maintaining comparable quality.&lt;/p&gt;
&lt;p&gt;This success suggests that the future of AI development might lie more
in clever engineering and efficient methodology than in raw
computational power. It challenges the assumption that advanced AI
development requires massive resources and suggests that innovative
approaches to training and model architecture might be more important
than sheer scale. The success of DeepSeek also suggests that
competitive advantage in AI might come from unexpected directions, and
that the barriers to entry for significant AI advancement might be
lower than previously thought. It’s a reminder that technological
breakthroughs often come not from doing things bigger, but from doing
them smarter.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;- Evan Koblentz&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://magazine.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://magazine.njit.edu/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI Raises Bigger Concerns for Students Than Teachers, Admins: Study</title>
      <link>http://localhost:1313/blog/20250416-technewsworld/</link>
      <pubDate>Wed, 16 Apr 2025 11:32:11 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250416-technewsworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John P. Mello Jr.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250416-technewsworld/students-in-classroom_hu_728ce1c1912c2ced.webp 400w,
               /blog/20250416-technewsworld/students-in-classroom_hu_a15121b2c9ecb058.webp 760w,
               /blog/20250416-technewsworld/students-in-classroom_hu_b9a1014aa0c8e82b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250416-technewsworld/students-in-classroom_hu_728ce1c1912c2ced.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A multinational study on the state of artificial intelligence in secondary and higher education released Tuesday found that students are more concerned about the impact of the technology on their learning than academic administrators and educators.&lt;/p&gt;
&lt;p&gt;The study, based on a survey of 3,500 academic administrators, students, and educators in seven nations, found that more than three out of five students (64%) worry about the use of AI in education, compared to 50% of educators and 41% of academic administrators.&lt;/p&gt;
&lt;p&gt;Top AI risks for educators and students were overreliance on the technology and potential loss of critical thinking skills, while prime risks for administrators were data privacy and security breaches, according to the survey conducted by &lt;a href=&#34;https://www.vansonbourne.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vanson Bourne&lt;/a&gt; for &lt;a href=&#34;https://www.turnitin.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Turnitin&lt;/a&gt;, an academic integrity and assessment solutions company, in Oakland, Calif.&lt;/p&gt;
&lt;p&gt;“I’m not surprised that students were concerned, but the depth of their concern was surprising to me,” said Turnitin’s Senior Director for Customer Engagement, Patti West-Smith.&lt;/p&gt;
&lt;p&gt;“I was surprised at the way the numbers shook out, that they expressed more concern about AI than admins or instructors,” she told TechNewsWorld.&lt;/p&gt;
&lt;h2 id=&#34;erosion-of-critical-thinking&#34;&gt;Erosion of Critical Thinking&lt;/h2&gt;
&lt;p&gt;Karen Kovacs North, a clinical professor of communication at the &lt;a href=&#34;https://annenberg.usc.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Annenberg&lt;/a&gt; School for Communication and Journalism at the University of Southern California, said that if students are concerned about AI, it isn’t stopping them from using it to complete assigned work.&lt;/p&gt;
&lt;p&gt;Nevertheless, she told TechNewsWorld, “It’s heartening to know that students are now embracing the idea that critical thinking is what’s lost when they hand off problem-solving to AI.”&lt;/p&gt;
&lt;p&gt;“If students have an increasing understanding and appreciation of their own critical thinking and their own ability to problem solve, then we’re at least moving toward a better world where people will do their own problem solving and critical thinking.”&lt;/p&gt;
&lt;p&gt;Nearly half the students in the survey (49%) said they worry about becoming overreliant on AI, while more than half (59%) fret about overreliance on &lt;a href=&#34;https://www.technewsworld.com/story/are-gen-ai-benefits-worth-the-risk-178484.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI reducing their critical thinking skills&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“Reading and writing are two fundamental ways that people make meaning of new information, so if you get into a situation where a student is over-relying on AI, they’re outsourcing a lot of meaning-making that they would normally be doing,” West-Smith explained.&lt;/p&gt;
&lt;p&gt;“The danger there is that if you outsource the reading and writing, you might also outsource the thinking that goes into reading and writing,” she said. “The result of that is you might learn less because you are turning over your thinking process to the technology.”&lt;/p&gt;
&lt;h2 id=&#34;overuse-of-ai-weakens-thinking-skills&#34;&gt;Overuse of AI Weakens Thinking Skills&lt;/h2&gt;
&lt;p&gt;Kaveh Vahdat, founder and president of &lt;a href=&#34;https://riseopp.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RiseOpp&lt;/a&gt;, a San Francisco marketing agency specializing in chief marketing officer services, maintained that overreliance on AI risks displacing the cognitive friction that critical thinking depends on.&lt;/p&gt;
&lt;p&gt;“When students defer too quickly to machine-generated answers, they may engage less in evaluating assumptions, weighing evidence, or forming independent judgments,” he told TechNewsWorld. “These are foundational to learning.”&lt;/p&gt;
&lt;p&gt;“I think of critical thinking as a muscle. It needs to be regularly exercised,” added Ryan Trattner, co-founder of &lt;a href=&#34;https://www.studyfetch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StudyFetch&lt;/a&gt;, an AI-powered learning platform in Los Angeles. “If students are not properly reasoning and evaluating information on a regular basis, this muscle will atrophy.”&lt;/p&gt;
&lt;p&gt;“We’re starting to see an entire generation of students who just magically makes answers appear without any effort,” he told TechNewsWorld. “When browsing the internet and looking for answers, it was still similar to a library. You had to find and read the information, but it wasn’t exactly answering your question, so you had to interpret and understand it, and then apply what you learned to answer a question. That is not the case with AI, where it’s just a simple copy and paste with zero critical thinking.”&lt;/p&gt;
&lt;h2 id=&#34;wide-agreement-on-ai-misuse&#34;&gt;Wide Agreement on AI Misuse&lt;/h2&gt;
&lt;p&gt;An overwhelming number of survey participants (95%) felt that AI was being misused. “The risk of intentional misuse will always exist with generative AI,” Turnitin Chief Product Officer Annie Chechitelli said in a statement. “Transparency throughout the student writing process enables educators to leverage the opportunities that AI technologies present while upholding the integrity of original student work.”&lt;/p&gt;
&lt;p&gt;“It can be helpful to have students do more of their writing in class, where it’s more difficult for them to use AI since they risk getting caught, but should we be teaching defensively and coming up with strategies that anticipate cheating?” asked &lt;a href=&#34;https://camd.northeastern.edu/people/dan-kennedy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Kennedy&lt;/a&gt;, a professor of journalism at Northeastern University in Boston.&lt;/p&gt;
&lt;p&gt;“I don’t think so,” he told TechNewsWorld. “Maybe having students produce a few writing samples in class at the beginning of the semester would be helpful, but overall, I’m uncomfortable with the idea of assuming my students will cheat.”&lt;/p&gt;
&lt;p&gt;AI is a huge challenge for people at universities because it is so enticing for students to get their homework done fast, added North. “It puts a burden on faculty to come up with assignments that will challenge students to approach a problem in a uniquely individual way,” she said. “I always try to come up with problems that would be very hard for AI to solve, but it’s exhausting to try to figure out how to circumvent AI.”&lt;/p&gt;
&lt;p&gt;Mark N. Vena, president and principal analyst at &lt;a href=&#34;https://www.smarttechresearch.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SmartTech Research&lt;/a&gt; in Las Vegas, maintains that misuse can be reduced through clear guidelines, ethical training, and early integration of AI literacy into the curriculum. “Educators should model responsible AI use and encourage students to see AI as a tool for exploration, not automation,” he told TechNewsWorld.&lt;/p&gt;
&lt;h2 id=&#34;getting-the-most-from-ai&#34;&gt;Getting the Most From AI&lt;/h2&gt;
&lt;p&gt;The survey also noted that while organizations may be expecting an AI-ready future workforce, more than two-thirds of the students surveyed (67%) felt they are shortcutting their learning by using AI. In addition to feeling they are shortcutting learning, 50% of students report not knowing how to get the most benefit from AI in their studies.&lt;/p&gt;
&lt;p&gt;“Often schools are falling behind in their understanding of how to use these systems, and by extension, how to help students get the most out of AI,” said Matt Mittelsteadt, a technology policy research fellow at the &lt;a href=&#34;https://www.cato.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cato Institute&lt;/a&gt;, a Washington, D.C. think tank.&lt;/p&gt;
&lt;p&gt;“To fill the gap, I’d encourage students to look to YouTube and other free-to-use online learning platforms,” he told TechNewsWorld. “Today there is a growing wealth of free resources on AI use cases, prompting techniques and limitations that students should dive into to fill the gaps in their education.”&lt;/p&gt;
&lt;p&gt;“I’d also recommend students investigate more ‘bespoke’ use cases of AI that could really supercharge their learning,” he added. “Machine translation, for instance, is now largely mature, opening the door for students to translate novel primary sources, follow international news, and engage with unique information sources that previously would have been walled behind a language barrier.”&lt;/p&gt;
&lt;p&gt;Vena added: “Students can derive the greatest benefit from employing artificial intelligence to augment — not supplant — their learning process. This entails utilizing AI for feedback, brainstorming, and conceptual clarification while simultaneously maintaining engagement and reflective thinking throughout the learning journey.”&lt;/p&gt;
&lt;h2 id=&#34;need-to-reimagine-education-for-the-ai-era&#34;&gt;Need To Reimagine Education for the AI Era&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://people.njit.edu/profile/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology in Newark, N.J., maintained that society is at an inflection point that requires reimagining education, not just adding AI as another tool in the existing framework.&lt;/p&gt;
&lt;p&gt;“The key question isn’t whether to use AI, but how to evolve education to prepare students for a world where AI is ubiquitous,” he told TechNewsWorld. “This means shifting emphasis from fact memorization to higher-order thinking skills that complement rather than compete with AI capabilities.”&lt;/p&gt;
&lt;p&gt;“It means reconsidering assessment fundamentally — what are we measuring and why?” he continued. “It means acknowledging that literacy now includes understanding algorithmic influences on information.”&lt;/p&gt;
&lt;p&gt;“Most importantly, we need to maintain focus on the uniquely human aspects of education — the creativity, ethical reasoning, and interpersonal skills that remain distinctly human domains,” he said. “AI should amplify these capacities, not replace them.”&lt;/p&gt;
&lt;p&gt;“Educational institutions have a responsibility to model thoughtful AI implementation, being neither uncritically enthusiastic nor fearfully resistant,” he added. “The decisions we make now about AI in education will shape not just learning outcomes but society’s relationship with these powerful technologies for years to come.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technewsworld.com/story/ai-raises-bigger-concerns-for-students-than-teachers-admins-study-179695.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technewsworld.com/story/ai-raises-bigger-concerns-for-students-than-teachers-admins-study-179695.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepSeek Ups Ante (Again) in Duel with OpenAI, Anthropic</title>
      <link>http://localhost:1313/blog/20250325-techstrong/</link>
      <pubDate>Tue, 25 Mar 2025 17:24:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250325-techstrong/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jon Swartz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250325-techstrong/DeepSeek_hu_af9f18c1ec562ce5.webp 400w,
               /blog/20250325-techstrong/DeepSeek_hu_820e3e650c9e983.webp 760w,
               /blog/20250325-techstrong/DeepSeek_hu_ce07150010d2da05.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250325-techstrong/DeepSeek_hu_af9f18c1ec562ce5.webp&#34;
               width=&#34;760&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;DeepSeek has raised the bar yet again in the artificial intelligence (AI) race.&lt;/p&gt;
&lt;p&gt;The Chinese startup this week released a major upgrade to its V3 large language model (LLM) in its escalating jousting with American rivals OpenAI and Anthropic. The new model, DeepSeek-V3-0324, has 685 billion parameters, compared with the original V3 model’s 671 billion, offering “significant improvements” in reasoning and coding, the company said. The new model was made available through the Hugging Face platform.&lt;/p&gt;
&lt;p&gt;The latest update, DeepSeek claims, boosts performance across several benchmark tests for the language model, as well as upgrades to front-end web development, Chinese writing proficiency and Chinese search capabilities such as “enhanced report analysis.”&lt;/p&gt;
&lt;p&gt;DeepSeek’s latest version of V3, which &lt;a href=&#34;https://techstrong.ai/articles/deepseek-has-ai-world-especially-silicon-valley-in-deep-duress/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;roiled markets and heightened blood pressure across Silicon Valley&lt;/a&gt; when it was originally released late last year, is bound to again rattle U.S. tech companies that are investing billions of dollars in advanced chips and large data centers used to train AI models. Amazon.com Inc., Meta Platforms Inc., and Microsoft Corp. plan to spend $371 billion on data centers and computing resources this year, up 44% from a year ago.&lt;/p&gt;
&lt;p&gt;When the Chinese company unveiled V3, it boasted the model was trained with less than $6 million worth of computing power from 2,000 NVIDIA’s H800 chips to achieve a level of performance on par with the most advanced models from OpenAI and Meta.&lt;/p&gt;
&lt;p&gt;“If U.S. tech giants and AI companies continue on their current trajectory, they will be outpaced by their smaller, more agile counterparts who open-source their models and continue reducing costs and compute needs,” Tory Green, CEO of GPU network io.net, warns. “Despite incredible, government-backed efforts to protect their monopolies, there is likely little U.S. tech giants can now do to stem the tide of open-source AI development.”&lt;/p&gt;
&lt;p&gt;So far, reviews of DeepSeek’s V3 have been generally positive among those in the tech industry, although a number of countries (Italy, Ireland, South Korea, Australia) and &lt;a href=&#34;https://techstrong.ai/agentic-ai/navy-congress-ban-deepseek-as-security-concerns-grow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;federal agencies have banned the use of the  startup’s model&lt;/a&gt; out of concerns over its ties to the Chinese government.&lt;/p&gt;
&lt;p&gt;Earlier this week during a visit to China, Apple Inc. CEO Tim Cook reportedly described DeepSeek’s AI models as “excellent” in an interview. When asked during an earnings call with analysts in January whether DeepSeek posed a risk to Apple’s future revenue, Cook said, “In general, I think innovation that drives efficiency is a good thing. And that’s what you see in that model.”&lt;/p&gt;
&lt;p&gt;American technologists, in general, are increasingly worried about AI models coming out of mainland China. Since DeepSeek debuted V3 in late 2024, a conga line of companies have ramped up their AI plans: Baidu Inc. has introduced &lt;a href=&#34;https://techstrong.ai/ai-at-the-edge/baidu-unleashes-speedy-new-ai-model-to-rival-deepseek/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ernie X1&lt;/a&gt; to compete with DeepSeek’s R1 model; Alibaba Group Holding announced its own AI agents and &lt;a href=&#34;https://techstrong.ai/agentic-ai/alibabas-new-ai-model-clocks-in-to-pass-deepseek-and-openai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reasoning model upgrade&lt;/a&gt;; Tencent Holdings unfurled an AI blueprint to rival R1; and Meituan, the world’s biggest meal-delivery service, said it was investing billions of dollars on AI.&lt;/p&gt;
&lt;p&gt;“We’re witnessing a fundamental shift in the AI landscape. DeepSeek’s rapid development cycle — from launching last year to releasing multiple competitive models in quick succession — demonstrates China’s serious commitment to becoming a contributor rather than just a consumer in the global AI ecosystem,” &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at New Jersey Institute of Technology, said in an email.&lt;/p&gt;
&lt;p&gt;“While regulatory concerns exist, with U.S. government restrictions already targeting DeepSeek’s products, the technical achievements can’t be ignored,” Bader added. “This isn’t just about national competition; it’s about how quickly AI capabilities are advancing globally and how different regulatory approaches might influence innovation trajectories in the years ahead.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://techstrong.ai/ai-at-the-edge/deepseek-ups-ante-again-in-duel-with-openai-anthropic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://techstrong.ai/ai-at-the-edge/deepseek-ups-ante-again-in-duel-with-openai-anthropic/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT&#39;s Bader, Who Helped Invent Modern Supercomputing, Gets HoF Honor</title>
      <link>http://localhost:1313/blog/20250317-njit/</link>
      <pubDate>Mon, 17 Mar 2025 09:25:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250317-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-njits-wulver-supercomputer-like-most-of-its-type-has-familiar-hardware-and-the-linux-operating-system&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;NJIT&amp;#39;s Wulver supercomputer, like most of its type, has familiar hardware and the Linux operating system&#34; srcset=&#34;
               /blog/20250317-njit/databank-ribboncutting-8545_hu_1c6d3db0efd8d8a1.webp 400w,
               /blog/20250317-njit/databank-ribboncutting-8545_hu_6e86118ef2e89086.webp 760w,
               /blog/20250317-njit/databank-ribboncutting-8545_hu_609cf4373a8d076c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250317-njit/databank-ribboncutting-8545_hu_1c6d3db0efd8d8a1.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      NJIT&amp;rsquo;s Wulver supercomputer, like most of its type, has familiar hardware and the Linux operating system
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey Institute of Technology distinguished professor &lt;strong&gt;David Bader&lt;/strong&gt; was &lt;a href=&#34;https://www.linkedin.com/posts/dbader13_im-honored-to-receive-the-2025-heatherington-activity-7303581245014224896-mWG4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;inducted recently&lt;/a&gt; into the Mimms Museum of Technology and Art Hall of Fame, located near Atlanta.&lt;/p&gt;
&lt;p&gt;The museum committee cited Bader, who also serves as director of NJIT’s Institute for Data Science, for “revolutioniz[ing] the computing industry through groundbreaking innovations that democratized high-performance computing.” His late-1990s supercomputer design combined off-the-shelf CPUs, high-performance networking, and the open-source Linux operating system.&lt;/p&gt;
&lt;p&gt;Until that time, supercomputers ran on proprietary hardware and commercially licensed software, making them inaccessible to all but massive corporations, government agencies and the largest universities. Computer scientists who advocated for high-performance computing through off-the-shelf hardware and Linux software were widely viewed as unrealistic optimists.&lt;/p&gt;
&lt;p&gt;Following innovations such as Bader’s, supercomputer designers began adopting these designs. Today, essentially all of the world’s highest-performing supercomputers run Linux. Many also use common processors, although those are gradually being replaced by high-end GPUs.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;When I built the Linux supercomputer in 1998, many believed proprietary systems were the only path forward for high-performance computing,” said Bader. “It demonstrated that commodity hardware with the Linux operating system could deliver supercomputing performance at a fraction of the cost. What began as my passion for building commodity off-the-shelf systems ultimately transformed the supercomputing landscape — today, every machine on the Top500 list is based on this design.”&lt;/p&gt;
&lt;p&gt;“The greatest satisfaction comes from seeing how this democratization has accelerated scientific discovery across disciplines, from astrophysics to climate science to genomics, by putting powerful computational tools in the hands of researchers worldwide.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader also taught and performed research at Georgia Institute of Technology from 2005-2019, where he founded and chaired the School of Computational Science and Engineering.&lt;/p&gt;
&lt;p&gt;Bader in 2023 had his early supercomputer research recognized by the Computer History Museum, in Mountain View, California. &lt;a href=&#34;https://news.njit.edu/njits-david-bader-future-ai-silver-linings-touch-grey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Read his thoughts&lt;/a&gt; on such research and its impact on modern artificial intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njits-bader-who-helped-invent-modern-supercomputing-gets-hof-honor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njits-bader-who-helped-invent-modern-supercomputing-gets-hof-honor&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumni Update: Oliver Alvarado Rodriguez</title>
      <link>http://localhost:1313/blog/20250313-tricounty/</link>
      <pubDate>Thu, 13 Mar 2025 13:13:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20250313-tricounty/</guid>
      <description>&lt;p&gt;Big News! On March 12th, Oliver successfully defended his PhD dissertation titled &amp;ldquo;On the Design of a Framework for Large-Scale Exploratory Graph Analytics&amp;rdquo; at New Jersey Institue of Technology Ying Wu College of Computing.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dr-oliver-alvarado-rodriguez-after-successfully-defending-his-phd-thesis-at-njit-photo-posted-on-linkedin-by-dr-david-a-bader-njit&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dr. Oliver Alvarado Rodriguez after successfully defending his PhD thesis at NJIT. Photo posted on LinkedIn by Dr. David A. Bader, NJIT.&#34; srcset=&#34;
               /blog/20250313-tricounty/pic_hu_33288976b55a5885.webp 400w,
               /blog/20250313-tricounty/pic_hu_3ae14b8afc8b3d92.webp 760w,
               /blog/20250313-tricounty/pic_hu_2f99a323e58e9214.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250313-tricounty/pic_hu_33288976b55a5885.webp&#34;
               width=&#34;760&#34;
               height=&#34;572&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dr. Oliver Alvarado Rodriguez after successfully defending his PhD thesis at NJIT. Photo posted on LinkedIn by Dr. David A. Bader, NJIT.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In a post on LinkedIn, &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, Distinguished Professor and Director, Institute for Data Science, New Jersey Institute of Technology wrote, &amp;ldquo;&lt;em&gt;Through rigorous research and innovative design, Oliver developed a framework that democratizes large-scale exploratory graph analytics for both developers and users. His work has been successfully applied in diverse scientific domains including scientometrics, cybersecurity, and neuroscience.&lt;/em&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Congratulations Dr. Alvarado Rodriguez! Tri-County is very proud to have been a part of your educational journey.&lt;/p&gt;
&lt;p&gt;Oliver&amp;rsquo;s journey:&lt;/p&gt;
&lt;p&gt;Oliver and his family immigrated from Costa Rica. His father was a part-time custodian for Blessed Sacrament School in Paterson where Oliver went to elementary school with the help of Tri-County Scholarship Fund. Oliver then qualified for the Tri-County Freedom Scholarship to attend DePaul Catholic High School where he continued to excel and graduated in 2016.&lt;/p&gt;
&lt;p&gt;With a strong academic foundation from his hardwork at the excellent Tri-County partner schools that he attended, Oliver was admitted to the Honors College at William Paterson University, where he majored in computer science and minored in mathematics. While pursuing his under graduate degree, Tri-County facilitated his connection with Chubb Insurance where he was offered a summer intership in data science.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Said Oliver, “Tri-County not only gave me the opportunity to receive an excellent education that I would not have gotten out of the Paterson school system, but also the connections to land a data science internship at Chubb Insurance in college. I’m so thankful for the opportunities Tri-County afforded me and how I have been able to grow since then.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Further congratulations to Oliver on having been offered a position at Hewlett Packard. Can&amp;rsquo;t wait to see what the future holds!&lt;/p&gt;
&lt;p&gt;Watch a recent video testimony by Oliver below:&lt;/p&gt;

      &lt;div
          style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
        &lt;iframe
          src=&#34;https://player.vimeo.com/video/1027353460?dnt=0&#34;
            style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allow=&#34;fullscreen&#34;&gt;
        &lt;/iframe&gt;
      &lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tcsfund.org/post/alumni-update-oliver-alvarado-rodriguez&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tcsfund.org/post/alumni-update-oliver-alvarado-rodriguez&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Museum of America rebrands as Mimms Museum of Technology and Art</title>
      <link>http://localhost:1313/blog/20250307-artdaily/</link>
      <pubDate>Fri, 07 Mar 2025 14:45:15 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250307-artdaily/</guid>
      <description>

















&lt;figure  id=&#34;figure-popular-roswell-museum-celebrates-evolution-during-byte25-fundraiser-announcing-new-name-and-debuting-exclusive-salvador-dalí-exhibit&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Popular Roswell museum celebrates evolution during BYTE25 fundraiser, announcing new name and debuting exclusive Salvador Dalí exhibit.&#34; srcset=&#34;
               /blog/20250307-artdaily/comp-2_hu_62a4e29f8144dd71.webp 400w,
               /blog/20250307-artdaily/comp-2_hu_cfde1c9052f8fcd8.webp 760w,
               /blog/20250307-artdaily/comp-2_hu_872d40ba9022fa6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250307-artdaily/comp-2_hu_62a4e29f8144dd71.webp&#34;
               width=&#34;760&#34;
               height=&#34;435&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Popular Roswell museum celebrates evolution during BYTE25 fundraiser, announcing new name and debuting exclusive Salvador Dalí exhibit.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;ROSWELL, GA.- During its BYTE fundraiser last evening, Computer Museum of America revealed an exciting announcement – moving forward, it will operate under a new name: Mimms Museum of Technology and Art. The rebrand marks an exciting new chapter for one of the area’s most popular attractions, reflecting its history, evolution and future. As the nonprofit museum celebrates five years of operation and prepares for expansion in 2026 – including the highly anticipated launch of its iNSPIRE: Fifty Years of Innovation at Apple exhibit – the timing of its transformation is setting the stage for a bold and dynamic future.&lt;/p&gt;
&lt;p&gt;“Our museum has always been about more than just computers,” said Lonnie Mimms, founder and board chair of Mimms Museum of Technology and Art. “We are thrilled to introduce a new brand that provides us the flexibility to promote the full scope of what we offer, aligning us with STEAM initiatives that are shaping both education and technology – two industries deeply embedded in our mission.”&lt;/p&gt;
&lt;p&gt;Since opening in 2019, the museum has expanded its scope and vision, as well as its on-site offerings, programming and experiences. While technology remains its foundation, the museum’s mission has evolved beyond a singular focus. The Mimms team recognized that technology and art are deeply interconnected, with examples of Innovation Past Forward—the museum’s tagline—reflected in both disciplines.&lt;/p&gt;
&lt;p&gt;Initially, the museum featured three main exhibits, A Tribute to Apollo 11, Supercomputing, and The STEAM Timeline. Since then, it has expanded to include an interactive Retro Gaming Corner and unveiled The Enigma Machine exhibit in 2020. iNSPIRE: Fifty Years of Innovation at Apple will open in Spring 2026, spanning an impressive 20,000 square feet, and featuring the world’s largest curation of Apple products, merchandise, early prototypes and uncommon collector’s items. In addition to its permanent exhibits, the museum now features rotating pop-up exhibits, currently showcasing a Strati 3D car.&lt;/p&gt;
&lt;p&gt;Embracing the synergy between art and technology, Mimms Museum of Technology and Art took the opportunity – while introducing its new brand during the BYTE25 event – to unveil its exclusive new Salvador Dalí Exhibit. Event attendees were given a first look at the remarkable collection featuring more than 20 signed Salvador Dalí prints, which opened to the public today. The evening also featured a special Hall of Fame induction ceremony, honoring three trailblazing pioneers in the technology industry: &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, Dan Bricklin, and John Yates. As the second class of distinguished inductees, their contributions have left a lasting impact on the field.&lt;/p&gt;
&lt;p&gt;“We remain committed to our core mission of preserving and celebrating the past while embracing the limitless possibilities of the future,” said Rena Youngblood, executive director of Mimms Museum of Technology and Art. “With our new name and several new initiatives planned, we are poised to grow, inspire, and engage the community in meaningful ways, just as technology and art continue to shape our lives.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://artdaily.com/news/178881/Computer-Museum-of-America-rebrands-as-Mimms-Museum-of-Technology-and-Art&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://artdaily.com/news/178881/Computer-Museum-of-America-rebrands-as-Mimms-Museum-of-Technology-and-Art&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader receives the 2025 Heatherington Award for Technological Innovation</title>
      <link>http://localhost:1313/blog/20250306-mimms/</link>
      <pubDate>Thu, 06 Mar 2025 13:35:37 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250306-mimms/</guid>
      <description>

















&lt;figure  id=&#34;figure-congratulations-to-the-2025-hall-of-fame-inductees-at-mimms-museum-of-technology-and-art-formerly-computer-museum-of-america-executive-director-rena-youngblood-had-the-honor-of-presenting-each-person-with-their-award-at-the-byte25-annual-fundraiser-held-thursday-march-6th-thank-you-david-a-bader-phd-dan-bricklin-and-john-yates-for-joining-us-this-world-would-be-a-different-place-without-youyour-innovations-your-willingness-to-collaborate-and-your-commitment-to-paying-it-forward&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Congratulations to the 2025 Hall of Fame inductees at Mimms Museum of Technology and Art (formerly Computer Museum of America). Executive Director, Rena Youngblood had the honor of presenting each person with their award at the BYTE25 annual fundraiser held Thursday, March 6th. Thank you David A. Bader, PhD, Dan Bricklin, and John Yates for joining us. This world would be a different place without you—your innovations, your willingness to collaborate, and your commitment to paying it forward.&#34; srcset=&#34;
               /blog/20250306-mimms/Rena_Youngblood-David_Bader_hu_6707fbdaafcc00dd.webp 400w,
               /blog/20250306-mimms/Rena_Youngblood-David_Bader_hu_293f40959a16ed74.webp 760w,
               /blog/20250306-mimms/Rena_Youngblood-David_Bader_hu_6a8c19b667f3d245.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250306-mimms/Rena_Youngblood-David_Bader_hu_6707fbdaafcc00dd.webp&#34;
               width=&#34;760&#34;
               height=&#34;608&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Congratulations to the 2025 Hall of Fame inductees at Mimms Museum of Technology and Art (formerly Computer Museum of America). Executive Director, Rena Youngblood had the honor of presenting each person with their award at the BYTE25 annual fundraiser held Thursday, March 6th. Thank you David A. Bader, PhD, Dan Bricklin, and John Yates for joining us. This world would be a different place without you—your innovations, your willingness to collaborate, and your commitment to paying it forward.
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-bader-receives-the-heatherington-award-for-technological-innovation-at-the-mimms-museum-for-technology-and-art-6-march-2025&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Bader receives the Heatherington Award for Technological Innovation at the Mimms Museum for Technology and Art, 6 March 2025.&#34; srcset=&#34;
               /blog/20250306-mimms/Bader-Heatherington_hu_e708139f2aa637be.webp 400w,
               /blog/20250306-mimms/Bader-Heatherington_hu_a151178a172854ad.webp 760w,
               /blog/20250306-mimms/Bader-Heatherington_hu_e814fbec0a92ce0a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250306-mimms/Bader-Heatherington_hu_e708139f2aa637be.webp&#34;
               width=&#34;570&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Bader receives the Heatherington Award for Technological Innovation at the Mimms Museum for Technology and Art, 6 March 2025.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Revolutionized the computing industry through groundbreaking innovations that democratized High-Performance Computing. Designed the first commodity-based supercomputer and prototyped a system using off-the-shelf components and a novel high-speed interconnection network, leading to “RoadRunner,” the first Linux supercomputer for open use by the national science and engineering community. Made seminal contributions to parallel computing software and pioneered general-purpose computing on accelerators. Led the team whose work was by used IBM in the first pre-assembled and configured Linux server clusters for business. Made significant research contributions in the field of novel parallel graph algorithms. Founded and chaired the School of Computational Science and Engineering at Georgia Tech.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250306-mimms/Mimms_hu_178457c76818cd83.webp 400w,
               /blog/20250306-mimms/Mimms_hu_4fae68e98d3e3e95.webp 760w,
               /blog/20250306-mimms/Mimms_hu_1dea3d67d642196c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250306-mimms/Mimms_hu_178457c76818cd83.webp&#34;
               width=&#34;760&#34;
               height=&#34;554&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>How Open-Source Models Are Disrupting the AI Industry</title>
      <link>http://localhost:1313/blog/20250304-laexaminer/</link>
      <pubDate>Tue, 04 Mar 2025 09:49:29 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250304-laexaminer/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jon Stojan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AI is in a rapidly moving modern-day arms race as companies worldwide try to create powerful, practical, and accessible generative AI models. Competitors are generally grouped into two categories: proprietary AI models developed by tech giants with a host of resources and open-source startups with significantly less funding. Can open-source AI truly challenge big tech? How can businesses ensure security and ethical practices when data is open to so many people?&lt;/p&gt;
&lt;h2 id=&#34;the-rise-of-efficient-ai-models&#34;&gt;The Rise of Efficient AI Models&lt;/h2&gt;
&lt;p&gt;The norm in AI development has long been standardized by singular corporations investing enormous amounts of resources into computing power and data processing. Some of the biggest generative AI companies, like OpenAI, have historically required hundreds of millions of dollars to produce a viable product.&lt;/p&gt;
&lt;p&gt;Instead, recent technological advancements show that AI models can achieve competitive performance with significantly fewer resources. DeepSeek R1 was trained with just 2,000 GPUs and a $5.7 million investment—a fraction of the cost of competing US tech giants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor and Director of the Institute for Data Science at &lt;a href=&#34;https://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Jersey Institute of Technology&lt;/a&gt;, is a nationally recognized expert on high-performance computing and AI. He says, “DeepSeek’s approach demonstrates that highly capable AI models no longer require vast financial and computational resources. This shift could open the door for more innovation across industries and make AI more accessible.”&lt;/p&gt;
&lt;h2 id=&#34;open-source-ai-a-double-edged-sword&#34;&gt;Open-Source AI: A Double-Edged Sword&lt;/h2&gt;
&lt;p&gt;There are both advantages and disadvantages to consider when naming open-source AI as the ultimate AI-creation solution.&lt;/p&gt;
&lt;p&gt;Open-source AI makes development more accessible than ever. Businesses and developers who lack the resources to develop a model from scratch can turn to community-driven improvements. With open collaboration, finding bugs, redundancies, and missing features is easier when several experienced minds are working on the same project. It can also be a cost-saving option for companies looking to integrate AI without relying on expensive third-party models.&lt;/p&gt;
&lt;p&gt;On the other hand, intellectual property may become an issue when an AI model is open-source. With many hands on deck and without a coordinated supervisor, data privacy is of increasing concern. Individuals may make unauthorized modifications or maliciously use the project.  While it may seem far from reality, autonomous AI systems have the potential to transgress ethical boundaries unintentionally or operate unpredictably.&lt;/p&gt;
&lt;h2 id=&#34;the-future-of-ai-development-and-security&#34;&gt;The Future of AI Development and Security&lt;/h2&gt;
&lt;p&gt;Predictions say that training costs will likely continue to decrease over the next five years. Cheaper AI development could lead to its adoption across industries like healthcare, finance, and customer service. Rather than hiring companies to integrate a pre-made system, existing businesses will be able to create AI that accommodates their unique needs more easily.&lt;/p&gt;
&lt;p&gt;There will continue to be a debate over data security when comparing proprietary versus open-source models. Open-source AI can offer complete transparency and customization, but it may also be vulnerable to internal security threats.&lt;/p&gt;
&lt;p&gt;Proprietary models have highly controlled access and security, but they are also limited in accessibility and have higher expenses associated with them. A hybrid AI approach could balance the benefits of openness with robust protective security measures.&lt;/p&gt;
&lt;h2 id=&#34;innovation-with-open-source-ai-models&#34;&gt;Innovation With Open-Source AI Models&lt;/h2&gt;
&lt;p&gt;While developing an AI model has traditionally been exclusive to established tech giants, open-source models and lower development costs make it more accessible. While open-source development democratizes good ideas, it also introduces challenges with cyber security and ethics.&lt;/p&gt;
&lt;p&gt;Several factors must be considered in AI development before creating a universal standard. However, open-source AI is a realistic option if companies want to create high-quality models without vast access to resources. Open-source development is far from perfect, but it’s a step in the right direction toward creating a world where developers aren’t limited by funding or location.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://laexaminer.com/how-open-source-models-are-disrupting-the-ai-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://laexaminer.com/how-open-source-models-are-disrupting-the-ai-industry/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fruit Fly Research Led NJIT Scientists and Edison Teens to Better AI Habits on Supercomputers</title>
      <link>http://localhost:1313/blog/20250303-njit/</link>
      <pubDate>Mon, 03 Mar 2025 11:07:35 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250303-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-fruit-fly-image-by-wikimedia-commons-user-jean-and-fred&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Fruit fly image by Wikimedia Commons user &amp;#39;Jean and Fred&amp;#39;&#34; srcset=&#34;
               /blog/20250303-njit/In_the_garden_%285039054744%29_hu_3f15db23d7d8f1ff.webp 400w,
               /blog/20250303-njit/In_the_garden_%285039054744%29_hu_763306ba0662f50b.webp 760w,
               /blog/20250303-njit/In_the_garden_%285039054744%29_hu_848b6ef02f6e2c97.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250303-njit/In_the_garden_%285039054744%29_hu_3f15db23d7d8f1ff.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Fruit fly image by Wikimedia Commons user &amp;lsquo;Jean and Fred&amp;rsquo;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;A competition to compare the brains of male vs. female fruit flies led New Jersey Institute of Technology researchers and two high school students to a second-place finish in the latest edition of the FlyWire Data Challenge, but more importantly the team learned new lessons about using artificial intelligence for setting up research with supercomputers.&lt;/p&gt;
&lt;p&gt;In the challenge, hosted earlier this year by Princeton University’s Neuroscience Institute, teams had to create efficient maps of the 19,000 synapse connections between neurons in tiny insect brains. The maps take the form of graphs, and the information they convey represents part of the fly brain called a ventral nerve cord connectome.&lt;/p&gt;
&lt;p&gt;Distinguished professor David Bader, who directs NJIT’s Institute for Data Science, led NJIT Principal Research Scientist Zhihui Du along with Edison Magnet School students Srijith Chinthalapudi and Harinarayan Asoori Sriram.&lt;/p&gt;
&lt;p&gt;The team decided to create algorithms that could run on &lt;a href=&#34;https://news.njit.edu/njit-and-databank-launch-high-performance-computing-environment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT’s Wulver supercomputer&lt;/a&gt;. To start, they selected Anthropic Claude as their AI system. Claude is a research-focused alternative to generic systems such as OpenAI’s ChatGPT.&lt;/p&gt;
&lt;p&gt;“The massive scale of matching two 19,000-node graphs required efficient parallel processing,” Bader explained. “Using Claude 3.5 Sonnet for rapid prototyping helped us quickly iterate through different matching strategies, but we needed to ensure the generated code would scale effectively on Wulver, including multiple parallel programming paradigms like multicore CPU, GPU acceleration and message passing. The challenge wasn&amp;rsquo;t just computational — we needed to maintain solution quality, while handling the complexity of weighted directed edges representing synaptic connections.”&lt;/p&gt;
&lt;p&gt;In using AI, “It served as an evaluation engine for new ideas. Claude could effectively evaluate proposed approaches against nearly every published paper in real-time, compressing what would typically be weeks of literature review into minutes of interactive discussion,” Bader observed.&lt;/p&gt;
&lt;p&gt;“Claude excelled at transforming these validated ideas into high-performance code with remarkable speed and accuracy. Rather than spending hours implementing each new approach, we could quickly generate optimized, HPC-ready code for multiple algorithmic variations.”&lt;/p&gt;
&lt;p&gt;“This dramatic acceleration of the prototyping cycle allowed for much more rapid iteration and experimentation than traditional development approaches. The combination of these capabilities created a powerful new research workflow: generate new algorithmic ideas, have them instantly evaluated against existing literature, get them implemented in efficient code, and rapidly test and iterate. This suggests a new paradigm for computational research where AI serves not merely as a coding assistant but as a comprehensive research partner.”&lt;/p&gt;
&lt;p&gt;The broader lesson is that researchers and students of any age should evaluate AI tools based on how the tools are tailored to technical requirements, not just on popularity or general capabilities, Bader noted.&lt;/p&gt;
&lt;p&gt;In 2023 the team participated in &lt;a href=&#34;https://news.njit.edu/njit-research-team-innovates-princeton-flywire-codex-challenge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a single-brain version&lt;/a&gt; of the competition using consumer-grade AI. They’ll learn about their next challenge on March 6 at the FlyWire awards ceremony.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/fruit-fly-research-led-njit-scientists-and-edison-teens-better-ai-habits-supercomputers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/fruit-fly-research-led-njit-scientists-and-edison-teens-better-ai-habits-supercomputers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Museum of America Announces 2025 Hall of Fame Inductees</title>
      <link>http://localhost:1313/blog/20250304-cmoa/</link>
      <pubDate>Tue, 04 Feb 2025 21:59:56 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250304-cmoa/</guid>
      <description>

















&lt;figure  id=&#34;figure-david-bader-at-the-hall-of-fame-ceremony-6-march-2025&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;20250306.jpg&#34; alt=&#34;David Bader at the Hall of Fame ceremony, 6 March 2025.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader at the Hall of Fame ceremony, 6 March 2025.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;ROSWELL, Ga. – (February 4, 2025) – &lt;a href=&#34;https://www.computermuseumofamerica.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer Museum of America (CMoA)&lt;/a&gt;, a metro Atlanta attraction featuring one of the world’s largest collections of digital-age artifacts, today announced the induction of three new distinct members to its Hall of Fame. The trailblazing pioneers will be honored on March 6 during BYTE25, the museum’s largest fundraiser of the year.&lt;/p&gt;
&lt;p&gt;“We have taken great pride in expanding the legacy of these innovators by adding them to the Computer Museum of America Hall of Fame,” said Lonnie Mimms, founder and board chair of CMoA. “These luminaries have left an indelible mark on the tech industry, shaping its trajectory for generations to come.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The 2025 Computer Museum of America Hall of Fame Inductees include:&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;dr-david-bader&#34;&gt;Dr. David Bader&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250304-cmoa/Bader-2022-square-300x300_hu_89cf446392c98e72.webp 400w,
               /blog/20250304-cmoa/Bader-2022-square-300x300_hu_66d2f7e590786c1.webp 760w,
               /blog/20250304-cmoa/Bader-2022-square-300x300_hu_c7a13f71bf2feb82.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250304-cmoa/Bader-2022-square-300x300_hu_89cf446392c98e72.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dr. David Bader revolutionized High-Performance Computing (HPC) supercomputing technology. Through his pioneering work in designing the first commodity-based supercomputer, Bader transformed the industry by drastically reducing costs while maintaining performance. His innovations paved the way for Linux-based supercomputers, now the global standard, generating over $100 trillion in economic impact. His contributions to hardware, software and algorithms have reshaped modern computing infrastructure, earning him the IEEE Sidney Fernbach Award and a place in the University of Maryland’s Innovation Hall of Fame.&lt;/p&gt;
&lt;h2 id=&#34;dan-bricklin&#34;&gt;Dan Bricklin&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250304-cmoa/Dan-Bricklin-Head-Shot-266x300_hu_201382663b4ed1b0.webp 400w,
               /blog/20250304-cmoa/Dan-Bricklin-Head-Shot-266x300_hu_be4e8747df91790a.webp 760w,
               /blog/20250304-cmoa/Dan-Bricklin-Head-Shot-266x300_hu_f1c065ff30b6897b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250304-cmoa/Dan-Bricklin-Head-Shot-266x300_hu_201382663b4ed1b0.webp&#34;
               width=&#34;266&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Known as the “Father of the Spreadsheet,” Dan Bricklin’s invention of VisiCalc in 1979 changed the face of personal computing. This groundbreaking software empowered businesses and individuals, establishing the spreadsheet as a cornerstone of modern productivity. Bricklin’s visionary contributions have been recognized with prestigious honors, including the Grace Murray Hopper Award and the Computer History Museum Fellow Awards. His work laid the foundation for countless innovations in data analysis and business management.&lt;/p&gt;
&lt;h2 id=&#34;john-yates&#34;&gt;John Yates&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250304-cmoa/John-Yates-Headshot-300x300_hu_e82b1766d29d6bf.webp 400w,
               /blog/20250304-cmoa/John-Yates-Headshot-300x300_hu_d396826377532434.webp 760w,
               /blog/20250304-cmoa/John-Yates-Headshot-300x300_hu_580de15abbb3b01f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250304-cmoa/John-Yates-Headshot-300x300_hu_e82b1766d29d6bf.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;An internationally recognized leader in technology law, John Yates has dedicated nearly 40 years to advancing the legal and business frameworks for technology, e-commerce and internet law. As co-chairman of Morris, Manning &amp;amp; Martin LLP and founder of its Technology Group, Yates has been instrumental in fostering Atlanta’s thriving tech ecosystem. He co-founded influential organizations such as the Southeastern Software Association, Technology Association of Georgia (TAG) and Southeastern Medical Device Association. His contributions have earned him accolades, including the first-ever “Leader of Influence Award” by TAG and the creation of the United Way’s John Yates Award for Community Leadership. John recently joined Gunderson Dettmer as a Partner in Corporate Technology.&lt;/p&gt;
&lt;p&gt;Taking place at CMoA in Roswell, BYTE25 will celebrate the achievements of these Hall of Fame inductees and raise support for the museum’s mission to preserve the history of computing. Attendees will enjoy an evening of music, the unveiling of new exhibits and artifacts, specialty ‘bytes,’ an auction and the induction ceremony where all three award winners are expected to attend.&lt;/p&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h3 id=&#34;about-computer-museum-of-america&#34;&gt;About Computer Museum of America&lt;/h3&gt;
&lt;p&gt;Computer Museum of America (CMoA) is a dedicated nonprofit with the mission of preserving and showcasing the history of technology and its impact on our society. Through interactive experiences, educational programs, and engaging events, CMoA provides visitors with a unique opportunity to explore the evolution of technology and its profound influence on our lives.&lt;/p&gt;
&lt;h3 id=&#34;media-contact&#34;&gt;Media Contact:&lt;/h3&gt;
&lt;p&gt;Kendall Bagley, Hemsworth&lt;br&gt;
812-483-7717 or &lt;a href=&#34;mailto:CMoA@HemsworthCommunications.com&#34;&gt;CMoA@HemsworthCommunications.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mimmsmuseum.org/news/cmoa-2025-hall-of-fame/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mimmsmuseum.org/news/cmoa-2025-hall-of-fame/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hemsworthcommunications.com/computer-museum-of-america-announces-2025-hall-of-fame-inductees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hemsworthcommunications.com/computer-museum-of-america-announces-2025-hall-of-fame-inductees/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computermuseumofamerica.org/news/computer-museum-of-america-announces-2025-hall-of-fame-inductees/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computermuseumofamerica.org/news/computer-museum-of-america-announces-2025-hall-of-fame-inductees/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Museum of America, 2025 Hall of Fame Inductees</title>
      <link>http://localhost:1313/blog/20250204-cmoa2/</link>
      <pubDate>Tue, 04 Feb 2025 15:28:30 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250204-cmoa2/</guid>
      <description>&lt;p&gt;Meet our 2025 Hall of Fame Inductees! &amp;#x2728;&lt;/p&gt;
&lt;p&gt;We’re thrilled to announce that three trailblazing leaders in technology will be honored at BYTE25 on March 6. These pioneers have shaped the tech industry and their legacy continues to inspire.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;#x1f3c6; &lt;strong&gt;&lt;a href=&#34;https://www.linkedin.com/in/dbader13/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BDOIyh%2BxEQ8mT0rXhAo6fTw%3D%3D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David A. Bader, PhD&lt;/a&gt;&lt;/strong&gt; – Revolutionized High-Performance Computing. Nearly all supercomputers being used today, can trace their lineage directly to Dr. Bader’s pioneering work.&lt;/li&gt;
&lt;li&gt;&amp;#x1f3c6; &lt;a href=&#34;https://www.linkedin.com/in/dan-bricklin-1b99/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BDOIyh%2BxEQ8mT0rXhAo6fTw%3D%3D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dan Bricklin&lt;/a&gt; – Referred to as the &amp;ldquo;Father of the Spreadsheet.&amp;rdquo; He was co-creator of VisiCalc, the first spreadsheet program for personal computers.&lt;/li&gt;
&lt;li&gt;&amp;#x1f3c6; &lt;a href=&#34;https://www.linkedin.com/in/johncyates/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BDOIyh%2BxEQ8mT0rXhAo6fTw%3D%3D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Yates&lt;/a&gt; – A leader in technology law and in shaping Atlanta’s thriving tech ecosystem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Join us as we celebrate their incredible achievements and raise support for the future of tech history by purchasing a ticket to BYTE25 at CMoA! :party_popper: [Iink: https://lnkd.in/emBt5JsF]&lt;/p&gt;
&lt;p&gt;#BYTE25 #Tech #ComputerMuseumOfAmerica&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20250204-cmoa2/1738691781209_hu_88e88fd8e0e718d9.webp 400w,
               /blog/20250204-cmoa2/1738691781209_hu_324627b4fdb84ce2.webp 760w,
               /blog/20250204-cmoa2/1738691781209_hu_991375348f015997.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250204-cmoa2/1738691781209_hu_88e88fd8e0e718d9.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/posts/computer-museum-of-america_byte25-tech-computermuseumofamerica-activity-7292601898379927553-tldb/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/posts/computer-museum-of-america_byte25-tech-computermuseumofamerica-activity-7292601898379927553-tldb/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepSeek has called into question Big AI’s trillion-dollar assumption</title>
      <link>http://localhost:1313/blog/20250128-fastcompany/</link>
      <pubDate>Tue, 28 Jan 2025 09:40:23 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250128-fastcompany/</guid>
      <description>

















&lt;figure  id=&#34;figure-photo-patrick-pleulpicture-alliance-via-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*[Photo: Patrick Pleul/picture alliance via Getty Images]*&#34; srcset=&#34;
               /blog/20250128-fastcompany/p-1-91268664-deepseek-has-called-into-question-big-ais-trillion-dollar-assumption_hu_2c6f342aa5cfaf03.webp 400w,
               /blog/20250128-fastcompany/p-1-91268664-deepseek-has-called-into-question-big-ais-trillion-dollar-assumption_hu_b14e490dce8ea224.webp 760w,
               /blog/20250128-fastcompany/p-1-91268664-deepseek-has-called-into-question-big-ais-trillion-dollar-assumption_hu_bb3613a53855dac8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250128-fastcompany/p-1-91268664-deepseek-has-called-into-question-big-ais-trillion-dollar-assumption_hu_2c6f342aa5cfaf03.webp&#34;
               width=&#34;750&#34;
               height=&#34;422&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;[Photo: Patrick Pleul/picture alliance via Getty Images]&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Mark Sullivan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Recently, Chinese startup &lt;a href=&#34;https://www.fastcompany.com/91267354/deepseek-explained-china-llm-chatgpt-nvidia-microsoft-stock-ai-rattled&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSeek&lt;/a&gt; created state-of-the art AI models using far less computing power and capital than anyone thought possible. It then showed its work in published research papers and by allowing its models to explain the reasoning process that led to this answer or that. It also scored at or near the top in a range of benchmark tests, &lt;a href=&#34;https://techcrunch.com/2025/01/27/deepseek-claims-its-reasoning-model-beats-openais-o1-on-certain-benchmarks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;besting OpenAI models&lt;/a&gt; in several skill areas. The surprising work seems to have let some of the air out of the AI industry’s main assumption—that the best way to make models smarter is by giving them more computing power, so that the AI lab with the most Nvidia chips will have the best models and shortest route to artificial general intelligence (AGI—which refers to AI that’s better than humans at most tasks).&lt;/p&gt;
&lt;p&gt;No wonder some Nvidia investors are questioning their faith in the unlimited demand for the most powerful AI chips in the future. And no wonder some in AI circles are questioning the world view and business strategy of OpenAI CEO Sam Altman, the biggest evangelist for the “brute force” approach to ever-smarter models.&lt;/p&gt;
&lt;p&gt;“The assumption behind all this investment is theoretical . . . the so-called scaling laws where when you double compute, the quality of your models increases in kind of the same way—it’s kind of a new Moore’s Law,” says Abhishek Nagaraj, a professor at the University of California–Berkeley’s Haas business school. (Moore’s Law said that software developers could expect microchips to become predictably more powerful as chipmakers packed more transistors into their microchips.)&lt;/p&gt;
&lt;p&gt;“And so if that holds, it effectively means that whoever controls the infrastructure will control a lot of the market,” adds Nagaraj. That’s why companies like OpenAI, Anthropic, and X are building data centers as fast as they can. OpenAI CEO Sam Altman last year said he &lt;a href=&#34;https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;needs to raise $7 trillion&lt;/a&gt; to build the data centers needed to reach AGI. OpenAI, Microsoft, Softbank, and Oracle &lt;a href=&#34;https://www.fastcompany.com/91265565/the-oval-office-stargate-project-reveal-was-just-more-tech-industry-genuflecting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said recently&lt;/a&gt; they’ll spend up to $500 billion over the next five years to build new data centers for AI in Texas.&lt;/p&gt;
&lt;p&gt;Attracting the money to do that, however, is something only “closed-source” companies like OpenAI can do, Nagaraj points out. OpenAI’s private equity backers (such as Andreessen Horowitz) and big tech backers (such as Microsoft) are willing to bankroll the AI infrastructure (chips, software, data centers, electricity), which OpenAI says it needs, if it keeps the recipes of its models secret. That’s the “moat” around their investment, after all. Establishing such a moat was the main reason OpenAI stopped being an “open” AI company back in 2019.&lt;/p&gt;
&lt;p&gt;DeepSeek shares the weights of its models (the mathematical calculations at each connection point in their neural networks) and allows any developer to build with them. After essentially giving away its research and eschewing a moat, DeepSeek was never going to attract the private equity funding needed to bankroll hundreds of thousands of Nvidia chips. Adding to its challenge were the &lt;a href=&#34;https://www.fastcompany.com/91267968/how-the-biden-chip-bans-created-a-monster-called-deepseek&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;U.S. chip bans&lt;/a&gt; that reserved the most powerful AI chips for U.S. companies. So DeepSeek found ways to build state-of-the-art models using far less computing power. In doing so, it appears to have collapsed Altman’s assumption that massive computing power is the only route to AGI.&lt;/p&gt;
&lt;p&gt;Not everybody thinks so, of course. Particularly in OpenAI circles. “I would never bet against compute as the upper bound for achievable intelligence in the long run,” says &lt;a href=&#34;https://x.com/karpathy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy&lt;/a&gt;, one of the original founders of OpenAI, in an X post. “Not just for an individual final training run, but also for the entire innovation/experimentation engine that silently underlies all the algorithmic innovations.”&lt;/p&gt;
&lt;p&gt;Altman, too, seemed undeterred. “We will obviously deliver much better models and also it’s legit invigorating to have a new competitor! We will pull up some releases . . . ,” he posted breezily on X. “But mostly we are excited to continue to execute on our research roadmap and believe more compute is more important now than ever before to succeed at our mission.” OpenAI’s “mission” is AGI.&lt;/p&gt;
&lt;p&gt;Lots of powerful chips will be needed, if only because the general demand for AI services is going to grow exponentially. More data centers will be needed just to respond to calls from millions of AI-infused apps built on OpenAI APIs, &lt;a href=&#34;https://x.com/sama/status/1884066338739830835&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;he added&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some have suggested that DeepSeek’s discovery of ways to build more compute-efficient advanced AI models could reduce the barrier to entry and allow far more developers to build such models of their own, therefore pushing up demand for AI chips.&lt;/p&gt;
&lt;p&gt;For example, DeepSeek’s most recent model, &lt;a href=&#34;https://go.skimresources.com/?id=122276X1583643&amp;amp;isjs=1&amp;amp;jv=15.7.1&amp;amp;sref=https%3A%2F%2Fwww.fastcompany.com%2F91268664%2Fdeepseek-called-into-question-big-ai-trillion-dollar-assumption-openai&amp;amp;url=https%3A%2F%2Farxiv.org%2Fhtml%2F2501.12948v1&amp;amp;xs=1&amp;amp;xtz=300&amp;amp;xuuid=a5ba779709559c07c9859f943db2add5&amp;amp;xjsf=other_click__auxclick%20%5B2%5D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSeek-R1&lt;/a&gt;, provided the open-source world with a reasoning model that appears to be comparable to OpenAI’s state-of-the-art &lt;a href=&#34;https://www.fastcompany.com/91189817/openais-new-o1-models-push-ai-to-phd-level-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;o1 series&lt;/a&gt;, which applies more computing power at inference time, when the model is reasoning through various routes to a good answer. In a statement Monday, Nvidia gives DeepSeek props for creating reasoning models using “widely available” Nvidia GPUs, and adds that such models require “significant numbers” of the GPUs as well as fast chip-to-chip networking technology.&lt;/p&gt;
&lt;p&gt;The latest DeepSeek models have only been available to developers for a short time. Just like when Meta introduced its open-source Llama models, it will take some time to understand the real economics of building new models and apps based on the DeepSeek models. It’s possible that more widely distributing the ability to build cutting edge models could put more brains to work on finding novel routes to AGI and, later, superintelligence. That’s the good news. The bad news may be that powerful models, and the means to build them, will become more available to people who might use them maliciously, or who may not be fastidious about using accepted safety guardrails.&lt;/p&gt;
&lt;p&gt;But DeepSeek is not perfect. The DeepSeek chatbot has in anecdotal cases emphatically misidentified itself as the creation of &lt;a href=&#34;https://x.com/susheel_c/status/1884340451060375838&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&lt;/a&gt; or &lt;a href=&#34;https://www.fastcompany.com/91267647/deepseek-told-me-made-by-microsoft-r1-openai-claude-anthropic-ai-model-copilot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft&lt;/a&gt;. Nor can the chatbot speak freely on all subjects. “Like all Chinese AI companies, DeepSeek operates within the People’s Republic of China’s regulatory framework, which includes restrictions on how language models handle politically sensitive topics,” says &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the New Jersey Institute of Technology. “These constraints are evident in how their models respond to queries about historical events and government policies.” If you ask the chatbot about the &lt;a href=&#34;https://go.skimresources.com/?id=122276X1583643&amp;amp;isjs=1&amp;amp;jv=15.7.1&amp;amp;sref=https%3A%2F%2Fwww.fastcompany.com%2F91268664%2Fdeepseek-called-into-question-big-ai-trillion-dollar-assumption-openai&amp;amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2F1989_Tiananmen_Square_protests_and_massacre&amp;amp;xs=1&amp;amp;xtz=300&amp;amp;xuuid=a5ba779709559c07c9859f943db2add5&amp;amp;xjsf=other_click__auxclick%20%5B2%5D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tiananmen Square protests&lt;/a&gt;, for example, it &lt;a href=&#34;https://x.com/peterbrodersen/status/1883824132590338466&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;responds&lt;/a&gt; with, “Let’s talk about something else.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.fastcompany.com/91268664/deepseek-called-into-question-big-ai-trillion-dollar-assumption-openai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.fastcompany.com/91268664/deepseek-called-into-question-big-ai-trillion-dollar-assumption-openai&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepSeek’s rise spooks investors, threatens to upend AI</title>
      <link>http://localhost:1313/blog/20250127-thehill/</link>
      <pubDate>Mon, 27 Jan 2025 10:35:27 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250127-thehill/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Julia Shapero and Miranda Nazzaro&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The rise in popularity of a high-performing and cheaply built Chinese artificial intelligence (AI) model has shaken the confidence of investors, while raising larger questions about the future of American-made AI and upping the stakes of Washington’s tech rivalry with Beijing.&lt;/p&gt;
&lt;p&gt;DeepSeek’s new AI model has taken the internet by storm, sparking a significant sell-off in the tech sector as investors fear the billions of dollars U.S. firms have invested into AI infrastructure may be unnecessary.&lt;/p&gt;
&lt;p&gt;“It upends the way that investors have thought about how AI needed to be developed and implemented,” said Steve Sosnick, chief strategist at Interactive Brokers, who suggested the industry might be at an “inflection point.”&lt;/p&gt;
&lt;p&gt;After launching its latest AI model, R1, last week, DeepSeek surged to the top of Apple’s App Store over the weekend. The application was the No. 1 free app on the store on Monday, while ChatGPT-maker OpenAI sat at the No. 2 spot.&lt;/p&gt;
&lt;p&gt;DeepSeek &lt;a href=&#34;https://x.com/deepseek_ai/status/1881318130334814301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;claims its R1 open-source reasoning&lt;/a&gt; model has a “performance on par with” OpenAI, which is regarded as one of the U.S.’s leading AI reasoning models.&lt;/p&gt;
&lt;p&gt;The company, founded in May 2023, claims to have spent just $5.6 million to train its latest models, The Wall Street Journal first reported.&lt;/p&gt;
&lt;p&gt;The price tag pales compared to that incurred by leading U.S. AI firms like OpenAI, Meta and Google, all of which have spent billions of dollars in recent years on AI infrastructure and development of large language models.&lt;/p&gt;
&lt;p&gt;A significant chunk of AI developers’ expenses in the U.S. goes to infrastructure, including the data centers and chips used to power the AI training process.&lt;/p&gt;
&lt;p&gt;Now, DeepSeek is disrupting the market and showing how AI can be developed at a fraction of the price, while allegedly not relying on the vast data sets, chips and infrastructure thought to be the holy grail of AI development.&lt;/p&gt;
&lt;p&gt;“Let’s say the convention wisdom as of yesterday, when you look at the AI market, is that essentially the best models come directly from those who have the deepest and broadest data sets and those that have the most brute force processing power behind them,” said Kenneth Lamont, a senior researcher at Morningstar, a financial services company.&lt;/p&gt;
&lt;p&gt;“There was assumed to be a natural monopoly where if you’re the biggest player, that’s why we saw this massive concentration of these companies and they were just seen to be the clear winners in this space,” Lamont said. “China found a way to not rely on the chips.”&lt;/p&gt;
&lt;p&gt;Nvidia, a leading producer of the chips behind the AI boom, saw its stock price plummet nearly 17 percent by market close Monday, shedding almost $600 billion in value. The semiconductor firm Broadcom similarly dipped 17.4 percent.&lt;/p&gt;
&lt;p&gt;Taiwan Semiconductor Manufacturing Company’s stock sank 13.3 percent over the course of the day, while Arm was down 10.2 percent and ASML Holding’s share price dipped 5.8 percent.&lt;/p&gt;
&lt;p&gt;Tech firms that have heavily invested in AI also took a hit in Monday’s sell-off. Shares in Oracle, one of three companies leading a new Trump administration project to invest in AI infrastructure, tumbled 13.8 percent.&lt;/p&gt;
&lt;p&gt;Microsoft also dipped 2.1 percent, while Alphabet, Google’s parent company, was down 4.2 percent.&lt;/p&gt;
&lt;p&gt;Callie Cox, chief market strategist at Ritholtz Wealth Management, noted that Monday’s market reaction was likely in part driven by the fact that the industry was “ripe for a sell-off.”&lt;/p&gt;
&lt;p&gt;“Tech has done incredibly well and that means that the bar for excellence is incredibly high, and any one skeptical headline can really knock the sector off its axis,” Cox told The Hill.&lt;/p&gt;
&lt;p&gt;With a disrupted narrative, Lamont warned that U.S. AI companies will now be facing increased pressure to justify their high expenditures.&lt;/p&gt;
&lt;p&gt;“It really puts a huge pressure on them to justify their fees, whether that be they make better products — I’m sure that’s what they’re trying to do anyway — but also almost certainly to lower their fees to compete with this,” Lamont said.&lt;/p&gt;
&lt;p&gt;OpenAI offers models at &lt;a href=&#34;https://openai.com/chatgpt/pricing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;different price points&lt;/a&gt;, including a free version and “plus” and “pro” plans, which cost $20 to $200 a month, respectively. &lt;a href=&#34;https://one.google.com/about/ai-premium/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google’s Gemini AI model&lt;/a&gt; can cost $0 a month with a Google account, but its premium plan costs $20 a month.&lt;/p&gt;
&lt;p&gt;The surge in the China-based app comes nearly a week after President Trump was sworn back into office. On his second day, the president announced a joint investment of up to $500 billion to build the infrastructure needed to power AI over the next four years.&lt;/p&gt;
&lt;p&gt;OpenAI is an initial investor in the venture, along with Oracle and SoftBank.&lt;/p&gt;
&lt;p&gt;However, DeepSeek’s development could indicate that AI does not require this level of hardware investment, Sosnick noted.&lt;/p&gt;
&lt;p&gt;“All of a sudden, there’s this upstart that threatens to do it more cheaply and more efficiently than what we thought,” he said.&lt;/p&gt;
&lt;p&gt;DeepSeek &lt;a href=&#34;https://time.com/7210296/chinese-ai-company-deepseek-stuns-american-ai-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reportedly claimed&lt;/a&gt; last year it had limited access to chips and used just 2,000 second-tier Nvidia chips to train its models v3 and R1.&lt;/p&gt;
&lt;p&gt;Some AI business leaders have cast doubt about the company’s claims.&lt;/p&gt;
&lt;p&gt;Scale AI CEO Alexandr Wang on Monday &lt;a href=&#34;https://x.com/unusual_whales/status/1883946417133383797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;claimed DeepSeek&lt;/a&gt; has about 50,000 H100 graphic processing units (GPU), which can be used for AI development, but cannot discuss them because of the U.S. chip export controls in place.&lt;/p&gt;
&lt;p&gt;When asked about DeepSeek, an Nvidia spokesperson told The Hill the Chinese company is “an excellent AI advancement and a perfect example of Test time scaling.”&lt;/p&gt;
&lt;p&gt;“DeepSeek’s work illustrates how new models can be created using that technique, leveraging widely-available models and compute that is fully export control compliant,” the spokesperson said. “Inference requires significant numbers of NVIDIA GPUs and high-performance networking. We now have three scaling laws: pre-training and post-training, which continue, and new test-time scaling.”&lt;/p&gt;
&lt;p&gt;The Hill reached out to DeepSeek for further comment.&lt;/p&gt;
&lt;p&gt;Up until its final days, the Biden administration aggressively tried to curtail China’s advancements in chipmaking and AI through export controls on some semiconductor chips and equipment.&lt;/p&gt;
&lt;p&gt;When asked about DeepSeek’s surge Monday, the Trump White House emphasized President Trump’s commitment to leading on AI and laid the recent advancements by China at the feet of the previous administration.&lt;/p&gt;
&lt;p&gt;“By stifling innovation at home and failing to cut off China’s access to American technology, President Biden created an opportunity for our foreign adversaries to make gains in AI development,” a White House Office of Science and Technology Policy spokesperson said in a statement.&lt;/p&gt;
&lt;p&gt;Trump repealed former President Biden’s AI executive order last week, stating it “established unnecessarily burdensome requirements” for AI developers.&lt;/p&gt;
&lt;p&gt;He later &lt;a href=&#34;https://apnews.com/article/trump-ai-artificial-intelligence-executive-order-eef1e5b9bec861eaf9b36217d547929c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;signed a new order&lt;/a&gt; to ensure AI development is “free from ideological bias,” and called for an AI development action plan within 180 days.&lt;/p&gt;
&lt;p&gt;“DeepSeek R1 shows that the AI race will be very competitive and that President Trump was right to rescind the Biden EO, which hamstrung American AI companies without asking whether China would do the same. (Obviously not.) I’m confident in the U.S. but we can’t be complacent,” David Sacks, White House AI and crypto czar, said in a post on the social platform X.&lt;/p&gt;
&lt;p&gt;DeepSeek’s rise comes as the Trump administration weighs the future of TikTok amid a looming government ban.&lt;/p&gt;
&lt;p&gt;After a brief shutdown earlier this month, TikTok came back online following assurances from Trump that he would not enforce a law that required the app’s China-based parent company ByteDance to &lt;a href=&#34;https://thehill.com/policy/technology/5101056-tiktok-buyers-trump-china/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;divest by Jan. 19&lt;/a&gt; or face a ban amid national security concerns.&lt;/p&gt;
&lt;p&gt;The Chinese government has tried to distance itself from ByteDance’s business decisions, though some observers still question how close the two are intertwined.&lt;/p&gt;
&lt;p&gt;“In China, the distance between the state and the private world is much more blurred than it is elsewhere,” Lamont remarked.&lt;/p&gt;
&lt;p&gt;DeepSeek’s arrival in the AI scene “really puts the U.S. leadership in doubt in such a surprising way,” said &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;“We’re in this incredible period of competition between the U.S. and China,” he said. “Here, we see a great example of China leapfrogging the U.S. based on their nimble, their agile development of this DeepSeek AI model, and I think this is just the start of what’s to come.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thehill.com/policy/technology/5109502-us-ai-rival-china-deepseek-rise/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thehill.com/policy/technology/5109502-us-ai-rival-china-deepseek-rise/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-us/money/markets/deepseek-s-rise-spooks-investors-threatens-to-upend-ai/ar-AA1xY0dr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-us/money/markets/deepseek-s-rise-spooks-investors-threatens-to-upend-ai/ar-AA1xY0dr&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Oval Office ‘Stargate Project’ reveal was just more tech industry genuflecting</title>
      <link>http://localhost:1313/blog/20250123-fastcompany/</link>
      <pubDate>Thu, 23 Jan 2025 16:28:58 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250123-fastcompany/</guid>
      <description>

















&lt;figure  id=&#34;figure-photo-jim-watsonafp-via-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*Photo: JIM WATSON/AFP via Getty Images*&#34; srcset=&#34;
               /blog/20250123-fastcompany/p-1-91265565-decoded-trump-crypto_hu_53e92720e2463bb3.webp 400w,
               /blog/20250123-fastcompany/p-1-91265565-decoded-trump-crypto_hu_96d5e1a1a49b4b5.webp 760w,
               /blog/20250123-fastcompany/p-1-91265565-decoded-trump-crypto_hu_98f8098c0728c9eb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250123-fastcompany/p-1-91265565-decoded-trump-crypto_hu_53e92720e2463bb3.webp&#34;
               width=&#34;750&#34;
               height=&#34;422&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Photo: JIM WATSON/AFP via Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Mark Sullivan&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;trumps-stargate-project-reveal-had-little-to-do-with-reality&#34;&gt;Trump’s ‘Stargate Project’ reveal had little to do with reality&lt;/h2&gt;
&lt;p&gt;“Stargate” sounds like a movie, and where Donald Trump is concerned it is indeed mainly a performance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://openai.com/index/announcing-the-stargate-project/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stargate&lt;/a&gt;, a new joint venture formed by OpenAI, Oracle, and SoftBank, aims to build up to $500 billion worth of new data centers over the next four years to run large AI models. OpenAI’s Sam Altman, SoftBank’s Masayoshi Son, and Oracle CEO Larry Elder joined Trump in the Oval Office Tuesday to outline the new project. Trump took to the podium to hitch his administration to the job creation and geopolitical rewards of the current AI boom.&lt;/p&gt;
&lt;p&gt;The announcement in the Oval was choreographed to look like a public-private partnership, but it’s not. Trump pledged no federal federal funding or tax breaks; he only talked vaguely about using “emergency declarations” to smooth Stargate’s way. (In theory the White House could work with Texas lawmakers to bypass regulatory land-use hurdles, etc.).&lt;/p&gt;
&lt;p&gt;“This is very much a private project—it’s no different than a big company deciding to [build] their factory in the U.S., and so that’s good for America in the sense that it creates jobs,” says Abhishek Nagaraj, a professor at the University of California–Berkeley’s Haas business school. “This isn’t necessarily going to do all the things that we’ve been asking the government to do in terms of supporting AI innovation, research, etc. within the U.S.”&lt;/p&gt;
&lt;p&gt;The initiative was already well underway before Trump’s second term began. Altman has been talking for months about raising trillions to build more data centers for his company’s next generation of compute-hungry models. OpenAI and Oracle had already been working together on those data centers, and have in fact broken ground on 10 of them, Oracle’s Ellison said. SoftBank has been trying to invest in OpenAI since 2023, finally getting its chance last November when it &lt;a href=&#34;https://go.skimresources.com/?id=122276X1583643&amp;amp;isjs=1&amp;amp;jv=15.7.1&amp;amp;sref=https%3A%2F%2Fwww.fastcompany.com%2F91265565%2Fthe-oval-office-stargate-project-reveal-was-just-more-tech-industry-genuflecting&amp;amp;url=https%3A%2F%2Fwww.cnbc.com%2F2024%2F11%2F26%2Fopenai-gets-1point5-billion-investment-from-softbank-in-tender-offer.html&amp;amp;xs=1&amp;amp;xtz=300&amp;amp;xuuid=0c11ee256c76822407a3e8a7a37e258d&amp;amp;xjsf=other_click__auxclick%20%5B2%5D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;put $1.3 billion&lt;/a&gt; into the company. Son pledged to invest $100 billion back in December.&lt;/p&gt;
&lt;p&gt;So the Stargate announcement now looks like another example of U.S. tech leaders hurrying to curry favor with Trump, in this case by offering him a chance at some unearned political points. That’s not to say the demand for new data centers isn’t real. “I generally think that there needs to be (and will be) a massive infrastructure buildout to really ‘enable’ AI to reach its potential,” says Anyscale cofounder &lt;a href=&#34;https://www.fastcompany.com/91244222/how-anyscale-cofounder-robert-nishihara-is-fixing-ai-infrastructure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robert Nishihara&lt;/a&gt; in an email. “This has been true of every major technology wave—internet, cloud, mobile—we’re just at the start.”&lt;/p&gt;
&lt;p&gt;And yet the hardware may play a unique role in this new tech wave compared to previous ones. The generative AI boom owes a lot to the availability of powerful GPUs. The game-changing intelligence gains that led to ChatGPT happened mostly because researchers discovered ways of &lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;applying more computing power&lt;/a&gt; to large transformer models. With enough GPUs, something remarkable happens (researchers don’t know exactly what) deep in the layers of the neural network that results in an alarmingly nuanced understanding of the relationships among words, code, or images. Even more surprising was the discovery that adding more and more GPUs leads to better and better results. Altman is betting OpenAI can ride this scaling law all the way to artificial general intelligence, then superintelligence. “If that holds,” Nagaraj says, “it effectively means that whoever controls the infrastructure will control a lot of the market.”&lt;/p&gt;
&lt;h2 id=&#34;chinese-deepseek-model-makes-strong-case-for-state-of-the-art-ai&#34;&gt;Chinese DeepSeek model makes strong case for state-of-the-art AI&lt;/h2&gt;
&lt;p&gt;A small Chinese research lab called DeepSeek has sent shockwaves through the AI world by releasing a new reasoning model called DeepSeek-R1 that rivals OpenAI’s o1 model in mathematics, coding, and general knowledge, according to benchmark tests. The model can break down problems into chunks, and scrutinize and verify its own results. DeepSeek reportedly &lt;a href=&#34;https://venturebeat.com/ai/open-source-deepseek-r1-uses-pure-reinforcement-learning-to-match-openai-o1-at-95-less-cost/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;costs 90-95% less&lt;/a&gt; to run than &lt;a href=&#34;https://www.fastcompany.com/91189817/openais-new-o1-models-push-ai-to-phd-level-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI’s o1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Remarkably, the Chinese company open-sourced the models, meaning developers can freely access and modify them for their own uses. In other words, DeepSeek is doing what OpenAI set out to do when it started in 2015: developing “open” AI models that can be used to benefit all of humanity. OpenAI has since &lt;a href=&#34;https://teqnoverse.medium.com/four-of-openais-founders-abandoned-its-mission-and-founded-closed-ai-companies-af0c3ee0eaca&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;locked down&lt;/a&gt; the details of its most capable models, and charged developers fees to use them. So why would DeepSeek give away the value it created with DeepSeek-R1?&lt;/p&gt;
&lt;p&gt;“It helps DeepSeek attract top talent by showcasing their technical capabilities and commitment to collaborative development,” says &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the New Jersey Institute of Technology. “The strategy also fosters a developer ecosystem around their technology, potentially accelerating innovation and adoption.” In the end it could position DeepSeek for broader influence in both domestic and international AI development, Bader says. DeepSeek has emerged as a leading force in China’s language model development, surpassing established tech giants like Baidu, Tencent, Alibaba, and ByteDance, Bader explains. DeepSeek has also set off a price war among China’s AI providers.&lt;/p&gt;
&lt;p&gt;DeepSeek is similar to o1 in some ways. For example the model does lots of computation at “test time” after it’s been prompted by a user to begin work on a problem. It can feel its way through various approaches to the problem, following promising pathways while steering away from nonproductive ones. But this doesn’t mean that DeepSeek “copied” OpenAI’s work on o1. “[T]he core knowledge for building these models is taught in universities and discussed openly at conferences,” Bader says. “When developments like OpenAI’s o1 model emerge, it’s natural for other implementations like DeepSeek’s R1 to follow, as researchers worldwide build upon shared technical foundations.”&lt;/p&gt;
&lt;h2 id=&#34;accenture-more-ai-budget-should-go-to-training-humans&#34;&gt;Accenture: More AI budget should go to training humans&lt;/h2&gt;
&lt;p&gt;Accenture, which helps enterprises adopt AI, is out with results from its latest round of &lt;a href=&#34;https://www.accenture.com/us-en/insights/pulse-of-change&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;survey work&lt;/a&gt; on executive and worker views on AI deployment in 2025. The firm surveyed 3,450 C-suite leaders and 3,000 employees across 22 industries in 20 countries in the last quarter of 2024.&lt;/p&gt;
&lt;p&gt;A few highlights from the survey:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“2024 was the year of gen AI—but after 12 months of rapid adoption, just 50% of C-suite leaders claim to be fully prepared for technological disruption and only 36% say they have scaled gen AI solutions. A mere 13% report seeing significant enterprise-level value.”&lt;/li&gt;
&lt;li&gt;“The top three reasons C-suite leaders cite are capitalizing on advances in the technology (28%); maintaining business competitiveness (20%); and increasing confidence in managing associated business risk (20%).”&lt;/li&gt;
&lt;li&gt;“Just 12% [of executives] consider improving skills as the main [reason to invest]. This is corroborated by Accenture’s own experience working with companies globally: the company found that three times more gen AI budget is spent on technology than on people.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.fastcompany.com/91265565/the-oval-office-stargate-project-reveal-was-just-more-tech-industry-genuflecting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.fastcompany.com/91265565/the-oval-office-stargate-project-reveal-was-just-more-tech-industry-genuflecting&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trump&#39;s new era is already shaking up the AI world. Here are 3 huge things that just happened in AI.</title>
      <link>http://localhost:1313/blog/20250122-businessinsider/</link>
      <pubDate>Wed, 22 Jan 2025 18:11:40 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250122-businessinsider/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Rosalie Chan&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-president-donald-trump-looking-toward-openai-ceo-sam-altman-as-altman-speaks-to-reporters-at-the-white-house-this-means-we-can-create-ai-and-agi-in-the-united-states-of-america-openai-ceo-sam-altman-said-of-president-donald-trumps-new-ai-infrastructure-project-stargate-andrew-harnik-via-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;President Donald Trump looking toward OpenAI CEO Sam Altman, as Altman speaks to reporters at the White House. &amp;#34;This means we can create AI and AGI in the United States of America,&amp;#34; OpenAI CEO Sam Altman said of President Donald Trump&amp;#39;s new AI infrastructure project, Stargate. *Andrew Harnik via Getty Images*&#34; srcset=&#34;
               /blog/20250122-businessinsider/img_hu_80ab63881fb17862.webp 400w,
               /blog/20250122-businessinsider/img_hu_3cfa3cf996915c5d.webp 760w,
               /blog/20250122-businessinsider/img_hu_bbb645b8dad952b4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250122-businessinsider/img_hu_80ab63881fb17862.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      President Donald Trump looking toward OpenAI CEO Sam Altman, as Altman speaks to reporters at the White House. &amp;ldquo;This means we can create AI and AGI in the United States of America,&amp;rdquo; OpenAI CEO Sam Altman said of President Donald Trump&amp;rsquo;s new AI infrastructure project, Stargate. &lt;em&gt;Andrew Harnik via Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Since President Donald Trump took office Monday, he&amp;rsquo;s quickly made moves on advancing AI.&lt;/p&gt;
&lt;p&gt;At the same time, the US can&amp;rsquo;t rest in the AI arms race as China launches new, advanced AI models.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a quick rundown of the biggest news in AI.&lt;/p&gt;
&lt;h2 id=&#34;1-the-trump-administration-and-tech-companies-launch-project-stargate&#34;&gt;1. The Trump administration and tech companies launch Project Stargate&lt;/h2&gt;
&lt;p&gt;The biggest news: The Trump administration announced Tuesday its backing of &lt;a href=&#34;https://www.businessinsider.com/trump-ai-stargate-openai-oracle-softbank-technology-investment-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Project Stargate&lt;/a&gt;, an AI venture led by OpenAI, Oracle, and SoftBank to invest billions into building American AI infrastructure.&lt;/p&gt;
&lt;p&gt;Project Stargate plans to &lt;a href=&#34;https://www.businessinsider.com/elon-musk-sam-atlman-spar-how-much-stargate-raised-trump-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;invest $500 billion&lt;/a&gt; over the next four years to build new AI projects in the US, including 20 new data centers, starting in Abilene, Texas. OpenAI has said it will deploy $100 billion &amp;ldquo;immediately.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Stargate aims to help position the US as an AI leader, create 100,000 jobs, and increase national security as the US gets into an AI arms race with China (more on that below). OpenAI &lt;a href=&#34;https://www.businessinsider.com/sam-altman-trump-stargate-build-agi-us-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEO Sam Altman&lt;/a&gt; has also said it will allow the US to achieve artificial general intelligence.&lt;/p&gt;
&lt;p&gt;SoftBank will have &amp;ldquo;financial responsibility&amp;rdquo; over this project, while OpenAI will have &amp;ldquo;operational responsibility.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Arm, Microsoft, Nvidia, and Oracle will also offer technology support, while OpenAI, Oracle, and the Abu Dhabi &lt;a href=&#34;https://www.businessinsider.com/abu-dhabi-uae-ai-powerhouse-mgx-openai-investment-2024-10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tech fund MGX&lt;/a&gt; (which has invested in OpenAI) will provide additional financial support.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Stargate adds fuel to the narrative that we are still early in the capex buildout required for AI and signals that the new administration is likely to be highly supportive of the investment and energy requirements of the AI platform shift,&amp;rdquo; William Blair analysts wrote in a note.&lt;/p&gt;
&lt;p&gt;Microsoft also announced a new partnership agreement with OpenAI. While the tenants are similar to much of the original agreement and include a &amp;ldquo;new, large &lt;a href=&#34;https://blogs.microsoft.com/blog/2025/01/21/microsoft-and-openai-evolve-partnership-to-drive-the-next-phase-of-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure commitment&lt;/a&gt;&amp;rdquo; to support OpenAI products, it allows OpenAI to use other cloud providers, while Microsoft has the right of first refusal for OpenAI&amp;rsquo;s computing capacity.&lt;/p&gt;
&lt;p&gt;At the same time, Stargate will drive up demand for energy, and OpenAI is already starting to &lt;a href=&#34;https://openai.com/form/stargate-infrastructure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;solicit interest&lt;/a&gt; from construction, power, and equipment providers to build infrastructure. Trump has said he wants to make it easier for AI developers to have access to electricity for powering data centers and to build power plants.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Trump administration&amp;rsquo;s involvement in Stargate underscores what is likely to be a friendlier, more accommodative administration that will work to ensure energy availability and remove stringent regulations that may slow down the buildout of large-scale AI infrastructure across the US,&amp;rdquo; William Blair analysts wrote.&lt;/p&gt;
&lt;p&gt;Elon Musk, founder of xAI and co-head of the &lt;a href=&#34;https://www.businessinsider.com/doge-different-musk-official-white-house-trump-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Government Efficiency&lt;/a&gt;, is notably absent from the list of companies involved with the Stargate project. &lt;a href=&#34;https://www.businessinsider.com/elon-musk-sam-atlman-spar-how-much-stargate-raised-trump-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Musk voiced concern about SoftBank&lt;/a&gt;&amp;rsquo;s ability to uphold its financial responsibility for the project.&lt;/p&gt;
&lt;h2 id=&#34;2-openai-has-a-formidable-rival-from-china-deepseek&#34;&gt;2. OpenAI has a formidable rival from China: DeepSeek&lt;/h2&gt;
&lt;p&gt;On Monday, the Chinese startup DeepSeek released a family of open-source AI models called &lt;a href=&#34;https://www.businessinsider.com/china-startup-deepseek-openai-america-ai-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSeek-R1&lt;/a&gt; — a warning shot from China as Trump begins his term. DeepSeek first &lt;a href=&#34;https://www.businessinsider.com/openai-o1-ai-model-rivals-google-deepseek-reasoning-inference-2024-12&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;launched the preview&lt;/a&gt; of this model in November, and it&amp;rsquo;s similar to OpenAI&amp;rsquo;s o1 series of AI models.&lt;/p&gt;
&lt;p&gt;DeepSeek also &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published a report&lt;/a&gt; that said its R1 model is &amp;ldquo;comparable&amp;rdquo; and even outperforms OpenAI&amp;rsquo;s o1 in several math, reasoning, and coding benchmarks.&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s more, DeepSeek is much cheaper to use. Its open-source models are free, while access to the DeepSeek R1 API costs a small fraction of what OpenAI charges.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s quite remarkable that DeepSeek is making their model openly available to the world,&amp;rdquo; &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of The Institute for Data Science at New Jersey Institute of Technology, told Business Insider. &amp;ldquo;This allows anyone, anywhere, to fine-tune the model for their data and use cases without the enormous expense and requirements to spend the cost of hundreds of millions of dollars and months of time on massive supercomputers to perform the initial training.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;OpenAI offers unlimited access to its o1 model through ChatGPT Pro for $2,400 annually. If DeepSeek or any other open-source AI model offers similar capabilities for free, then it poses a challenge to AI companies that seek to profit from selling their technology.&lt;/p&gt;
&lt;p&gt;This launch shows that China is still a formidable competitor in the AI arms race, keeping up with Silicon Valley and, in some cases, even surpassing it. At the same time, concerns about the Chinese government&amp;rsquo;s censorship have already arisen. For example, R1 won&amp;rsquo;t answer questions about Tiananmen Square or Taiwan&amp;rsquo;s autonomy. Given the political risks, American companies may be unable to use the R1 model in their products.&lt;/p&gt;
&lt;p&gt;Meanwhile, OpenAI is getting ready to launch its &lt;a href=&#34;https://www.businessinsider.com/everything-openai-released-12-days-of-shipmas-sora-o1-2024-12&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next-generation model, o3&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;3-trump-repeals-the-executive-order-on-ai&#34;&gt;3. Trump repeals the executive order on AI&lt;/h2&gt;
&lt;p&gt;On Monday, Trump also repealed former president Joe Biden&amp;rsquo;s &lt;a href=&#34;https://www.businessinsider.com/white-house-advisor-catching-up-on-ai-biden-executive-order-2023-10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 executive order&lt;/a&gt; on AI, ushering in a deregulation of the technology.&lt;/p&gt;
&lt;p&gt;The executive order aimed to reduce AI risks and required companies that train advanced AI models to disclose details about them, including the results of safety testing, to the federal government. It also directed agencies to set standards for testing and address any risks.&lt;/p&gt;
&lt;p&gt;Trump has long criticized this executive order. He also rescinded nearly 80 &lt;a href=&#34;https://www.businessinsider.com/trump-day-one-agenda-executive-orders-inauguration-day-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other executive orders&lt;/a&gt;. One &lt;a href=&#34;https://www.businessinsider.com/biden-executive-order-ai-infrastructure-build-data-center-energy-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI executive order&lt;/a&gt; from Biden that Trump notably did not repeal called for federal support to ensure power for AI data centers.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s still unclear how Trump will handle other AI issues, including restrictions on AI chip and technology exports and addressing how much information AI developers should provide to the government.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.businessinsider.com/project-stargate-ai-deepseek-donald-trump-2025-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.businessinsider.com/project-stargate-ai-deepseek-donald-trump-2025-1&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nvidia’s tiny $3,000 computer for AI developers steals the show at CES</title>
      <link>http://localhost:1313/blog/20250109-cnbc/</link>
      <pubDate>Thu, 09 Jan 2025 16:36:15 -0500</pubDate>
      <guid>http://localhost:1313/blog/20250109-cnbc/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Kif Leswing&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-nvidia-ceo-jensen-huang-speaks-about-the-project-digits-personal-ai-supercomputer-for-researchers-and-students-during-a-keynote-address-at-the-ces-tech-conference-in-las-vegas-jan-6-2025-patrick-t-fallon--afp--getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Nvidia CEO Jensen Huang speaks about the Project Digits personal AI supercomputer for researchers and students during a keynote address at the CES tech conference in Las Vegas, Jan. 6, 2025. *Patrick T. Fallon | Afp | Getty Images*&#34; srcset=&#34;
               /blog/20250109-cnbc/108083597-1736346594840-gettyimages-2192223994-AFP_36T83RL_hu_8f26b443e11cc45c.webp 400w,
               /blog/20250109-cnbc/108083597-1736346594840-gettyimages-2192223994-AFP_36T83RL_hu_9f3f40781aaace86.webp 760w,
               /blog/20250109-cnbc/108083597-1736346594840-gettyimages-2192223994-AFP_36T83RL_hu_81b08eeec5ea2c79.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250109-cnbc/108083597-1736346594840-gettyimages-2192223994-AFP_36T83RL_hu_8f26b443e11cc45c.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Nvidia CEO Jensen Huang speaks about the Project Digits personal AI supercomputer for researchers and students during a keynote address at the CES tech conference in Las Vegas, Jan. 6, 2025. &lt;em&gt;Patrick T. Fallon | Afp | Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cnbc.com/quotes/NVDA/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia&lt;/a&gt; CEO Jensen Huang was greeted as a rock star this week at CES in Las Vegas, following an artificial intelligence &lt;a href=&#34;https://www.cnbc.com/2024/10/14/nvidia-shares-hit-a-record-as-chipmaker-market-cap-tops-3point4-trillion.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;boom&lt;/a&gt; that’s made the chipmaker the second-most-valuable company in the world.&lt;/p&gt;
&lt;p&gt;At his nearly two-hour keynote on Monday kicking off the annual tech conference, Huang packed a 12,000-seat arena, drawing comparisons to the way the late Steve Jobs revealed products at &lt;a href=&#34;https://www.cnbc.com/quotes/AAPL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple&lt;/a&gt; events.&lt;/p&gt;
&lt;p&gt;Huang concluded with an Apple-like trick: a surprise product reveal. He presented one of Nvidia’s server racks and, using some stage magic, held up a much smaller version, which looked like a tiny cube of a computer.&lt;/p&gt;
&lt;p&gt;“This is an AI supercomputer,” said Huang, wearing an alligator-skin leather jacket. “It runs the entire Nvidia AI stack. All of Nvidia’s software runs on this.”&lt;/p&gt;
&lt;p&gt;Huang said the computer is called Project Digits and runs off a relative of the Grace Blackwell graphics processing units, or GPUs, that are currently powering the most advanced AI server clusters. The GPU is paired with an &lt;a href=&#34;https://www.cnbc.com/quotes/ARM/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ARM&lt;/a&gt;-based Grace central processing unit, or CPU. Nvidia worked with Chinese semiconductor company &lt;a href=&#34;https://www.cnbc.com/quotes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MediaTek&lt;/a&gt; to create the system-on-a-chip called GB10.&lt;/p&gt;
&lt;p&gt;Formerly known as the Consumer Electronics Show, CES is typically the spot to launch flashy and futuristic consumer gadgets. At this year’s show, which started Tuesday and wraps up Friday, several companies announced AI integrations with appliances, laptops and even grills. Other major announcements included a laptop from Lenovo with a rollable screen that can expand vertically. There were also new robots, including a Roomba competitor with a robotic arm.&lt;/p&gt;
&lt;p&gt;Unlike Nvidia’s traditional GPUs for gaming, Project Digits isn’t targeting consumers. Instead, it’s aimed at machine-learning researchers, smaller companies, and universities that want to develop advanced AI but don’t have billions of dollars to build massive data centers or buy enough cloud credits.&lt;/p&gt;
&lt;p&gt;“There’s a gaping hole for data scientists and ML researchers and who are actively working, who are actively building something,” Huang said. “Maybe you don’t need a giant cluster. You’re just developing the early versions of the model, and you’re iterating constantly. You could do it in the cloud, but it just costs a lot more money.”&lt;/p&gt;
&lt;p&gt;The supercomputer will cost about $3,000 when it hits the market in May, Nvidia said, and it will be available from the company itself as well as some of its manufacturing partners. Huang said Project Digits is a placeholder name, indicating it may change by the time the computer goes on sale.&lt;/p&gt;
&lt;p&gt;“If you have a good name for it, reach out to us,” Huang said.&lt;/p&gt;
&lt;h2 id=&#34;diversifying-its-business&#34;&gt;Diversifying its business&lt;/h2&gt;
&lt;p&gt;It’s a dramatically different kind of product from the GPUs that have driven Nvidia’s historic boom in the past two years. OpenAI, which launched ChatGPT in late 2022, and other AI model creators such as Anthropic have joined with large cloud providers in snapping up Nvidia’s data center GPUs because of their ability to power the most intensive models and computing workloads.&lt;/p&gt;
&lt;p&gt;Data center sales accounted for 88% of Nvidia’s $35 billion in revenue in the &lt;a href=&#34;https://www.cnbc.com/2024/11/20/nvidia-nvda-earnings-report-q3-2025.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;most recent quarter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Wall Street is focused on Nvidia’s ability to diversify its business so that it’s less reliant on a handful of customers buying massive AI systems.&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-nvidia-project-digits-supercomputer-during-the-2025-ces-event-in-las-vegas-nevada-us-on-wednesday-jan-8-2025-people-flock-to-the-gambling-capital-in-the-tens-of-thousands-to-soak-up-keynote-events-and-fight-through-the-throngs-on-show-floors-for-a-glimpse-of-some-gadget-that-might-be-a-harbinger-of-the-future-photographer-bridget-bennett--bloomberg--getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Nvidia Project Digits supercomputer during the 2025 CES event in Las Vegas, Nevada, US, on Wednesday, Jan. 8, 2025. People flock to the gambling capital in the tens of thousands to soak up keynote events and fight through the throngs on show floors for a glimpse of some gadget that might be a harbinger of the future. Photographer: *Bridget Bennett | Bloomberg | Getty Images*&#34; srcset=&#34;
               /blog/20250109-cnbc/108084405-1736447789464-gettyimages-2192471522-CES_2025_hu_7002a3d72d43c480.webp 400w,
               /blog/20250109-cnbc/108084405-1736447789464-gettyimages-2192471522-CES_2025_hu_dc8b1b2ecfa1a7c3.webp 760w,
               /blog/20250109-cnbc/108084405-1736447789464-gettyimages-2192471522-CES_2025_hu_56375ff4c50c34ec.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20250109-cnbc/108084405-1736447789464-gettyimages-2192471522-CES_2025_hu_7002a3d72d43c480.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Nvidia Project Digits supercomputer during the 2025 CES event in Las Vegas, Nevada, US, on Wednesday, Jan. 8, 2025. People flock to the gambling capital in the tens of thousands to soak up keynote events and fight through the throngs on show floors for a glimpse of some gadget that might be a harbinger of the future. Photographer: &lt;em&gt;Bridget Bennett | Bloomberg | Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;“It was a little scary to see Nvidia come out with something so good for so little in price,” Melius Research analyst Ben Reitzes wrote in a note this week. He said Nvidia may have “stolen the show,” due to Project Digits as well as other announcements, including graphics cards for gaming, new robot chips and a deal with &lt;a href=&#34;https://www.cnbc.com/quotes/TM/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Toyota&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Project Digits, which runs Linux and the same Nvidia software used on the company’s GPU server clusters, represents a huge increase in capabilities for researchers and universities, said &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;Bader, who has worked on research projects with Nvidia in the past, said the computer appears to be able to handle enough data and information to train the biggest and most cutting-edge models. He told CNBC that Anthropic, &lt;a href=&#34;https://www.cnbc.com/quotes/GOOGL/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;, &lt;a href=&#34;https://www.cnbc.com/quotes/AMZN/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon&lt;/a&gt; and others “would pay $100 million to build a super computer for training” to get a system with these sorts of capabilities.&lt;/p&gt;
&lt;p&gt;For $3,000, users can soon get a product they can plug into a standard electrical outlet in their home or office, Bader said. It’s particularly exciting for academics, who have often left for private industry in order to access bigger and more powerful computers, he said.&lt;/p&gt;
&lt;p&gt;“Any student who is able to have one of these systems that cost roughly the same as a high-end laptop or gaming laptop, they’ll be able to do the same research and build the same models,” Bader said.&lt;/p&gt;
&lt;p&gt;Reitzes said the computer may be Nvidia’s first move into the $50 billion market for PC and laptop chips.&lt;/p&gt;
&lt;p&gt;“It’s not too hard to imagine it would be easy to just do it all themselves and allow the system to run Windows someday,” Reitzes wrote. “But I guess they don’t want to step on too many toes.”&lt;/p&gt;
&lt;p&gt;Huang didn’t rule out that possibility when asked about it by Wall Street analysts Tuesday.&lt;/p&gt;
&lt;p&gt;He said MediaTek may be able to sell the GB10 chip to other computer makers in the market. He made sure to leave some mystery in the air.&lt;/p&gt;
&lt;p&gt;“Obviously, we have plans,” Huang said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnbc.com/2025/01/09/nvidias-tiny-3000-computer-steals-the-show-at-ces-.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnbc.com/2025/01/09/nvidias-tiny-3000-computer-steals-the-show-at-ces-.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nbcnewyork.com/news/business/money-report/nvidias-tiny-3000-computer-steals-the-show-at-ces/6101187/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nbcnewyork.com/news/business/money-report/nvidias-tiny-3000-computer-steals-the-show-at-ces/6101187/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://techbullion.com/nvidia-unveils-3000-mini-computer-for-ai-developers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://techbullion.com/nvidia-unveils-3000-mini-computer-for-ai-developers/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neuron.expert/news/nvidias-tiny-3000-computer-for-ai-developers-steals-the-show-at-ces/10255/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://neuron.expert/news/nvidias-tiny-3000-computer-for-ai-developers-steals-the-show-at-ces/10255/en/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.techwrix.com/nvidia-introduces-3000-ai-mini-computer-at-ces-2025-revolutionizing-research-and-development/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.techwrix.com/nvidia-introduces-3000-ai-mini-computer-at-ces-2025-revolutionizing-research-and-development/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aiwereld.nl/nieuws/nvidias-kleine-ai-supercomputer-van-3000-steelt-de-show-op-techbeurs-ces&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://aiwereld.nl/nieuws/nvidias-kleine-ai-supercomputer-van-3000-steelt-de-show-op-techbeurs-ces&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://comercio.tv/2025/01/10/nvidia-revoluciona-la-inteligencia-artificial-con-su-supercomputadora-de-3-000/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://comercio.tv/2025/01/10/nvidia-revoluciona-la-inteligencia-artificial-con-su-supercomputadora-de-3-000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://criptotendencia.com/2025/01/10/project-digits-de-nvidia-revoluciona-el-ces-2025/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://criptotendencia.com/2025/01/10/project-digits-de-nvidia-revoluciona-el-ces-2025/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.portaltela.com/tecnologia/inteligencia-artificial/2025/01/09/nvidia-apresenta-supercomputador-compacto-de-usdollar-3-mil-na-ces-e-surpreende-publico&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.portaltela.com/tecnologia/inteligencia-artificial/2025/01/09/nvidia-apresenta-supercomputador-compacto-de-usdollar-3-mil-na-ces-e-surpreende-publico&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pchome.megatime.com.tw/news/cat1/20250110/73650203313449329005.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pchome.megatime.com.tw/news/cat1/20250110/73650203313449329005.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.pchome.com.tw/science/sunmedia/20250110/index-73650203313449329005.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.pchome.com.tw/science/sunmedia/20250110/index-73650203313449329005.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.marketersgo.com/media-collaboration/sunmedia/202501/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.marketersgo.com/media-collaboration/sunmedia/202501/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://firenews.com.tw/2025/01/10/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://firenews.com.tw/2025/01/10/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.moneyweekly.com.tw/ArticleData/Info/Article/162877&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.moneyweekly.com.tw/ArticleData/Info/Article/162877&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lifenews.com.tw/276218/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lifenews.com.tw/276218/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://enn.tw/591620/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://enn.tw/591620/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.scooptw.com/sunmedia/330006/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.scooptw.com/sunmedia/330006/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sunmedia.tw/news/technology/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8AI%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6Project-Digits-%E6%94%B9%E5%AF%ABAI%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB-1736502025875&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sunmedia.tw/news/technology/%E9%9C%87%E6%92%BC%E5%BD%88%EF%BC%81%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8AI%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6Project-Digits-%E6%94%B9%E5%AF%ABAI%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB-1736502025875&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tcpttw.com/2025/01/150585/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tcpttw.com/2025/01/150585/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://leho.com.tw/archives/112900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://leho.com.tw/archives/112900&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://tw.stock.yahoo.com/news/%E9%9C%87%E6%92%BC%E5%BD%88-%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB-094033038.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tw.stock.yahoo.com/news/%E9%9C%87%E6%92%BC%E5%BD%88-%E8%BC%9D%E9%81%94%E7%99%BC%E8%A1%A8ai%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6project-digits-%E6%94%B9%E5%AF%ABai%E8%A8%AD%E5%82%99%E9%96%80%E6%AA%BB-094033038.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vnreview.vn/threads/san-pham-duoc-quan-tam-nhat-tai-ces-2025-la-1-sieu-may-tinh-rat-nho-be.52678/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vnreview.vn/threads/san-pham-duoc-quan-tam-nhat-tai-ces-2025-la-1-sieu-may-tinh-rat-nho-be.52678/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://new-reporter.com/news/346344&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://new-reporter.com/news/346344&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vietnet24h.vn/cong-nghe/chiec-may-tinh-nho-3-000-do-la-cua-nvidia-danh-cho-cac-nha-phat-trien-ai-da-chiem-tron-su-chu-y-tai-ces&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.vietnet24h.vn/cong-nghe/chiec-may-tinh-nho-3-000-do-la-cua-nvidia-danh-cho-cac-nha-phat-trien-ai-da-chiem-tron-su-chu-y-tai-ces&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nguoiquansat.vn/sieu-may-tinh-ai-nho-gon-gia-chi-3-000-usd-cua-nvidia-gay-sot-hua-hen-mo-ra-mo-vang-moi-191409.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nguoiquansat.vn/sieu-may-tinh-ai-nho-gon-gia-chi-3-000-usd-cua-nvidia-gay-sot-hua-hen-mo-ra-mo-vang-moi-191409.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://genk.vn/sieu-may-tinh-ai-cua-nvidia-thuc-hien-1000-ty-phep-tinh-ai-moi-giay-kich-co-chi-bang-chiec-macbook-nho-sinh-vien-cung-co-the-so-huu-20250111083922727.chn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://genk.vn/sieu-may-tinh-ai-cua-nvidia-thuc-hien-1000-ty-phep-tinh-ai-moi-giay-kich-co-chi-bang-chiec-macbook-nho-sinh-vien-cung-co-the-so-huu-20250111083922727.chn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cafef.vn/sieu-may-tinh-ai-cua-nvidia-thuc-hien-1000-ty-phep-tinh-ai-moi-giay-kich-co-chi-bang-chiec-macbook-nho-sinh-vien-cung-co-the-so-huu-188250110093803105.chn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cafef.vn/sieu-may-tinh-ai-cua-nvidia-thuc-hien-1000-ty-phep-tinh-ai-moi-giay-kich-co-chi-bang-chiec-macbook-nho-sinh-vien-cung-co-the-so-huu-188250110093803105.chn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vietnamnet.vn/sieu-may-tinh-3-000-usd-cua-nvidia-thu-hut-su-chu-y-tai-ces-2025-2361938.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vietnamnet.vn/sieu-may-tinh-3-000-usd-cua-nvidia-thu-hut-su-chu-y-tai-ces-2025-2361938.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://baomoi.com/sieu-may-tinh-3-000-usd-cua-nvidia-gay-an-tuong-tai-ces-c51211977.epi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://baomoi.com/sieu-may-tinh-3-000-usd-cua-nvidia-gay-an-tuong-tai-ces-c51211977.epi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.owlting.com/articles/912301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.owlting.com/articles/912301&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://n.yam.com/Article/20250110305228&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://n.yam.com/Article/20250110305228&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://buzzorange.com/techorange/2025/01/10/lock-nvidia-project-digits/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://buzzorange.com/techorange/2025/01/10/lock-nvidia-project-digits/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.stiripesurse.ro/acesta-este-un-supercomputer-ai-mutarea-uriasa-a-nvidia-pe-o-piata-evaluata-la-50-de-miliarde-de-dolari_3546763.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.stiripesurse.ro/acesta-este-un-supercomputer-ai-mutarea-uriasa-a-nvidia-pe-o-piata-evaluata-la-50-de-miliarde-de-dolari_3546763.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pescurt.ro/stiri-tehnologie/nvidia-lanseaza-un-supercomputer-ai-accesibil-pentru-cercetatori-oportunitate-revolutionara-pentru-mediul-academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pescurt.ro/stiri-tehnologie/nvidia-lanseaza-un-supercomputer-ai-accesibil-pentru-cercetatori-oportunitate-revolutionara-pentru-mediul-academic&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://economedia.ro/nvidia-a-doua-cea-mai-valoroasa-companie-din-lume-lanseaza-un-supercomputer-ai-compact.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://economedia.ro/nvidia-a-doua-cea-mai-valoroasa-companie-din-lume-lanseaza-un-supercomputer-ai-compact.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hotnews.ro/prezentarea-surpriza-a-sefului-nvidia-la-ces-2025-un-supercomputer-ai-pe-care-poti-sa-l-tii-in-palma-si-are-un-pret-similar-cu-un-laptop-de-gaming-1878566&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hotnews.ro/prezentarea-surpriza-a-sefului-nvidia-la-ces-2025-un-supercomputer-ai-pe-care-poti-sa-l-tii-in-palma-si-are-un-pret-similar-cu-un-laptop-de-gaming-1878566&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.ro/economic/micul-computer-al-nvidia-pentru-dezvoltatorii-ai-de-3-000-de-dolari-a-furat-spectacolul-la-ces-1922405111002025010721893559&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.ro/economic/micul-computer-al-nvidia-pentru-dezvoltatorii-ai-de-3-000-de-dolari-a-furat-spectacolul-la-ces-1922405111002025010721893559&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ftnews.cz/revolucni-superpocitac-nvidia-na-veletrhu-v-las-vegas/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ftnews.cz/revolucni-superpocitac-nvidia-na-veletrhu-v-las-vegas/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sky.pro/media/news/kompyuter-nvidia-stoimostyu-vsego-3000-stal-zvezdoj-ces/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sky.pro/media/news/kompyuter-nvidia-stoimostyu-vsego-3000-stal-zvezdoj-ces/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sigma Xi, American Scientist: Distinguished Lecturers 2025-2026</title>
      <link>http://localhost:1313/blog/20241201-sigmaxi/</link>
      <pubDate>Sun, 01 Dec 2024 15:56:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241201-sigmaxi/</guid>
      <description>&lt;p&gt;For the 87th year, Sigma Xi presents its panel of Distinguished Lecturers as an opportunity for chapters to host visits from outstanding individuals who are at the leading edge of science.  These experts have agreed to visit chapters and share their insights and excitement on the topics detailed below.&lt;/p&gt;
&lt;p&gt;The Distinguished Lecturers are available from July 1, 2025, to June 30, 2026. Each speaker has consented to a modest honorarium together with full payment of travel costs and subsistence.&lt;/p&gt;
&lt;p&gt;Local chapters may apply for subsidies to support expenses related to hosting a Distinguished Lecturer. Subsidy applications must be submitted online by March 1, 2025, for funds to be available in the next fiscal year.&lt;/p&gt;
&lt;p&gt;Additional support for the program comes from the American Meteorological Society. Lecturer biographies, contact information, and additional details can be found online at sigmaxi/lectureships or by sending an email to &lt;a href=&#34;mailto:lectureships@sigmaxi.org&#34;&gt;lectureships@sigmaxi.org&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, New Jersey Institute of Technology&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>4 YWCC Faculty Contribute to Report to the Governor as Part of AI Task Force</title>
      <link>http://localhost:1313/blog/20241127-njit/</link>
      <pubDate>Wed, 27 Nov 2024 09:32:37 -0500</pubDate>
      <guid>http://localhost:1313/blog/20241127-njit/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20241127-njit/News%20story%20graphic_0_hu_ed4ad767f413e0cd.webp 400w,
               /blog/20241127-njit/News%20story%20graphic_0_hu_494f567b93554585.webp 760w,
               /blog/20241127-njit/News%20story%20graphic_0_hu_e7d7aa81e12214fb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241127-njit/News%20story%20graphic_0_hu_ed4ad767f413e0cd.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Distinguished Professors &lt;strong&gt;David Bader&lt;/strong&gt; and Guiling “Grace” Wang and Professors Cristian Borcea and Vincent Oria in the Ying Wu College of Computing (YWCC) served as part of Governor Phil Murphy’s AI Task Force for the state of New Jersey, a team comprised of industry leaders, academic experts, consumer advocates, and government innovators tasked with establishing programs, trainings and tools that advance the state’s leadership in AI.&lt;/p&gt;
&lt;p&gt;Recommendations by the Task Force were recently published in &lt;a href=&#34;https://www.nj.gov/governor/docs/Final-2024-NJ-AI-Task-force-Report-to-Governor.pdf?v=2024-11-13&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Report to the Governor on Artificial Intelligence&lt;/a&gt;, which aims to create economic opportunities for residents and businesses, encourage ethical use of AI technologies, promote  equitable outcomes, support public and private workforces and improve government services and the resident experience.&lt;/p&gt;
&lt;p&gt;The YWCC faculty members each contributed to four AI Taskforce subgroups as part of four primary Working Groups in the following areas: security, safety and privacy considerations for AI use cases; workforce training, jobs of the future and training public professionals; AI, equity and literacy; and making New Jersey a hub for AI innovation.&lt;/p&gt;
&lt;p&gt;Governor Murphy signed &lt;a href=&#34;https://nj.gov/infobank/eo/056murphy/pdf/EO-346.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Executive Order No. 346&lt;/a&gt; in October 2023, which established the &lt;a href=&#34;https://nj.gov/governor/news/news/562023/approved/20231010b.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial Intelligence (“AI”) Task Force&lt;/a&gt; to study emerging technologies, issue findings on their potential societal impacts, and offer recommendations for government actions to encourage the ethical and responsible use of AI technologies.&lt;/p&gt;
&lt;p&gt;Recommendations included ways to expand opportunities for AI education and literacy, promote a strong workforce and AI talent pipeline, address biases and discrimination, foster a collaborative AI innovation ecosystem and economy across the state, and bolster the state’s use of GenAI to support policy outcomes and improve the resident experience.&lt;/p&gt;
&lt;p&gt;New Jersey is the first state in the nation to launch comprehensive surveys of its state public and private workforces, residents, businesses and higher education institutions to understand their views on AI. The report will act as a roadmap for the future of the AI industry in New Jersey, further positioning the state as the nation’s leader in AI innovation.&lt;/p&gt;
&lt;p&gt;“As AI technologies continue to expand and advance at an unprecedented pace, New Jersey remains at the forefront, building up the Garden State as a hub for innovation,” Governor Murphy said in a statement. “…I am grateful to the Task Force for their recommendations and look forward to implementing them in the coming months.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/four-ywcc-faculty-contribute-report-governor-part-ai-task-force&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/four-ywcc-faculty-contribute-report-governor-part-ai-task-force&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>7 Questions for David Bader: Graph Analytics at Scale with Arkouda and Chapel</title>
      <link>http://localhost:1313/blog/20241107-chapel/</link>
      <pubDate>Thu, 07 Nov 2024 15:12:46 -0500</pubDate>
      <guid>http://localhost:1313/blog/20241107-chapel/</guid>
      <description>&lt;p&gt;&lt;em&gt;By: &lt;a href=&#34;https://chapel-lang.org/blog/authors/engin-kayraklioglu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Engin Kayraklioglu&lt;/a&gt;, &lt;a href=&#34;https://chapel-lang.org/blog/authors/brad-chamberlain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brad Chamberlain&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this installment of our &lt;a href=&#34;https://chapel-lang.org/blog/series/7-questions-for-chapel-users/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7 Questions for Chapel Users&lt;/a&gt; series, we welcome &lt;strong&gt;&lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;&lt;/strong&gt;, a Distinguished Professor in the &lt;a href=&#34;https://computing.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ying Wu College of Computing&lt;/a&gt; at the New Jersey Institute of Technology (NJIT). With a deep focus on high-performance computing and data science, David has consistently driven innovation in solving some of the most complex and large-scale computational problems. Read on to dive into his journey with Chapel, his current projects, and how tools like &lt;a href=&#34;https://arkouda-www.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arkouda&lt;/a&gt; and &lt;a href=&#34;https://github.com/Bears-R-Us/arkouda-njit/blob/main/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arachne&lt;/a&gt; are accelerating data science at scale.&lt;/p&gt;
&lt;h2 id=&#34;1-who-are-you&#34;&gt;1. Who are you?&lt;/h2&gt;
&lt;p&gt;I am David Bader, currently serving as the Director of the Institute for Data Science at NJIT. My expertise lies in scalable data analytics and parallel computing, where I focus on addressing large-scale data challenges that impact our daily lives, such as social network analysis, cybersecurity, and bioinformatics. I have led the team that has developed Arachne, an Arkouda package that extends its interactive data exploration capabilities to also handle complex graph analytics through scalable algorithms. Over the years, I have collaborated with teams across academia, government, and industry to develop solutions that push the boundaries of computing.&lt;/p&gt;
&lt;h2 id=&#34;2-what-do-you-do-what-problems-are-you-trying-to-solve&#34;&gt;2. What do you do? What problems are you trying to solve?&lt;/h2&gt;
&lt;p&gt;My work revolves around developing efficient algorithms and frameworks for processing and analyzing massive datasets. The challenges I tackle often have broad societal impact, such as improving our understanding of disease pathways, detecting cyber threats in real time, and optimizing infrastructure networks. Many of these problems require harnessing the power of modern supercomputers and parallel architectures to deliver solutions at unprecedented scales and speeds.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Arachne helps us execute complex graph analytics on dynamic datasets with extreme scalability and performance.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With frameworks like Arkouda, we are able to provide data scientists the ability to work interactively on large datasets with the simplicity of Python, but at the performance levels of HPC systems. Similarly, Arachne helps us execute complex graph analytics on dynamic datasets with extreme scalability and performance. These tools are essential for translating cutting-edge research into real-world applications.&lt;/p&gt;
&lt;h2 id=&#34;3-how-does-chapel-help-you-with-these-problems&#34;&gt;3. How does Chapel help you with these problems?&lt;/h2&gt;
&lt;p&gt;Chapel provides a unique advantage by allowing its users to express parallelism and locality in a more intuitive manner compared to traditional parallel programming languages like MPI or OpenMP. It offers a modern programming model that is both high-level and highly performant, making it easier to develop and maintain complex data analytics applications. We run Chapel on a variety of systems, from local clusters to national-scale supercomputers, and its flexibility and scalability have been invaluable in bridging the gap between research prototypes and real-world deployments.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Students in my group, without prior experience in parallel programming, were able to write and implement scalable graph algorithms in Chapel within just a few weeks.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Arkouda’s Chapel-powered backend supports scalable and interactive processing of dataframes with millions of elements, giving data scientists the power to analyze data interactively without worrying about the complexity of parallel processing. For Arachne, Chapel allows us to efficiently manage large graph structures and parallelize analytics workloads, which is crucial when working with massive social networks or biological interaction networks.&lt;/p&gt;


















&lt;figure  id=&#34;figure-two-implementations-of-subgraph-isomorphism-in-arachne-perform-97x-better-than-networkx-and-dotmotif-commonly-used-in-neuroscience-workflows-for-motif-matching-in-graphs&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Two implementations of subgraph isomorphism in Arachne perform 97x better than NetworkX and DotMotif, commonly used in neuroscience workflows for motif matching in graphs.&#34; srcset=&#34;
               /blog/20241107-chapel/neuro_hu_114d2f05aa098009.webp 400w,
               /blog/20241107-chapel/neuro_hu_334fb8c1465f77fc.webp 760w,
               /blog/20241107-chapel/neuro_hu_1d07fb0da194cdb8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241107-chapel/neuro_hu_114d2f05aa098009.webp&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Two implementations of subgraph isomorphism in Arachne perform 97x better than NetworkX and DotMotif, commonly used in neuroscience workflows for motif matching in graphs.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In addition to Chapel’s strengths in parallelism and locality, it has proven to be highly accessible for new users. For instance, students in my group, without prior experience in parallel programming, were able to write and implement scalable graph algorithms in Chapel within just a few weeks. This intuitive learning curve allowed them to quickly contribute to Arachne’s graph analytics, demonstrating how Chapel can be leveraged to rapidly develop solutions even by those new to parallel programming and HPC.&lt;/p&gt;
&lt;h2 id=&#34;4-what-initially-drew-you-to-chapel&#34;&gt;4. What initially drew you to Chapel?&lt;/h2&gt;
&lt;p&gt;I first learned about Chapel decades ago as an investigator in the DARPA High Productivity Computing Systems (HPCS) program. At the time, I was drawn to its potential for transforming productivity in parallel computing, making it feasible to tackle challenging problems at scale. What stood out to me was how Chapel abstracted away the low-level details of parallel programming while retaining the performance characteristics necessary for high-end computing. Chapel’s promise to improve productivity and facilitate high-performance application development made it a compelling choice for our research projects.&lt;/p&gt;
&lt;h2 id=&#34;5-what-are-your-biggest-successes-that-chapel-has-helped-achieve&#34;&gt;5. What are your biggest successes that Chapel has helped achieve?&lt;/h2&gt;
&lt;p&gt;Using Chapel, we’ve been able to implement parallel algorithms for graph analytics that scale efficiently across thousands of cores. One of Chapel’s most impactful successes has been the development of Arkouda, a tool that allows data scientists to manipulate large datasets interactively and perform exploratory data analysis at unprecedented scales. This wouldn’t have been possible without Chapel’s robust parallel programming capabilities.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“With Arachne, we’ve successfully demonstrated how complex graph problems can be tackled using Chapel, providing solutions that are not only scalable but also highly efficient in real-world scenarios.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With Arachne, the Arkouda package we design and develop here at NJIT, we’ve successfully demonstrated how complex graph problems can be tackled using Chapel to process dynamic and irregular data structures, providing solutions that are not only scalable but also highly efficient in real-world scenarios. These tools have helped us advance the field of data science by providing new avenues for scalable data processing and analytics.&lt;/p&gt;


















&lt;figure  id=&#34;figure-arachne-shows-strong-scalability-on-two-different-types-of-systems-for-distributed-memory-multilocale-breadth-first-search&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Arachne shows strong scalability on two different types of systems for distributed-memory (multilocale) breadth-first search.&#34; srcset=&#34;
               /blog/20241107-chapel/bfs_hu_51e937fffa59867f.webp 400w,
               /blog/20241107-chapel/bfs_hu_9fa4bf591f2ea583.webp 760w,
               /blog/20241107-chapel/bfs_hu_923940f4e62547b1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241107-chapel/bfs_hu_51e937fffa59867f.webp&#34;
               width=&#34;760&#34;
               height=&#34;456&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Arachne shows strong scalability on two different types of systems for distributed-memory (multilocale) breadth-first search.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;6-if-you-could-improve-chapel-with-a-finger-snap-what-would-you-do&#34;&gt;6. If you could improve Chapel with a finger snap, what would you do?&lt;/h2&gt;
&lt;p&gt;I’d focus on enhancing Chapel’s integration with existing machine learning and data science libraries, particularly those in Python. Seamlessly combining Chapel’s parallelism with the extensive ecosystem of data science tools would significantly broaden its applicability and reduce the friction for users transitioning between traditional and parallel environments. For frameworks like Arkouda and Arachne, tighter integration with Python would make it even easier for data scientists to leverage the power of Chapel in their existing workflows.&lt;/p&gt;
&lt;h2 id=&#34;7-anything-else-youd-like-people-to-know&#34;&gt;7. Anything else you’d like people to know?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;“I’m excited about the growing intersections between data science, graph analytics, and real-world applications. And I believe that languages like Chapel will play a pivotal role in shaping the future of computational research.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I encourage researchers and developers interested in high-performance data analytics to explore Chapel. Its modern approach to parallel computing makes it a valuable tool for tackling today’s data challenges. As for my own work, I’m excited about the growing intersections between data science, graph analytics, and real-world applications. And I believe that languages like Chapel will play a pivotal role in shaping the future of computational research.&lt;/p&gt;
&lt;p&gt;Frameworks like Arkouda and Arachne demonstrate how Chapel can serve as a powerful foundation for building scalable, high-performance tools. Whether you’re dealing with massive dataframes or complex graph structures, Chapel’s expressive parallelism and scalability can be a game-changer.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We’d like to thank David Bader for participating in the 7 Questions for Chapel Users series. To learn more about David’s team’s work, make sure to check out &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his website&lt;/a&gt;. David’s work showcases how powerful and flexible modern parallel programming languages like Chapel can be for addressing large-scale data challenges. With tools like Arkouda and Arachne, he continues to push the boundaries of data science and parallel computing. We look forward to seeing how his work evolves and inspires the next generation of computational breakthroughs!&lt;/p&gt;
&lt;p&gt;If you have any questions for David, or comments, please direct them to the &lt;a href=&#34;https://chapel.discourse.group/t/7-questions-for-chapel-users-series-questions-comments/37200&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7 Questions for Chapel Users&lt;/a&gt; thread on Discourse.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chapel-lang.org/blog/posts/7qs-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chapel-lang.org/blog/posts/7qs-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comes A Time Podcast</title>
      <link>http://localhost:1313/blog/20241101-comesatime/</link>
      <pubDate>Fri, 01 Nov 2024 15:14:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241101-comesatime/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/GX3dTbTXJGg?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. Bader is an elected Board Member of the Computing Research Association (CRA). He is a Fellow of the IEEE, ACM, AAAS, and SIAM; a recipient of the IEEE Sidney Fernbach Award; and the 2022 Innovation Hall of Fame inductee of the University of Maryland’s A. James Clark School of Engineering. The Computer History Museum recognizes Bader for developing the first Linux-based supercomputer which became the predominant architecture for all major supercomputers in the world.  Bader followed the Grateful Dead during their heyday, immersing himself in the counterculture movement. As a member of the Legion of the Sufficiently Twisted (LOST), an offshoot of Ken Kesey&amp;rsquo;s Merry Pranksters, he embarked on wild adventures that shaped his unique perspective on life. Alongside John Perry Barlow, a lyricist for the Grateful Dead and an early internet pioneer, Bader experienced memorable escapades that bridged the worlds of technology, music, and freedom of expression.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cybersecurity Roundtable</title>
      <link>http://localhost:1313/blog/20241101-njbusiness/</link>
      <pubDate>Fri, 01 Nov 2024 15:08:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241101-njbusiness/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20241101-njbusiness/Cyber-RT_hu_728053feb013bbf0.webp 400w,
               /blog/20241101-njbusiness/Cyber-RT_hu_4fa0c6cff9223417.webp 760w,
               /blog/20241101-njbusiness/Cyber-RT_hu_27a19e4aeb18cb5a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241101-njbusiness/Cyber-RT_hu_728053feb013bbf0.webp&#34;
               width=&#34;760&#34;
               height=&#34;490&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In this installment of New Jersey Business Magazine’s continuing Business Roundtable Series, conducted in conjunction with the New Jersey Business &amp;amp; Industry Association, we present a discussion on cybersecurity. Five experts in the field discuss today’s common digital threats, the growing dangers of AI coupled with supercomputing, and why employees are still the best of line defense against cyberattacks. It is a frank discussion with experts who are passionate about protecting their organizations from faceless foes who lurk in the digital underworld.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding professor and chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matt Darlage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Matt Darlage joined Citizens in August 2017 as the director of security engineering and architecture (SEA). In September 2021, he assumed the chief information security officer (CISO) role for Corporate Security and Resilience (CS&amp;amp;R) at Citizens Bank. He is responsible for Citizens information security program, which protects enterprise IT data and assets from various threats.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Darrin Maggy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Darrin Maggy of Integris is a Certified Information Systems Security Professional who helps organizations connect to, and solve, their information security challenges. He is a founding member and past vice president of the New Hampshire (ISC)² Chapter and is the New Hampshire State Organizer for the Cloud Security Alliance (CSA) New England Chapter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Barbara Romano&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Barbara A. Romano is the first woman to take on the chief information officer role at South Jersey Industries (SJI). She provides strategic guidance, leadership, and direct oversight at the corporate level for all technology and business information system initiatives for SJI and its subsidiaries. Romano is also responsible for advancing and sustaining the corporate infrastructure and network for new technology initiatives at SJI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hussein Syed&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hussein Syed is the chief information security officer (CISO) at RWJBarnabas Health. With decades of IT experience, Syed joined Barnabas Health in 2002 as a security architect. He later established the system’s first security program and led the creation of the security department. He serves on advisory boards and leads internal security committees at RWJBarnabas Health.&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-most-significant-cybersecurity-threats-you-are-facing-today&#34;&gt;What are the most significant cybersecurity threats you are facing today?&lt;/h2&gt;
&lt;p&gt;Darlage: The big things for me are phishing, spear phishing, and ransomware. However, what really scares me right now are third-party vulnerabilities and exploiting libraries [a collection of pre-written code used to reduce the amount of code a programmer needs to write] that are compiled into applications that a company uses. The other thing is the amplification of AI. The technology has provided a way for attackers to get faster and increase the blast radius. The way that we’re looking at that is very simple: integrate technologies with machine learning (ML) to help us manage it.&lt;/p&gt;
&lt;p&gt;Bader: At NJIT, we’re doing research on securing the opensource software supply chain, and it becomes harder when an exploit is not just a single library or package, but a seemingly benevolent code that is inserted across multiple libraries that, when compiled together, become malware or malicious attacks. So, we are conducting research to try and discover those hard-to-detect vulnerabilities that could have a large impact downstream.&lt;/p&gt;
&lt;p&gt;Maggy: From a threat standpoint, the thing I always see is human behavior. We’ve got malicious humans. We have careless humans. Statistically speaking, we have determined that 97% of all breaches can be solid-lined right back to a human action.&lt;/p&gt;
&lt;h2 id=&#34;how-often-are-businesses-being-attacked&#34;&gt;How often are businesses being attacked? &lt;/h2&gt;
&lt;p&gt;Syed: There are thousands, if not millions, of phishing emails targeted at people. It depends on the nature of it. Some of it could be just a wider phishing net, like sending a ton of emails, or it could be a targeted attack, which is also starting to become pervasive, where [a cybercriminal] scours social media to identify high value targets.&lt;/p&gt;
&lt;p&gt;It may be a ransomware type of thing, compromising someone’s computer or trying to get money redirected into an offshore account, for example. So, we’re looking at thousands if not millions of attacks per day against organizations.&lt;/p&gt;
&lt;p&gt;Darlage: With AI, the amplification of the attacks is spreading like wildfire. We cannot keep up with this unless we use the exact same technologies against the attacks.&lt;/p&gt;
&lt;p&gt;Bader: According to Forbes, there’s been a 72% increase in data breaches in the last two years, and each data breach costs, on average, $4.88 million, so it’s quite substantial.&lt;/p&gt;
&lt;p&gt;Syed: If you were to look at a 90-day period, it would be to the tune of 100 to 200 billion attacks that need to be taken, triaged, quantified, and processed down to actionable incidents. We’re fighting an adversary that plays by no rules. Anything is fair game for them, whereas we must follow processes, policies, and regulatory requirements.&lt;/p&gt;
&lt;h2 id=&#34;what-are-the-data-risks-in-your-specific-fields&#34;&gt;What are the data risks in your specific fields?&lt;/h2&gt;
&lt;p&gt;Syed: If you look at an FBI report from last year, healthcare was the most targeted business sector, and that’s primarily because healthcare [information is] valuable. For that reason, there are lots of attempts to compromise systems. Then, there’s a preconceived notion that systems are not adequately protected, and that gives [cybercriminals] another incentive to attempt to compromise systems.&lt;/p&gt;
&lt;p&gt;Romano: We’re a natural gas business, so our primary concern is protecting our customer’s Personally Identifiable Information (PII) data. We are heavily encrypted when it comes to storing that information. The other major concern is the pipeline. Our Supervisory Control and Data Acquisition (SCADA) systems are used to monitor the pipeline pressure throughout our entire system. That’s one of our primary concerns; just protecting that SCADA infrastructure.&lt;/p&gt;
&lt;p&gt;Darlage: There’s been a sustained credential abuse campaign going against online banking websites for almost 10 years. It’s what we have to deal with. We’ve been focused on building a digital identity perimeter around that and docking that in with our applications.&lt;/p&gt;
&lt;p&gt;One of the things important to me is the exploitation of elderly banking customers. My mom was subject to an elder abuse campaign. Thankfully in her case, the branch manager called me up because I was next of kin. They said, “We’ve got this,” but fraud here is absolutely out of control.&lt;/p&gt;
&lt;p&gt;Syed: Regarding elder abuse, statistics from an FBI report last year revealed that the 50-to-59-year-old age group had $1.7 billion in losses and about 65,000 complaints filed: for 60-year-olds and up, it was $3.4 billion in losses and about 101,000 complaints. The 20-and-under age group lost about $40 million. So, it just proves that elder abuse is out of control. These people are losing their hard-earned money that they desperately need to live on.&lt;/p&gt;
&lt;h2 id=&#34;regarding-ai-how-smart-could-attacks-become&#34;&gt;Regarding AI, how smart could attacks become?&lt;/h2&gt;
&lt;p&gt;Bader: As we see with large language models – pick your favorite one, ChatGPT, Gemini, Claude, etc. – the sophistication that AI brings today takes phishing, for instance, from badly worded broken English emails to targeting specific individuals with carefully crafted messages that are taking in personal information, that may have been stolen in other breaches. Emails are being created that look perfectly unique and very realistic. We’re also now seeing audio, and in some cases video, which has been created through generative AI. These are sophisticated attacks. It’s very scary today and it is going to be quite a challenge for the foreseeable future.&lt;/p&gt;
&lt;p&gt;Additionally, with AI, the rate at which [attacks] can be created has gone up exponentially. Rather than an individual being able to push one [attack] out every five minutes, they can now push a million every second.&lt;/p&gt;
&lt;p&gt;Romano: We should also be thinking about AI in a positive way. AI can handle and understand data a lot faster than people. We can utilize AI to discover new threats that are coming in to advance our strategies and combat the bad guys.&lt;/p&gt;
&lt;h2 id=&#34;how-much-would-a-small-business-pay-to-implement-a-cybersecurity-strategy-or-technology&#34;&gt;How much would a small business pay to implement a cybersecurity strategy or technology?&lt;/h2&gt;
&lt;p&gt;Maggy: So much of it is based on the context of an organization: where we find them [in terms of operations]; is there any maturity to what it may have already implemented; was it cobbled together over time to react to things? There’s no hard, fast answer to the question.&lt;/p&gt;
&lt;p&gt;What my team does first is gather the context of the organization. What does it do to remain commercially viable? Then, we look at the organization through the lens of risk. Once we understand your risk profile and the valuation of your assets … we rank things in order by their criticality. So, the first approach is always the same; we need to understand.&lt;/p&gt;
&lt;h2 id=&#34;are-you-experiencing-a-shortage-of-highly-skilled-cyber-security-experts&#34;&gt;Are you experiencing a shortage of highly skilled cyber security experts?&lt;/h2&gt;
&lt;p&gt;Syed: Cybersecurity is still facing a talent shortage, and that’s going stay for some time – until we get to entice more people into the field. You may find a skilled professional who you can bring in, but it takes time for them to get to know your business and how to adapt to it. Growing talent from within your organization is the best way to cultivate that long-term retention.&lt;/p&gt;
&lt;p&gt;Romano: For the longest time, I would agree that we were seeing a shortage. We’ve approached that by developing a robust intern program. Probably 50% [of our staff] have come through our internship program.&lt;/p&gt;
&lt;p&gt;Bader: The US Bureau of Labor Statistics lists cybersecurity professionals as one of the most in-demand jobs, with average salaries over $120,000. At NJIT, we have a number of degree programs [for students interested in careers in the field]. We have a Master’s Degree in Cybersecurity and Privacy and a Master’s Degree in Information Technology, Administration and Security. We are one of the largest producers of computing talent in the tri-state area. Many students are getting great jobs in the region and around the world. In fact, NJIT is designated as a National Center of Academic Excellence in Cyber Defense by the National Security Agency.&lt;/p&gt;
&lt;h2 id=&#34;what-thrills-you-about-your-jobs&#34;&gt;What thrills you about your jobs?&lt;/h2&gt;
&lt;p&gt;Romano: I have spent 30 plus years in the technology field. I could not even have imagined all the changes that have happened over the course of my career. I look at this field optimistically, it is always evolving … I can’t imagine a day where it’s not evolving. It is very rewarding to work with a group of passionate individuals who want to come to work, solve problems, and meet our customers’ needs.&lt;/p&gt;
&lt;p&gt;Syed: There’s never going to be any shortage of excitement in this field. A person [must] love change because change is the dynamic here. This is where passionate people come together, who love to work together, and everyone feeds off that energy.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://njbmagazine.com/monthly-articles/cybersecurity-roundtable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njbmagazine.com/monthly-articles/cybersecurity-roundtable/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Research Team Innovates in Princeton FlyWire Codex Challenge</title>
      <link>http://localhost:1313/blog/20241030-njit/</link>
      <pubDate>Wed, 30 Oct 2024 11:04:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241030-njit/</guid>
      <description>

















&lt;figure  id=&#34;figure-njits-team--zhihui-du-david-bader-harinarayan-sriram-and-srijith-chinthalapudi--is-advancing-computational-neuroscience&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;NJIT’s team — Zhihui Du, David Bader, Harinarayan Sriram and Srijith Chinthalapudi — is advancing computational neuroscience&#34; srcset=&#34;
               /blog/20241030-njit/PXL_20241028_180405267_hu_7ec091da6b2bd3c3.webp 400w,
               /blog/20241030-njit/PXL_20241028_180405267_hu_e806f7a950334a55.webp 760w,
               /blog/20241030-njit/PXL_20241028_180405267_hu_3608fef7b23e9a9c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241030-njit/PXL_20241028_180405267_hu_7ec091da6b2bd3c3.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      NJIT’s team — Zhihui Du, David Bader, Harinarayan Sriram and Srijith Chinthalapudi — is advancing computational neuroscience
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;An NJIT computational research team made impressive strides at the Princeton FlyWire Codex Data Challenge. Teams were tasked to solve a highly complex problem: analyzing the intricate network of neurons in the brain of a &lt;em&gt;Drosophila melanogaster&lt;/em&gt; — commonly known as the fruit fly. Teams had to treat this neural network as a massive, interconnected graph and work to determine the best possible ordering of neurons. The goal was to maximize the efficiency of the brain&amp;rsquo;s neural pathways, creating an optimal flow of information within this tiny but surprisingly complex system.&lt;/p&gt;
&lt;p&gt;NJIT’s team was led by Distinguished Professor David Bader, and included NJIT’s Institute for Data Science principal research scientist Dr. Zhihui Du, and high school student interns Harinarayan Asoori Sriram and Srijith Chinthalapudi from Edison Academy Magnet School. By the competition&amp;rsquo;s deadline, they achieved a score of 35,231,406 — dramatically improving on the 29 million baseline using the well-known greedy method, which prioritizes the locally optimal choice at each stage.&lt;/p&gt;
&lt;p&gt;However, teams were allowed to continue to refine their algorithm. NJIT’s team improved to first place with a score of 35,459,212 in time for the awards ceremony held Oct. 28 at the Princeton Neuroscience Institute (PNI). Princeton research scientist Arie Matsliah ran the competition and NJIT’s results are directly applicable to the neuroscience research community, including for Mala Murthy, director of PNI, and H. Sebastian Seung, the Evnin Professor in Neuroscience at Princeton.&lt;/p&gt;
&lt;h2 id=&#34;optimizing-neural-pathways-reveals-insights-for-neuroscience-and-advanced-ai-systems&#34;&gt;Optimizing neural pathways reveals insights for neuroscience and advanced AI systems&lt;/h2&gt;
&lt;p&gt;“The significance of this event lies in its ability to test our advanced data science and optimization techniques in a real-world setting with a challenging dataset of the Drosophila brain connectome,” Bader said. Supported by the National Science Foundation, the NJIT team applied innovative techniques like parallelized simulated annealing and integer programming to yield high-level results.&lt;/p&gt;
&lt;p&gt;“Working with the Drosophila brain is significant because its neural architecture provides insights that could translate to more complex systems.” This model offers an ideal testing ground for fundamental studies in neural connectivity, with implications for understanding human brain function and neurological conditions. Beyond the competition, NJIT’s approach to analyzing the connectome graph of the Drosophila brain has potential applications across various fields, from AI systems inspired by neural networks to bioinformatics and logistics.&lt;/p&gt;
&lt;p&gt;This challenge also highlights the pursuit for more efficient algorithms that require less power. Training AI large language models, like ChatGPT, can require several megawatts to train — which is roughly the same power as a small town uses in a day. The human brain requires about 20 watts of power to function, which is roughly the energy needed to power a dim light bulb.&lt;/p&gt;
&lt;h2 id=&#34;innovative-strategies-introduce-controlled-algorithmic-chaos&#34;&gt;Innovative strategies introduce controlled algorithmic chaos&lt;/h2&gt;
&lt;p&gt;Achieving such a high score required a combination of strategies, including a novel technique dubbed toposhuffling. When running complex simulations, the algorithm might get “stuck” in a suboptimal path, referred to as local maxima in this case. Toposhuffling effectively jolts the algorithm out of this groove through controlled disorder to escape the undesired pathways, which will ultimately score higher.&lt;/p&gt;
&lt;p&gt;“Simulated annealing helps to minimize a quantity called energy, and in this challenge, we targeted the weighted sum of forward edges,” Bader explained. “Our team parallelized this approach, running multiple simulations to improve performance, but once progress slowed, we introduced toposhuffling—a method inspired by topological sorting that increased disorder while maintaining solution integrity. This helped us overcome local maxima.”&lt;/p&gt;
&lt;p&gt;For the high school student participants, the challenge provided transformative educational value. They gained hands-on experience in optimization techniques and teamwork, applying theoretical knowledge to solve high-impact problems. “The experience prepared them for future STEM careers, deepening their understanding of data analysis and programming,” said Bader. “The students also performed impressively, I viewed them more like colleagues in this challenge — they were fantastic.”&lt;/p&gt;
&lt;p&gt;The event also aligned with NJIT’s broader institutional goals, emphasizing the university’s commitment to data science and interdisciplinary problem-solving. “Our use of techniques like parallelized simulated annealing and integer programming underscores NJIT’s mission to address complex problems,” Bader remarked. “This accomplishment not only strengthens our position in computational research but inspires future applications in neuroscience and AI.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-research-team-innovates-princeton-flywire-codex-challenge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-research-team-innovates-princeton-flywire-codex-challenge&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amid an A.I. Chip Shortage, the GPU Rental Market Is Booming</title>
      <link>http://localhost:1313/blog/20241023-observer/</link>
      <pubDate>Wed, 23 Oct 2024 19:57:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241023-observer/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://observer.com/author/aaron-mok/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aaron Mok&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-gpu-rentals-allow-small-companies-to-access-high-performance-ai-chips-for-specific-projects-igor-omilaevunsplash&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;GPU rentals allow small companies to access high-performance A.I. chips for specific projects. *Igor Omilaev/Unsplash*&#34; srcset=&#34;
               /blog/20241023-observer/igor-omilaev-eGGFZ5X2LnA-unsplash_hu_b8cf37f7dcf0bd0e.webp 400w,
               /blog/20241023-observer/igor-omilaev-eGGFZ5X2LnA-unsplash_hu_8bd80d1d0a8c264e.webp 760w,
               /blog/20241023-observer/igor-omilaev-eGGFZ5X2LnA-unsplash_hu_8861dcb2e0625afe.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241023-observer/igor-omilaev-eGGFZ5X2LnA-unsplash_hu_b8cf37f7dcf0bd0e.webp&#34;
               width=&#34;635&#34;
               height=&#34;529&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      GPU rentals allow small companies to access high-performance A.I. chips for specific projects. &lt;em&gt;Igor Omilaev/Unsplash&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;GPUs, or graphic processing units, have become increasingly difficult to acquire as tech giants like &lt;a href=&#34;https://observer.com/company/openai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&lt;/a&gt; and Meta purchase mountains of them to power A.I. models. Amid &lt;a href=&#34;https://observer.com/2024/07/andreessen-horowitz-stocking-ai-chips-win-deals-gpu-shortage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an ongoing chip shortage&lt;/a&gt;, a crop of startups are stepping up to increase access to the highly sought-after A.I. chips—by renting them out.&lt;/p&gt;
&lt;p&gt;The GPU rental market is part of a niche, existing industry known as GPU-as-a-service where chip owners use an online marketplace to sell compute power to clients over fixed periods of time through the cloud. Typically, companies turn to major cloud providers like &lt;a href=&#34;https://observer.com/company/amazon-web-services/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Web Services&lt;/a&gt;, Microsoft Azure and Google Cloud—which collectively hold a &lt;a href=&#34;https://www.canalys.com/newsroom/worldwide-cloud-services-q2-2024&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;63 percent market share&lt;/a&gt; of the global cloud computing market—to run A.I. workloads on their on-premise data centers.&lt;/p&gt;
&lt;p&gt;GPU-as-as-service, however, provides a more decentralized approach. Providers in that space partner with data centers and GPU owners globally to rent their clusters of chips to clients whenever the need arises. Renting computer power allows organizations with tight budgets, such as startups and academic institutions, access to high-performance GPUs for specific projects, said &lt;a href=&#34;https://observer.com/person/david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;“GPU-as-a-service has significantly leveled the playing field in A.I. and high-performance computing,” Bader told Observer. “Instead of making substantial upfront investments in hardware that quickly depreciates and becomes obsolete, companies can now access GPU power on-demand.”&lt;/p&gt;
&lt;p&gt;Even as supply chain constraints around GPUs start to ease, the rental market continues to grow. The GPU-as-a-service market, valued at $3.79 billion as of 2023, is expected to &lt;a href=&#34;https://www.grandviewresearch.com/industry-analysis/gpu-as-a-service-gpuaas-market-report&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;grow 21.5 percent annually to $12.26 billion&lt;/a&gt; by 2030 as the demand for advanced data analytics, like running machine learning algorithms, increases, according to data from Grand View Research.&lt;/p&gt;
&lt;h2 id=&#34;generative-ai-has-spurred-interest-in-gpu-rentals&#34;&gt;Generative A.I. has spurred interest in GPU rentals&lt;/h2&gt;
&lt;p&gt;Some startups in the GPU rental space have seen demand surging since ChatGPT came out in November of 2022 as companies seek out compute power to build A.I.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://observer.com/person/jake-cannell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jake Cannell&lt;/a&gt;, founder and CEO of Vast.ai, said his company’s customers were primarily cryptocurrency miners before the generative A.I. hype began. Today, more than half of the projects run on Vast.ai’s GPU rentals are A.I.-related. Clients include A.I. entrepreneurs, startups and academics building custom large language models with foundational models like OpenAI’s GPT and deploying LLMs on A.I.-related workloads like A.I.-image generator Stable Diffusion, Cannell said.&lt;/p&gt;
&lt;p&gt;The release of ChatGPT, combined with high demand for major cloud providers and the GPU shortage, pushed more customers to look for alternative options, which has in part accelerated demand for Vast.ai’s GPU rentals, according to Cannell. “That’s probably relaxed a bit now that production has caught up, but demand still seems really high and growing,” the CEO said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://observer.com/company/nvidia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nvidia (NVDA)&lt;/a&gt; CEO Jensen Huang recently said &lt;a href=&#34;https://www.cnbc.com/2024/10/03/nvidia-ceo-demand-for-blackwell-ai-chip-is-insane.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;demand for Nvidia’s new Blackwell chips has been “insane”&lt;/a&gt; and that the company, which owns about 90 percent of the GPU market, plans to ramp up Blackwell production this year through 2026.&lt;/p&gt;
&lt;p&gt;Launched in 2017, Vast.ai is behind an online marketplace that connects owners of GPU clusters from Nvidia and AMD with organizations looking to rent compute power. As of late October, the marketplace offers 109 clusters of GPUs—including Nvidia’s popular H100 chips—housed in data centers and, in some cases, the owners’ garages scattered across the U.S. Europe, Asia and Australia, according to Cannell.&lt;/p&gt;
&lt;p&gt;By offering GPU clusters with different capacities, speeds and system requirements for various lengths of time, Vast.ai aims to provide renters with the freedom to pick GPUs required for specific projects and scale up-and-down depending on their need. For instance, a client developing an A.I. chatbot may initially rent 100 GPUs to train their model. If their product takes off, the client could ramp up their compute capacity by renting out thousands of GPUs. The flexibility to access different amounts of compute at different stages of product development, the company claims, is what makes GPU rentals over purchasing chips appealing.&lt;/p&gt;
&lt;p&gt;“Buying would only make sense if you have a much more predictable, steady demand for GPUs over a very long period of time,” Cannell said. “Only the hypercalers can formulate that,” referring to industry giants like OpenAI.&lt;/p&gt;
&lt;p&gt;While startups like Vast.ai launched prior to ChatGPT are seeing an uptick in interest, new startups have emerged following the chatbot’s release to tap into the growing GPU rental market.&lt;/p&gt;
&lt;p&gt;Foundry, a GPU marketplace built specifically for A.I. workloads, claims it has attracted “dozens” of customers since it launched its cloud platform in August and can significantly reduce compute costs by tapping into the excess power supply of pre-existing chips, according to CEO Jared Quincy Davis.&lt;/p&gt;
&lt;p&gt;The startup, which raised $80 million from investors like Sequoia and Lightspeed Ventures as of March, rents out GPUs through a mix of compute clusters the company owns and “underutilized clusters” sourced from partnerships with data centers.&lt;/p&gt;
&lt;p&gt;Foundry’s customers include companies in the technology, telecommunications, media and health care industries. Foundations and academic labs also use Foundry’s services. Common use cases include fine-tuning models like Meta’s Llama to exhibit desirable properties, building neural networks from scratch, and performing sentiment analyses—a deep learning technique used to analyze text to determine its emotional tone. Foundry even has clients renting GPUs to predict protein sequences for drug discovery, train models to translate rare languages, and build A.I. agents that can control websites without human intervention.&lt;/p&gt;
&lt;p&gt;“Much of the cutting-edge development that could previously only be conducted by labs like OpenAI and &lt;a href=&#34;https://observer.com/company/deepmind/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind&lt;/a&gt; will now be obtainable by others as Foundry makes GPU compute more accessible and affordable,” Davis, who previously worked at Google DeepMind as an engineer, told Observer.&lt;/p&gt;
&lt;p&gt;Some organizations are seeing the benefits of GPU rentals materialize. Bader, the professor at New Jersey Institute of Technology, said he’s seen his university use the GPU rental approach to “free up resources” for “critical activities” like research and development. The GPU rental model, he claims, is ideal for projects with “temporary” or “seasonal compute needs” and “eliminates the burden” of costly hardware management and maintenance. Bader said he has also seen small businesses the university collaborates with access the same GPU power as larger businesses.&lt;/p&gt;
&lt;p&gt;“I’ve witnessed countless startups benefit from this,” Bader said. “They no longer need millions in upfront investment for specialized hardware. Instead, they can prototype, test and iterate their algorithms using rented GPUs, ensuring that funds are directed towards development rather than infrastructure.”&lt;/p&gt;
&lt;h2 id=&#34;renting-out-gpus-may-not-save-that-much-money-long-term&#34;&gt;Renting out GPUs may not save that much money long-term&lt;/h2&gt;
&lt;p&gt;Still, Bader noted that renting out GPUs over purchasing them comes with some trade-offs.&lt;/p&gt;
&lt;p&gt;Performance on shared infrastructure can be inconsistent, which could slow down the execution of tasks like A.I. model training if there’s service disruptions. GPU rentals could also get expensive down the line despite upfront cost savings. The costs of transferring data between the cloud and the company could “add up quickly,” and for workloads that require real-time processing, clients that continuously hit latency issues might end up paying more than if they owned GPUs, according to Bader. The lack of control over the infrastructure could also be “problematic” for companies with strict security and compliance protocols.&lt;/p&gt;
&lt;p&gt;The future of the GPU rental market could also depend on how the chip industry evolves. After all, major cloud providers like Amazon Web Services are expected to &lt;a href=&#34;https://observer.com/2023/11/microsoft-ceo-satya-nadella-unveils-the-companys-first-in-house-a-i-chip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continue expanding their product lines&lt;/a&gt; and are likely to absorb smaller companies, which could lower prices in the short term and limit consumer choice in the long run, according to Bader. Plus, supply chain delays could make it harder for cloud giants to get their hands on GPUs.&lt;/p&gt;
&lt;p&gt;Despite these concerns, the startups that spoke to Observer remain confident there will still be a need for their services in the following years as A.I. continues to grow. Vast.ai continues to improve its GPU matchmaker service and is getting more directly involved in use cases like LLM inference, especially for A.I. agents. Foundry plans to release additional features to increase the accessibility for its platform and make it more useful for A.I. developers to build advanced models.&lt;/p&gt;
&lt;p&gt;“Nvidia is still a leader, and I don’t see that changing overnight, but there is increasingly more competition,” Vast.ai CEO Cannnell said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://observer.com/2024/10/ai-gpu-rental-startup/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://observer.com/2024/10/ai-gpu-rental-startup/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Panel Reminds Us That Artificial Intelligence Can Only Guess, Not Reason for Itself</title>
      <link>http://localhost:1313/blog/20241022-njit/</link>
      <pubDate>Tue, 22 Oct 2024 13:00:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241022-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20241022-njit/2_0_hu_d3cfd1eeaa3f2893.webp 400w,
               /blog/20241022-njit/2_0_hu_7726014030ffbd0b.webp 760w,
               /blog/20241022-njit/2_0_hu_f03cc40774387fb9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241022-njit/2_0_hu_d3cfd1eeaa3f2893.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Expert panelists took a measured tone about the trends, challenges and ethics of artificial intelligence, at a campus forum organized by NJIT’s Institute for Data Science this month.&lt;/p&gt;
&lt;p&gt;The panel moderator was institute director &lt;strong&gt;David Bader&lt;/strong&gt;, who is also a distinguished professor in NJIT Ying Wu College of Computing and who shared his own thoughts on AI in a separate Q&amp;amp;A recently. The panel members were Kevin Coulter, field CTO for AI, Dell Technologies; Grace Wang, distinguished professor and director of NJIT’s Center for Artificial Intelligence Research; and Mengjia Xu, assistant professor of data science. DataBank Ltd., a data center firm that hosts NJIT’s Wulver high-performance computing cluster, was the event sponsor.&lt;/p&gt;
&lt;p&gt;“If you have not heard about AI, you must be in a cocoon. AI is really creating new and extensive opportunities for innovation and knowledge creation,” NJIT President Teik C. Lim said while introducing the panel. “AI is arguably going to have the greatest effect on the creation and the delivery of knowledge goods and services, since the invention of the internet and smartphone.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;We have been working on AI before AI became a buzzword.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;“We have been working on AI before AI became a buzzword,” Lim noted. An archive search found that AI was part of university research before New Jersey Institute of Technology existed, with results as far back as 1974 by Newark College of Engineering doctoral student John Comerford.&lt;/p&gt;
&lt;p&gt;Following are highlights of the panel comments, lightly edited for clarity, in the order they were presented.&lt;/p&gt;
&lt;p&gt;Bader: “What do you see as the most pressing challenges and opportunities in the field of AI today?”&lt;/p&gt;
&lt;p&gt;Xu: “I think we have a lot of different scientific questions that we can apply AI techniques like the large language model and foundation models. I think we need collaborations from different domains. I think we need understanding of the underlying principles of deep learning, including the very powerful transformer model [and] GPT models and so on, from the mathematical side and also working with computer scientists [to] bridge the gap between the different domains.”&lt;/p&gt;
&lt;p&gt;Wang: “The first thing I did is open ChatGPT and ask, what does ChatGPT believe is the challenge and also the opportunity. And I got a very long essay on ethical issues. There&amp;rsquo;s a lot of different topics related to challenges and opportunities, but I wasn’t satisfied to be frank. I think the biggest challenge and also the opportunity is how to monetize the investment. … So for a very big company, it&amp;rsquo;s very hard and you have to make a big investment. For the startup, the entrance barrier is actually very low because if you want to play, you just call the larger language model API. So basically everyone can have a small AI startup.&lt;/p&gt;
&lt;p&gt;Bader: “I should come clean. You mentioned you looked at ChatGPT about the questions I was asking. Okay, I&amp;rsquo;ll come clean now. So I wrote a bunch of questions for the panel, and they&amp;rsquo;re okay, but I went to ChatGPT with the questions, with your biographies, with some of the goals I had for the panel, and my gosh, it came up with some much better questions. And I thought this is an AI panel anyway [laughs]. So for full disclosure, Thank you to ChatGPT and to Anthropic’s Claude, because I wanted to see if they would all give me different questions. And in fact, they agreed with each other as to the questions I should ask. So they really did help out. And I figured maybe we don&amp;rsquo;t even need me as moderator. There could be some AI sitting here! Maybe next year that&amp;rsquo;s what you&amp;rsquo;ll see at this lecture, but today you have us.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Every day something new comes out in AI, some new tool, a new startup. So as soon as you think you know something, all of a sudden you don&amp;rsquo;t have it.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Coulter: “It&amp;rsquo;s the vastness of everything that&amp;rsquo;s out there. There&amp;rsquo;s so much to take in with AI, whether it&amp;rsquo;s the technology stack, whether it&amp;rsquo;s a software stack, whether it&amp;rsquo;s talking in terms of AI or who you&amp;rsquo;re talking to. Talking to a data scientist is very different than talking to a CFO and being able to integrate these conversations. We talk within our company a lot around value versus feasibility, and how to align those two with our customers, because they vary from ‘I don&amp;rsquo;t know where to begin’ to organizations like NJIT which has a whole board associated with AI. So it&amp;rsquo;s filling that gap, I think. And the other thing I think is a big challenge is just the amount of change. Every day something new comes out in AI, some new tool, a new startup. So as soon as you think you know something, all of a sudden you don&amp;rsquo;t have it.”&lt;/p&gt;
&lt;p&gt;Bader: “There&amp;rsquo;s also a lot of concerns that get raised with AI in terms of privacy, in terms of ethics, in terms of its usage. So I really want to understand your thoughts on how we ensure that AI systems are developed and deployed ethically. And are there specific frameworks or guidelines that you would follow?”&lt;/p&gt;
&lt;p&gt;Coulter: “It&amp;rsquo;s interesting just seeing the dynamics going on right now. … I&amp;rsquo;m a musician that&amp;rsquo;s playing music, so I follow the music industry quite a bit, and there&amp;rsquo;s somebody who created literally hundreds of ideas and made ten million dollars on AI music. It wasn&amp;rsquo;t even his own, he just created a program to do this. And I think the way to tackle this, really is in an ecosystem. We talk about ecosystems all the time, talking to each other, working with each other, having diversity in every aspect of what&amp;rsquo;s happening out there. That way you get different perspectives. … In innovation, I think ethics will start to become just a natural part of developing these AI systems.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Sometimes it looks ethical but maybe what&amp;rsquo;s behind it is amplifying the bias.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Wang: “Well, I always believe that AI at its core is just a tool, so there&amp;rsquo;s no difference for the AI and say, lock picking tools. Now, picking tools can open your door if you lock yourself out and it can also open others. That&amp;rsquo;s a crime, right? So it depends on how AI is used. From that perspective, there&amp;rsquo;s not much special when we talk about AI ethics, or, say, computer security ethics, or the ethics related to how to use a gun, for example. But what is different is, as AI is too complex, it&amp;rsquo;s beyond the knowledge of many of us how it works. Sometimes it looks ethical but maybe what&amp;rsquo;s behind it is amplifying the bias by using the AI tools without our knowledge. So whenever we talk about AI ethics, I think the most important one is education if you know what AI is about, how it works and what AI can do and what AI cannot. I think for now we have the fear that AI is so powerful it can do anything, but actually, many of the things that people believe AI can do now can be done in the past by just any software system. So education is very, very important to help us to demystify AI accordingly, so we can talk about AI ethics. I want to emphasize transparency. If AI is used for decision making, if we understand how the decision is made, that becomes very, very important. And another important topic related to AI ethics is auditing if we don&amp;rsquo;t know what&amp;rsquo;s inside. At least we have some assessment tools to know whether there&amp;rsquo;s a risk or not in certain circumstances. Whether it can generate a harmful result or is not very much like the stress testing to the financial system after 2008.”&lt;/p&gt;
&lt;p&gt;Bader: “AI has really come into the mainstream. It’s probably the fastest that we&amp;rsquo;ve seen go from a technology lab into interacting with the general public, and we see AI used for everything from lawyers writing cases, we&amp;rsquo;ve seen it being used in healthcare for evaluating patients by looking at imagery, we&amp;rsquo;ve seen it really in many, many sectors. And what I&amp;rsquo;m wondering about for AI research, something that NJIT does and is a leader in, is how should research interact with other fields such as law, healthcare and ethics, and how do we approach more of that interdisciplinary collaboration?”&lt;/p&gt;
&lt;p&gt;Coulter: “Every time I talk to a customer about some of the concerns that they bring out, I always have to highlight the fact that human discernment does not go away in AI. We still have to interact. Human annotation is one of the greatest aspects of working with retraining an AI model. That interaction is important using scientific methodologies. I think sometimes when we talk day-to-day to scientists, we forget they’re scientists. Let&amp;rsquo;s start with theory. Improve the theory. Have the peers all talk to each other. Can it be proven over and over again? All those methods are critical to how we&amp;rsquo;re using and leveraging the AI space. We really have to make sure we&amp;rsquo;re looking at this from a holistic perspective and using these research methods as part of our natural process of developing these tools. I think that&amp;rsquo;s the way we become more and more successful.”&lt;/p&gt;
&lt;p&gt;Audience member: “There is industry debate about whether AI can already, or soon will, have the ability to reason. What do you think?”&lt;/p&gt;
&lt;p&gt;Wang: “This question has been asked many times, and there is ongoing heated debate about it. Someone believed that the current path of all our language models seems hopeless, but others believe this is the right way. My own opinion is we’re not there.”&lt;/p&gt;
&lt;p&gt;Coulter: “It&amp;rsquo;s a tool. It&amp;rsquo;s a tool as far as learning from each other. It&amp;rsquo;s interesting because there is this tendency I find with a lot of times when customers come in concerned about putting human psychology over AI. I don&amp;rsquo;t think it&amp;rsquo;s even close to that. In fact, we had one customer who&amp;rsquo;s a banking customer who said, AI doesn&amp;rsquo;t really know anything about investment banking really. It just knows the language about it, and can sort of figure out what to look for that would be investment banking-related, right? So I think that’s a long way from anything that&amp;rsquo;s sentient or knows what&amp;rsquo;s going on.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Be open to any new tools and the new skills, and be prepared that whatever skill you have may no longer be needed shortly. So just be prepared. … Being a lifetime learner is very, very important.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Bader: “What advice would you give to aspiring AI professionals?&lt;/p&gt;
&lt;p&gt;Wang: “Be open to any new tools and new skills, and be prepared that whatever skill you have may no longer be needed shortly. So just be prepared. … Being a lifetime learner is very, very important. Because of the availability of the AI tools, that means whenever you need help, you can have help. Well, in the past, if you write an essay and you want someone to revise it, you have to submit it to your professor or turn to friends. There is a long time for you to get the feedback, but now, because of the availability of the tools, you can immediately get the feedback to improve yourself. [Also] if you did not keep up with the technology for five years, then probably you are really out. Others accumulated the skills that you don&amp;rsquo;t have. But now I think the nice thing is, if you are out for five years because of the tools, you can quickly catch up. So this is a very dynamic time.”&lt;/p&gt;
&lt;p&gt;Coulter: “I&amp;rsquo;m going to put my dad hat on for a minute. Keep in mind, the answer you&amp;rsquo;re getting from AI might not be the right answer. Verify, validate! It’s this whole trust-but-verify conversation that I have with a lot of customers. I think the other important thing, too, is the social aspect of AI. I think you&amp;rsquo;re learning a lot, you&amp;rsquo;re going to be learning a lot, you&amp;rsquo;re going to continue to learn at an accelerated pace, because the information is going to be readily available. Work with your peers. Talk about what you&amp;rsquo;re finding, talk about what&amp;rsquo;s going on. Talk about what&amp;rsquo;s right, what&amp;rsquo;s wrong and ways they might be using AI. … Find out where the information is coming from and who wrote it. That&amp;rsquo;s just as important as finding the answer itself. So that&amp;rsquo;d be my recommendation. Just stay curious and do diligence.”&lt;/p&gt;
&lt;p&gt;Wang: “Personally, I feel that the government should pay more attention to the regulations. But on the other hand, at least in this Supreme Court they are super open to new technology. So here I want to emphasize again about the importance of education transparency as well as auditing. They require the judges and the attorneys to be aware of how AI works, so when they are presented with certain evidence that is related to AI, they know how to treat this evidence and how they should make a decision that this particular evidence can be presented properly or not.&lt;/p&gt;
&lt;p&gt;Bader: “How do you bridge the gap between the technical complexities of AI and the strategic needs of businesses?”&lt;/p&gt;
&lt;p&gt;Coulter: What we&amp;rsquo;re trying to do as a company is operationalize a lot of what&amp;rsquo;s happening. From a technology staff perspective, I think it&amp;rsquo;s simpler to deploy a use test, which then from a business perspective, I can talk about value, I can talk about use cases, and then it&amp;rsquo;s more of here’s the value. Let&amp;rsquo;s take a look at the underpinnings of what&amp;rsquo;s going on in your organization — do you have the right technology today? Do you have the data? Is it clean data? I can focus on business processes, and I don&amp;rsquo;t have to worry about the underlying technology stack, because we&amp;rsquo;re always deploying those and innovating them and working on them every single day. Nvidia and AMD and Qualcomm are all our partners out there who are always innovating, so it&amp;rsquo;s really just making the technology conversation easier in a way.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/panel-reminds-us-artificial-intelligence-can-only-guess-not-reason-itself&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/panel-reminds-us-artificial-intelligence-can-only-guess-not-reason-itself&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Tech Goes Nuclear To Quench AI Energy Demands</title>
      <link>http://localhost:1313/blog/20241017-techstrongai/</link>
      <pubDate>Thu, 17 Oct 2024 13:12:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241017-techstrongai/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://techstrong.ai/author/jon-swartz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jon Swartz&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20241017-techstrongai/Nuke-scaled-e1729204393334-1200x516_hu_d7dba12215e1bb55.webp 400w,
               /blog/20241017-techstrongai/Nuke-scaled-e1729204393334-1200x516_hu_664437bcc56f933.webp 760w,
               /blog/20241017-techstrongai/Nuke-scaled-e1729204393334-1200x516_hu_7e4d7e33966dbf04.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241017-techstrongai/Nuke-scaled-e1729204393334-1200x516_hu_d7dba12215e1bb55.webp&#34;
               width=&#34;760&#34;
               height=&#34;327&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Big Tech is going nuclear in an escalating race to meet growing energy demands.&lt;/p&gt;
&lt;p&gt;Amazon.com Inc., Alphabet Inc.’s Google and Microsoft Corp. are pouring billions of dollars into nuclear energy facilities to supply companies with emissions-free electricity to feed their artificial intelligence services.&lt;/p&gt;
&lt;p&gt;The tech giants are scrambling to strike deals and partnerships with operators and developers of nuclear power plants to refurbish traditional power plants or build small modular reactors, which offer a smaller footprint and quicker building times.&lt;/p&gt;
&lt;p&gt;Amazon is making perhaps the biggest splash. It is leading a $500 million funding round for X-Energy Reactor, a company that specializes in SMRs through which Amazon intends to bring more than 5GW of new power projects online in the U.S. by 2039.&lt;/p&gt;
&lt;p&gt;Amazon is exploring options in Washington, Pennsylvania and Virginia, where it is pursuing an SMR project with Dominion Energy near the North Anna nuclear power station.&lt;/p&gt;
&lt;p&gt;“One of the fastest ways to address climate change is by transitioning our society to carbon-free energy sources, and nuclear energy is both carbon-free and able to scale — which is why it is an important area of investment for Amazon,” Amazon Web Services Chief Executive Matt Garman said in a statement. “Our agreements will encourage the construction of new nuclear technologies that will generate energy for decades to come.”&lt;/p&gt;
&lt;p&gt;Google, meanwhile, recently inked a partnership with Kairos Power to develop advanced reactor plants under Power Purchase Agreements in a bid to supply clean electricity to Google data centers by 2030.&lt;/p&gt;
&lt;p&gt;Last year, Microsoft let it be known what it thinks of nuclear energy, a source of power long supported by co-founder and former CEO Bill Gates. [Gates has invested more than $1 billion in startup TerraPower, which is developing smaller reactors in partnership with Warren Buffett’s utility company PacifiCorp.]&lt;/p&gt;
&lt;p&gt;In 2023, the software giant listed a job for a principal program manager to lead its nuclear energy strategy. Since then, Microsoft has agreed to pay Constellation Energy to resuscitate the shuttered Three Mile Island nuclear power plant in Pennsylvania, and it has agreed to buy power from Helion Energy, a Seattle startup that seeks to build the world’s first nuclear fusion power plant by 2028.&lt;/p&gt;
&lt;p&gt;Indeed, even the Biden Administration sees the wisdom in nuclear power, which provides about 20% of the U.S.’s electricity in reducing greenhouse gas emissions. “Revitalizing America’s nuclear sector is key to adding more carbon-free energy to the grid and meeting the needs of our growing economy — from AI and data centers to manufacturing and health care,” Energy Secretary Jennifer M. Granholm said in a statement.&lt;/p&gt;
&lt;p&gt;As AI grows, so has the need for more energy despite a spike in data centers, hence the nod to nuclear.&lt;/p&gt;
&lt;p&gt;“These tech companies are doing this not because they want to, but because they have to,” James Walker, a nuclear physicist and CEO of &lt;a href=&#34;https://nanonuclearenergy.com/?v=0b3b97fa6688&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NANO Nuclear Energy Inc.&lt;/a&gt;, said in an interview. “NVIDIA will be in the same boat. They and others are capped by what they can remove from the grid. And for many, SMRs are the most likely path because they are more affordable, scalable, and an deployable option.”&lt;/p&gt;
&lt;p&gt;There are 94 active nuclear power plant reactors in the U.S. — the most of any country — though the nation has tried to build 250 since 1960. Many fell by the wayside because of steep costs, construction delays and a paucity of permanent storage for spent nuclear fuel.&lt;/p&gt;
&lt;p&gt;“The predictability of nuclear energy output contrasts with the intermittency of renewable sources like solar or wind, making it a reliable base load energy supply for AI-driven operations,” &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of the Institute for Data Science at New Jersey Institute of Technology, said in an email. “The development and deployment of advanced nuclear technologies, such as SMRs, also align with sustainability goals by providing a clean, long-term energy source that reduces dependency on fossil fuels. As AI continues to drive global economic growth, the scalability and environmental benefits of nuclear power make it a vital component of the future energy mix.”&lt;/p&gt;
&lt;p&gt;Given their insatiable thirst for clean energy, tech’s largest players are expected to look at nuclear power plants, SMRs and other alternatives.&lt;/p&gt;
&lt;p&gt;One option under consideration are &lt;a href=&#34;https://nanonuclearenergy.com/microreactors/?v=0b3b97fa6688&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;micro nuclear reactors&lt;/a&gt;, which are smaller than SMRs and can be built faster and at a fraction of the cost. Such reactors generate less than 20 megawatts of thermal energy that can be used to generate electricity and/or provide heat for industrial uses such as data centers or military bases. The compact reactors are small enough to fit on a semi-truck and can be easily deployed in remote locations.&lt;/p&gt;
&lt;p&gt;Microreactors cost less than conventional nuclear reactors and SMRs, and do not use any liquid coolant, thus eliminating the possibility of a meltdown.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://techstrong.ai/articles/big-tech-goes-nuclear-to-quench-ai-energy-demands/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://techstrong.ai/articles/big-tech-goes-nuclear-to-quench-ai-energy-demands/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Renowned data scientist David Bader to present Sigma Xi Distinguished Lectureship</title>
      <link>http://localhost:1313/blog/20241004-ksu/</link>
      <pubDate>Fri, 04 Oct 2024 17:45:43 -0500</pubDate>
      <guid>http://localhost:1313/blog/20241004-ksu/</guid>
      <description>&lt;p&gt;&lt;em&gt;Submitted by Martha Mather&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20241004-ksu/2024-Oct-03_2154_12-thumbna_hu_cba2e6747988f870.webp 400w,
               /blog/20241004-ksu/2024-Oct-03_2154_12-thumbna_hu_697d4fb6566a80c8.webp 760w,
               /blog/20241004-ksu/2024-Oct-03_2154_12-thumbna_hu_e296050b7ac3a5db.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241004-ksu/2024-Oct-03_2154_12-thumbna_hu_cba2e6747988f870.webp&#34;
               width=&#34;500&#34;
               height=&#34;500&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Kansas State University chapter of Sigma Xi, The Scientific Research Honor Society, presents a distinguished lecture by &lt;strong&gt;David Bader&lt;/strong&gt; on Tuesday, Oct. 8. In his distinguished lectureship, Bader will share his views on &amp;ldquo;Solving Global Grand Challenges with High Performance Data Analytics&amp;rdquo; in the Wildcat Chamber at the K-State Student Union. All are encouraged to attend. You do not need to be a member.&lt;/p&gt;
&lt;p&gt;Bader is a distinguished professor and director of the Institute for Data Science at the New Jersey Institute of Technology. He is also a distinguished professor and founder of the department of data science in the Ying Wu College of Computing; and a selected Distinguished Sigma Xi Lecturer. He served as founding professor and chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. Bader is an elected board member of the Computing Research Association. He is a fellow of the IEEE, ACM, UAAAS and SIAM; a recipient of the IEEE Sidney Fernbach Award; and the 2022 Innovation Hall of Fame inductee of the University of Maryland&amp;rsquo;s A. James Clark School of Engineering. The Computer History Museum recognizes Bader for developing the first Linux-based supercomputer, which became the predominant architecture for all major supercomputers in the world.&lt;/p&gt;
&lt;p&gt;As part of his visit, Bader will participate in the following three opportunities on Oct. 8 for faculty, students and staff to discuss innovations in data science and opportunities for interdisciplinary science collaborations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;9:30-10:45 a.m., 121 Eisenhower Hall: Open discussion on linking data science and other disciplines to initiate collaborations. All faculty and students are welcome. Contact &lt;a href=&#34;mailto:mmather@k-state.edu&#34;&gt;mmather@k-state.edu&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;Noon to 1 p.m., 118 Eisenhower Hall: &amp;ldquo;Conversations with a Renowned Scientist.&amp;rdquo; Students are invited to exchange science and career ideas with Bader. Contact &lt;a href=&#34;mailto:sytsmaj134@k-state.edu&#34;&gt;sytsmaj134@k-state.edu&lt;/a&gt; for more information.&lt;/li&gt;
&lt;li&gt;3-5 p.m. Wildcat Chamber, K-State Student Union: Contact &lt;a href=&#34;mailto:cpetrescu@k-state.edu&#34;&gt;cpetrescu@k-state.edu&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sigma Xi lecture schedule:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3-3:25 p.m.: Social&lt;/li&gt;
&lt;li&gt;3:25-3:30 p.m.: Introduction of David Bader&lt;/li&gt;
&lt;li&gt;3:30-4 p.m.: Bader&amp;rsquo;s presentation&lt;/li&gt;
&lt;li&gt;4-4:20 p.m.: Q&amp;amp;A session&lt;/li&gt;
&lt;li&gt;4:20-5 p.m.: Presentation of K-State Sigma Xi 2024 Outstanding Scientist Awards — Martha Mather and Ajay Sharda.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The K-State chapter of Sigma Xi is a unique opportunity for scientists on campus with diverse training to interact. With changes in the world and the university, we hope you will join us in taking a renewed interest in developing and strengthening our campuswide professional scientific interactions.&lt;/p&gt;
&lt;p&gt;If you have not already done so, please &lt;a href=&#34;https://www.sigmaxi.org/members/becoming-a-member&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;complete&lt;/a&gt; or &lt;a href=&#34;https://www.sigmaxi.org/members/renew&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;renew&lt;/a&gt; your Sigma Xi membership.&lt;/p&gt;
&lt;p&gt;For more information, contact the K-State Sigma Xi leadership team, &lt;a href=&#34;cpetrescu@k-state.edu&#34;&gt;Claudia Petrescu&lt;/a&gt;, &lt;a href=&#34;ciampitti@k-state.edu&#34;&gt;Ignacio Ciampitti&lt;/a&gt;, &lt;a href=&#34;mmather@k-state.edu&#34;&gt;Martha Mather&lt;/a&gt; and &lt;a href=&#34;sytsmaj134@k-state.edu&#34;&gt;Jack Sytsma&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.k-state.edu/today/announcement/?id=98953&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.k-state.edu/today/announcement/?id=98953&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CNN Video: Helene Damage Threatens Global Tech Supply Chain</title>
      <link>http://localhost:1313/blog/20241004-cnn/</link>
      <pubDate>Fri, 04 Oct 2024 14:33:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241004-cnn/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/C6FxLDUn2-w?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;CNN correspondent Ivan Rodriguez video story on Hurricane Helene damage threatens global tech supply chain. Expert commentary from &lt;strong&gt;David Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.lee.net/news/national/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_30b7dbbf-a44f-51e3-be05-63f40b134e11.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.lee.net/news/national/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_30b7dbbf-a44f-51e3-be05-63f40b134e11.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.local3news.com/regional-national/helene-damage-threatens-global-tech-supply-chain-3pmet/video_e3e92b7c-0052-526e-b0a2-2554581cf726.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.local3news.com/regional-national/helene-damage-threatens-global-tech-supply-chain-3pmet/video_e3e92b7c-0052-526e-b0a2-2554581cf726.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.koamnewsnow.com/news/national-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_4018c676-1589-54d3-bfb7-fa5115e85f6e.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.koamnewsnow.com/news/national-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_4018c676-1589-54d3-bfb7-fa5115e85f6e.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.channel3000.com/news/national-and-world-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_b0adc48b-a7de-5bcb-98be-8e0d839c6b6f.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.channel3000.com/news/national-and-world-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_b0adc48b-a7de-5bcb-98be-8e0d839c6b6f.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.applevalleynewsnow.com/news/national-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_36ed2d87-e6b7-5692-8f5d-e89564a5599c.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.applevalleynewsnow.com/news/national-news/dnt-helene-damage-threatens-global-tech-supply-chain-3pmet/video_36ed2d87-e6b7-5692-8f5d-e89564a5599c.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.2news.com/weather/ap_weather_headlines/helene-damage-threatens-global-tech-supply-chain-3pmet/video_27923d26-60c8-53fa-b340-459c40b45539.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.2news.com/weather/ap_weather_headlines/helene-damage-threatens-global-tech-supply-chain-3pmet/video_27923d26-60c8-53fa-b340-459c40b45539.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kxly.com/news/national-and-world-news/helene-damage-threatens-global-tech-supply-chain-3pmet/video_c1341b92-25a9-5d10-9ce5-a80fc8882d2f.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kxly.com/news/national-and-world-news/helene-damage-threatens-global-tech-supply-chain-3pmet/video_c1341b92-25a9-5d10-9ce5-a80fc8882d2f.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.facebook.com/watch/live/?ref=watch_permalink&amp;amp;v=535520112457805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/watch/live/?ref=watch_permalink&amp;v=535520112457805&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Devastation from Hurricane Helene could bring semiconductor chipmaking to a halt</title>
      <link>http://localhost:1313/blog/20241002-cnn/</link>
      <pubDate>Wed, 02 Oct 2024 13:47:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20241002-cnn/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.cnn.com/profiles/clare-duffy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clare Duffy&lt;/a&gt; and &lt;a href=&#34;https://www.cnn.com/profiles/dianne-gallagher-profile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dianne Gallagher&lt;/a&gt;, CNN&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-aftermath-of-hurricane-helene-seen-near-spruce-pine-north-carolina-which-supplies-much-of-the-worlds-high-purity-quartz-for-semiconductor-manufacturing-courtesy-dr-barbara-a-stagg&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Aftermath of Hurricane Helene seen near Spruce Pine, North Carolina, which supplies much of the world&amp;#39;s high-purity quartz for semiconductor manufacturing. *Courtesy Dr. Barbara A. Stagg*&#34; srcset=&#34;
               /blog/20241002-cnn/pic1_hu_d47c68bf6c24f2a9.webp 400w,
               /blog/20241002-cnn/pic1_hu_8f31b55334eb7706.webp 760w,
               /blog/20241002-cnn/pic1_hu_f99344181f427f02.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241002-cnn/pic1_hu_d47c68bf6c24f2a9.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Aftermath of Hurricane Helene seen near Spruce Pine, North Carolina, which supplies much of the world&amp;rsquo;s high-purity quartz for semiconductor manufacturing. &lt;em&gt;Courtesy Dr. Barbara A. Stagg&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;New York, CNN —&lt;/strong&gt;
The devastation in North Carolina in the wake of Hurricane Helene could have serious implications for a niche — but extremely important — corner of the tech industry.&lt;/p&gt;
&lt;p&gt;Tucked in the Blue Ridge Mountains on the outskirts of Spruce Pine, a town of less than 2,200, are two mines that produce the world’s purest quartz, which formed in the area some 380 million years ago. The material is a key component in the global supply chain for semiconductor chips, which power everything from smartphones and cars to medical devices and solar panels.&lt;/p&gt;
&lt;p&gt;But operations at the facilities have halted since &lt;a href=&#34;https://www.cnn.com/2024/09/27/us/video/asheville-north-carolina-flooding-helene-rosales-cnc-digvid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hurricane Helene&lt;/a&gt; tore through the southeast United States over the weekend, causing &lt;a href=&#34;https://www.cnn.com/2024/09/27/us/video/asheville-north-carolina-flooding-helene-rosales-cnc-digvid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;historic flooding&lt;/a&gt; and landslides, cutting off roads and power and endangering millions of residents.&lt;/p&gt;
&lt;p&gt;Sibelco and The Quartz Corp, the companies that separately manage the two mines, say they shut down operations on September 26 ahead of the storm and are working to restart. But it’s not clear just how severely damaged the mines are and how long it may take to get them back up and running.&lt;/p&gt;
&lt;p&gt;What is clear is that the facilities, like the rest of the surrounding area, are grappling with infrastructure disruptions such as flooding, power outages, road shutdowns and a lack of phone service. And, crucially, they are still trying to get in contact with all of their local employees — who make up their specialized workforce — many of whom have been displaced or have seen their homes damaged.&lt;/p&gt;
&lt;p&gt;Supply chain experts say it could take weeks to make the mines operational again, which could mean chip shortages and price hikes at a particularly bad time for the tech industry, as Silicon Valley giants plow billions of dollars into chips to run artificial intelligence systems.&lt;/p&gt;
&lt;p&gt;“If you wanted to identify one mine complex that is critically important to the semiconductor manufacturing industry, and also solar panel industry, it’s the Sibelco and The Quartz Corp mines in Spruce Pine,” said Seaver Wang, co-director of the climate and energy program at environmental research center The Breakthrough Institute.&lt;/p&gt;
&lt;p&gt;Spencer Bost, executive director of Downtown Spruce Pine, an economic development organization, told CNN Tuesday that “the level of devastation here is insane.”&lt;/p&gt;
&lt;p&gt;“We know nothing right now about damage for (the mining companies) right now, but the damage in Spruce Pine is so devastating that I’m not sure when they will have employees able to return,” Bost said. “People are hurting, properties are destroyed, there are places where the roads no longer exist … My fiancée is a third-grade teacher, and her principal got a hold of her yesterday and said basically ‘the school is gone.’”&lt;/p&gt;
&lt;p&gt;Sibelco, the Belgian company whose Spruce Pine mine is the county’s largest employer, said in a post on its website that it had “confirmed the safety of most employees and are working diligently to contact those still unreachable due to ongoing power outages and communication challenges.”&lt;/p&gt;
&lt;p&gt;“Please rest assured that Sibelco is actively collaborating with government agencies and third-party rescue and recovery operations to mitigate the impact of this event and to resume operations as soon as possible,” it said.&lt;/p&gt;
&lt;p&gt;The Quartz Corp, Sibelco’s smaller but still important neighbor that’s owned jointly by a French and a Norwegian mineral company, called the fallout from the hurricane a “dramatic situation for the region” and said it has “no visibility” into when its operations might restart.&lt;/p&gt;
&lt;p&gt;“Our focus is to ensure that our employees and their families are safe while all efforts are made to contact those which are still unreachable. In addition to this, our teams are joining the local taskforces to try to restore the most basic services and bring further supplies to Spruce Pine,” The Quartz Corp said in a statement.&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-quartz-corp-facility-near-spruce-pine-north-carolina-is-seen-during-a-cnn-flyover-of-the-area-on-monday-september-30-cnn&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Quartz Corp facility near Spruce Pine, North Carolina, is seen during a CNN flyover of the area on Monday, September 30. *CNN*&#34; srcset=&#34;
               /blog/20241002-cnn/pic2_hu_9c0e443622a5fff2.webp 400w,
               /blog/20241002-cnn/pic2_hu_967ad8e0bd2bf87b.webp 760w,
               /blog/20241002-cnn/pic2_hu_7c2f85d42d7e9a1a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20241002-cnn/pic2_hu_9c0e443622a5fff2.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Quartz Corp facility near Spruce Pine, North Carolina, is seen during a CNN flyover of the area on Monday, September 30. &lt;em&gt;CNN&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;a-rare-ingredient-for-a-critical-industry&#34;&gt;A rare ingredient for a critical industry&lt;/h2&gt;
&lt;p&gt;Quartz is an essential ingredient in the semiconductor manufacturing process. And its purity is crucial to avoid damaging the chips.&lt;/p&gt;
&lt;p&gt;“You’re building these incredibly complicated chips that have, in some cases, 100 billion transistors, 100 billion tiny little machines, on a chip that is the size of your thumbnail … One atom being out of place could mean a defect that breaks the chip,” said Gregory Allen, director of the Wadhwani Center for AI and Advanced Technologies at the Center for Strategic and International Studies.&lt;/p&gt;
&lt;p&gt;And while quartz is abundant around the world, the kind of ultra-high-purity quartz mined in Spruce Pine is not. The Spruce Pine mines provide an estimated 80% to 90% of the world’s high-purity quartz — experts say the exact amount is proprietary and unknown — supplying semiconductor manufacturers like chipmaking giant Taiwan Semiconductor Manufacturing Company.&lt;/p&gt;
&lt;p&gt;Some chip manufacturers may have up to several weeks’ supply of high-purity quartz to be able to continue production, but a longer shutdown at the mines will likely mean chip shortages. In some cases, purifying regular quartz can be used as an alternative, but the world doesn’t have the capacity to do that purification in high enough quantities to make up for the loss of Spruce Pine.&lt;/p&gt;
&lt;p&gt;“I would expect there to be a pause and a disruption in the supply chain for major chip manufacturers … as they wait for these mines to reopen,” said &lt;strong&gt;David Bader, professor and director of the Institute for Data Science at New Jersey Institute of Technology&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What’s more, even if the mines themselves are able to reopen, they will need local infrastructure, like roads, to get the product out to customers.&lt;/p&gt;
&lt;p&gt;A chip shortage would cause many industries to grind to a halt. The 2021 global chip shortage caused by the Covid-19 pandemic meant, for example, that car manufacturers were building nearly complete cars but couldn’t ship them because they lacked chips to power critical features, driving up car prices.&lt;/p&gt;
&lt;p&gt;“Most of the American economy, to some greater or lesser extent, is downstream from the semiconductor industry as a critical input,” CSIS’s Allen said.&lt;/p&gt;
&lt;p&gt;However, Wang said that despite the scale of the devastation in Spruce Pine, there is still reason for optimism.&lt;/p&gt;
&lt;p&gt;“This mine is considered a national strategic asset, and I would imagine that the federal government is going to move hell and high water to get it up and running as fast as possible … everyone knows this mining complex is important,” Wang said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnn.com/2024/10/02/tech/semiconductor-supply-chain-north-carolina-helene/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnn.com/2024/10/02/tech/semiconductor-supply-chain-north-carolina-helene/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-us/news/us/devastation-from-hurricane-helene-could-bring-semiconductor-chipmaking-to-a-halt/ar-AA1rAZkG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-us/news/us/devastation-from-hurricane-helene-could-bring-semiconductor-chipmaking-to-a-halt/ar-AA1rAZkG&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aol.com/finance/devastation-hurricane-helene-could-bring-164952109.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.aol.com/finance/devastation-hurricane-helene-could-bring-164952109.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Devotes Over $10 Million in New Funds for Push in Artificial Intelligence</title>
      <link>http://localhost:1313/blog/20240916-njit/</link>
      <pubDate>Mon, 16 Sep 2024 07:22:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240916-njit/</guid>
      <description>

















&lt;figure  id=&#34;figure-establishes-grace-hopper-ai-research-institute-with-support-from-anonymous-donor&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Establishes Grace Hopper AI Research Institute with Support from Anonymous Donor&#34; srcset=&#34;
               /blog/20240916-njit/eberhardt_HDR_hu_1e292b6eadef98e3.webp 400w,
               /blog/20240916-njit/eberhardt_HDR_hu_eedcaeff41eda9ad.webp 760w,
               /blog/20240916-njit/eberhardt_HDR_hu_b07dbb9491cd86bd.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240916-njit/eberhardt_HDR_hu_1e292b6eadef98e3.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Establishes Grace Hopper AI Research Institute with Support from Anonymous Donor
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey Institute of Technology (NJIT) is launching a new, $10+ million initiative that will significantly advance the university’s strength in the field of artificial intelligence (AI) and position NJIT to become a leader in both AI research and application in higher education. This augments already significant existing academic and research activity in AI and related programs. In 2023 alone, NJIT research related to AI totaled nearly $60 million. This new effort will leverage the university’s extensive expertise in machine learning, computer vision, natural language processing, and robotics, with the goal of becoming a major player in AI innovation.&lt;/p&gt;
&lt;p&gt;As part of the new investment, NJIT will establish the Grace Hopper AI Research Institute with support from an anonymous donor and matching funds that total $6 million. The Institute is named for American computer scientist, mathematician, and United States Navy rear admiral Grace Brewster Hopper, who was a pioneer of computer programming. The new institute will include existing initiatives, such as the &lt;a href=&#34;https://research.njit.edu/ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Center for AI Research&lt;/a&gt; and the &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt; — where faculty members and researchers, along with students, study topics such as data analysis and scientific computing — and also will apply AI across diverse fields, such as architecture, biology, engineering, and management, ensuring that AI research benefits multiple sectors. Through collaboration with the New Jersey Innovation Institute, an NJIT corporation, the Grace Hopper AI Research Institute also will form partnerships with healthcare, defense, finance, and manufacturing industries, creating AI solutions for real-world challenges.&lt;/p&gt;
&lt;p&gt;An additional $4+ million investment by NJIT will support initiatives that include an expansion of the top-tier AI talent at NJIT and the creation of a Center for Educational Innovation Excellence, where experts will study AI’s role in enhancing curricula and effective teaching/learning. NJIT is already well underway in equipping faculty to use generative learning, as part of an integrated partnership between the administration, instructional staff and the Office of Information Services &amp;amp; Technology.&lt;/p&gt;
&lt;p&gt;The expansion of NJIT’s AI prowess in research and application aligns perfectly with the university’s 2030 Strategic Plan, which calls for NJIT to become a nexus of innovation that is a physical and intellectual focal point for ideas, actions and people and brings together researchers, learners, entrepreneurs and partners from government, industry and the community to pursue innovation. The AI push will dramatically enhance learner advancement, faculty success, digital transformation and expansion of industry partnerships, all of which are areas of focus within that strategic plan.&lt;/p&gt;
&lt;p&gt;“Generative AI is creating new opportunities for innovation and knowledge creation while also challenging the traditional models of R&amp;amp;D and operations,” said NJIT President Teik C. Lim. “At the same time, the development of intellectual property and the translation and commercialization of research requires dedicated investment. AI is arguably going to have the greatest effect on the creation and delivery of knowledge goods and services since the advent of the Internet and the smartphone. NJIT will focus its research enterprise on collaborative, large-scale, applied projects in areas of high impact.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-devotes-over-10-million-new-funds-push-artificial-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-devotes-over-10-million-new-funds-push-artificial-intelligence&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT and Illinois Research on Data Analytics Will Measure Impact of Scientific Literature</title>
      <link>http://localhost:1313/blog/20240826-njit/</link>
      <pubDate>Mon, 26 Aug 2024 11:37:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240826-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-us-government--public-domain-image-via-wikimedia-commons&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;U.S. government / public domain image, via Wikimedia Commons&#34; srcset=&#34;
               /blog/20240826-njit/DARPA_Big_Data_hu_cbbdbb914ec7c26d.webp 400w,
               /blog/20240826-njit/DARPA_Big_Data_hu_790432619f296e9e.webp 760w,
               /blog/20240826-njit/DARPA_Big_Data_hu_a508b5f1c9be3f15.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240826-njit/DARPA_Big_Data_hu_cbbdbb914ec7c26d.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      U.S. government / public domain image, via Wikimedia Commons
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Three distinct problems in data science — trend identification in graphs, the quantitative study of scientific literature and evaluation of single-cell genomics — will all be addressed by new research in large-scale network analytics, jointly led by &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; at New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;The problems have a common challenge of finding patterns, known as community detection, from inside incredibly large datasets. Work is funded by a $648,000 National Science Foundation grant, &lt;a href=&#34;https://www.nsf.gov/awardsearch/showAward?AWD_ID=2402560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cyber-Infrastructure for Community Detection, Extraction, and Search in Large Networks&lt;/a&gt;. Bader is a principal investigator, receiving $250,000 for research. He is working with George Chacko and Tandy Warnow, both of University of Illinois Urbana-Champaign.&lt;/p&gt;
&lt;p&gt;What will happen now is the development of new algorithms to identify clusters within those graphs. Bader’s role is scalability and performance. He brings Arachne to the table, which is open-source software that he and colleagues have worked on since 2022, designed to organize graphs with trillions of vertices and edges while presenting a Python interface that almost anyone can learn. Chacko is working on project benchmarking and Warnow is developing new algorithms and interoperability with other methods.&lt;/p&gt;
&lt;p&gt;They are planning to test the new method for various applications. In one field, single-cell genomics, it will involve “clustering very large networks whose vertices represent cells and where the weight of an edge between two vertices is computed based on gene expression profiles,” the team explained.&lt;/p&gt;
&lt;p&gt;But it’s scientometrics that has Bader most excited. This is a meta-field where researchers study how to mathematically evaluate the impact of papers and citations.&lt;/p&gt;
&lt;p&gt;“A lot of information can be learned by looking at the metadata of published literature,” said Bader, director of NJIT’s Institute for Data Science. “For instance, looking at which papers cite others, which authors co-published together, and so on. And when there are questions emerging, like who are the experts on the following topic, or what are the papers I need to read to really get an understanding of a particular field, that&amp;rsquo;s where scientometrics comes into play.”&lt;/p&gt;
&lt;p&gt;“Right now, the data sets for scientometrics are very, very large in size, and current tools, for instance, NetworkX in Python, or other such packages, don&amp;rsquo;t scale to the size problems needed by the scientometrics community. So what this grant is enabling is new algorithms and new implementations that are scalable to be able to solve their problems. And so this is where we&amp;rsquo;re going to pick up Arkouda and Arachne to work on them,” he noted, referring to &lt;a href=&#34;https://news.njit.edu/institute-data-science-aims-democratize-supercomputing-nsf-grant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;his own prior research&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So far, “There are prototypes for some of the algorithms that [Warnow] has developed, and they work on small test cases. So the idea is, we essentially have a proof of concept, but there&amp;rsquo;s sequential implementations, there&amp;rsquo;s slow implementations and our challenge is going to be able to get them to scale to the problem sizes that the scientometrics community needs for their data sets, and to be able to make it run fast.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-and-illinois-research-data-analytics-will-measure-impact-scientific-literature&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-and-illinois-research-data-analytics-will-measure-impact-scientific-literature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://scienmag.com/njit-and-illinois-research-on-data-analytics-will-measure-impact-of-scientific-literature/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scienmag.com/njit-and-illinois-research-on-data-analytics-will-measure-impact-of-scientific-literature/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Joseph Jaja Announces Retirement</title>
      <link>http://localhost:1313/blog/20240826-umcp/</link>
      <pubDate>Mon, 26 Aug 2024 11:06:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240826-umcp/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240826-umcp/article16213.large_hu_84e0dfdca831fa2.webp 400w,
               /blog/20240826-umcp/article16213.large_hu_b9f89dce8d18ff7e.webp 760w,
               /blog/20240826-umcp/article16213.large_hu_a717e51397380b72.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240826-umcp/article16213.large_hu_84e0dfdca831fa2.webp&#34;
               width=&#34;300&#34;
               height=&#34;301&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Professor Joseph Jaja has announced his retirement from the ECE Department after more than 40 years of service. Jaja joined the ECE faculty in 1983 as an Associate Professor in computer engineering. Prior to joining UMD, he served on the faculty of Penn State University. He earned a BS in Mathematics from the American University of Beirut in 1974 and a Ph.D. in Applied Mathematics from Harvard University in 1977. He holds joint appointments with ECE and the University of Maryland Institute for Advanced Computer Studies (UMIACS).&lt;/p&gt;
&lt;p&gt;During his tenure with ECE, Jaja served in a variety of roles. From 1988 to 1994, he was the Associate Director for Research for the Institute of Systems Research (ISR). From 1994 to 2014, he served as the Director of UMIACS.&lt;/p&gt;
&lt;p&gt;In 2018, he accepted the position of Interim Chair of the ECE Department, a one-year assignment that turned into four years. He guided the department through the pandemic years, successfully leading the transition from in-person to remote learning and providing support and resources for students, faculty and staff who were facing numerous challenges during those years. During his years as Interim Chair, the department hired eight new faculty members, launched a new EE curriculum, revamped the ECE honors program, completely revised and updated the ECE Plan of Organization and Bylaws, and established several endowed ECE undergraduate labs.&lt;/p&gt;
&lt;p&gt;Jaja’s research interests include high performance computing, machine learning with applications to neuroscience, and data science. Over the years, he has graduated 29 Ph.D. students and published hundreds of journal and conference papers. He is a Fellow of the Institute of Electrical and Electronics Engineers (IEEE) and the Association of Computing Machinery (ACM).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Professor JaJa&amp;rsquo;s dedication and passion have profoundly shaped the ECE Department and the careers of many students, including myself. His unwavering commitment to excellence in teaching and research has set a high standard for all of us,” says &lt;strong&gt;David Bader&lt;/strong&gt; (ECE Ph.D ’96), Distinguished Professor at the New Jersey Institute of Technology, and 2022 inductee in the A. James Clark School of Engineering Innovator Hall of Fame.&lt;/p&gt;
&lt;p&gt;Sangchul Song (ECE Ph.D. 2010, Corporate Vice President for Samsung Electronics and ECE 2023 Distinguished Alum) adds, “It was my privilege to have the opportunity to pursue my Ph.D. under Professor Jaja’s guidance, witnessing firsthand how my lousy &amp;ldquo;draft&amp;rdquo; paper magically turned into a beautiful &amp;ldquo;photo-ready&amp;rdquo; version after his edits!”.&lt;/p&gt;
&lt;p&gt;In July 2024, Jaja was elevated to Professor Emeritus. Post-retirement, he plans to continue research in his fields and mentor graduate students.&lt;/p&gt;
&lt;p&gt;In addition, Jaja will further his collaboration with FDA researchers on new methods for AI medical devices through the University of Maryland Institute for Health Computing (UM-IHC). As a strategic partner with Montgomery County, Maryland, the UM-IHC combines computational expertise and biomedical research with the aim of improving the quality of life and health for all citizens of Maryland and beyond.&lt;/p&gt;
&lt;p&gt;“Professor Jaja’s innovative approach to high performance computing and data science has not only advanced the field but also inspired us to push the boundaries of our own research,” notes Bader.&lt;/p&gt;
&lt;p&gt;Song agrees, “as he embarks on this new chapter of his life, may it be as fulfilling and impactful as the high-performance algorithms he has crafted.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/joseph-jaja-announces-retirement&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/joseph-jaja-announces-retirement&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why did Delta take days to restore normal service after CrowdStrike outage? Experts weigh in.</title>
      <link>http://localhost:1313/blog/20240727-abcnews/</link>
      <pubDate>Sat, 27 Jul 2024 10:35:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240727-abcnews/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://abcnews.go.com/author/Max_Zahn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Max Zahn&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An &lt;a href=&#34;https://abcnews.go.com/US/american-airlines-issues-global-ground-stop-flights/story?id=112092372&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;outage&lt;/a&gt; caused by a software update distributed by cybersecurity firm &lt;a href=&#34;https://abcnews.go.com/Business/crowdstrike-stock-price-plummets-amid-worldwide-outage/story?id=112098344&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrowdStrike&lt;/a&gt; triggered a wave of flight cancellations at several major U.S. airlines – but the disruption was most severe and prolonged at Delta &lt;a href=&#34;https://abcnews.go.com/alerts/airlines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Airlines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In all, the carrier canceled more than 2,500 flights over a period that stretched from last Friday, when the outage began, into the middle of this week.&lt;/p&gt;
&lt;p&gt;The U.S. Department of &lt;a href=&#34;https://abcnews.go.com/alerts/tsa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transportation&lt;/a&gt; &lt;a href=&#34;https://abcnews.go.com/Business/department-transportation-opens-investigation-delta-flight-disruptions/story?id=112191447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;opened&lt;/a&gt; an investigation into Delta this week over its uniquely severe flight disruptions.&lt;/p&gt;
&lt;p&gt;“All airline passengers have the right to be treated fairly,” Transportation Secretary Pete Buttigieg said on Tuesday in a &lt;a href=&#34;https://x.com/SecretaryPete/status/1815719031741022573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt; on X.&lt;/p&gt;


















&lt;figure  id=&#34;figure-people-looking-for-missing-bags-wait-in-line-to-speak-with-delta-air-lines-baggage-in-the-delta-air-lines-baggage-claim-area-los-angeles-international-airport-lax-july-24-2024-in-los-angeles-patrick-t-fallonafp-via-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;People looking for missing bags wait in line to speak with Delta Air Lines baggage in the Delta Air Lines baggage claim area Los Angeles International Airport (LAX), July 24, 2024, in Los Angeles. Patrick T. Fallon/AFP via Getty Images&#34; srcset=&#34;
               /blog/20240727-abcnews/delta-2-gty-er-240726_1722024395835_hpEmbed_3x2_hu_b77b2349fc505606.webp 400w,
               /blog/20240727-abcnews/delta-2-gty-er-240726_1722024395835_hpEmbed_3x2_hu_a14e1025e1650ba4.webp 760w,
               /blog/20240727-abcnews/delta-2-gty-er-240726_1722024395835_hpEmbed_3x2_hu_aaf3cca931a512cb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240727-abcnews/delta-2-gty-er-240726_1722024395835_hpEmbed_3x2_hu_b77b2349fc505606.webp&#34;
               width=&#34;750&#34;
               height=&#34;500&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      People looking for missing bags wait in line to speak with Delta Air Lines baggage in the Delta Air Lines baggage claim area Los Angeles International Airport (LAX), July 24, 2024, in Los Angeles. Patrick T. Fallon/AFP via Getty Images
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In a &lt;a href=&#34;https://news.delta.com/tags/july-2024-operation?mkcpgn=SM_TWIT____20240721_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statement&lt;/a&gt; on Tuesday, Delta said it is fully cooperating with the investigation. “Across our operation, Delta teams are working tirelessly to care for and make it right for customers impacted by delays and cancellations as we work to restore the reliable, on-time service they have come to expect from Delta,&amp;quot; the company said.&lt;/p&gt;
&lt;p&gt;The company also issued an apology on Wednesday for the outage-related problems.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Please accept our sincere apologies for the disruption to your recent travel plans caused by a vendor technology outage affecting airlines and companies worldwide,” the airline &lt;a href=&#34;https://abcnews.go.com/US/delta-issues-apology-passengers-after-crowdstrike-outage-bonus/story?id=112223364&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said&lt;/a&gt; in a statement.&lt;/p&gt;
&lt;p&gt;“It’s a surprise that a multi-billion-dollar corporation like Delta would allow this to happen,” Henry Harteveldt, a travel industry analyst at Atmosphere Research Group, told ABC News.&lt;/p&gt;
&lt;p&gt;“I’m hopeful that the worst is behind us now. While we can breathe a sigh of relief, I think a lot of people are understandably nervous about flying Delta,” Harteveldt added.&lt;/p&gt;
&lt;p&gt;Delta did not immediately respond to an ABC News request for comment.&lt;/p&gt;
&lt;p&gt;Airline and cybersecurity experts spoke to ABC News about what made the CrowdStrike outage so disruptive, and why it took days for Delta to resume normal service.&lt;/p&gt;
&lt;h2 id=&#34;what-made-the-crowdstrike-outage-so-disruptive-for-delta&#34;&gt;What made the CrowdStrike outage so disruptive for Delta?&lt;/h2&gt;
&lt;p&gt;The CrowdStrike outage was so impactful because of the severity of the IT failure and the scale of its reach within the internal operating systems at Delta, experts told ABC News.&lt;/p&gt;
&lt;p&gt;“For a company such as Delta, they rely on countless partner services for everything from scheduling pilots and planes to providing meal service and snacks to allowing customers to select their seats,” &lt;strong&gt;David Bader&lt;/strong&gt;, a professor of cybersecurity and the director of the Institute of Data Science at the New Jersey Institute of Technology, told ABC News.&lt;/p&gt;
&lt;p&gt;“The CrowdStrike bug disrupted many of those critical services that keep the airline running at full capacity,” Bader added.&lt;/p&gt;
&lt;p&gt;Mark Lanterman, the chief technology officer at the cybersecurity firm Computer Forensic Services, said the outage resulted from a faulty software update initiated by CrowdStrike. The resulting computer bug interrupted core services because of the degree to which CrowdStrike pervades the Delta operating systems, he added.&lt;/p&gt;
&lt;p&gt;“The CrowdStrike update is deep inside the operating system. When that was installed, there was bad code inside of this update. And when Windows came across the bad code, it panicked and it crashed,” Lanterman said.&lt;/p&gt;


















&lt;figure  id=&#34;figure-delta-airlines-passenger-jets-are-pictured-outside-terminal-c-at-laguardia-airport-in-new-york-june-1-2022-mike-segarreuters-file&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Delta Airlines passenger jets are pictured outside Terminal C at LaGuardia Airport in New York, June 1, 2022. Mike Segar/Reuters, FILE&#34; srcset=&#34;
               /blog/20240727-abcnews/delta-1-rt-er-240726_1722024268158_hpMain_hu_f55f39027f856ae.webp 400w,
               /blog/20240727-abcnews/delta-1-rt-er-240726_1722024268158_hpMain_hu_3b2046689ce91fa2.webp 760w,
               /blog/20240727-abcnews/delta-1-rt-er-240726_1722024268158_hpMain_hu_93499d3cbc162208.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240727-abcnews/delta-1-rt-er-240726_1722024268158_hpMain_hu_f55f39027f856ae.webp&#34;
               width=&#34;750&#34;
               height=&#34;500&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Delta Airlines passenger jets are pictured outside Terminal C at LaGuardia Airport in New York, June 1, 2022. Mike Segar/Reuters, FILE
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The outage, which affected CrowdStrike clients that use Windows operating systems, disrupted a critical system that ensures each flight has a full crew, Delta said in a statement on Monday.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Upward of half of Delta’s IT systems worldwide are Windows based,” Delta said.&lt;/p&gt;
&lt;h2 id=&#34;why-did-it-take-days-for-delta-to-resume-normal-service&#34;&gt;Why did it take days for Delta to resume normal service?&lt;/h2&gt;
&lt;p&gt;The reason for the prolonged recovery from the outage was because the CrowdStrike update disruption required a manual fix at each individual computer system, experts told ABC News. While each fix can be completed in no more than 10 minutes, the vast number of Delta&amp;rsquo;s digital terminals required significant manpower to address, expert said.&lt;/p&gt;
&lt;p&gt;“This isn’t a fix that could be done automatically; IT resources can’t just sit at a computer and push out an update and everything is fixed,” Lanterman said. “It took so long because Delta has a lot of computers and likely they have limited IT resources to go from computer to computer.”&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://news.delta.com/tags/july-2024-operation?mkcpgn=SM_TWIT____20240721_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statement&lt;/a&gt; on Tuesday, the airline acknowledged the challenge posed by the manual fix requirement.&lt;/p&gt;
&lt;p&gt;“The CrowdStrike error required Delta’s IT teams to manually repair and reboot each of the affected systems, with additional time then needed for applications to synchronize and start communicating with each other,&amp;rdquo; Delta said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abcnews.go.com/Business/delta-days-restore-normal-service-after-crowdstrike-outage/story?id=112299966&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abcnews.go.com/Business/delta-days-restore-normal-service-after-crowdstrike-outage/story?id=112299966&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ca.news.yahoo.com/why-did-delta-days-restore-091025535.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ca.news.yahoo.com/why-did-delta-days-restore-091025535.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.msn.com/en-us/travel/other/why-did-it-take-days-for-delta-to-restore-normal-service-after-outage/ar-BB1qIQrK?ocid=BingNewsSerp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-us/travel/other/why-did-it-take-days-for-delta-to-restore-normal-service-after-outage/ar-BB1qIQrK?ocid=BingNewsSerp&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Here&#39;s what the CrowdStrike outage exposed about our connected world. It&#39;s not good.</title>
      <link>http://localhost:1313/blog/20240726-yahoofinance/</link>
      <pubDate>Fri, 26 Jul 2024 15:59:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240726-yahoofinance/</guid>
      <description>&lt;p&gt;Nearly a week after a massive IT outage shut down computer systems around the world, cybersecurity company CrowdStrike (&lt;a href=&#34;https://finance.yahoo.com/quote/CRWD/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CRWD&lt;/a&gt;) &lt;a href=&#34;https://www.crowdstrike.com/falcon-content-update-remediation-and-guidance-hub/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issued a statement Thursday&lt;/a&gt; revealing that a single software update was responsible for grounding planes, curtailing hospital procedures, and closing businesses for days.&lt;/p&gt;
&lt;p&gt;The announcement came as the majority of companies returned to business as usual. But it points to the &lt;a href=&#34;https://finance.yahoo.com/news/explainer-how-crowdstrike-knocked-the-world-offline-175318168.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vulnerability of our modern internet infrastructure&lt;/a&gt; and how taking out even a relatively small number of devices — Microsoft (&lt;a href=&#34;https://finance.yahoo.com/quote/MSFT/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MSFT&lt;/a&gt;) &lt;a href=&#34;https://blogs.microsoft.com/blog/2024/07/20/helping-our-customers-through-the-crowdstrike-outage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estimates 8.5 million systems&lt;/a&gt; were affected — can impact our lives.&lt;/p&gt;
&lt;p&gt;“What we see here is the cascading effect that a minor software update, or in the future, maybe a cyberattack or malicious code, can have a huge impact,” &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science at the New Jersey Institute of Technology, told Yahoo Finance.&lt;/p&gt;
&lt;p&gt;And without some kind of broader plan to address the matter, another widespread outage is all but guaranteed to happen.&lt;/p&gt;
&lt;p&gt;“What we&amp;rsquo;re seeing today is these types of cascading failures occurring more and more frequently,” Bader said. “These will continue as we see AI, and as we move toward [artificial general intelligence], that these types of failures, whether they&amp;rsquo;re accidental, some bad programming, such as CrowdStrike, or whether their malicious attacks, will continue showing the vulnerability of our technological world.”&lt;/p&gt;
&lt;h2 id=&#34;a-lack-of-coherent-rules&#34;&gt;A lack of coherent rules&lt;/h2&gt;
&lt;p&gt;According to the announcement statement by CrowdStrike, the company issued a software update on July 19 that included a flaw that went undetected in validation checks. The error immediately crashed certain Windows systems connected to the web, causing them to display a crash message known as the blue screen of death.&lt;/p&gt;
&lt;p&gt;CrowdStrike says it’s responding to the matter by reworking how it prepares its software updates, including more stringent testing and staggering deployment to prevent a global systems collapse in the future.&lt;/p&gt;
&lt;p&gt;Screens show a blue error message at a departure floor of LaGuardia Airport in New York on Friday, July 19, 2024, after a faulty CrowdStrike update caused a major internet outage for computers running Microsoft Windows. (AP Photo/Yuki Iwamura)&lt;/p&gt;


















&lt;figure  id=&#34;figure-screens-show-a-blue-error-message-at-a-departure-floor-of-laguardia-airport-in-new-york-on-friday-july-19-2024-after-a-faulty-crowdstrike-update-caused-a-major-internet-outage-for-computers-running-microsoft-windows-ap-photoyuki-iwamura-associated-press&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Screens show a blue error message at a departure floor of LaGuardia Airport in New York on Friday, July 19, 2024, after a faulty CrowdStrike update caused a major internet outage for computers running Microsoft Windows. (AP Photo/Yuki Iwamura) (ASSOCIATED PRESS)&#34; srcset=&#34;
               /blog/20240726-yahoofinance/crowdstrike_hu_d9810fad92491323.webp 400w,
               /blog/20240726-yahoofinance/crowdstrike_hu_555348fb1a76655.webp 760w,
               /blog/20240726-yahoofinance/crowdstrike_hu_18fceb25d2219fc7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240726-yahoofinance/crowdstrike_hu_d9810fad92491323.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Screens show a blue error message at a departure floor of LaGuardia Airport in New York on Friday, July 19, 2024, after a faulty CrowdStrike update caused a major internet outage for computers running Microsoft Windows. (AP Photo/Yuki Iwamura) (ASSOCIATED PRESS)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;It’s important to note that software is developed by people. And while they’re usually incredibly capable people, they’re still human, and humans make mistakes. That’s generally how flaws enter software ecosystems, whether it’s CrowdStrike’s programs or some other company’s platform.&lt;/p&gt;
&lt;p&gt;“Even the best testing processes fail,” explained Gartner analyst Jon Amato. “You can do a certain amount of automated testing, but those automated tests are themselves designed by human beings and human beings are fallible.”&lt;/p&gt;
&lt;p&gt;And while CrowdStrike is certainly looking to improve its own internal processes as far as ensuring the stability of its software updates, that doesn’t mean every other software company will do the same.&lt;/p&gt;
&lt;p&gt;“We really don&amp;rsquo;t have any organization in the US that is looking holistically at our technological resilience,” Bader said.&lt;/p&gt;
&lt;p&gt;He added, “We don&amp;rsquo;t have a body that can generate the best practices needed for private industry to both protect against the delivery of the software updates and what a customer should do, for instance, the banks, the hospitals, the airlines, how they should protect themselves to ensure that these problems don&amp;rsquo;t impact them in the future.”&lt;/p&gt;
&lt;p&gt;And while the Department of Homeland Security’s Cybersecurity and Infrastructure Security Agency offers tips, there’s no major enforcement mechanism in place to force companies to follow specific strategies when issuing software updates or addressing program failures and malicious attacks.&lt;/p&gt;
&lt;p&gt;Without those, Bader said, a larger outage and prolonged recovery are bound to happen.&lt;/p&gt;
&lt;h2 id=&#34;a-bigger-problem&#34;&gt;A bigger problem?&lt;/h2&gt;
&lt;p&gt;Outside of a need for a regimented approach to IT failures, the CrowdStrike outage also points to a broader problem within the backbone of the world’s tech infrastructure: A small number of companies have an outsized impact on how the web operates.&lt;/p&gt;
&lt;p&gt;“We definitely know that these are very fragile systems, and the fact that they work as well as they do is, frankly, a miracle, given all of the different players, the lack of heterogeneity of the stack,” Gregory Falco, assistant professor of mechanical and aerospace engineering and systems engineering at Cornell University&amp;rsquo;s Sibley School, told Yahoo Finance.&lt;/p&gt;
&lt;p&gt;But expanding the number of companies that plug directly into our internet infrastructure isn’t exactly an easy fix either. That’s because the more companies there are, the more opportunities there are for failures.&lt;/p&gt;
&lt;p&gt;Ultimately, the solution to these kinds of world-scale problems might just come down to forcing companies to be better prepared for catastrophe. And if software does fail, understanding how to contain the fallout.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Daniel Howley&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Email Daniel Howley at &lt;a href=&#34;mailto:dhowley@yahoofinance.com&#34;&gt;dhowley@yahoofinance.com&lt;/a&gt;. Follow him on X at @DanielHowley.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aol.com/finance/crowdstrike-outage-exposed-connected-world-150321849.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.aol.com/finance/crowdstrike-outage-exposed-connected-world-150321849.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://headtopics.com/ca/what-the-crowdstrike-outage-exposed-about-our-connected-56460861&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://headtopics.com/ca/what-the-crowdstrike-outage-exposed-about-our-connected-56460861&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data breach hits Rite Aid pharmacy. How NJ residents can protect their personal info</title>
      <link>http://localhost:1313/blog/20240718-northjersey/</link>
      <pubDate>Thu, 18 Jul 2024 12:44:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240718-northjersey/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Daniel Munoz&lt;/em&gt;, NorthJersey.com&lt;/p&gt;
&lt;p&gt;Pharmacy chain Rite Aid revealed this week that 2.2 million customers were exposed in a data breach in June.&lt;/p&gt;
&lt;p&gt;The leaked data includes driver’s license numbers, addresses and dates of birth, Rite Aid said in a &lt;a href=&#34;https://news.riteaid.com/press-releases/press-release-details/2024/Rite-Aid-Provides-Notice-of-Security-Incident/default.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;July 15 press release&lt;/a&gt; The stolen data was associated with completed attempted purchases made between June 6, 2017 and July 30, 2018, Rite Aid said.&lt;/p&gt;
&lt;p&gt;According to the Rite Aid press release, the breached data did not include Social Security numbers, financial information or patient information.&lt;/p&gt;
&lt;p&gt;Rite Aid said it notified state authorities in &lt;a href=&#34;https://www.maine.gov/agviewer/content/ag/985235c7-cb95-4be2-8792-a1252b4f8318/c4bace65-85df-4fff-b99f-f8fd390bb41a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maine&lt;/a&gt;, &lt;a href=&#34;https://www.mass.gov/doc/2024-1308-rite-aid-corporation/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Massachusetts&lt;/a&gt;, &lt;a href=&#34;https://justice.oregon.gov/consumer/DataBreach/Home/Details/1364031490&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oregon&lt;/a&gt; and &lt;a href=&#34;https://ago.vermont.gov/document/2024-07-15-rite-aid-data-breach-notice-consumers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vermont&lt;/a&gt;. It is not clear if New Jersey authorities were contacted by Rite Aid regarding the breach — representatives for the state Attorney General’s office could not be immediately reached by email Thursday morning.&lt;/p&gt;


















&lt;figure  id=&#34;figure-signs-on-the-front-doors-of-the-rite-aid-on-south-street-in-morristown-announce-the-stores-closure-thursday-sept-14-2023-kyle-morelnorthjerseycom&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Signs on the front doors of the Rite Aid on South Street in Morristown announce the store&amp;#39;s closure Thursday, Sept. 14, 2023. *Kyle Morel/NorthJersey.com*&#34; srcset=&#34;
               /blog/20240718-northjersey/70852352007-img-20230914-115419_hu_3e714b041ec828d9.webp 400w,
               /blog/20240718-northjersey/70852352007-img-20230914-115419_hu_db6c0bc258ff37c4.webp 760w,
               /blog/20240718-northjersey/70852352007-img-20230914-115419_hu_31fe648328a83f40.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240718-northjersey/70852352007-img-20230914-115419_hu_3e714b041ec828d9.webp&#34;
               width=&#34;660&#34;
               height=&#34;495&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Signs on the front doors of the Rite Aid on South Street in Morristown announce the store&amp;rsquo;s closure Thursday, Sept. 14, 2023. &lt;em&gt;Kyle Morel/NorthJersey.com&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;“We regret that this incident occurred and are implementing additional security measures to prevent potentially similar attacks in the future,” Rite Aid said in a statement this week.&lt;/p&gt;
&lt;p&gt;“We take our obligation to safeguard personal information very seriously and are alerting affected consumers about this incident.”&lt;/p&gt;
&lt;h2 id=&#34;rite-aid-bankruptcy-has-led-to-closing-stores&#34;&gt;Rite Aid bankruptcy has led to closing stores&lt;/h2&gt;
&lt;p&gt;June’s breach announcement comes as the beleaguered retail pharmacy chain emerges from bankruptcy protection — a process that saw hundreds of stores close across the nation.&lt;/p&gt;
&lt;p&gt;The chain faces slumping sales and battles lawsuits over its alleged role in fueling the opioid epidemic by illegally filling prescriptions for painkillers.&lt;/p&gt;
&lt;p&gt;Rite Aid had more than 2,000 stores when it filed for bankruptcy in October last year. Under a plan approved in federal court late June, Rite Aid will have 1,300 locations remaining, &lt;a href=&#34;https://www.reuters.com/business/healthcare-pharmaceuticals/rite-aid-bankruptcy-plan-approved-cutting-2-bln-debt-2024-06-28/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;according to Reuters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A spokesperson for Rite Aid could not be immediately reached for comment by email Thursday morning.&lt;/p&gt;
&lt;h2 id=&#34;data-breaches-become-more-common&#34;&gt;Data breaches become more common&lt;/h2&gt;
&lt;p&gt;Telecom giant AT&amp;amp;T recently announced that the call and text message records of nearly all of its cellular customers were &lt;a href=&#34;https://www.northjersey.com/story/tech/2024/07/12/att-data-breach/74377926007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;exposed in a massive breach&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“What this means is that criminals now have a better way to impersonate us because they know who we talk to, who our friends are, and where we may live and work,” said &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the Ying Wu College of Computing at the New Jersey Institute of Technology in Newark.&lt;/p&gt;
&lt;p&gt;“This makes it much easier for a scammer to impersonate a family member, our boss, or ourselves,” Bader said.&lt;/p&gt;
&lt;p&gt;Scammers and hackers could use the information gleaned by data breaches such as AT&amp;amp;T and Rite Aid to create a “digital twin” and request things like money or passwords, Bader said.&lt;/p&gt;
&lt;p&gt;“Criminals can use this information to commit targeted acts of phishing by convincing you their communications are from a legitimate source&amp;quot; such as your bank or a government official, the credit monitoring company Experian said in an April blog post. “Their goal may be to con you into handing over more sensitive information, or to trick you into providing access to your financial accounts.”&lt;/p&gt;
&lt;p&gt;And what to do if you suspect someone is trying to scam you?&lt;/p&gt;
&lt;p&gt;“Any time we get a request that sounds too urgent or requires money or a transfer or seems a little unusual, where we get a gut feeling that, ‘Hmm, this is strange,’ should take a deep breath and inspect the message closely,” Bader said.&lt;/p&gt;
&lt;p&gt;“Is it coming from the email address we would expect or not, from a friend, family member or boss? It’s always fine to pick up the phone to verify that it’s a valid email before taking some urgent action.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Daniel Munoz covers business, consumer affairs, labor and the economy for NorthJersey.com and The Record.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.northjersey.com/story/news/business/2024/07/18/rite-aid-data-breach/74452806007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.northjersey.com/story/news/business/2024/07/18/rite-aid-data-breach/74452806007/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI Visionaries Podcast: Exploring the Future of AI and HPC with Dr. David A. Bader</title>
      <link>http://localhost:1313/blog/20240610-zinfi/</link>
      <pubDate>Mon, 10 Jun 2024 15:50:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240610-zinfi/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240610-zinfi/splash_hu_bf765c012536133e.webp 400w,
               /blog/20240610-zinfi/splash_hu_750c54d24db250c5.webp 760w,
               /blog/20240610-zinfi/splash_hu_34138c78fcbbffe3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240610-zinfi/splash_hu_bf765c012536133e.webp&#34;
               width=&#34;760&#34;
               height=&#34;409&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Welcome to our podcast series, where we delve into artificial intelligence and high-performance computing. Hosted by ZINFI CEO and Founder Sugata Sanyal and sponsored by Databank, this series features distinguished speakers like &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, who share their insights on cutting-edge technologies and their impact on various industries.&lt;/p&gt;
&lt;p&gt;Tune in to explore groundbreaking research, future trends, and the transformative potential of AI and HPC in today’s rapidly evolving technological landscape.&lt;/p&gt;
&lt;p&gt;Watch the full video to uncover how AI and HPC shape our future!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zinfi.com/video-podcast/future-of-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zinfi.com/video-podcast/future-of-ai/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ticketmaster hacked: Here&#39;s how experts say New Jerseyans can protect their info</title>
      <link>http://localhost:1313/blog/20240603-njherald/</link>
      <pubDate>Mon, 03 Jun 2024 12:58:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240603-njherald/</guid>
      <description>&lt;p&gt;&lt;em&gt;by &lt;a href=&#34;https://www.northjersey.com/staff/9414111002/daniel-munoz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniel Munoz&lt;/a&gt;&lt;/em&gt;, NorthJersey.com&lt;/p&gt;
&lt;p&gt;More than &lt;a href=&#34;https://www.njherald.com/videos/money/2024/05/31/ticketmaster-customers-personal-info-credit-card-numbers-hacked-reports-say/73909301007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;half a billion customers&lt;/a&gt; at the event ticketing company Ticketmaster had their personal information hacked and potentially sold on the dark web last month, but security experts say there are measures you can take to minimize the impact.&lt;/p&gt;
&lt;p&gt;The data includes credit card details, names and addresses, with the hacking group ShinyHunters reportedly claiming responsibility for the breach. They are seeking a ransom payment of $500,000 to prevent the data from being sold.&lt;/p&gt;
&lt;p&gt;“That may run up your credit and it may be used in other countries to buy weapons, it may be used on the dark web, and so it’s very important to protect your identity and your credit information,” said &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the Ying Wu College of Computing at the New Jersey Institute of Technology.&lt;/p&gt;


















&lt;figure  id=&#34;figure-in-this-photo-illustration-a-ticketmaster-ticket-is-shown-on-a-cellphone-on-november-18-2022-in-miami-florida-joe-raedle-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;In this photo illustration, a Ticketmaster ticket is shown on a cellphone on November 18, 2022 in Miami, Florida. *Joe Raedle, Getty Images*&#34; srcset=&#34;
               /blog/20240603-njherald/73820716007-1442595194_hu_ac64314e44faa48e.webp 400w,
               /blog/20240603-njherald/73820716007-1442595194_hu_b59a1ca22471d1e2.webp 760w,
               /blog/20240603-njherald/73820716007-1442595194_hu_c5658c4c932089b0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240603-njherald/73820716007-1442595194_hu_ac64314e44faa48e.webp&#34;
               width=&#34;660&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      In this photo illustration, a Ticketmaster ticket is shown on a cellphone on November 18, 2022 in Miami, Florida. &lt;em&gt;Joe Raedle, Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Ticketmaster, which is owned by  Live Nation Entertainment, did not immediately respond to a request for comment.&lt;/p&gt;
&lt;p&gt;Live Nation confirmed in a &lt;a href=&#34;https://www.sec.gov/Archives/edgar/data/1335258/000133525824000081/lyv-20240520.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;May 20 filing&lt;/a&gt; with the U.S. Securities and Exchange Commission that a “criminal threat actor offered what it alleged to be company user data for sale via the dark web.”&lt;/p&gt;
&lt;h2 id=&#34;alert-your-credit-card-company&#34;&gt;Alert your credit card company&lt;/h2&gt;
&lt;p&gt;There are “general hygiene” security measures people can take to protect their data, said Raffi Jamgotchian, founding CEO of Triada Networks, an IT security firm based in Norwood.&lt;/p&gt;
&lt;p&gt;Odysseas Papadimitriou, a former executive at credit card company Capital One, said to monitor activity on any cards tied to your Ticketmaster account.&lt;/p&gt;
&lt;p&gt;Bader, the NJIT professor, said that if you suspect your information was compromised by the Ticketmaster data breach, then you have the option to contact your credit card provider. They might provide a replacement card.&lt;/p&gt;
&lt;h2 id=&#34;monitor-your-credit-reports&#34;&gt;Monitor your credit reports&lt;/h2&gt;
&lt;p&gt;Papadimitriou, who’s now CEO of personal finance website &lt;a href=&#34;https://wallethub.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WalletHub&lt;/a&gt;, said that you can sign up for round-the-clock credit monitoring.&lt;/p&gt;
&lt;p&gt;There are three agencies that monitor your credit report: TransUnion, Experian and Equifax.&lt;/p&gt;
&lt;h2 id=&#34;dont-overshare-information&#34;&gt;Don&amp;rsquo;t overshare information&lt;/h2&gt;
&lt;p&gt;“The more information a bad actor has on you, the more easy it is for them to assume your identity and, for instance, get credit issued in your name,” said Bader.&lt;/p&gt;
&lt;p&gt;Even information as basic as your birthday can be misused, Bader said.&lt;/p&gt;
&lt;p&gt;Papadimitriou said to never respond to “unexpected requests for information,” especially over the phone.&lt;/p&gt;
&lt;p&gt;“No legitimate financial institution or service provider would call you up and ask for your personal identifying information,” Bader said.&lt;/p&gt;
&lt;p&gt;Though if you are called, you should notify your service provider, as you might not be the only person affected and the provider “may be under an attack,” he added.&lt;/p&gt;
&lt;h2 id=&#34;protect-your-login-information&#34;&gt;Protect your login information&lt;/h2&gt;
&lt;p&gt;One option is multi-factor authentication. That’s when you use your mobile phone to authenticate your identity when logging in, whether through a “pop up, a generated code or a text message,” said Jamgotchian of Triada.&lt;/p&gt;
&lt;p&gt;Papadimitriou said you should also update your Ticketmaster password.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Daniel Munoz covers business, consumer affairs, labor and the economy for NorthJersey.com and The Record.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njherald.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.njherald.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.northjersey.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.northjersey.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.courierpostonline.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.courierpostonline.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.app.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.app.com/story/news/2024/06/03/ticketmaster-data-breach-protect-your-personal-info-from-dark-web/73956957007/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mexicobusiness.news/cybersecurity/news/ticketmaster-cyberattack-impact-and-response-measures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mexicobusiness.news/cybersecurity/news/ticketmaster-cyberattack-impact-and-response-measures&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Northeast Big Data Hub Newsletter: Community Spotlight (May 2024)</title>
      <link>http://localhost:1313/blog/20240508-nebdih/</link>
      <pubDate>Wed, 08 May 2024 14:37:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240508-nebdih/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240508-nebdih/41afbf18-75b9-176b-6332-059ee8929184_hu_edee850276545efa.webp 400w,
               /blog/20240508-nebdih/41afbf18-75b9-176b-6332-059ee8929184_hu_79c5a543ad063924.webp 760w,
               /blog/20240508-nebdih/41afbf18-75b9-176b-6332-059ee8929184_hu_89df74c2044415a8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240508-nebdih/41afbf18-75b9-176b-6332-059ee8929184_hu_edee850276545efa.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt; is a Distinguished Professor and a founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology. He is also Chair of the NEBDHub Seed Fund Steering Committee. Earlier this year, David hosted a Masterclass with the NEBDHub. &lt;a href=&#34;https://youtu.be/mVfHHx93n1E&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Watch to learn about open-source frameworks for massive-scale graph analytics.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mailchi.mp/covidinfocommons/nehubnewsletter-6725962?e=e425191e8b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mailchi.mp/covidinfocommons/nehubnewsletter-6725962?e=e425191e8b&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sigma Xi, American Scientist: 2024 Distinguished Lecturers</title>
      <link>http://localhost:1313/blog/20240501-sigmaxi/</link>
      <pubDate>Wed, 01 May 2024 15:56:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240501-sigmaxi/</guid>
      <description>&lt;p&gt;Since 1937, Sigma Xi has presented its Distinguished Lecturers. This program is an opportunity for chapters to host visits from outstanding individuals who are at the leading edge of science.  Lecturers communicate their insights and excitement to a broad range of scholars and to the community at large. Each year, thousands of Sigma Xi members, students, and the public have an opportunity to hear exceptional talks and to ask questions of experts.&lt;/p&gt;
&lt;p&gt;The Lectureship Program is supported by Sigma Xi members with additional support from partnering organizations such as the American Meteorological Society, the National Academy of Engineering, and the National Cancer Insitute.&lt;/p&gt;
&lt;p&gt;To learn more about specific speakers and lecture topics, visit &lt;a href=&#34;http://sigmaxi.org/lectureships&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sigmaxi.org/lectureships&lt;/a&gt;. To schedule a speaker or sponsor a lecture, email &lt;a href=&#34;mailto:lectureships@sigmaxi.org&#34;&gt;lectureships@sigmaxi.org&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, New Jersey Institute of Technology&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Powerful earthquake in Taiwan should raise concerns for Apple, Qualcomm, MediaTek and others</title>
      <link>http://localhost:1313/blog/20240403-phonearena/</link>
      <pubDate>Wed, 03 Apr 2024 14:09:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240403-phonearena/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Alan Friedman&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240403-phonearena/Powerful-earthquake-in-Taiwan-should-raise-concerns-for-Apple-Qualcomm-MediaTek-and-others_hu_7a42bb2fce5d0e78.webp 400w,
               /blog/20240403-phonearena/Powerful-earthquake-in-Taiwan-should-raise-concerns-for-Apple-Qualcomm-MediaTek-and-others_hu_c39a788e2f3fd4b2.webp 760w,
               /blog/20240403-phonearena/Powerful-earthquake-in-Taiwan-should-raise-concerns-for-Apple-Qualcomm-MediaTek-and-others_hu_6a5c86f90935cb1f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-phonearena/Powerful-earthquake-in-Taiwan-should-raise-concerns-for-Apple-Qualcomm-MediaTek-and-others_hu_7a42bb2fce5d0e78.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;This morning, the U.S. woke up to the news that a powerful 7.4 magnitude earthquake had hit Taiwan, the strongest earthquake in the region over the last 25 years. After worrying about the human toll of this tragedy and how expensive and time-consuming it will be to rebuild, the next thought many had was whether the world&amp;rsquo;s leading contract chip foundry, TSMC, suffered serious damage and what the effect might be to its top customers such as Apple, MediaTek, AMD, and Qualcomm.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnn.com/2024/04/03/tech/taiwan-earthquake-risks-semiconductor-chip-industry-tsmc/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;CNN&lt;/em&gt;&lt;/a&gt; reports that the quake killed nine people while buildings collapsed and landslides were spotted. TSMC is located on the opposite side of the island from where the center of the earthquake hit allowing it to escape without sustaining more serious damages. Shaking was felt in the company&amp;rsquo;s offices and fabs. Some of those production facilities did shake so much that workers had to be evacuated. Later in the day on Wednesday, TSMC allowed workers to return to the assembly lines after announcing that everyone was safe.&lt;/p&gt;
&lt;p&gt;While the news was good for TSMC and its customers, the world&amp;rsquo;s largest foundry did report some damage to some of its chip-making equipment. &amp;ldquo;A small number of tools were damaged at certain facilities, partially impacting their operations. However, there is no damage to our critical tools,&amp;rdquo; TSMC said late Wednesday. More than 70% of TSMC&amp;rsquo;s tools inside its fabs were recovered within 10 hours of the earthquake. In newer fabs, recovery levels were higher.&lt;/p&gt;


















&lt;figure  id=&#34;figure-tsmc-managed-to-survive-the-earthquake-in-taiwan-with-minimal-damage&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;TSMC managed to survive the earthquake in Taiwan with minimal damage&#34; srcset=&#34;
               /blog/20240403-phonearena/tsmcquakinginboots_hu_f014d11930d717a1.webp 400w,
               /blog/20240403-phonearena/tsmcquakinginboots_hu_cdf1039713738614.webp 760w,
               /blog/20240403-phonearena/tsmcquakinginboots_hu_472080bfbb2eb5af.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-phonearena/tsmcquakinginboots_hu_f014d11930d717a1.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      TSMC managed to survive the earthquake in Taiwan with minimal damage
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In an investor note Wednesday, Barclays analysts pointed out that it might take TSMC weeks to recover from a shutdown that lasted a few hours. &amp;ldquo;Some of the high-end chips need 24/7 seamless operations in vacuum state for a few weeks,&amp;rdquo; Barclays analysts said. With TSMC having to halt some of its operations, Barclays says that &amp;ldquo;some high-end chips in production may be spoiled.&amp;rdquo; Overall impact to the foundry could be a $60 million reduction in second quarter earnings.&lt;/p&gt;
&lt;p&gt;So this time, &lt;a href=&#34;https://www.phonearena.com/apple&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple&lt;/a&gt;, Qualcomm and others counting on TSMC can breathe a sigh of relief. But what happens next time a major earthquake hits Taiwan? And more worrisome is the large communist elephant in the room, the Chinese Communist Party (CCP). CNN says that TSMC is responsible for the production of a whopping 90% of the world&amp;rsquo;s most advanced chips. That makes the foundry a tempting target for a country practically begging to be self-sufficient when it comes to semiconductors.&lt;/p&gt;
&lt;p&gt;So there is an uneasy feeling bordering on fear when talk turns to the importance of TSMC in the tech world. &lt;strong&gt;David Bader&lt;/strong&gt;, professor and director of the Institute for Data Science at New Jersey Institute of Technology says, &amp;ldquo;I believe it’s an existential threat. The entire world now works on semiconductor devices powering everything that we do, whether we’re driving in our cars, whether we are talking on our cell phones, even our military defenses or weapons systems, airlines, everything uses chips. If production were to halt … this would be devastating.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Angelo Zino, CFRA research analyst said on Wednesday, &amp;ldquo;We think the earthquake should serve as a reminder to investors of the risks associated with having so much foundry exposure coming from one region.&amp;rdquo; The tech industry needs to come up with a solution.&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alan Friedman&lt;/strong&gt;&lt;br&gt;
Mobile Tech News Journalist&lt;/p&gt;
&lt;p&gt;Alan, an ardent smartphone enthusiast and a veteran writer at PhoneArena since 2009, has witnessed and chronicled the transformative years of mobile technology. Owning iconic phones from the original iPhone to the iPhone 15 Pro Max, he has seen smartphones evolve into a global phenomenon. Beyond smartphones, Alan has covered the emergence of tablets, smartwatches, and smart speakers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.phonearena.com/news/apples-chip-supplier-survives-powerful-taiwan-earthquake_id156963&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.phonearena.com/news/apples-chip-supplier-survives-powerful-taiwan-earthquake_id156963&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Taiwan earthquake is a stark reminder of the risks to the region’s chipmaking industry</title>
      <link>http://localhost:1313/blog/20240403-cnn/</link>
      <pubDate>Wed, 03 Apr 2024 08:53:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240403-cnn/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.cnn.com/profiles/clare-duffy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clare Duffy&lt;/a&gt;, CNN&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-emergency-personnel-stand-in-front-of-a-partially-collapsed-building-leaning-over-a-street-in-hualien-on-april-3-2024-after-a-major-earthquake-hit-taiwans-east-coast-sam-yehafpgetty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Emergency personnel stand in front of a partially collapsed building leaning over a street in Hualien on April 3, 2024, after a major earthquake hit Taiwan&amp;#39;s east coast. Sam Yeh/AFP/Getty Images&#34; srcset=&#34;
               /blog/20240403-cnn/gettyimages-2128591882_hu_655cbaf9eb9cc440.webp 400w,
               /blog/20240403-cnn/gettyimages-2128591882_hu_3584286cc97eea82.webp 760w,
               /blog/20240403-cnn/gettyimages-2128591882_hu_6476681bd1743048.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/gettyimages-2128591882_hu_655cbaf9eb9cc440.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Emergency personnel stand in front of a partially collapsed building leaning over a street in Hualien on April 3, 2024, after a major earthquake hit Taiwan&amp;rsquo;s east coast. Sam Yeh/AFP/Getty Images
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;New York (CNN) &amp;ndash;&lt;/strong&gt; The world’s biggest chipmaker is working to resume operations following the massive earthquake that struck Taiwan Wednesday — a welcome sign for makers of products ranging from iPhones and computers to cars and washing machines that rely on advanced semiconductors.&lt;/p&gt;
&lt;p&gt;A 7.4 magnitude earthquake struck the island’s east coast Wednesday morning, the strongest in 25 years, killing nine and causing landslides and collapsed structures.&lt;/p&gt;
&lt;p&gt;Taiwan Semiconductor Manufacturing Company, the leading chipmaker also known as TSMC, operates largely on the opposite side of the island, although the company said its facilities did experience some shaking. TSMC temporarily evacuated some manufacturing plants following the quake but said later Wednesday that staff were safe and had returned to their workplaces.&lt;/p&gt;
&lt;p&gt;“A small number of tools were damaged at certain facilities, partially impacting their operations. However, there is no damage to our critical tools,” TSMC said in a statement late Wednesday.&lt;/p&gt;
&lt;p&gt;While Wednesday’s earthquake appears unlikely to have any long-term implications for the semiconductor supply chain, it gave a stark reminder of the risks of concentrating crucial microchip manufacturing on an island that is both &lt;a href=&#34;https://www.cnn.com/2017/09/08/world/ring-of-fire-explainer/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prone to earthquakes&lt;/a&gt; and a &lt;a href=&#34;https://www.cnn.com/2024/01/13/economy/taiwan-china-election-economy-intl-hnk/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hotspot for geopolitical tensions&lt;/a&gt;. Chipmakers and governments, including the US government, have in recent years invested billions in efforts to diversify chip production, but many experts worry that process is not happening fast enough.&lt;/p&gt;
&lt;p&gt;TSMC produces an estimated 90% of the world’s most advanced semiconductor chips, which power countless devices people rely on daily. Its chips are used by tech giants including Apple, Qualcomm, Nvidia and AMD — and they’re essential for the burgeoning artificial intelligence industry, where chip supply is already constrained.&lt;/p&gt;
&lt;p&gt;“I believe it’s an existential threat,” said &lt;strong&gt;David Bader&lt;/strong&gt;, professor and director of the Institute for Data Science at New Jersey Institute of Technology, of the concentration of chipmaking in Taiwan.&lt;/p&gt;
&lt;p&gt;“The entire world now works on semiconductor devices powering everything that we do, whether we’re driving in our cars, whether we are talking on our cell phones, even our military defenses or weapons systems, airlines, everything uses chips,” Bader said. “If production were to halt … this would be devastating.”&lt;/p&gt;
&lt;h2 id=&#34;recovery-efforts-underway&#34;&gt;Recovery efforts underway&lt;/h2&gt;
&lt;p&gt;TSMC &lt;a href=&#34;https://esg.tsmc.com/en/update/governance/caseStudy/1/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bolstered its earthquake protections&lt;/a&gt; in the years following Taiwan’s last major earthquake in 1999.&lt;/p&gt;
&lt;p&gt;As of late Wednesday, the company said that more than 70% of the tools in its fabs had been recovered within 10 hours of the earthquake, with recovery levels higher in some newer facilities. TSMC said impacted facilites were expected to resume production throughout Wednesday night.&lt;/p&gt;
&lt;p&gt;Still, even an hours-long shutdown of certain chip production could take weeks to recover from.&lt;/p&gt;
&lt;p&gt;“Some of the high-end chips need 24/7 seamless operations in vacuum state for a few weeks,” Barclays analysts said in an investor note Wednesday, adding that operation halts could mean “some high-end chips in production may be spoiled.” They noted that TSMC could see a $60 million impact to its second quarter earnings from the disruption.&lt;/p&gt;
&lt;p&gt;The potential broader ripple effects for the tech industry will also depend on what kind of chip manufacturing was affected, something that wasn’t immediately clear on Wednesday, according to Gartner analyst Joe Unsworth. Tech companies that rely on GPU chips that help power AI applications, which are already in short supply, will likely be watching closely for potential impacts on that area of production.&lt;/p&gt;
&lt;p&gt;Nvidia, the leading designer of GPUs, said in a statement Wednesday that “after consulting with our manufacturing partners, we don’t expect any impact on our supply from the Taiwan earthquake.”&lt;/p&gt;
&lt;p&gt;Some other semiconductor and technology manufacturers — including smaller chipmaker United Microelectronics Corporation, memory and storage chipmaker Micron and Apple supplier Foxconn — said Wednesday they were also evaluating the potential impacts of the earthquake on their Taiwan facilities but indicated that they expect little fallout.&lt;/p&gt;
&lt;h2 id=&#34;the-race-to-diversify-chipmaking&#34;&gt;The race to diversify chipmaking&lt;/h2&gt;
&lt;p&gt;Wednesday’s earthquake will likely only add pressure to efforts underway for years to grow chipmaking capacity outside of Taiwan.&lt;/p&gt;
&lt;p&gt;Other disasters, including the &lt;a href=&#34;https://www.cnn.com/2021/06/10/tech/taiwan-chip-shortage-covid-climate-crisis-intl-hnk/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Covid-19 pandemic and droughts&lt;/a&gt;, have previously weighed on semiconductor production in the region and caused &lt;a href=&#34;https://www.cnn.com/2021/04/29/business/chip-shortages-smartphones-consumer-goods/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;chip shortages&lt;/a&gt; that raised the prices of consumer goods. Supply chain experts and US officials also worry that US-China trade tensions and military aggression from China against the island could bring consequences for the industry.&lt;/p&gt;
&lt;p&gt;“We think the earthquake should serve as a reminder to investors of the risks associated with having so much foundry exposure coming from one region,” CFRA Research analyst Angelo Zino said in an investor note Wednesday.&lt;/p&gt;
&lt;p&gt;In 2022, US President Joe Biden signed into &lt;a href=&#34;https://www.cnn.com/2022/08/09/politics/chips-semiconductor-manufacturing-science-act/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;law the CHIPS and Science Act&lt;/a&gt;, which allocated more than $200 billion in investment over the next five years to help the United States regain a leadership position in semiconductor manufacturing.&lt;/p&gt;
&lt;p&gt;And in recent years, TSMC has announced plans for &lt;a href=&#34;https://www.cnn.com/2024/02/07/tech/tsmc-taiwan-japan-second-factory-intl-hnk/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new semiconductor fabs&lt;/a&gt; in Japan, Germany and the United States. But plans for its second factory in Arizona — which was announced in 2022 and originally expected to be operational this year — have been &lt;a href=&#34;https://www.cnn.com/2024/01/19/tech/tsmc-taiwan-arizona-project-delay-intl-hnk/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repeatedly delayed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Experts say it’s a sign that the diversification of the chip supply chain is not moving quickly enough to account for the risks of remaining concentrated in Taiwan. Locations for new fabs must have companies or governments willing and able to invest billions to construct the facilities, as well as a large workforce with the skills to do advanced semiconductor manufacturing.&lt;/p&gt;
&lt;p&gt;“I think that we are in a critical period for the next several years, until there is a location for a major fab like TSMC that is in a less geopolitical hot zone than Taiwan,” Bader said. “We are really working in a challenging arena for several more years as we wait for that to happen.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnn.com/2024/04/03/tech/taiwan-earthquake-risks-semiconductor-chip-industry-tsmc/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnn.com/2024/04/03/tech/taiwan-earthquake-risks-semiconductor-chip-industry-tsmc/index.html&lt;/a&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-httpswwwfacebookcomsharepmp6xgybgndfyghwumibextidofdknk&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.facebook.com/share/p/MP6xGYbGnDfyghwu/?mibextid=oFDknk&#34; srcset=&#34;
               /blog/20240403-cnn/CNN_hu_baa2645ab70ed392.webp 400w,
               /blog/20240403-cnn/CNN_hu_c6f8d0e06b0e9f9f.webp 760w,
               /blog/20240403-cnn/CNN_hu_f255d8a870859c6d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/CNN_hu_baa2645ab70ed392.webp&#34;
               width=&#34;760&#34;
               height=&#34;362&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.facebook.com/share/p/MP6xGYbGnDfyghwu/?mibextid=oFDknk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/share/p/MP6xGYbGnDfyghwu/?mibextid=oFDknk&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwfacebookcomsharepxwnranfgyl4gm7zzmibextidofdknk&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.facebook.com/share/p/xWNraNfGyL4gm7Zz/?mibextid=oFDknk&#34; srcset=&#34;
               /blog/20240403-cnn/CNNInternational_hu_2237077044d7b828.webp 400w,
               /blog/20240403-cnn/CNNInternational_hu_517cd55ca7ffaebb.webp 760w,
               /blog/20240403-cnn/CNNInternational_hu_5a89f6aa3bf5644f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/CNNInternational_hu_2237077044d7b828.webp&#34;
               width=&#34;225&#34;
               height=&#34;225&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.facebook.com/share/p/xWNraNfGyL4gm7Zz/?mibextid=oFDknk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/share/p/xWNraNfGyL4gm7Zz/?mibextid=oFDknk&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwmsncomen-usmoneycompaniesthe-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industryar-bb1l1lbk&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.msn.com/en-us/money/companies/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/ar-BB1l1Lbk&#34; srcset=&#34;
               /blog/20240403-cnn/msn-logo_hu_e500bec6be06d8df.webp 400w,
               /blog/20240403-cnn/msn-logo_hu_eaea3a2b85338a73.webp 760w,
               /blog/20240403-cnn/msn-logo_hu_9b0ad0e6a678b46c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/msn-logo_hu_e500bec6be06d8df.webp&#34;
               width=&#34;760&#34;
               height=&#34;291&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.msn.com/en-us/money/companies/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/ar-BB1l1Lbk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.msn.com/en-us/money/companies/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/ar-BB1l1Lbk&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpsfinanceyahoocomnewstaiwan-earthquake-stark-reminder-risks-202307994html&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://finance.yahoo.com/news/taiwan-earthquake-stark-reminder-risks-202307994.html&#34; srcset=&#34;
               /blog/20240403-cnn/Yahoo!_Finance_logo_2021_hu_bca6eccf38a03732.webp 400w,
               /blog/20240403-cnn/Yahoo!_Finance_logo_2021_hu_182bfdbb93d99cad.webp 760w,
               /blog/20240403-cnn/Yahoo!_Finance_logo_2021_hu_805426356b11f277.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/Yahoo!_Finance_logo_2021_hu_bca6eccf38a03732.webp&#34;
               width=&#34;760&#34;
               height=&#34;279&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://finance.yahoo.com/news/taiwan-earthquake-stark-reminder-risks-202307994.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://finance.yahoo.com/news/taiwan-earthquake-stark-reminder-risks-202307994.html&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwktencomstory50637756the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.kten.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; srcset=&#34;
               /blog/20240403-cnn/KTEN_hu_98b85ddfb26c6950.webp 400w,
               /blog/20240403-cnn/KTEN_hu_6d5a1cfd3dc29b5c.webp 760w,
               /blog/20240403-cnn/KTEN_hu_ed6062b6ec0bc0d6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/KTEN_hu_98b85ddfb26c6950.webp&#34;
               width=&#34;396&#34;
               height=&#34;140&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.kten.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kten.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwwiczcomstory50637756the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.wicz.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; srcset=&#34;
               /blog/20240403-cnn/FOX40_hu_10e26bb4029b375d.webp 400w,
               /blog/20240403-cnn/FOX40_hu_b61546968a7d65a2.webp 760w,
               /blog/20240403-cnn/FOX40_hu_eec9d19c7edbc6f2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/FOX40_hu_10e26bb4029b375d.webp&#34;
               width=&#34;424&#34;
               height=&#34;140&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.wicz.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wicz.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwalbanyheraldcomnewsbusinessthe-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmakingarticle_b9309378-bfa9-55ca-b696-482f44e88c45html&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.albanyherald.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_b9309378-bfa9-55ca-b696-482f44e88c45.html&#34; srcset=&#34;
               /blog/20240403-cnn/AlbanyHerald_hu_cd651e405b7bcf25.webp 400w,
               /blog/20240403-cnn/AlbanyHerald_hu_6a58255b1b492a8.webp 760w,
               /blog/20240403-cnn/AlbanyHerald_hu_3340d5f6c864d5ce.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/AlbanyHerald_hu_cd651e405b7bcf25.webp&#34;
               width=&#34;750&#34;
               height=&#34;309&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.albanyherald.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_b9309378-bfa9-55ca-b696-482f44e88c45.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.albanyherald.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_b9309378-bfa9-55ca-b696-482f44e88c45.html&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwwralcomstorythe-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry21361509&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.wral.com/story/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/21361509/&#34; srcset=&#34;
               /blog/20240403-cnn/WRALNewsDaily1920x1080-DMID1-5m0ymhfus-640x360_hu_9ae17a5eb9f9067c.webp 400w,
               /blog/20240403-cnn/WRALNewsDaily1920x1080-DMID1-5m0ymhfus-640x360_hu_906400fa8b86dba5.webp 760w,
               /blog/20240403-cnn/WRALNewsDaily1920x1080-DMID1-5m0ymhfus-640x360_hu_8898c73b2260749.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/WRALNewsDaily1920x1080-DMID1-5m0ymhfus-640x360_hu_9ae17a5eb9f9067c.webp&#34;
               width=&#34;640&#34;
               height=&#34;360&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.wral.com/story/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/21361509/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wral.com/story/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking-industry/21361509/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpsnortheastnewschannelnebraskacomstory50637756the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://northeast.newschannelnebraska.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; srcset=&#34;
               /blog/20240403-cnn/NewsChannelNebraska_hu_d2a84d0bb39189e3.webp 400w,
               /blog/20240403-cnn/NewsChannelNebraska_hu_37514e21cd5691fe.webp 760w,
               /blog/20240403-cnn/NewsChannelNebraska_hu_9164c6cf63c553a0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/NewsChannelNebraska_hu_d2a84d0bb39189e3.webp&#34;
               width=&#34;553&#34;
               height=&#34;140&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://northeast.newschannelnebraska.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://northeast.newschannelnebraska.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwerienewsnowcomstory50637756the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.erienewsnow.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; srcset=&#34;
               /blog/20240403-cnn/ErieNewsNow_hu_482436a5a2f7547b.webp 400w,
               /blog/20240403-cnn/ErieNewsNow_hu_9ca010c3400c5b8e.webp 760w,
               /blog/20240403-cnn/ErieNewsNow_hu_992b34f1a8322456.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/ErieNewsNow_hu_482436a5a2f7547b.webp&#34;
               width=&#34;280&#34;
               height=&#34;140&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.erienewsnow.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.erienewsnow.com/story/50637756/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpswwwkitvcomnewsbusinessthe-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmakingarticle_d2e4f040-4319-5124-9e03-9e22a6cfc9c8html&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://www.kitv.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_d2e4f040-4319-5124-9e03-9e22a6cfc9c8.html&#34; srcset=&#34;
               /blog/20240403-cnn/IslandNews_hu_16e43e63aad2080a.webp 400w,
               /blog/20240403-cnn/IslandNews_hu_d174b7ed786ac906.webp 760w,
               /blog/20240403-cnn/IslandNews_hu_3acba85f10945ab0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/IslandNews_hu_16e43e63aad2080a.webp&#34;
               width=&#34;540&#34;
               height=&#34;147&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://www.kitv.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_d2e4f040-4319-5124-9e03-9e22a6cfc9c8.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kitv.com/news/business/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region-s-chipmaking/article_d2e4f040-4319-5124-9e03-9e22a6cfc9c8.html&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpsespottingcomthe-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://espotting.com/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; srcset=&#34;
               /blog/20240403-cnn/espotting_hu_c5d9fa3bf608cbf8.webp 400w,
               /blog/20240403-cnn/espotting_hu_36b71b88f84aab4a.webp 760w,
               /blog/20240403-cnn/espotting_hu_5040c12efb33e53e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/espotting_hu_c5d9fa3bf608cbf8.webp&#34;
               width=&#34;760&#34;
               height=&#34;363&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://espotting.com/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://espotting.com/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpsromaniapostsencomtrends375871the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regione28099s-chipmaking-industryhtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://romania.postsen.com/trends/375871/The-Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&#34; srcset=&#34;
               /blog/20240403-cnn/Romania_hu_3914082ac4467c8b.webp 400w,
               /blog/20240403-cnn/Romania_hu_6a875d8f62131ba4.webp 760w,
               /blog/20240403-cnn/Romania_hu_79f439fabb826834.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/Romania_hu_3914082ac4467c8b.webp&#34;
               width=&#34;548&#34;
               height=&#34;166&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://romania.postsen.com/trends/375871/The-Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://romania.postsen.com/trends/375871/The-Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpsalkhaleejtodaycointernational5830222taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regione28099s-chipmaking-industryhtml&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://alkhaleejtoday.co/international/5830222/Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&#34; srcset=&#34;
               /blog/20240403-cnn/alKhaleejToday_hu_b651d31963cdf271.webp 400w,
               /blog/20240403-cnn/alKhaleejToday_hu_2fc92205e3b3c8b9.webp 760w,
               /blog/20240403-cnn/alKhaleejToday_hu_731c08f5fd3eb6c8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/alKhaleejToday_hu_b651d31963cdf271.webp&#34;
               width=&#34;400&#34;
               height=&#34;90&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://alkhaleejtoday.co/international/5830222/Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://alkhaleejtoday.co/international/5830222/Taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-region%E2%80%99s-chipmaking-industry.html&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpskrdocomnews20240403the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://krdo.com/news/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; srcset=&#34;
               /blog/20240403-cnn/krdo_logo_100px_width_hu_faa06051a0d291d2.webp 400w,
               /blog/20240403-cnn/krdo_logo_100px_width_hu_aec45a0f045c9852.webp 760w,
               /blog/20240403-cnn/krdo_logo_100px_width_hu_94101bccb5065a42.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/krdo_logo_100px_width_hu_faa06051a0d291d2.webp&#34;
               width=&#34;100&#34;
               height=&#34;42&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://krdo.com/news/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://krdo.com/news/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpskesqcommoneycnn-business-consumer20240403the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://kesq.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; srcset=&#34;
               /blog/20240403-cnn/nc3xsm_hu_77b275a160a8087f.webp 400w,
               /blog/20240403-cnn/nc3xsm_hu_e9e7111a8943f3ce.webp 760w,
               /blog/20240403-cnn/nc3xsm_hu_2819d1229a197c59.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/nc3xsm_hu_77b275a160a8087f.webp&#34;
               width=&#34;150&#34;
               height=&#34;49&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://kesq.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kesq.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpslocalnews8commoneycnn-business-consumer20240403the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://localnews8.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; srcset=&#34;
               /blog/20240403-cnn/Channel8_hu_644c406f0b651cdc.webp 400w,
               /blog/20240403-cnn/Channel8_hu_3fde7e1ab4461d4e.webp 760w,
               /blog/20240403-cnn/Channel8_hu_286e63b7364a646d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/Channel8_hu_644c406f0b651cdc.webp&#34;
               width=&#34;252&#34;
               height=&#34;55&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://localnews8.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://localnews8.com/money/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-httpskeytcomnewsmoney-and-businesscnn-business-consumer20240403the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;https://keyt.com/news/money-and-business/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; srcset=&#34;
               /blog/20240403-cnn/54270-KEYT-Logo-Tri_Logo_Jan2022-80_hu_37bb5ee2fd1f67e4.webp 400w,
               /blog/20240403-cnn/54270-KEYT-Logo-Tri_Logo_Jan2022-80_hu_a0d50f861c092e83.webp 760w,
               /blog/20240403-cnn/54270-KEYT-Logo-Tri_Logo_Jan2022-80_hu_bc0fa5883200dca7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240403-cnn/54270-KEYT-Logo-Tri_Logo_Jan2022-80_hu_37bb5ee2fd1f67e4.webp&#34;
               width=&#34;170&#34;
               height=&#34;80&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://keyt.com/news/money-and-business/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://keyt.com/news/money-and-business/cnn-business-consumer/2024/04/03/the-taiwan-earthquake-is-a-stark-reminder-of-the-risks-to-the-regions-chipmaking-industry/&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Chapel 2.0 Released: Scalable and Productive Computing for All</title>
      <link>http://localhost:1313/blog/20240322-hpcwire/</link>
      <pubDate>Fri, 22 Mar 2024 13:40:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240322-hpcwire/</guid>
      <description>&lt;p&gt;The Chapel team is excited to announce the release of Chapel version 2.0. After years of hard work and continuous improvements, Chapel shines as an enjoyable and productive programming language for distributed and parallel computing.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240322-hpcwire/chapel-logo-300x197_hu_991626985b4eeb48.webp 400w,
               /blog/20240322-hpcwire/chapel-logo-300x197_hu_c72515513a5b12a4.webp 760w,
               /blog/20240322-hpcwire/chapel-logo-300x197_hu_4499e44c3ce93f1e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240322-hpcwire/chapel-logo-300x197_hu_991626985b4eeb48.webp&#34;
               width=&#34;300&#34;
               height=&#34;197&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Users with diverse application goals are leveraging Chapel to quickly develop fast and scalable software, including physical simulations, massive data and graph analytics, portions of machine learning pipelines, and more. The 2.0 release brings stability guarantees to Chapel’s battle-tested features, making it possible to write performant and elegant code for laptops, GPU workstations, and supercomputers with confidence and convenience.&lt;/p&gt;
&lt;p&gt;In addition to numerous usability and performance improvements — &lt;a href=&#34;https://www.hpcwire.com/off-the-wire/chapel-programming-language-moves-toward-2-0-with-significant-updates-and-stabilizations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;including many over the previous release candidate&lt;/a&gt; — the 2.0 release of Chapel is stable: the core language and library features are designed to be backwards-compatible from here on. As Chapel continues to grow and evolve, additions or changes to the language should not require adjusting any existing code.&lt;/p&gt;
&lt;p&gt;Those new to Chapel might be wondering how the language came about. Chapel was pioneered by a small team at Cray Inc. who wanted to help anyone make full use of any parallel hardware available to them — from their multicore laptop to a large-scale supercomputer.&lt;/p&gt;
&lt;h2 id=&#34;what-are-people-doing-with-chapel&#34;&gt;What Are People Doing with Chapel?&lt;/h2&gt;
&lt;p&gt;Chapel is being used for a diverse set of production-grade applications; these applications have guided the language’s development, pushing it to grow and support real-world computing workloads.&lt;/p&gt;
&lt;h3 id=&#34;arkouda-interactive-data-analysis-at-scale&#34;&gt;&lt;span style=&#34;text-decoration:underline&#34;&gt;Arkouda: Interactive Data Analysis at Scale&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Arkouda is one exciting application of the Chapel language. It is an open-source data science library for Python written in Chapel. Arkouda enables users to interactively analyze massive datasets. Using Chapel enables Arkouda to scale easily; as a concrete example, its &lt;code&gt;argsort&lt;/code&gt; operation, written in around 100 lines of Chapel code, was able to sort 256 TiB of data in 31 seconds, a rate of 8500 GiB/s! That throughput was achieved by scaling Arkouda to 8192 compute nodes on an &lt;a href=&#34;https://www.hpe.com/us/en/compute/hpc/supercomputing/cray-exascale-supercomputer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPE Cray EX&lt;/a&gt; using its &lt;a href=&#34;https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slingshot-11 network&lt;/a&gt;. The scaling was made possible through Chapel’s built-in support for multi-node execution.&lt;/p&gt;


















&lt;figure  id=&#34;figure-scaling-results-from-arkoudas-argsort-function&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Scaling results from Arkouda’s argsort function&#34; srcset=&#34;
               /blog/20240322-hpcwire/arkouda-argsort-1024x532_hu_8dc9a2db8923f3e2.webp 400w,
               /blog/20240322-hpcwire/arkouda-argsort-1024x532_hu_db789fe2553a1160.webp 760w,
               /blog/20240322-hpcwire/arkouda-argsort-1024x532_hu_2f503f0b28df2594.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240322-hpcwire/arkouda-argsort-1024x532_hu_8dc9a2db8923f3e2.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scaling results from Arkouda’s argsort function
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Arkouda allows data scientists to iterate at the speed of thought, even with datasets spanning terabytes of data — scales that could never fit into the memory of a single machine. All the performance and scalability of Chapel does not come at the cost of convenience or ergonomics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, the NJIT professor leading development of the &lt;a href=&#34;https://github.com/Bears-R-Us/arkouda-njit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arachne&lt;/a&gt; Arkouda module for scalable graph algorithms, has this to say about the language: “Chapel serves as a powerful tool for the rapid development of scalable graph analysis algorithms, standing alone in its capability to enable such advancements.”&lt;/p&gt;
&lt;p&gt;Further examples can be found &lt;a href=&#34;https://chapel-lang.org/blog/posts/announcing-chapel-2.0/#:~:text=CHAMPS%3A%20A%20Framework%20for%20Computational%20Fluid%20Dynamics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Chapel has supported developers with various computing needs in creating performant and maintainable software since 2015, a track record of nearly 10 years. The 2.0 release represents the culmination of a concerted, multi-year effort to refine Chapel’s core functionality based on these many years of experience.&lt;/p&gt;
&lt;p&gt;Chapel is now stable, but it is not finished: the language will continue to see performance improvements, new features, and better tooling. Whether you are working with a personal machine or a multi-node cluster, there has been no better time to get started with Chapel to get the most out of your computing hardware.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Source: Chapel Parallel Programming Language Development Team&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/chapel-2-0-released-scalable-and-productive-computing-for-all/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/chapel-2-0-released-scalable-and-productive-computing-for-all/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chapel 2.0: Scalable and Productive Computing for All</title>
      <link>http://localhost:1313/blog/20240321-chapel/</link>
      <pubDate>Thu, 21 Mar 2024 09:32:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240321-chapel/</guid>
      <description>&lt;p&gt;Chapel Language Blog&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Daniel Fedorin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, the Chapel team is excited to announce the release of Chapel version 2.0. After years of hard work and continuous improvements, Chapel shines as an enjoyable and productive programming language for distributed and parallel computing. People with diverse application goals are leveraging Chapel to quickly develop fast and scalable software, including physical simulations, massive data and graph analytics, portions of machine learning pipelines, and more. The 2.0 release brings stability guarantees to Chapel’s battle-tested features, making it possible to write performant and elegant code for laptops, GPU workstations, and supercomputers with confidence and convenience.&lt;/p&gt;
&lt;p&gt;In addition to &lt;a href=&#34;https://github.com/chapel-lang/chapel/blob/main/CHANGES.md#version-20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numerous usability and performance improvements&lt;/a&gt; — including many &lt;a href=&#34;https://chapel-lang.org/blog/posts/changes-since-2.0-rc1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;over the previous release candidate&lt;/a&gt; — the 2.0 release of Chapel is &lt;strong&gt;stable&lt;/strong&gt;: the core language and library features are designed to be backwards-compatible from here on. As Chapel continues to grow and evolve, additions or changes to the language should not require adjusting any existing code.&lt;/p&gt;
&lt;p&gt;Those new to Chapel might be wondering how the language came about. Chapel was pioneered by a small team at Cray Inc. who wanted to help anyone make full use of any parallel hardware available to them — from their multicore laptop to a large-scale supercomputer.&lt;/p&gt;
&lt;h2 id=&#34;what-are-people-doing-with-chapel&#34;&gt;What Are People Doing With Chapel?&lt;/h2&gt;
&lt;p&gt;As I mentioned earlier, Chapel is being used for a diverse set of production-grade applications; these applications have guided the language’s development, pushing it to grow and support real-world computing workloads.&lt;/p&gt;
&lt;h3 id=&#34;arkouda-interactive-data-analysis-at-scale&#34;&gt;Arkouda: Interactive Data Analysis at Scale&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Bears-R-Us/arkouda/blob/master/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arkouda&lt;/a&gt; is one exciting application of the Chapel language. It is an open-source data science library for Python written in Chapel. Arkouda enables users to interactively analyze massive datasets. Using Chapel enables Arkouda to scale easily; as a concrete example, its &lt;code&gt;argsort&lt;/code&gt; operation, written in around 100 lines of Chapel code, was able to sort 256 TiB of data in 31 seconds, a rate of 8500 GiB/s! That throughput was achieved by scaling Arkouda to 8192 compute nodes on an &lt;a href=&#34;https://www.hpe.com/us/en/compute/hpc/supercomputing/cray-exascale-supercomputer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HPE Cray EX&lt;/a&gt; using its &lt;a href=&#34;https://www.hpe.com/us/en/compute/hpc/slingshot-interconnect.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slingshot-11 network&lt;/a&gt;. The scaling was made possible through Chapel’s built-in support for multi-node execution.&lt;/p&gt;


















&lt;figure  id=&#34;figure-scaling-results-from-arkoudas-argsort-function&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Scaling results from Arkouda’s `argsort` function&#34; srcset=&#34;
               /blog/20240321-chapel/arkouda-argsort_hu_5a4ff25102f241a1.webp 400w,
               /blog/20240321-chapel/arkouda-argsort_hu_c0e29238443f87aa.webp 760w,
               /blog/20240321-chapel/arkouda-argsort_hu_84719c1a9166099b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240321-chapel/arkouda-argsort_hu_5a4ff25102f241a1.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scaling results from Arkouda’s &lt;code&gt;argsort&lt;/code&gt; function
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Arkouda allows data scientists to iterate at the speed of thought, even with datasets spanning terabytes of data — scales that could never fit into the memory of a single machine.&lt;/p&gt;
&lt;p&gt;All the performance and scalability of Chapel does not come at the cost of convenience or ergonomics. &lt;strong&gt;David Bader&lt;/strong&gt;, the NJIT professor leading development of the &lt;a href=&#34;https://github.com/Bears-R-Us/arkouda-njit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arachne&lt;/a&gt; Arkouda module for scalable graph algorithms, has this to say about the language:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Chapel serves as a powerful tool for the rapid development of scalable graph analysis algorithms, &lt;strong&gt;standing alone in its capability to enable such advancements&lt;/strong&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;champs-a-framework-for-computational-fluid-dynamics&#34;&gt;CHAMPS: A Framework for Computational Fluid Dynamics&lt;/h3&gt;
&lt;p&gt;Another excellent example of Chapel’s ergonomics is CHAMPS, a software framework focused on three-dimensional, unstructured computational fluid dynamics. CHAMPS is made up of over 125,000 lines of Chapel code, making it the largest project written in the language. The principal investigator of CHAMPS, Éric Laurendeau, has &lt;a href=&#34;https://youtu.be/wD-a_KyB8aI?t=1904&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;remarked on the productivity&lt;/a&gt; gained by switching to Chapel:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;We ask students at the master’s degree to do stuff that would take 2 years and they do it in 3 months&lt;/strong&gt;. . . . So, if you want to take a summer internship and you say, . . . “program a new turbulence model,” well they manage. And before, it was impossible to do.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;At the time of this quote, CHAMPS was 48k lines of code, a near threefold decrease from the 120k lines of its C-based predecessor. In addition to this, CHAMPS improved upon its predecessor by extending the fluid dynamics simulations to use distributed memory and be three-dimensional, unstructured, and multi-physics. Each of these advances represented a step up in complexity. Despite this, the Chapel-based CHAMPS proved to be easier to learn, program, and maintain.&lt;/p&gt;
&lt;h3 id=&#34;coral-biodiversity-computation&#34;&gt;Coral Biodiversity Computation&lt;/h3&gt;
&lt;p&gt;One area particularly suited for parallel computing — and one at which Chapel excels — is image analysis. Scott Bachman, an oceanographer and climate scientist, learned Chapel from the ground up to develop software for coral reef biodiversity analysis. Scott made use of Chapel’s parallel programming features to develop efficient algorithms for finding ideal spots for coral preservation. He did so by analyzing an intricate high-resolution satellite image of an ocean island. Below is how Scott described his experience:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have now written three major programs for my work using Chapel, and each time I was able to significantly increase performance and achieve excellent parallelism with a low barrier to entry. &lt;strong&gt;Chapel is my go-to language if I need to stand up a highly performant software stack quickly&lt;/strong&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;By making use of parallelism, Scott was able to greatly improve on existing, serial implementations in Matlab. From Scott’s &lt;a href=&#34;https://chapel-lang.org/presentations/Bachman-PAW-ATM.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;presentation at SC23&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Previous performance (serial, MATLAB): ~ 10 days
Current performance (360x cores, Chapel): ~ 2 seconds
&lt;strong&gt;Roughly 5 orders of magnitude improvement&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It’s also worth highlighting Chapel’s portability. Scott wrote and tested his programs on a laptop. With only minor modifications — including tweaks to enable GPU support — his code was able to be executed on &lt;a href=&#34;https://www.olcf.ornl.gov/frontier/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Frontier&lt;/a&gt;, the fastest system in the &lt;a href=&#34;https://www.top500.org/lists/top500/2023/11/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TOP500&lt;/a&gt;, and the only one in that list to reach a computation speed of one exaflop. Scott’s code distributed the problem across several of Frontier’s nodes, and across each of the eight GPUs present in each node. By doing so, it was able to achieve a massive speedup: back-of-the-envelope estimations suggest that Scott’s program would take 648 hours to run on an 8-core CPU, whereas the 64-node version on Frontier ran in 20 minutes. In other words, what would’ve taken an 8-core machine almost a month was sped up to less than the length of a lunch break.&lt;/p&gt;


















&lt;figure  id=&#34;figure-scaling-the-coral-reef-biodiversity-program-on-frontier&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Scaling the Coral Reef Biodiversity program on Frontier&#34; srcset=&#34;
               /blog/20240321-chapel/coral-reef-frontier_hu_2c733b180d268ca2.webp 400w,
               /blog/20240321-chapel/coral-reef-frontier_hu_900f0142d4a28611.webp 760w,
               /blog/20240321-chapel/coral-reef-frontier_hu_94e8d17bcf34eaad.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240321-chapel/coral-reef-frontier_hu_2c733b180d268ca2.webp&#34;
               width=&#34;760&#34;
               height=&#34;391&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Scaling the Coral Reef Biodiversity program on Frontier
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In addition to being a significant performance win, this use case highlights an essential aspect of Chapel: the tools that Chapel provides to developers can be used for programming diverse parallel hardware. Programs can be scaled from a laptop, to a GPU-enabled cloud instance, to a supercomputer, using the same language concepts and features. At the same time, Chapel programs are vendor-neutral — they work on a variety of processors, networks, and GPUs. As a concrete example, GPU-enabled Chapel programs work with both NVIDIA and AMD GPUs.&lt;/p&gt;
&lt;h2 id=&#34;a-language-built-with-scalable-parallel-computing-in-mind&#34;&gt;A Language Built with Scalable Parallel Computing in Mind&lt;/h2&gt;
&lt;p&gt;Chapel was built specifically for parallel computing at scale, guided by experts and real-world high-performance use cases. Because of this, the language has many unique features. These features interconnect to form a consistent and general model for programming diverse parallel hardware, as I alluded to in the previous section. To get a taste of Chapel, take a look at the following serial program:&lt;/p&gt;
&lt;pre&gt;
var Space = {1..n};  
var Dst, Src, Inds: [Space] int;  
initArrays(Src, Indices);  // populate input arrays with data  

for i in Space do  
  Dst[i] = Src[Inds[i]];
&lt;/pre&gt;
&lt;p&gt;This program copies elements from an integer array &lt;code&gt;Src&lt;/code&gt; to another integer array &lt;code&gt;Dst&lt;/code&gt;. However, it reorders the numbers it copies — the third array, &lt;code&gt;Inds&lt;/code&gt;, contains the destination indices for each source index. In other words, &lt;code&gt;destinationIndex = Inds[sourceIndex]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When trying to improve the performance of a program like this, one important thing to consider is that today, even the CPUs of relatively cheap consumer laptops have multiple processor cores. This means that they support parallelism. To tweak the above program to allow it to make use of multiple CPU cores, it’s only necessary to change the &lt;code&gt;for&lt;/code&gt; to a &lt;code&gt;forall&lt;/code&gt;. If you were to run this on your 8-core M1 Mac, the iterations of the loop would be divided between those cores, likely resulting in a noticeable speedup over the original for-loop.&lt;/p&gt;
&lt;pre&gt;
var Space = {1..n};
var Dst, Src, Inds: [Space] int;
initArrays(Src, Indices);

&lt;span style=&#34;background-color:yellow&#34;&gt;forall i in Space do&lt;/span&gt;
  Dst[i] = Src[Inds[i]];
&lt;/pre&gt;
&lt;p&gt;With just one more tweak — the addition of an &lt;code&gt;on&lt;/code&gt; statement — the above program can be made to execute on a GPU:&lt;/p&gt;
&lt;pre&gt;
&lt;span style=&#34;background-color:yellow&#34;&gt;on here.gpus[0] {&lt;/span&gt;
  var Space = {1..n};
  var Dst, Src, Inds: [Space] int;
  initArrays(Src, Indices);

  forall i in Space do
    Dst[i] = Src[Inds[i]];
}
&lt;/pre&gt;
&lt;p&gt;In this case, iterations of the loop will be executed in a massively parallel fashion by the multiple threads of the GPU. I covered Chapel’s GPU support in some detail in &lt;a href=&#34;https://chapel-lang.org/blog/posts/intro-to-gpus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Introduction to GPU Programming in Chapel&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To scale the program to something like a cluster, cloud instance, or supercomputer, the programmer has to make use of several compute nodes at the same time. In this example, that can be achieved by asking Chapel to distribute the arrays across all compute nodes available to the program. When doing so, Chapel divides the iterations of the loop — as well as the array data itself — across the nodes’ memories and processors.&lt;/p&gt;
&lt;pre&gt;
&lt;span style=&#34;background-color:yellow&#34;&gt;use BlockDist;&lt;/span&gt;

&lt;span style=&#34;background-color:yellow&#34;&gt;var Space = blockDist.createDomain(1..n);&lt;/span&gt;
var Dst, Src, Inds: [Space] int;
initArrays(Src, Indices);

forall i in Space do
  Dst[i] = Src[Inds[i]];
&lt;/pre&gt;
&lt;p&gt;With additional tweaks, the multi-node and multi-GPU versions of this program can be combined to distribute computations across all GPUs in an entire cluster. These are the exact sorts of changes that were made to Scott Bachman’s Coral Reef software above to make it run on Frontier.&lt;/p&gt;
&lt;p&gt;The main takeaway is that the same tools — &lt;code&gt;forall&lt;/code&gt; loops and &lt;code&gt;on&lt;/code&gt; statements — interplay elegantly to fit a wide variety of use cases. Moreover, they don’t sacrifice performance. The multi-node version of the program above (which is the &lt;a href=&#34;https://github.com/jdevinney/bale/blob/master/src/bale_classic/apps/ig_src/README.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bale IndexGather&lt;/a&gt; benchmark, by the way), outperforms standard parallel computing technologies:&lt;/p&gt;


















&lt;figure  id=&#34;figure-performance-of-chapels-indexgather-benchmark-with-the---auto-aggregation-compiler-flag&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Performance of Chapel’s IndexGather benchmark, with the `--auto-aggregation` compiler flag&#34; srcset=&#34;
               /blog/20240321-chapel/bale-indexgather_hu_45dfe6a3eeb7b6bf.webp 400w,
               /blog/20240321-chapel/bale-indexgather_hu_e2bb2c0c84a3cc88.webp 760w,
               /blog/20240321-chapel/bale-indexgather_hu_1c681ccb6893d746.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240321-chapel/bale-indexgather_hu_45dfe6a3eeb7b6bf.webp&#34;
               width=&#34;760&#34;
               height=&#34;336&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Performance of Chapel’s IndexGather benchmark, with the &lt;code&gt;--auto-aggregation&lt;/code&gt; compiler flag
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The code presented up to this point is far from a complete picture of all of the unique features that Chapel brings to the table; there’s just not enough space here to cover them all. Some other features to check out are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;sync&lt;/code&gt; variables and &lt;code&gt;coforall&lt;/code&gt; loops&lt;/strong&gt; for explicit parallelism, covered in depth in one of our &lt;a href=&#34;https://chapel-lang.org/blog/posts/aoc2022-day11-monkeys/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous blog posts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reductions&lt;/strong&gt;, for combining a set of elements into a scalar, covered in &lt;a href=&#34;https://chapel-lang.org/blog/posts/aoc2022-day01-calories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;another blog post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;promotion&lt;/strong&gt;, for seamlessly applying scalar functions to multiple elements at once, covered in &lt;a href=&#34;https://chapel-lang.org/blog/posts/aoc2022-day02-rochambeau/#argument-promotion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a third blog post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And those are just the parallelism-specific features! Take a look at the &lt;a href=&#34;https://chapel-lang.org/blog/series/advent-of-code-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Advent of Code&lt;/a&gt; series for a gentle introduction to Chapel, including its general-purpose features.&lt;/p&gt;
&lt;h2 id=&#34;rich-tooling-support&#34;&gt;Rich Tooling Support&lt;/h2&gt;
&lt;p&gt;Chapel’s 2.0 release also brings improvements to developer tooling. Along with the latest version of the language, the Chapel team is publishing a &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=chpl-hpe.chapel-vscode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visual Studio Code extension for Chapel&lt;/a&gt;. This extension provides support for a number of features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code diagnostics (syntax and semantic errors), including quick fixes&lt;/li&gt;
&lt;li&gt;documentation on hover&lt;/li&gt;
&lt;li&gt;go-to-definition, find references&lt;/li&gt;
&lt;li&gt;linting for common issues using &lt;a href=&#34;https://chapel-lang.org/docs/2.0/tools/chplcheck/chplcheck.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;chplcheck&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and much more!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The VSCode extension is an exciting development, bringing a large number of modern editor features to users of Chapel. Much like the features of Chapel itself, there’s simply not enough room in this article to show all of them off. Stay tuned for another article that walks through what the new extension can do in more detail. In the meantime, as a little teaser, here’s a demonstration of hover information and go-to-definition:&lt;/p&gt;


















&lt;figure  id=&#34;figure-hovering-and-going-to-definition-using-chapels-vscode-extension&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Hovering and going-to-definition using Chapel’s VSCode extension&#34;
           src=&#34;http://localhost:1313/blog/20240321-chapel/lsp-hover.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Hovering and going-to-definition using Chapel’s VSCode extension
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Much of the extension’s functionality is provided by a general tool that uses the &lt;a href=&#34;https://microsoft.github.io/language-server-protocol/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language Server Protocol&lt;/a&gt;. This new tool, the &lt;a href=&#34;https://chapel-lang.org/docs/2.0/tools/chpl-language-server/chpl-language-server.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapel language server&lt;/a&gt;, can also be used from other editors. For example, members of the Chapel team use it with Neovim on a daily basis.&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-looking-forward&#34;&gt;Conclusion and Looking Forward&lt;/h2&gt;
&lt;p&gt;Chapel has supported developers with various computing needs in creating performant and maintainable software since 2015, a track record of nearly 10 years. The 2.0 release represents the culmination of a concerted, multi-year effort to refine Chapel’s core functionality based on these many years of experience. Chapel is now stable, but it is not finished: the language will continue to see performance improvements, new features, and better tooling. Whether you are working with a personal machine or a multi-node cluster, there has been no better time to get started with Chapel to get the most out of your computing hardware.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chapel-lang.org/blog/posts/announcing-chapel-2.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chapel-lang.org/blog/posts/announcing-chapel-2.0/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expert on government-commissioned AI threat report: A lot of hype, but a good plan</title>
      <link>http://localhost:1313/blog/20240320-thestreet/</link>
      <pubDate>Wed, 20 Mar 2024 18:18:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20240320-thestreet/</guid>
      <description>&lt;p&gt;&lt;em&gt;Ian Krietzberg&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;fast-facts&#34;&gt;Fast Facts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Gladstone AI last week published a nearly 300-page government-commissioned report detailing the &amp;ldquo;catastrophic&amp;rdquo; risks posed by AI.&lt;/li&gt;
&lt;li&gt;TheStreet sat down with &lt;strong&gt;David Bader&lt;/strong&gt;, the Director of the Institute for Data Science at New Jersey Institute of Technology, to break down the report.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Certainly there&amp;rsquo;s a lot of hype,&amp;rdquo; Bader said, &amp;ldquo;but there&amp;rsquo;s also a lot of real-world threats.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Last week, Gladstone AI &lt;a href=&#34;https://www.gladstone.ai/action-plan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published a report&lt;/a&gt; — commissioned for $250,000 by the U.S. State Department — that detailed the apparent &amp;ldquo;catastrophic&amp;rdquo; risks posed by untethered artificial intelligence technology. It was first reported on by &lt;a href=&#34;https://time.com/6898967/ai-extinction-national-security-risks-report/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The report, the result of more than 200 interviews with AI researchers, leading AI labs and prominent AI executives, was conducted by Gladstone&amp;rsquo;s &lt;a href=&#34;https://twitter.com/harris_edouard/status/1767183618131038569&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Edouard Harris&lt;/a&gt;, Jeremie Harris and Mark Beall over the past year. It warns that, if it remains unregulated, AI could “pose an extinction-level threat to the human species.”&lt;/p&gt;
&lt;p&gt;The report, which TheStreet reviewed &lt;a href=&#34;https://www.scribd.com/document/715366873/Defense-in-Depth-An-Action-Plan-to-Increase-the-Safety-and-Security-of-Advanced-AI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in full&lt;/a&gt;, focuses on two key risks: the weaponization of AI and a potential loss of human control. In terms of weaponization, the report warns that models can be used to power everything from mass disinformation campaigns to large-scale cyberattacks, going on to suggest that advanced, future models might be able to assist in the creation of biological weaponry.&lt;/p&gt;
&lt;p&gt;The risk of control, according to the report, is based on future, highly advanced (and theoretical) models that, in achieving &amp;ldquo;superhuman&amp;rdquo; capabilities, &amp;ldquo;may engage in power-seeking behaviors,&amp;rdquo; becoming &amp;quot; effectively uncontrollable.&amp;quot;&lt;/p&gt;
&lt;p&gt;Though the report says there is evidence to support this, the report does not explore that evidence.&lt;/p&gt;
&lt;h2 id=&#34;the-lines-of-effort&#34;&gt;The lines of effort&lt;/h2&gt;
&lt;p&gt;To address these two risks, the report lays out five lines of effort.&lt;/p&gt;
&lt;p&gt;The government, according to the &lt;a href=&#34;https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e7779f72417554f7958260_Gladstone%20Action%20Plan%20Executive%20Summary.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;report&lt;/a&gt;, should establish an interim set of safeguards, including the creation of an AI Observatory to monitor developments in the space, a set of responsible safeguards for developers and an AI Safety Task Force to enforce those safeguards. It calls here for more control over the advanced AI &lt;a href=&#34;https://www.thestreet.com/dictionary/s/supply-chain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;supply chain&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The report also calls for the funding of open AI safety research, as well as the establishment of deployment safety standards.&lt;/p&gt;
&lt;p&gt;The report&amp;rsquo;s fourth line of effort calls for the establishment of an AI regulatory agency, which would have licensing powers over the companies developing the tech.&lt;/p&gt;
&lt;p&gt;And the fifth line of effort calls for the creation of international safeguards and an international rule-making and licensing agency that would oversee and monitor AI projects around the globe.&lt;/p&gt;
&lt;h2 id=&#34;the-agi-of-it-all&#34;&gt;The AGI of it all&lt;/h2&gt;
&lt;p&gt;The glaring hole in the report — as pointed out by a number of experts on X — is that it is predicated around catastrophic risks stemming from the possible future creation of artificial general intelligence (AGI), a theoretical AI system that would have human-adjacent knowledge.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;There&amp;rsquo;s no science in X risk.&amp;rdquo; — Dr. Suresh Venkatasubramanian&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;AGI, however, does not exist, and many experts &lt;a href=&#34;https://www.thestreet.com/technology/elon-musk-anthropic-claude-3-self-awareness-consciousness-artificial-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;do not believe it will ever be possible&lt;/a&gt;, especially considering that researchers have yet to understand the &lt;a href=&#34;https://www.thestreet.com/technology/elon-musk-anthropic-claude-3-self-awareness-consciousness-artificial-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;whys and hows behind human intelligence, cognition or consciousness&lt;/a&gt;; the replication of that inexplicable human reality is, therefore, a significant challenge.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a ploy by some. It&amp;rsquo;s an actual belief by others. And it&amp;rsquo;s a cynical tactic by even more,&amp;rdquo; Dr. Suresh Venkatasubramanian, an AI researcher and professor who in 2021 served as a White House tech advisor, &lt;a href=&#34;https://www.thestreet.com/technology/the-ethics-of-artificial-intelligence-responsible-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said of the so-called AI extinction risk last year&lt;/a&gt;. &amp;ldquo;It&amp;rsquo;s a great degree of religious fervor sort of masked as rational thinking.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I believe that we should address the harms that we are seeing in the world right now that are very concrete,&amp;rdquo; he added. &amp;ldquo;And I do not believe that these arguments about future risks are either credible or should be prioritized over what we&amp;rsquo;re seeing right now. There&amp;rsquo;s no science in X risk.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;And though current language models might seem intelligent, &lt;a href=&#34;https://www.thestreet.com/technology/elon-musk-anthropic-claude-3-self-awareness-consciousness-artificial-intelligence&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;researchers have decried that as nothing but a mirage&lt;/a&gt; — &lt;a href=&#34;https://www.thestreet.com/technology/tech-news-now-more-lawsuits-pile-up-against-openai-salesforce-announces-dividend-and-more&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the models are, in actuality, predictive generators&lt;/a&gt; trained on an enormous amount of content.&lt;/p&gt;
&lt;p&gt;The report, acknowledging that it views AGI as the driver behind all this AI risk — and ignoring &lt;a href=&#34;https://www.thestreet.com/technology/ai-cybersecurity-nonprofit-civai-deepfake-fraud-identity-theft-hijacking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mention of the many current harms incited by the tech&lt;/a&gt; — even states that while companies including OpenAI, Google DeepMind, Anthropic and Nvidia have suggested AGI is only a few years away, &amp;ldquo;they may also face incentives to exaggerate their near-term capabilities.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The report says that, in an effort to address this problem, it asked a handful of technical sources what the odds are of AI leading to &amp;ldquo;global and irreversible effects.&amp;rdquo; The lowest estimate was 4%; the highest was 20%.&lt;/p&gt;
&lt;p&gt;But Harris later added in a &lt;a href=&#34;https://twitter.com/harris_edouard/status/1767281977395159476&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post on X&lt;/a&gt; that he and his team surveyed only &amp;ldquo;5-10&amp;rdquo; people, saying that the &amp;ldquo;20% was slightly more anomalous, but the folks working on the most cutting edge systems gave higher numbers in every case.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The report goes on to acknowledge that &amp;ldquo;there is no direct empirical evidence that a future AGI system will behave dangerously&amp;rdquo; in regards to a potential loss of control, adding that on the flip side, there is also no evidence to suggest that AGI will behave safely, either.&lt;/p&gt;
&lt;p&gt;Without that evidence, the report instead relies on &amp;ldquo;theoretical arguments guided by experiments on today’s less capable AI systems,&amp;rdquo; a model that it says has &amp;ldquo;significant limitations.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;“The same people cycle between selling AGI utopia and doom,” Timnit Gebru, a former Google computer scientist, told &lt;a href=&#34;https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the New Yorker&lt;/a&gt;. “They are all endowed and funded by the tech billionaires who build all the systems we’re supposed to be worried about making us extinct.”&lt;/p&gt;
&lt;p&gt;On the same day the report was published, Beall left Gladstone to launch what he called &amp;ldquo;the first AI safety Super PAC,&amp;rdquo; according to &lt;a href=&#34;https://venturebeat.com/ai/money-and-politics-continue-to-merge-in-ai-safety-including-a-new-super-pac-the-ai-beat/#:%7e:text=As%20the%20debate%20swirled%20on,first%20AI%20safety%20Super%20PAC.%E2%80%9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VentureBeat&lt;/a&gt;. His plan is to &amp;ldquo;run a national voter education campaign on AI policy.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;He told VentureBeat that the PAC has already secured initial investments and plans to raise millions of dollars in the coming weeks and months.&lt;/p&gt;
&lt;h2 id=&#34;sifting-through-the-hype&#34;&gt;Sifting through the hype&lt;/h2&gt;
&lt;p&gt;While it was &lt;a href=&#34;https://twitter.com/billyperrigo/status/1767176713492980011/quotes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dismissed by many&lt;/a&gt; for its lack of evidence in arguments not supported by science, others lauded the report, not for its fear of nonexistent AGI, but for its action-oriented suggestions that could allow the government to better rein in the industry.&lt;/p&gt;
&lt;p&gt;The nonprofit Control AI &lt;a href=&#34;https://twitter.com/ai_ctrl/status/1767250108586856894&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;said that&lt;/a&gt; it is &amp;ldquo;heartening to see this taken seriously at the highest levels,&amp;rdquo; adding that &amp;ldquo;we should be well-prepared, and regulations and guard rails should be in place to ensure that it only benefits humanity.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Ed Newton-Rex, the CEO of the nonprofit Fairly Trained, &lt;a href=&#34;https://twitter.com/ednewtonrex/status/1767321351017406761&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;called&lt;/a&gt; the report &amp;ldquo;extremely important&amp;rdquo; for recommending that the government &amp;ldquo;act fast&amp;rdquo; on AI.&lt;/p&gt;
&lt;p&gt;TheStreet sat down with &lt;strong&gt;David Bader&lt;/strong&gt;, the Director of the Institute for Data Science at New Jersey Institute of Technology, to discuss the report, who acknowledged that &amp;ldquo;certainly there&amp;rsquo;s a lot of hype, but there&amp;rsquo;s also a lot of real-world threats.&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Now is the time that the world has to have these conversations.&amp;rdquo; — Dr. David Bader&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;With the technology moving as quickly as it is, he said that people are hearing everything from promises of an AI-fueled utopia to an AI-fueled apocalypse.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;My thought is that there are some real concerns to think about, but we do have some time to think about it,&amp;rdquo; he said. &amp;ldquo;This report raises some interesting directions for trying to understand what to do.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The report&amp;rsquo;s recommendation to safeguard the supply chains behind AI technology in an effort to increase what &lt;a href=&#34;https://twitter.com/harris_edouard/status/1767183624330203573&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;it found to be lacking security at the labs building the tech&lt;/a&gt;, according to Bader, is a good one.&lt;/p&gt;
&lt;p&gt;He did say, however, that there are a number of other risks and harms posed by AI that are important to not ignore, &lt;a href=&#34;https://www.thestreet.com/technology/the-ethics-of-artificial-intelligence-responsible-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;including issues of algorithmic bias&lt;/a&gt;, &lt;a href=&#34;https://www.thestreet.com/technology/ai-cybersecurity-nonprofit-civai-deepfake-fraud-identity-theft-hijacking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;harms stemming from deepfake creation&lt;/a&gt; and &lt;a href=&#34;https://www.thestreet.com/technology/cybersecurity-expert-says-the-next-generation-of-identity-theft-is-here-identity-hijacking&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mounting problems concerning disinformation&lt;/a&gt;, cyberattacks and &lt;a href=&#34;https://www.thestreet.com/technology/whats-stopping-tesla-from-achieving-level-3-self-driving&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;self-driving&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;He went on to acknowledge the national security risks laid out in the report, as well as the value of the five lines of effort, saying that they represent a number of good ways forward in terms of policies and laws regarding the mitigation of AI risk.&lt;/p&gt;
&lt;p&gt;Still, he said that he is pessimistic that the government will be able to &amp;ldquo;put the genie back in the battle and control AI,&amp;rdquo; even through laws and regulations. &amp;ldquo;I think this regulation is probably going to be one of the hardest regulations to create,&amp;rdquo; he added, citing the difficulty inherent to the report&amp;rsquo;s lofty goals of international regulatory efforts.&lt;/p&gt;
&lt;p&gt;But when it comes to the AGI of it all, Bader isn&amp;rsquo;t sold.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I&amp;rsquo;m still a little bit leery that that&amp;rsquo;s something that we&amp;rsquo;ll achieve. I think the hype over AI at the moment &amp;hellip; we see it isn&amp;rsquo;t a panacea of excellent and fantastic information,&amp;rdquo; Bader said. &amp;ldquo;It hallucinates a lot. There&amp;rsquo;s a lot of bias. We&amp;rsquo;re getting there but I still think it&amp;rsquo;s a long way off before we see AGI.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;There is still a lot of hype with AI but it&amp;rsquo;s getting better and better every day,&amp;rdquo; he added. &amp;ldquo;Now is the time that the world has to have these conversations.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;why-not-shut-it-down&#34;&gt;Why not shut it down?&lt;/h2&gt;
&lt;p&gt;In this conversation, stemming from the report, about the risk of world-destroying AI, I asked Bader: &amp;ldquo;Why not shut it down?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Every technology, every basic and foundational technology we create, can be used for good purposes and it can be used for nefarious purposes,&amp;rdquo; Bader said. &amp;ldquo;So whether it&amp;rsquo;s a ballpoint pen, or whether it is a weather satellite or whether it&amp;rsquo;s a new medicine, everything that we create can do good or do harm.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;AI, he said, may be able to help humanity mitigate climate change (&lt;a href=&#34;https://www.thestreet.com/technology/ibm-highlights-the-actual-promise-of-ai-not-chatgpt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an effort some companies are already exploring&lt;/a&gt;), help solve geopolitical crises or help feed populations around the world.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;When we look at these technologies, I don&amp;rsquo;t think the right thing to do is shut it down. If we did that we would be left in this country without automobiles, without electricity, without lightbulbs, without all the technology that got us to where we are today,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I think technology has a way of making lives better. So I&amp;rsquo;m more of an optimist that we should continue developing these technologies, but then we have to understand how to mitigate the risks and reduce the potential bad uses of that technology.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;BY IAN KRIETZBERG&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ian Krietzberg is a tech reporter for The Street. He covers artificial intelligence companies, safety and ethics extensively. As an offshoot of his tech beat, Ian also covers Elon Musk and his many companies, namely SpaceX and Tesla.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.thestreet.com/technology/expert-on-government-commissioned-ai-threat-report-a-lot-of-hype-but-a-good-plan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.thestreet.com/technology/expert-on-government-commissioned-ai-threat-report-a-lot-of-hype-but-a-good-plan&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT&#39;s David Bader on the Future of AI: Silver Linings, a Touch of Grey</title>
      <link>http://localhost:1313/blog/20240309-njit/</link>
      <pubDate>Sat, 09 Mar 2024 15:19:56 -0500</pubDate>
      <guid>http://localhost:1313/blog/20240309-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Artificial intelligence, data science and the emerging field of quantum computing are among the hottest research topics in computing overall. &lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor in NJIT’s Ying Wu College of Computing and director of the university’s &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt;, shared his thoughts on big-picture questions about each area. Bader is known globally for his innovative work &lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the history&lt;/a&gt; and &lt;a href=&#34;https://news.njit.edu/streaming-data-analytics-has-new-method-planned-njit-usc-harvard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cutting-edge&lt;/a&gt; of computing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In 2023 you were recognized by the Computer History Museum for developing a Linux supercomputer using commodity hardware. Your approach is now the dominant method in high-performance computing. Was that a life-shaping lesson for you in not being afraid to try unconventional things? How do you explain this lesson to your students?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Certainly, being recognized by the Computer History Museum for my early work in developing a Linux supercomputer using commodity hardware was a pivotal moment in my career, and it underscored a fundamental lesson that has shaped not only my approach to computing but also how I mentor and teach. In the world of research and innovation, the courage to explore unconventional paths is often the key to groundbreaking discoveries. This project was a testament to that belief. At a time when the idea of assembling a supercomputer from off-the-shelf components was unconventional, I saw an opportunity to democratize access to high-performance computing. It was a venture into the unknown, leveraging the emerging potential of Linux and commodity hardware to build something that was both accessible and powerful. This experience taught me the importance of embracing risk and the value of resilience. There were technical hurdles, skepticism from peers and the daunting task of venturing beyond established norms.&lt;/p&gt;
&lt;p&gt;Yet, it was through navigating these challenges that I not only succeeded in this immediate goal but also laid the groundwork for future advancements in high-performance computing. When I share this journey with my students, I emphasize the importance of curiosity and perseverance. I encourage them to question the status quo and to see beyond conventional wisdom. The lesson is not just about the success of the project itself but about the mindset it fostered. I stress that failure is not the opposite of success but a stepping stone towards it. The Linux supercomputer project was not just about the technology, it was about building a community around an idea.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You were recently elected to the Computing Research Association’s Board of Directors. CRA helps set academic and industrial research directions through its many activities, which include advising Congress and the National Science Foundation. What inspired you to seek this role and how do you anticipate being involved?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my bid to join the Computing Research Association board, I&amp;rsquo;m committed to aligning the organization&amp;rsquo;s efforts with the transformative wave of the AI revolution. The CRA&amp;rsquo;s mandate extends beyond the confines of traditional computer science, encompassing the burgeoning fields of machine learning, data science and artificial intelligence. These areas are critical to the future of computing research and must be integrated into the CRA&amp;rsquo;s vision. At NJIT, we&amp;rsquo;ve established ourselves as pioneers by introducing one of the nation&amp;rsquo;s first data science academic programs and departments. This initiative exemplifies the kind of leadership and foresight I believe the CRA should embody, ensuring it remains at the forefront of data science and computing research advancements. My vision extends to nurturing the next generation of innovators and thought leaders, essential for maintaining the US&amp;rsquo;s competitive edge in technology.&lt;/p&gt;
&lt;p&gt;A significant barrier to this goal is the inadequate Ph.D. stipends that deter many talented individuals from pursuing research careers. I advocate for doubling the standard Ph.D. stipend, making research careers more attractive and reflective of the value these roles bring to our society. Joining the Computing Research Association board represents a significant milestone and responsibility that aligns with my professional values and aspirations in the field of computing research.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You’re an advocate for democratizing the power of data science and supercomputing. How does the rise of user-friendly artificial intelligence systems such as ChatGPT impact your work? How does it impact the overall work of the Institute for Data Science?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Absolutely, the rise of user-friendly artificial intelligence systems like ChatGPT marks a pivotal moment in our pursuit to democratize data science and supercomputing. For my work, this evolution serves as both a tool and a testament to the power of making complex computational capabilities accessible to a broader audience. It enriches the palette of methodologies and technologies at our disposal, enabling us to tackle more ambitious projects with greater efficiency and creativity. By integrating these AI systems into our research and educational programs, we&amp;rsquo;re not just enhancing our ability to process and analyze data, we&amp;rsquo;re also empowering students and researchers with the means to innovate and explore new horizons in data science without being hindered by the technical complexities that once acted as barriers. For the Institute for Data Science, the impact of such AI systems is transformative. They serve as a bridge between advanced computational technologies and a diverse range of disciplinary domains, facilitating interdisciplinary research and collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A lot of people who are information workers are afraid that AI will make their careers obsolete. Technology progress can’t be stopped, so how should people adapt?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the face of technological progress, particularly with the rapid advancement of AI, it&amp;rsquo;s understandable that information workers may feel apprehensive about the future of their careers. However, rather than viewing AI as a harbinger of obsolescence, it&amp;rsquo;s crucial to see it as a catalyst for evolution and innovation in our work practices. The key to adapting is in embracing these technologies, learning to work alongside them, and leveraging their capabilities to enhance our own skill sets and productivity. The first step in this adaptation process is to cultivate a mindset of lifelong learning. As AI and other technologies continue to evolve, so too must our skills and knowledge. This means staying informed about new technologies, seeking out educational opportunities, and being open to acquiring new competencies that complement the capabilities of AI.&lt;/p&gt;
&lt;p&gt;For instance, developing skills in data literacy, AI ethics, and understanding the principles of machine learning can make workers more versatile and valuable in an AI-integrated workplace. Additionally, it&amp;rsquo;s important to focus on the uniquely human skills that AI cannot replicate, such as creativity, emotional intelligence, and critical thinking. By honing these abilities, workers can ensure they remain irreplaceable components of the workforce, capable of tasks that require a human touch, from complex decision-making to empathetic interactions with customers or clients.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other than creative prompt-making, what should non-programmers learn now about AI?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For non-programmers looking to delve deeper into AI, understanding the ethical implications and societal impacts of AI is paramount. It’s important to be aware of how AI decisions are made, the potential biases in AI systems, and the ethical considerations of AI use. Additionally, developing data literacy is crucial, as it enables individuals to evaluate AI outputs and understand the importance of data quality and biases. A basic grasp of AI and machine learning concepts, even without programming skills, can demystify AI technologies and reveal their potential applications. Staying informed about AI advancements across various sectors can also inspire innovative ideas and foster interdisciplinary collaborations. By focusing on these areas, non-programmers can contribute meaningfully to the AI conversation and its future direction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There’s a popular sci-fi plot where the computers get so smart that people lose control. The new class of user-friendly AI is certainly making people excited but also nervous. Should we be afraid?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The emergence of user-friendly AI technologies has indeed brought this conversation into the mainstream, highlighting the balance we must strike between harnessing the benefits of AI and addressing valid concerns about its implications. It’s critical to recognize that the AI technologies we&amp;rsquo;re creating today are built with numerous safeguards, are subject to ethical guidelines, and operate within evolving regulatory environments. These measures are designed to ensure AI systems augment human abilities and decision-making, rather than supplanting or undermining human control.&lt;/p&gt;
&lt;p&gt;While it’s natural to harbor concerns about the rapid progression of AI, allowing fear to dominate the discourse would be a disservice to the potential benefits these technologies can offer. Instead, this moment calls for proactive engagement with AI, an investment in understanding its inner workings, limitations, and the ethical dilemmas it presents. By advocating for responsible AI development, emphasizing education, and promoting transparency, we can foster an environment where AI serves as a tool for societal advancement. This approach ensures that we remain at the helm of AI&amp;rsquo;s trajectory, steering it towards outcomes that uplift humanity rather than scenarios that fuel dystopian fears.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other than AI, which emerging technologies excite you the most right now in terms of their potential to transform computing?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the realm of computing, beyond the transformative power of AI, quantum computing stands out as an especially exciting frontier. This technology, with its potential to solve complex problems exponentially faster than classical computers, could revolutionize fields ranging from cryptography to drug discovery, climate modeling, and beyond. Quantum computing&amp;rsquo;s promise to tackle challenges currently beyond our reach, due to its fundamentally different approach to processing information, represents a leap forward in our computational capabilities. Its convergence with AI could lead to unprecedented advancements, making this era an incredibly thrilling time to be at the forefront of computing and data science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njits-david-bader-future-ai-silver-linings-touch-grey&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njits-david-bader-future-ai-silver-linings-touch-grey&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2024 CRA Board of Directors Election Results</title>
      <link>http://localhost:1313/blog/20240229-cra/</link>
      <pubDate>Thu, 29 Feb 2024 10:25:22 -0500</pubDate>
      <guid>http://localhost:1313/blog/20240229-cra/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Matt Hazenbush, Director of Communications&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/2024%20CRA%20Board%20Election%20Winners-2-David%20Bader_hu_4b55695d794de7d1.webp 400w,
               /blog/20240229-cra/2024%20CRA%20Board%20Election%20Winners-2-David%20Bader_hu_b98f1f92fc4c49ad.webp 760w,
               /blog/20240229-cra/2024%20CRA%20Board%20Election%20Winners-2-David%20Bader_hu_f7d59e92c5336524.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/2024%20CRA%20Board%20Election%20Winners-2-David%20Bader_hu_4b55695d794de7d1.webp&#34;
               width=&#34;760&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;CRA members have elected four new individuals to its Board of Directors: &lt;strong&gt;David Bader&lt;/strong&gt; (New Jersey Institute of Technology), Bruce Hendrickson (Lawrence Livermore National Lab), Fatma Ozcan (Google), and Manuel Pérez-Quiñones (University of North Carolina at Charlotte).&lt;/p&gt;
&lt;p&gt;In addition, five current Board members won reelection: James Allan (University of Massachusetts Amherst), Ran Libeskind-Hadas (Claremont McKenna College), Rachel Pottinger (University of British Columbia), Eve Schooler (Intel, retired and University of Oxford), and Katie Siek (Indiana University Bloomington).&lt;/p&gt;
&lt;p&gt;CRA would like to thank all of the candidates who ran for a seat on the Board of Directors this election cycle. Those named above will serve from July 1, 2024 through June 30, 2027.&lt;/p&gt;
&lt;p&gt;Four Board members’ terms of service will end June 30, 2024: Stephanie Forrest (Arizona State University), Diana Franklin (University of Chicago), Chris Ramming (VMWare by Broadcom), and Jing Xiao (Worcester Polytechnic Institute). CRA thanks them for their contributions during their time with the Board.&lt;/p&gt;
&lt;h2 id=&#34;new-board-members&#34;&gt;New Board Members&lt;/h2&gt;
&lt;p&gt;CRA is thrilled to welcome the following four new members to its Board of Directors.&lt;/p&gt;
&lt;h3 id=&#34;david-bader&#34;&gt;David Bader&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/David-Bader-180x180_hu_de11f82c5ccab36e.webp 400w,
               /blog/20240229-cra/David-Bader-180x180_hu_d8477a8fec940e23.webp 760w,
               /blog/20240229-cra/David-Bader-180x180_hu_bae28717440da19b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/David-Bader-180x180_hu_de11f82c5ccab36e.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; is a Distinguished Professor and a founder of the Department of Data Science in the Ying Wu College of Computing at the New Jersey Institute of Technology, where he also serves as the Director of the Institute for Data Science.&lt;/p&gt;
&lt;p&gt;Dr. Bader is a Fellow of the IEEE, ACM, AAAS, and SIAM, and a recipient of the IEEE Sidney Fernbach Award in 2021. He was inducted into the Innovation Hall of Fame at the University of Maryland’s A. James Clark School of Engineering in 2022, the same institution from which he earned his Ph.D. in electrical engineering.&lt;/p&gt;
&lt;p&gt;Through his service on the Board, Dr. Bader aims to strategically position CRA’s activities with the AI revolution.&lt;/p&gt;
&lt;h3 id=&#34;bruce-hendrickson&#34;&gt;Bruce Hendrickson&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Bruce-180x180_hu_929e13a1bf5386ac.webp 400w,
               /blog/20240229-cra/Bruce-180x180_hu_1cf2d36874a6efbc.webp 760w,
               /blog/20240229-cra/Bruce-180x180_hu_e29849a279b8585a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Bruce-180x180_hu_929e13a1bf5386ac.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Bruce Hendrickson is the Principal Associate Director for Computing at the Lawrence Livermore National Lab. In this role, he leads an organization of around 1,400 computing professionals with responsibility for the full breadth of the Laboratory’s computational needs, including research, platforms, and services.&lt;/p&gt;
&lt;p&gt;Dr. Hendrickson is a Fellow of SIAM and AAAS, and a winner of the SuperComputing Conference Test of Time Award and the George R. Cotter Award, among many honors received during his distinguished career. He earned his Ph.D. in computer science from Cornell University.&lt;/p&gt;
&lt;p&gt;As a CRA Board member, Dr. Hendrickson hopes to build the organization’s ties to US national laboratories.&lt;/p&gt;
&lt;h3 id=&#34;fatma-ozcan&#34;&gt;Fatma Ozcan&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Fatma-180x180_hu_413cc2ac7052f1.webp 400w,
               /blog/20240229-cra/Fatma-180x180_hu_7ff11fa9c2903da4.webp 760w,
               /blog/20240229-cra/Fatma-180x180_hu_91fb480db0385d6d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Fatma-180x180_hu_413cc2ac7052f1.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Fatma Ozcan is a Principal Software Engineer at Google, leading data analysis research in Systems Research@Google. Previously, she was a Distinguished Research Staff Member and a Senior Manager at IBM Almaden Research Center.&lt;/p&gt;
&lt;p&gt;Among Dr. Ozcan’s many honors are the VLDB Women in Database Research Award (2022), the IBM Corporate Award for PureXML Database Technology (2008) and the IBM Extraordinary Technical Accomplishment Award (2010 &amp;amp; 2009). She is an ACM Distinguished Member and the Vice Chair of ACM SIGMOD. She earned her Ph.D. in computer science from the University of Maryland, College Park.&lt;/p&gt;
&lt;p&gt;Dr. Ozcan currently serves on the CRA-Industry Steering Committee, and will become its Co-Chair in July 2024.&lt;/p&gt;
&lt;p&gt;She aims to continue to deepen CRA collaboration with industry through her reappointment to the Board.&lt;/p&gt;
&lt;h3 id=&#34;manuel-pérez-quiñones&#34;&gt;Manuel Pérez-Quiñones&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Manuel-180x180_hu_3546f86128c58117.webp 400w,
               /blog/20240229-cra/Manuel-180x180_hu_5a13b35719dfda23.webp 760w,
               /blog/20240229-cra/Manuel-180x180_hu_87a92cddb46390d0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Manuel-180x180_hu_3546f86128c58117.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Manuel Pérez-Quiñones is a Professor in the Department of Software and Information Systems in the College of Computing and Informatics at the University of North Carolina at Charlotte.&lt;/p&gt;
&lt;p&gt;Dr. Pérez-Quiñones has received a number of honors and awards over his distinguished career, including being named an ACM Distinguished Member for Outstanding Educational Contributions to Computing (2019), the CRA Nico A. Haberman Award (2018), and the Richard A. Tapia Achievement Award for Scientific Scholarship, Civic Science and Diversifying Computing (2017), among others. He earned a D.Sc. in computer science from The George Washington University.&lt;/p&gt;
&lt;p&gt;Dr. Pérez-Quiñones has been involved with a number of CRA activities, recently including serving on the Organizing Committee for the CRA Leadership Academy (2023), the Search Committee for the CRA Executive Director (2022), and the BPCnet.org Steering Committee (2020-2023).&lt;/p&gt;
&lt;p&gt;Through his service on the Board, Dr. Pérez-Quiñones seeks to continue his collaboration with CRA on efforts to broaden participation in computing.&lt;/p&gt;
&lt;h2 id=&#34;reelected-board-members&#34;&gt;Reelected Board Members&lt;/h2&gt;
&lt;p&gt;CRA is pleased to share news of the reelection of the following five members of the Board of Directors.&lt;/p&gt;
&lt;h3 id=&#34;james-allan&#34;&gt;James Allan&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/James-Allan-180x180_hu_18b1c3cd74cb04a7.webp 400w,
               /blog/20240229-cra/James-Allan-180x180_hu_ce763794d3fdd30c.webp 760w,
               /blog/20240229-cra/James-Allan-180x180_hu_4abd4bb91535a776.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/James-Allan-180x180_hu_18b1c3cd74cb04a7.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;James Allan is a Professor and Associate Dean of Research and Engagement at the Manning College of Information and Computer Sciences at the University of Massachusetts Amherst. Dr. Allan is a Fellow of the ACM and was elected to the ACM SIGIR Academy in 2021. He won the ACM SIGIR Test of Time Award in 2016, among other career honors. He earned his Ph.D. in computer science from Cornell University.&lt;/p&gt;
&lt;p&gt;Dr. Allan has served as a CRA Board member since 2018, and as the Treasurer and member of the Executive Committee since 2019.&lt;/p&gt;
&lt;h3 id=&#34;ran-libeskind-hadas&#34;&gt;Ran Libeskind-Hadas&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Ran-180x180_hu_5b46806ac24b2034.webp 400w,
               /blog/20240229-cra/Ran-180x180_hu_63aa706664dd6e1e.webp 760w,
               /blog/20240229-cra/Ran-180x180_hu_11245619062cb86.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Ran-180x180_hu_5b46806ac24b2034.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Ran Libeskind-Hadas is the Founding Chair of the Kravis Department of Integrated Sciences at Claremont McKenna College. He is the recipient of several honors, including the NCWIT EngageCSEdu Engagement Excellence Award and the Harvey T. Mudd Prize, among others. He earned his Ph.D. in computer science from the University of Illinois Urbana-Champaign, and was honored in 2012 with its CS Distinguished Alumni Educator Award.&lt;/p&gt;
&lt;p&gt;Dr. Libeskind-Hadas has served as a CRA Board member since 2018, and as a member of the Executive Committee since 2019, serving as Secretary from 2019 to 2023 and as Vice Chair since 2023. He previously served as a member of the CCC and Co-Chair of CRA-E.&lt;/p&gt;
&lt;h3 id=&#34;rachel-pottinger&#34;&gt;Rachel Pottinger&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Rachel-180x180_hu_e350e29b63d9658.webp 400w,
               /blog/20240229-cra/Rachel-180x180_hu_ee57f1aa29c590db.webp 760w,
               /blog/20240229-cra/Rachel-180x180_hu_4c938cde668b752f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Rachel-180x180_hu_e350e29b63d9658.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Rachel Pottinger is a Professor of Computer Science at the University of British Columbia. She is a recipient of the Denice Denton Emerging Leader Award from the Anita Borg Institute and is a multi-time winner of the UBC Computer Science Department Faculty Teaching Award. She earned her Ph.D. in computer science from the University of Washington.&lt;/p&gt;
&lt;p&gt;Dr. Pottinger has served as a CRA Board member since 2018, and is a Co-Chair of the CRA Committee on Organizing the 2024 CRA Conference at Snowbird.&lt;/p&gt;
&lt;h3 id=&#34;eve-schooler&#34;&gt;Eve Schooler&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Eve-180x180_hu_65b6384b90aef8e7.webp 400w,
               /blog/20240229-cra/Eve-180x180_hu_9477e04f746dd13d.webp 760w,
               /blog/20240229-cra/Eve-180x180_hu_2cfcea4ba72f853d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Eve-180x180_hu_65b6384b90aef8e7.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Eve Schooler is a RAEng Visiting Professor of Sustainable Computing at University of Oxford. She previously worked for nearly two decades at Intel, where she was a Principal Engineer and Director. She is an IEEE Fellow and co-recipient of the IEEE Internet Award (2020). She earned her Ph.D. in computer science from the California Institute of Technology.&lt;/p&gt;
&lt;p&gt;Dr. Schooler has served as a CRA Board member since 2021, and has been a CRA-Industry Council member since 2023.&lt;/p&gt;
&lt;h3 id=&#34;katie-siek&#34;&gt;Katie Siek&lt;/h3&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240229-cra/Katie-180x180_hu_58611209bb1e9409.webp 400w,
               /blog/20240229-cra/Katie-180x180_hu_356fb672889e817b.webp 760w,
               /blog/20240229-cra/Katie-180x180_hu_52d47e28371e7589.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240229-cra/Katie-180x180_hu_58611209bb1e9409.webp&#34;
               width=&#34;180&#34;
               height=&#34;180&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Katie Siek is a Professor of Informatics at Indiana University Bloomington. She is an ACM Distinguished Member and recipient of several prestigious honors, including the Indiana University Trustees Teaching Award, NCWIT Undergraduate Research Mentoring Award, and the Borg Early Career Award, among others. She earned her Ph.D. in computer science from Indiana University.&lt;/p&gt;
&lt;p&gt;Dr. Siek has served as a CRA Board member since 2021, and as Secretary and a member of the Executive Committee since 2023. She also serves on the CCC Council, is Vice Chair of the CRA Committee on Surveys, and is Chair of the CRA Working Group on Misconduct Issues.&lt;/p&gt;
&lt;h2 id=&#34;stay-up-to-date-on-cra-board-activities&#34;&gt;Stay Up to Date on CRA Board Activities &lt;/h2&gt;
&lt;p&gt;The CRA Board of Directors will convene next from July 22-23 in Snowbird, Utah. For updates on the CRA Board, including programmatic committee updates and board meeting summaries, follow the CRA Bulletin and Computing Research News.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cra.org/2024-cra-board-of-directors-election-results/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cra.org/2024-cra-board-of-directors-election-results/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>National Student Data Corps (NSDC) MasterClass Video Series: David Bader</title>
      <link>http://localhost:1313/blog/20240228-nsdc/</link>
      <pubDate>Wed, 28 Feb 2024 16:48:58 -0500</pubDate>
      <guid>http://localhost:1313/blog/20240228-nsdc/</guid>
      <description>&lt;p&gt;Are you interested in #datascience and #graph #analytics? Join the NSDC and &lt;strong&gt;Dr. David Bader (NJIT)&lt;/strong&gt; for a fascinating look into Open-Source Frameworks for Interactive Massive-Scale Graph Analytics!&lt;/p&gt;
&lt;p&gt;Watch on the NEBDHub YT channel: bit.ly/NEastYT&lt;/p&gt;
&lt;p&gt;#data #datascience #R #python #analytics #dataanalytics #computerscience #STEM #science #math #ethics #AI #ML&lt;/p&gt;
&lt;p&gt;David A. Bader, PhD, Florence Hudson, Midwest Big Data Innovation Hub, New Jersey Institute of Technology&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/mVfHHx93n1E?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/feed/update/urn%3Ali%3Aactivity%3A7168715014034460674/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/feed/update/urn%3Ali%3Aactivity%3A7168715014034460674/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fernando Vera Buschmann, Fulbright Scholar from Chile, Chose NJIT to Learn from Great Minds</title>
      <link>http://localhost:1313/blog/20240202-njit/</link>
      <pubDate>Fri, 02 Feb 2024 13:40:12 -0500</pubDate>
      <guid>http://localhost:1313/blog/20240202-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240202-njit/Fernando%20Buschmann2_hu_ca54e6fad431f909.webp 400w,
               /blog/20240202-njit/Fernando%20Buschmann2_hu_19dec57d62559dc9.webp 760w,
               /blog/20240202-njit/Fernando%20Buschmann2_hu_d2e26fb128cdd48c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240202-njit/Fernando%20Buschmann2_hu_ca54e6fad431f909.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Ying Wu College of Computing (YWCC) and the department of Computer Science are proud to count Fulbright Scholar Fernando Vera Buschmann as a member of its Ph.D. candidate team of researchers, one of a growing number of Fulbright award recipients who have chosen NJIT as an incubator for their professional goals and objectives. He came to NJIT from Osorno, a small town in Patagonia, far south of Santiago, Chile, with a background in physics and pedagogy having obtained his M.Sc. at the University of Chile.&lt;/p&gt;
&lt;p&gt;His fascination with better understanding the human brain inspired him to pursue computational neuroscience as a means of uniting physical science and computer science in an effort to study and design algorithms that could lead to new discoveries in mental health, from autism, depression, traumatic changes to psychological states. Seeking to understand from the very complexity of large-scale elements of the brain.&lt;/p&gt;
&lt;p&gt;“We still don’t know enough about the brain. There are more connections in the brain than the number of stars in the complete universe,&amp;quot; he said.&lt;/p&gt;
&lt;p&gt;As a Fulbright Scholar, Vera Buschmann was presented with several options on where to pursue his Ph.D., focusing on either Europe or the U.S. Recognizing the NY metro area as one of the world’s major tech hubs, he narrowed his choices to just a few select universities of interest.&lt;/p&gt;
&lt;p&gt;However, when mining potential discoveries in the brain through computing, the great mind under which you will conduct your research is just as important as where you do it. Buschmann ultimately chose YWCC for the opportunity to work with &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;, cofounder of the Graph500 List for benchmarking “Big Data” computing platforms, one of the pioneers of Linux supercomputing, and a leading researcher in High Performance Computing.&lt;/p&gt;
&lt;p&gt;Under Bader’s mentorship, Buschmann analyzes large-scale knowledge graphs in open sources to understand how certain data behaves within them. It is within the application of these findings that Buschmann seeks to assist in advancing the intersection of the science of data and the study of what makes us human.&lt;/p&gt;
&lt;p&gt;He observed, “Physics, computer science and neuroscience are a good way to understand the nature of the world.”&lt;/p&gt;
&lt;p&gt;Upon completion of his Ph.D., Buschmann hopes to bring his experience back to Chile, mainly to Osorno, which, as an underserved population, has significant education challenges, particularly with developmental science. This is a sentiment shared by his brother Jorge Vera Buschmann, who happens to be a neuroscientist at Albert Einstein College of Medical Bronx, NY.&lt;/p&gt;
&lt;p&gt;According to Buschmann, the program thus far has been “diverse, competitive, and quite demanding,” but also cites the unique opportunity to work side by side with people who are as passionate about finding solutions as invaluable.&lt;/p&gt;
&lt;p&gt;He added, “Working permanently on the frontier of knowledge, seeking answers to questions that are currently being discussed in industry and attending academic conferences worldwide is nothing short of incredible.”&lt;/p&gt;
&lt;p&gt;The “how” and “why” of the brain is what motivates him to join millions of others who are still seeking answers.&lt;/p&gt;
&lt;p&gt;“The most important computer we have is our brain. And yet, we still are only scratching the surface when trying to understand it.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/fernando-vera-buschmann-fulbright-scholar-chile-chose-njit-learn-great-minds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/fernando-vera-buschmann-fulbright-scholar-chile-chose-njit-learn-great-minds&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best online courses and certificate programs to master new skills in 2024</title>
      <link>http://localhost:1313/blog/20240112-nypost/</link>
      <pubDate>Fri, 12 Jan 2024 10:37:22 -0500</pubDate>
      <guid>http://localhost:1313/blog/20240112-nypost/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Social Links forCamryn La Sala and Social Links forVictoria Giardina&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-new-york-post-composite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New York Post Composite&#34; srcset=&#34;
               /blog/20240112-nypost/onlineclasses_hu_e6476f28760d1478.webp 400w,
               /blog/20240112-nypost/onlineclasses_hu_7349dc7ffb274462.webp 760w,
               /blog/20240112-nypost/onlineclasses_hu_ecc4c9a78fb8197d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/onlineclasses_hu_e6476f28760d1478.webp&#34;
               width=&#34;744&#34;
               height=&#34;496&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New York Post Composite
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Let’s be real — once the New Year approaches, we’re all about looking into personal development. Of course, this extends to online courses to sharpen a rusty skill or gain a new one.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;https://www.coursera.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coursera&lt;/a&gt; to &lt;a href=&#34;https://www.edx.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;edX&lt;/a&gt; to &lt;a href=&#34;https://nypost.com/shopping/masterclass-review/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MasterClass&lt;/a&gt; and &lt;a href=&#34;https://nypost.com/article/masterclass-vs-skillshare-review-pros-cons-best-courses/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Skillshare&lt;/a&gt; (the latter two &lt;a href=&#34;https://nypost.com/article/masterclass-vs-skillshare-review-pros-cons-best-courses/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;we reviewed&lt;/a&gt;), there are an endless array of options for you to learn from.&lt;/p&gt;
&lt;p&gt;Aside from listing the 40 best online courses and certificate programs in a variety of different categories, we called upon skilled experts in their crafts to touch upon specific skills — and even recommend classes — in a variety of industries. More, you’ll find some classes we have taken and could probably write an entire thesis about, too.&lt;/p&gt;
&lt;p&gt;Whether you’re interested in the beauty industry at large or want to learn how to say “hello” in more than one language, the world is pretty much your oyster when it comes to the repertoire of online classes at your disposal.&lt;/p&gt;
&lt;p&gt;Be sure to note that if one of the classes listed has a monthly price, you may be able to access other classes on that same platform during the time of your membership. Who doesn’t love more opportunities to learn?&lt;/p&gt;
&lt;h2 id=&#34;are-online-classes-a-good-idea&#34;&gt;Are online classes a good idea?&lt;/h2&gt;
&lt;p&gt;From our firsthand experience, the answer is yes. Regardless of age, interest or skill level, there’s surely a fit for you on any of the online platforms we rounded up below.&lt;/p&gt;
&lt;p&gt;Not to mention, most online classes are now entertaining. With immersive, studio-quality virtual lessons and engaging talking points, you may resort to one of the below options quicker than your new &lt;a href=&#34;http://go.nypost.com/?id=93051X1547088&amp;amp;isjs=1&amp;amp;jv=15.4.2-stackpath&amp;amp;sref=https%3A%2F%2Fnypost.com%2Farticle%2Fbest-online-classes%2F&amp;amp;url=https%3A%2F%2Fwww.amazon.com%2FAmazon-Video%2Fb%3Fie%3DUTF8%26node%3D2858778011%26tag%3Dnypost-20%26asc_refurl%3Dhttps%3A%2F%2Fnypost.com%2Farticle%2Fbest-online-classes%2F%26asc_source%3Dweb&amp;amp;xs=1&amp;amp;xtz=300&amp;amp;xuuid=5c8bed0d0a5d4de50a4df9d4d5e83122&amp;amp;xcust=xid%3Afr1705160635906bgb&amp;amp;xjsf=other_click__auxclick%20%5B2%5D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prime Video&lt;/a&gt; show.&lt;/p&gt;
&lt;p&gt;Click to jump to specific classes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-cooking-nutrition-wine-classes&#34;&gt;Best Online Cooking, Nutrition and Wine Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-fitness-classes&#34;&gt;Best Online Fitness Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-computer-coding-it-classes&#34;&gt;Best Online Computer, Coding and IT Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-classes-for-learning-a-new-language&#34;&gt;Best Online Classes for Learning a New Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-art-fashion-classes&#34;&gt;Best Online Art and Fashion Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-writing-classes&#34;&gt;Best Online Writing Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-music-dance-classes&#34;&gt;Best Online Music and Dance Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-psychology-classes&#34;&gt;Best Online Psychology Classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#best-online-real-estate-classes&#34;&gt;Best Online Real Estate Classes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-cooking-nutrition-wine-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-fitness-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1 id=&#34;best-online-computer-coding-and-it-classes&#34;&gt;&lt;a name=&#34;#best-online-computer-coding-it-classes&#34;&gt;Best Online Computer, Coding and IT Classes&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;“Every day, more and more cyber criminals try to trick us into sharing our personal information, providing access to our social media accounts, and ultimately taking our hard-earned savings from the bank,” &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader, PhD&lt;/strong&gt;&lt;/a&gt;, distinguished professor and founder of the Department of Data Science in the Ying Wu College of Computing and director of the Institute for Data Science at New Jersey Institute of Technology (NJIT), told. The Post.&lt;/p&gt;
&lt;p&gt;“With the recent explosion of chatbots such as OpenAI’s ChatGPT and Google’s Bard, the attacks have escalated to a point where it’s virtually impossible to differentiate between a malicious bot or a trusted colleague.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What to look for&lt;/strong&gt;: There’s a huge breadth of topics a student can master, from machine learning at artificial intelligence, to data science and data engineering, information security and technology, and cybersecurity.&lt;/p&gt;
&lt;p&gt;“Many online education platforms offer beginner, easy-to-follow classes that will appeal the masses since many require no prerequisites and their offerings are typically available online 24/7, when one’s schedule permits learning activities,” Bader shares. “Students should gain knowledge with some of the leading cloud platforms including Amazon’s AWS, Google Cloud, and Microsoft Azure, which are all quite capable for solving computing needs from novices to the most sophisticated and complex businesses alike.”&lt;/p&gt;
&lt;h2 id=&#34;google-cybersecurity-professional-certificate&#34;&gt;&lt;a href=&#34;https://www.coursera.org/professional-certificates/google-cybersecurity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cybersecurity Professional Certificate&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240112-nypost/Google-Cybersecurity-Professional-Certificate_hu_9cc830916f95e3a8.webp 400w,
               /blog/20240112-nypost/Google-Cybersecurity-Professional-Certificate_hu_c0238962b5b09329.webp 760w,
               /blog/20240112-nypost/Google-Cybersecurity-Professional-Certificate_hu_9acc16ec59f2ae60.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/Google-Cybersecurity-Professional-Certificate_hu_9cc830916f95e3a8.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: $49 a month
“A search for “Cybersecurity” on Coursera yields 241 courses to choose from, including some leading companies such as Google, Microsoft and IBM,” Dr. Bader lists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, the Google Cybersecurity Professional Certificate allows you to receive professional-level training from Google and a glossy employer-recognized certificate from Google. More granularly, this course helps better prepare you for careers as a cybersecurity analyst, security analyst and security operations center (SOC) analyst.&lt;/p&gt;
&lt;h2 id=&#34;university-of-michigan-programming-for-everybody-getting-started-with-python&#34;&gt;&lt;a href=&#34;https://www.edx.org/learn/python/the-university-of-michigan-programming-for-everybody-getting-started-with-python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Michigan Programming for Everybody (Getting Started with Python)&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240112-nypost/University-of-Michigan-Programming-for-Everybody-Getting-Started-with-Python_hu_a04244e0e61fce2.webp 400w,
               /blog/20240112-nypost/University-of-Michigan-Programming-for-Everybody-Getting-Started-with-Python_hu_873d3f0be92e3b11.webp 760w,
               /blog/20240112-nypost/University-of-Michigan-Programming-for-Everybody-Getting-Started-with-Python_hu_6614529d0c98ace7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/University-of-Michigan-Programming-for-Everybody-Getting-Started-with-Python_hu_a04244e0e61fce2.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Free, or $59 for a shareable certification upon completion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“A great first course in programming with no prerequisites is offered by University of Michigan: ‘Programming for Everybody’ and teaches the Python programming language, quickly becoming the lingua de franca of data science,” Bader notes.&lt;/p&gt;
&lt;p&gt;More, this class is self-paced and typically takes two to four hours per week over the course of seven weeks to complete it. “All of the homework is performed in a web browser, so learners can do all of the programming assignments from their smart phones or public computers.” he adds.&lt;/p&gt;
&lt;h2 id=&#34;introduction-to-machine-learning-on-aws&#34;&gt;&lt;a href=&#34;https://www.futurelearn.com/courses/introduction-to-machine-learning-on-aws&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Machine Learning on AWS&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240112-nypost/Introduction-to-Machine-Learning-on-AWS_hu_697ab6209b817348.webp 400w,
               /blog/20240112-nypost/Introduction-to-Machine-Learning-on-AWS_hu_75f560938d7904a9.webp 760w,
               /blog/20240112-nypost/Introduction-to-Machine-Learning-on-AWS_hu_f53708077f9d7f11.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/Introduction-to-Machine-Learning-on-AWS_hu_697ab6209b817348.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: $89&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“FutureLearn offers 299 short courses in IT and computer science,” Bader explains. “Typically, a learner would enroll in an annual unlimited subscription to pay for the access to the courseware.”&lt;/p&gt;
&lt;p&gt;FutureLearn also provides premium content – such as Amazon Web Services (AWS)’s Introduction to Machine Learning on AWS for $89. “The course requires four hours a week over two weeks and provides students the knowledge and tools to navigate AWS and  begin using machine learning services,” Bader adds.&lt;/p&gt;
&lt;h2 id=&#34;university-of-maryland-cybersecurity-for-everyone&#34;&gt;&lt;a href=&#34;https://www.coursera.org/learn/cybersecurity-for-everyone&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Maryland Cybersecurity for Everyone&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240112-nypost/University-of-Maryland-Cybersecurity-for-Everyone_hu_a5192b325791c222.webp 400w,
               /blog/20240112-nypost/University-of-Maryland-Cybersecurity-for-Everyone_hu_bae072a7284731af.webp 760w,
               /blog/20240112-nypost/University-of-Maryland-Cybersecurity-for-Everyone_hu_831ebf9cbc761312.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/University-of-Maryland-Cybersecurity-for-Everyone_hu_a5192b325791c222.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: $49 a month&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“For a beginner, I’d recommend a free course on cyberattacks and computer and network security called ‘Cybersecurity for Everyone’ taught by a University of Maryland College Park expert who has served on the front lines with the NSA,” Bader says. “This course allows a flexible schedule and takes about seven hours a week for three weeks.”&lt;/p&gt;
&lt;p&gt;With this course, you’ll explore topics like global communications architecture, threat actors and motivations and the hacking process, among other learnings.&lt;/p&gt;
&lt;h2 id=&#34;intro-to-ux-designing-with-a-user-centered-approach&#34;&gt;&lt;a href=&#34;https://www.skillshare.com/en/tedx?via=search-layout-grid&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intro to UX: Designing with a User-Centered Approach&lt;/a&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20240112-nypost/Intro-to-UX-Designing-with-a-User-Centered-Approach_hu_c01a789d077eadce.webp 400w,
               /blog/20240112-nypost/Intro-to-UX-Designing-with-a-User-Centered-Approach_hu_e1f405f16fff3881.webp 760w,
               /blog/20240112-nypost/Intro-to-UX-Designing-with-a-User-Centered-Approach_hu_2d4a655af79a4b22.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20240112-nypost/Intro-to-UX-Designing-with-a-User-Centered-Approach_hu_c01a789d077eadce.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: $32 a month&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“Skillshare offers thousands of video classes on a variety of topics to the online learner,” Bader shares. “Most classes run from one to three hours, and a highly recommended class on user-interface design is offered by Cinthya Mohr ‘Intro to UX: Designing with a User-Centered Approach’ running just 44 minutes.”&lt;/p&gt;
&lt;p&gt;Many other UI/UX design courses are available teaching online learners how to use popular Adobe and Figma software as well. “However, Skillshare does not offer coding and cybersecurity videos; so if this is your focus, you may want to consider other platforms,” he adds.&lt;/p&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-classes-for-learning-a-new-language&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-art-fashion-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-writing-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-music-dance-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-psychology-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;a name=&#34;#best-online-real-estate-classes&#34;&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://nypost.com/article/best-online-classes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nypost.com/article/best-online-classes/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI Will Level the Academic Playing Field, Eventually</title>
      <link>http://localhost:1313/blog/20231219-volt/</link>
      <pubDate>Tue, 19 Dec 2023 16:16:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/20231219-volt/</guid>
      <description>&lt;p&gt;&lt;em&gt;By: Chris Kudialis&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231219-volt/Will-AI-level-the-academic-playing-field-1120_hu_7f9f532258b60111.webp 400w,
               /blog/20231219-volt/Will-AI-level-the-academic-playing-field-1120_hu_56034a3eefcdd3c4.webp 760w,
               /blog/20231219-volt/Will-AI-level-the-academic-playing-field-1120_hu_1cad5128c5f44ce3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231219-volt/Will-AI-level-the-academic-playing-field-1120_hu_7f9f532258b60111.webp&#34;
               width=&#34;760&#34;
               height=&#34;422&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Imagine a university lecture hall of 200 students in which a single professor gives every student a personalized lesson plan based entirely on individual strengths and learning style: a unique syllabus tailored to each student that offers the absolute best possible chance for them to succeed in the class.&lt;/p&gt;
&lt;p&gt;Now imagine that instead of a small handful of major-milestone exams —  like the midterm, a few homework assignments, a term project and the final exam — the class offers &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S2666920X22000303&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continuous assessments&lt;/a&gt; of students’ performance throughout the semester, complete with &lt;a href=&#34;https://hbsp.harvard.edu/inspiring-minds/ai-as-feedback-generator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;personalized feedback&lt;/a&gt; based on their learning strengths.&lt;/p&gt;
&lt;p&gt;Higher-ed has dreamed of and slowly crawled toward such an ideal for the past 20 years — mostly in the name of &lt;a href=&#34;https://www.washington.edu/doit/self-examination-how-inclusive-your-campus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;accessibility&lt;/a&gt; and &lt;a href=&#34;https://www.utep.edu/extendeduniversity/utepconnect/blog/june-2023/importance-of-dei-in-education.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;social equity&lt;/a&gt;. But schools could only do so much with finite budgets, manpower and hours in a day.&lt;/p&gt;
&lt;p&gt;Yet artificial intelligence, particularly the AI boom of the past 12 months, has &lt;a href=&#34;https://voltedu.com/enrollment/the-promise-and-peril-of-chatgpt-and-ai-platforms-in-higher-ed/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;changed everything&lt;/a&gt;. Interviewed scholars at the cutting edge of this fast-growing technology say the coming years will make the personalized student learning experience a reality, speeding up a decades-long process in just months.&lt;/p&gt;
&lt;h2 id=&#34;ai-already-permeates-everything&#34;&gt;AI Already “Permeates Everything” &lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; taught at Georgia Tech’s School of Computational Science and Engineering before moving to the New Jersey Institute of Technology, where he founded the school’s department of data science in 2019. Bader also oversees NJIT’s data science programs for its bachelor’s, master’s and doctoral degrees.&lt;/p&gt;
&lt;p&gt;“We teach AI and use it within our curriculum,” he said. “AI permeates everything we do here.”&lt;/p&gt;
&lt;p&gt;Even in some of his department’s most analytical and technical courses, Bader says students take vastly different approaches to interpreting class material.&lt;/p&gt;
&lt;p&gt;Before AI, most NJIT computer science professors would teach cut-and-dry, “straightforward statements” for writing algorithms and coding. The new tech opened the door wider for more visually inclined learners, hands-on learners and even students with disabilities to understand class material better.&lt;/p&gt;
&lt;p&gt;“Some students are very good essentially at having a manual regurgitated to them and learning it in that style,” Bader explained. “But a visual learner may want to see that information flow-charted and see how things connect together, and we can more easily provide that with AI.”&lt;/p&gt;
&lt;p&gt;“Additionally, a student who is more hands-on, an active learner, may want to type in the lesson materials and run them during the class to figure out how they work, rather than just learning from statements on the board,” he added. “Instead of doing separate activities in the classroom, each student can now have lectures tailored toward their best way of learning how to program.”&lt;/p&gt;
&lt;p&gt;As an alternative to traditional lesson plans made exclusively of standardized tests, courses, textbooks and materials, Bader said his department took “bold steps” into AI to avoid creating students that are all “puppies or clones of each other.” Large language models, such as ChatGPT, and generative AI, such as DALL-E that creates fanciful images from descriptive prompts, have already offered promising returns.&lt;/p&gt;
&lt;h2 id=&#34;ai-facilitates-out-of-class-assistance&#34;&gt;AI Facilitates Out-of-Class Assistance&lt;/h2&gt;
&lt;p&gt;Although AI in individualized lesson planning is still in a nascent stage, the technology already plays a huge role in tutoring and live help, Bader said. At NJIT, students in certain programs can launch a digital teacher’s assistant to ask questions and have “meaningful conversations” with the click of a button.&lt;/p&gt;
&lt;p&gt;“They don’t have to commute to campus, find office hours that work, sit down, and schedule a session with another person,” Bader said. “As they’re doing homework now, they ask a question and get an immediate response.”&lt;/p&gt;
&lt;p&gt;At Emory University, computer science chair and professor Vaidy Sunderam has similarly helped lead the rollout of AI for personalized student learning during office hours. &lt;a href=&#34;https://github.com/emora-chat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Emora Chat&lt;/a&gt;,  a university-wide AI bot, combines features of contracted mainstream chatbots with a proprietary twist.&lt;/p&gt;
&lt;p&gt;Like Bader, Sunderam says the idea of personalized syllabi, coursework and curricula driven by AI is still a work in progress. But it’s already been a godsend for tutoring and office hours, freeing up TAs and professors to deal with more advanced student questions and needs.&lt;/p&gt;
&lt;p&gt;“That ability to use AI TAs is phenomenal and revolutionizes the classroom experience,” he said.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/E-jit5cEKEg?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2 id=&#34;students-offer-mixed-reviews&#34;&gt;Students Offer Mixed Reviews&lt;/h2&gt;
&lt;p&gt;Some university students who went on the record with Volt said they’ve enjoyed the early returns of AI in the classroom. But most still hadn’t seen their schools implement any significant forms of the new technology.&lt;/p&gt;
&lt;p&gt;Christina Forbes, a sophomore &lt;a href=&#34;https://sse.tulane.edu/cs/undergraduate-studies-computer-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;coordinate major in computer science&lt;/a&gt; at Tulane, said staff and faculty within the program “heavily encourage” students to use AI as a tool. Although Tulane doesn’t yet have its own school-based AI teaching assistant in place, Forbes said she and her classmates regularly use ChatGPT for help with homework, asking the chatbot to diagnose any problems with the code they’ve written.&lt;/p&gt;
&lt;p&gt;“It’s like a tutor that you can ask hundreds of questions without annoying it,” she said.&lt;/p&gt;
&lt;p&gt;Aaron West, a senior forensic science major at &lt;a href=&#34;https://www.sjsu.edu/justicestudies/degrees/undergrad/major-fs.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;San Jose State University&lt;/a&gt;, said his professors have not used AI “in any teaching capacity,” in part because they don’t yet feel any pressing need to implement the technology. West, who was already in his third of four years when the &lt;a href=&#34;https://www.forbes.com/sites/bernardmarr/2023/05/19/a-short-history-of-chatgpt-how-we-got-to-where-we-are-today/?sh=183cfc56674f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT-led AI boom&lt;/a&gt; hit late last fall, said the idea of a personalized curriculum based on his learning strengths would have appealed more to him if he’d begun college with that option.&lt;/p&gt;
&lt;p&gt;After two years of learning “the traditional way,” a shift to AI-based learning would be “interesting, but probably unnecessary,” West opined. His personal experience with AI includes using a grammar-correction website Grammarly to proofread the occasional essays. But with generative AI, West said he only asked ChatGPT for jokes and restaurant ideas when it first became popular.&lt;/p&gt;
&lt;p&gt;“I’m not a huge fan of AI but think everybody will need to be familiar with certain aspects of it at some point,” West said. “I don’t use it, and I haven’t heard of any classmates using it.”&lt;/p&gt;
&lt;h2 id=&#34;ai-driven-learning-is-still-growing&#34;&gt;AI-Driven Learning Is Still Growing&lt;/h2&gt;
&lt;p&gt;So when, exactly, can the average American university student — not just one in a top-ranked computer science program — expect to see AI-driven personalized learning in action? Interviewed scholars said it could take three to five years for most students to notice &lt;a href=&#34;https://voltedu.com/student-experience/dont-assume-your-students-are-ai-savvy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;any differences&lt;/a&gt; at all. Some schools, leery of the tech’s reputation as a &lt;a href=&#34;https://thehill.com/opinion/education/4162766-ai-cheating-has-hopelessly-irreparably-corrupted-us-higher-education/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tool for cheating&lt;/a&gt;, may wait for longer or hold off implementing AI tools entirely, to the extent possible.&lt;/p&gt;
&lt;p&gt;Bader believes the “authoritarian” model of a teacher having a superior role will gradually take a backseat to AI diagnosing individual students’ learning styles. When AI fills personalized models for each student, those students become in control of their education, he said.&lt;/p&gt;
&lt;p&gt;“We’ll be able to customize the learning experience by changing how curriculums are put together,” said Bader. “Right now, we’re just at the start of this AI revolution.&lt;/p&gt;
&lt;p&gt;“We’re at the baby phase of just crawling,” he added. “But we’re not yet a toddler walking, let alone a child running.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://voltedu.com/student-experience/ai-teaching-will-level-the-academic-playing-field-eventually/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://voltedu.com/student-experience/ai-teaching-will-level-the-academic-playing-field-eventually/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sigma Xi Distinguished Lecturers, 2023–2024</title>
      <link>http://localhost:1313/blog/20231215-sigmaxi/</link>
      <pubDate>Fri, 15 Dec 2023 12:22:52 -0500</pubDate>
      <guid>http://localhost:1313/blog/20231215-sigmaxi/</guid>
      <description>&lt;p&gt;For the 86th year, Sigma Xi presents its panel of Distinguished Lecturers as an opportunity for chapters to host visits from outstanding individuals who are at the leading edge of science.  These vistors communicate their insights and excitement on a broad range of topics.&lt;/p&gt;
&lt;p&gt;The Distinguished Lecturers are available form July 1, 2024, to June 30, 2025. Each speaker has consented to a modest honorariuam together with full payment of travel costs and subsistence.&lt;/p&gt;
&lt;p&gt;Local chapters may apply for subsidies to support expenses related to hosting a Distinguished Lecturer. Applications must be submitted online by March 1, 2024, for funds to be available the next fiscal year.&lt;/p&gt;
&lt;p&gt;Additional support for the program comes from the American Meteorological Society. Lecturer biographies, contact information, and additional details can be found online at sigmaxi.org/lectureships or by sending an email to &lt;a href=&#34;mailto:lectureships@sigmaxi.org&#34;&gt;lectureships@sigmaxi.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marc Imhoff, Chair&lt;br&gt;
&lt;em&gt;Committee on Lectureships&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231215-sigmaxi/bader3_hu_3c66baab4991a37b.webp 400w,
               /blog/20231215-sigmaxi/bader3_hu_1bd4005feb10ff40.webp 760w,
               /blog/20231215-sigmaxi/bader3_hu_c8976044627c4822.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231215-sigmaxi/bader3_hu_3c66baab4991a37b.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Distinguished Professor and Director of the Institute for Data Science, New Jersey Institute of Technology&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Solving Global Grand Challenges with High Performance Data Analytics (P, G, S)&lt;/li&gt;
&lt;li&gt;Predictive Analysis from Massive Knowledge Graphs (P, G, S)&lt;/li&gt;
&lt;li&gt;Interactive Data Science at Scale (P, G, S)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;David A. Bader is a Distinguished Professor and founder of the Department of Data Science and inaugural Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;Dr. Bader is a Fellow of the IEEE, ACM, AAAS, and SIAM, and a recipient of the IEEE Sidney Fernbach Award. He advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE). Bader is a leading expert in solving global grand challenges in science, engineering, computing, and data science. His interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics, and he has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and IEEE/ACM SC.&lt;/p&gt;
&lt;p&gt;Dr. Bader is Editor-in-Chief of the ACM Transactions on Parallel Computing, and previously served as Editor-in-Chief of the IEEE Transactions on Parallel and Distributed Systems. ROI-NJ recognized Bader as a technology influencer on its 2021 inaugural and 2022 lists. In 2012, Bader was the inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award.&lt;/p&gt;
&lt;p&gt;In 2014, Bader received the Outstanding Senior Faculty Research Award from Georgia Tech. Bader has also served as Director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor and Director of an NVIDIA GPU Center of Excellence. In 1998, Bader built the first Linux supercomputer that led to a high-performance computing (HPC) revolution, and Hyperion Research estimates that the total economic value of Linux supercomputing pioneered by Bader has been over $100 trillion over the past 25 years.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sigmaxi.org/programs/lectureships/2023-2024-lecturers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.sigmaxi.org/programs/lectureships/2023-2024-lecturers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cybersecurity Challenges in the Age of Generative AI</title>
      <link>http://localhost:1313/blog/20231120-cxotech/</link>
      <pubDate>Mon, 20 Nov 2023 10:34:16 -0500</pubDate>
      <guid>http://localhost:1313/blog/20231120-cxotech/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231120-cxotech/David-Bader_New-Jersey-Institute-of-Technology_hu_6a784588970328b8.webp 400w,
               /blog/20231120-cxotech/David-Bader_New-Jersey-Institute-of-Technology_hu_8874b45ca7910a64.webp 760w,
               /blog/20231120-cxotech/David-Bader_New-Jersey-Institute-of-Technology_hu_2114eb5c82de4b85.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231120-cxotech/David-Bader_New-Jersey-Institute-of-Technology_hu_6a784588970328b8.webp&#34;
               width=&#34;760&#34;
               height=&#34;423&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Cybersecurity professionals will not only have to discover malicious events at the time of occurrence, but also proactively implement preventative measures before an attack. For these professionals, the significant challenge will be protecting against new behaviors and methods that they are not yet familiar with.&lt;/p&gt;
&lt;h2 id=&#34;evolving-attack-vectors&#34;&gt;Evolving Attack Vectors&lt;/h2&gt;
&lt;p&gt;Bad actors can use AI to enhance dynamic and sophisticated cyberattack methods, including advanced persistent threats (APT), deepfake attacks, DDoS attacks, phishing, and targeted malware. As AI continues to advance, existing attack vectors will be deployed at exponentially increasing speed and scale.&lt;/p&gt;
&lt;p&gt;For instance, phishing is a prevalent and well-known way for attackers to cast a wide net and reach thousands of people at once. Over 500 million phishing attacks were reported in 2022, resulting in a total loss of over $52 million in the U.S. alone. Spam filters and broader awareness of these scams have traditionally helped many avoid the generic and poorly written requests, but with AI, these emails will no longer be read in broken English when sent en masse.&lt;/p&gt;
&lt;p&gt;An advanced form of phishing, spear phishing attacks target specific individuals or groups to obtain sensitive information. Typically, attackers have to construct phishing attacks against their victims carefully, customizing their scams with relevant information to induce targets to reveal confidential information such as user credentials. Due to the time needed to prepare, cybercriminals have not been able to traditionally operate spear phishing attacks on a large scale.&lt;/p&gt;
&lt;p&gt;However, GenAI now enables attackers to organize and deploy these highly targeted attacks at exponentially faster speed. Attackers can construct a massive number of phishing attacks with a click of a button, directing the AI to write customized emails for each victim with highly sophisticated and fluent impersonations of our bosses, co-workers, and customers. Bad actors who have collected personal information can use AI technology to parse and organize cultivated information in no time, then customize individual attacks to unsuspecting victims. For example, a scammer knows a new employee has joined a company and will send them an email claiming to be an HR representative needing to re-verify information, tricking them into handing over their Social Security number.&lt;/p&gt;
&lt;p&gt;Social engineering attacks will also be further devastating due to AI’s increased capabilities of camouflage. Attackers can use generative AI to create real-time conversations, mimic voices, alter videos, and generate realistic images that are virtually impossible to differentiate from reality. Deepfake technology’s chilling ability to impersonate real people can deceive even the savviest cybersecurity professionals.&lt;/p&gt;
&lt;p&gt;In an example of social engineering attacks, victims can get lured into connecting with online communities of “friends,” only to realize later that they’ve been sharing highly sensitive information (such as a company’s confidential information, product plans, and customer records) with malicious actors.&lt;/p&gt;
&lt;p&gt;In malware-focused attacks, AI can also be used to find vulnerabilities in an organization’s IT infrastructure and launch APT or DDoS attacks quickly targeting their Achilles heel and remaining undetected for longer periods of time. Rather than using a single instance of malware, generative AI is capable of constructing tailored attacks against specific targets at an unprecedented rate. This is akin to going from testing a single key on thousands of locked cars in a parking lot to, thanks to AI, now being able to cut a working key for every car in the lot.&lt;/p&gt;
&lt;h2 id=&#34;adapting-cybersecurity-defense&#34;&gt;Adapting Cybersecurity Defense&lt;/h2&gt;
&lt;p&gt;From a risk management perspective, it’s important to recognize that AI creates novel attack vectors for existing threats as well as the potential for new threats. Adaptive responses and new controls will be the new norm for evaluating an organization’s cybersecurity risk posture. However, do not overlook the importance of having robust and foundational cybersecurity protections effective against traditional risks.&lt;/p&gt;
&lt;p&gt;The first step should be to understand the organization’s people and infrastructure, as well as monitor it for anomalies that could be an indication of an attack. Many of the same cybersecurity protections for traditional threats will also be important to employ against AI-generated threats, so keep up a strong security posture. Regular training among employees to recognize these new types of threats will also go a long way to making the organization safer against these attacks.&lt;/p&gt;
&lt;p&gt;Additionally, harnessing AI and automation to strengthen defense mechanisms will be part of adapting to the evolving landscape of AI-related attacks. According to IBM’s 2023 Cost of Data Breach Report, organizations with extensive use of both AI and automation experienced a data breach lifecycle that was 108 days shorter compared to studied organizations that have not deployed these technologies (214 days versus 322 days).When organizations discovered breaches themselves, they experienced nearly $1 million less in breach costs than breaches disclosed by attackers.&lt;/p&gt;
&lt;p&gt;As the capabilities of AI continue to advance, both its potential for positive transformation by cybersecurity professionals and its potential for misuse by malicious actors become increasingly evident. Whether an attack is generated by AI or not, the best CISOs and risk managers will follow the timeless practice of applying the latest security patches and minimizing potential risks for systems and data to be compromised by hackers.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt;, a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cxotechmagazine.com/cybersecurity-challenges-in-the-age-of-generative-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cxotechmagazine.com/cybersecurity-challenges-in-the-age-of-generative-ai/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A detailed, end-to-end assessment of a quantum algorithm for portfolio optimization, released by Goldman Sachs and AWS</title>
      <link>http://localhost:1313/blog/20231113-aws/</link>
      <pubDate>Mon, 13 Nov 2023 13:35:52 -0700</pubDate>
      <guid>http://localhost:1313/blog/20231113-aws/</guid>
      <description>&lt;p&gt;To date, the financial services industry has been a pioneer in quantum technology, owing to its plethora of computationally hard use cases and the potential for significant impact from quantum solutions. Daily operations at financial firms such as banks, insurance companies, and hedge funds require solving well-defined computational problems on underlying data that is constantly changing. If quantum computers can perform the same calculations faster or with better accuracy than existing solutions, these customers stand to reap significant benefit.&lt;/p&gt;
&lt;p&gt;In this post we’ll walk you through some key takeaways from a body of work by scientists from Goldman Sachs and AWS that was &lt;a href=&#34;https://doi.org/10.1103/PRXQuantum.4.040325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;published today in the journal PRX Quantum&lt;/a&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;A commonly studied use case in quantitative finance is the task of portfolio optimization — consequently, customers are eager to learn what impacts quantum computing can have on this task, if any. Several distinct quantum algorithms have been proposed for portfolio optimization, each of which leverage quantum effects to provide a theoretical speedup over comparable classical algorithms.&lt;/p&gt;
&lt;p&gt;However, it has been difficult to assess whether these theoretical speedups can be realized in practice. Such an assessment requires an &lt;a href=&#34;https://aws.amazon.com/blogs/quantum-computing/constructing-end-to-end-quantum-algorithm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;end-to-end&lt;/em&gt; resource estimation of the quantum algorithm&lt;/a&gt; — that is, a calculation of the total number of qubits and quantum gates (or other relevant metrics) required to solve the actual problem of interest, without sweeping any costs or caveats under the rug.&lt;/p&gt;
&lt;p&gt;End-to-end assessments are challenging because certain elements of the quantum algorithm are heuristic, and there are caveats related to the way the quantum computer accesses the data. Thus, it can be difficult to make apples-to-apples comparisons with existing methods&lt;/p&gt;
&lt;p&gt;Despite the challenges, scientists from Goldman Sachs and AWS took on the task of performing an end-to-end resource estimate for a leading quantum approach to portfolio optimization — the quantum interior point method (QIPM), which we will discuss in this post. You can read the full paper in the journal PRX Quantum, but let’s walk through some of the main takeaways of that work &lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;how-do-we-determine-if-a-quantum-algorithm-is-practical&#34;&gt;How do we determine if a quantum algorithm is practical?&lt;/h2&gt;
&lt;p&gt;For quantum computers to provide value over classical algorithms, we need to develop practical, end-to-end quantum algorithms, for which we identify the following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The quantum algorithm produces a &lt;strong&gt;classical output that allows for benchmarking against classical methods&lt;/strong&gt;.&lt;/em&gt; This requirement rules out some quantum “algorithms” that efficiently produce quantum states as output, but are much less efficient when forced to turn that quantum state into an answer to the original (classical) computational problem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The quantum algorithm relies on a &lt;strong&gt;reasonable input model&lt;/strong&gt;.&lt;/em&gt; In particular, it does not utilize any unspecified black-box subroutines (often called “oracles”) for which the costs are not counted, nor does it assume fast data access without accounting for the &lt;a href=&#34;https://aws.amazon.com/blogs/quantum-computing/goldman-sachs-and-aws-examine-efficient-ways-to-load-data-into-quantum-computers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;true cost of quantum access to the classical data&lt;/a&gt;. For example, several quantum algorithms for Machine Learning were thought to offer exponential advantages over classical methods until it was pointed out that this was primarily an artifact of unreasonable assumptions about the input model &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;The quantum algorithm has a &lt;strong&gt;plausible case for asymptotic quantum speedup&lt;/strong&gt; —&lt;/em&gt; that is, for sufficiently large instance sizes, the quantum algorithm requires substantially fewer elementary operations than the classical algorithm. Since classical computers can perform elementary operations at a much faster rate than quantum computers, without an asymptotic speedup, the quantum computer will always lose.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;For classically challenging problem sizes, the &lt;strong&gt;quantum algorithm can solve the same end-to-end problem in a reasonable run-time&lt;/strong&gt;.&lt;/em&gt; Asymptotic speedups are of no value if the crossover point where quantum computers outperform classical computers occurs at instance sizes that are too large to be useful to customers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;portfolio-optimization&#34;&gt;Portfolio optimization&lt;/h2&gt;
&lt;p&gt;Portfolio optimization is a well-defined computational task that appears in many fields, but its utility is particularly clear for financial operations. Suppose you have a fixed budget to invest in the stock market. You’d like your investments to provide predictable growth; in particular, you’d like to construct a portfolio that maximizes the expected return of the portfolio, while minimizing the risk, that is, the amount by which the return might deviate from its expectation.&lt;/p&gt;
&lt;h2 id=&#34;the-model&#34;&gt;The model&lt;/h2&gt;
&lt;p&gt;Part of the problem is thus determining which stocks are expected to perform well in a given timeframe, based on historical data — we assume this has already been done. If there are &lt;em&gt;n&lt;/em&gt; stocks to invest in, we are provided as input with a length-&lt;em&gt;n&lt;/em&gt; vector containing the expected return for each of the stocks, as well as an &lt;em&gt;n x n&lt;/em&gt; matrix 𝛴 containing the historical volatilities of each stock and the covariance of each pair of stocks — the performance of different stocks is correlated, and the optimal portfolio will exploit these correlations to minimize portfolio risk.&lt;/p&gt;
&lt;p&gt;Let &lt;em&gt;w&lt;/em&gt; be the length-&lt;em&gt;n&lt;/em&gt; vector representing the fraction of our budget we allocate to each stock. In general, entries of &lt;em&gt;w&lt;/em&gt; can be negative, which represents a short sale. Here, though, we will require that the portfolio contain only “long” assets — i.e., no short sales: &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt; ≥ 0&lt;/em&gt;. The portfolio optimization problem is then formulated as a convex optimization problem:&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.50.28_hu_ccc9281be812bd1d.webp 400w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.50.28_hu_1bd065ee67056492.webp 760w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.50.28_hu_410de18709b6f063.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/CleanShot-2023-11-08-at-08.50.28_hu_ccc9281be812bd1d.webp&#34;
               width=&#34;272&#34;
               height=&#34;148&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The parameter &lt;em&gt;q&lt;/em&gt; is called the “risk-aversion parameter”, and it is set by the user according to how much risk they are willing to take on to gain additional return.&lt;/p&gt;
&lt;p&gt;The first constraint is a total budget constraint. We’ve normalized w so that the total budget available is just 1, but without normalizing this sum would equal the total amount of money available to invest.&lt;/p&gt;
&lt;p&gt;The second constraint enforces the “long-only” condition above; if we allow short selling, this can be relaxed. Additional constraints, such as minimum or maximum investments in certain stock sectors, cardinality constraints (i.e., the maximum allowed number of assets), or transaction costs, can also be incorporated into this framework (see &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;The set of portfolios that obey all the constraints is known as the &lt;em&gt;feasible&lt;/em&gt; region. The feasible region can be divided into the &lt;em&gt;boundary&lt;/em&gt;, where at least one of the inequality constraints holds as an equality (e.g. &lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt; = 0&lt;/em&gt;), and the &lt;em&gt;interior&lt;/em&gt;, where all inequality constraints are strict (&lt;em&gt;w&lt;sub&gt;i&lt;/sub&gt; &amp;gt; 0&lt;/em&gt; for all &lt;em&gt;i&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;One widely used classical approach for solving portfolio optimization is to map the convex optimization problem above to a type of optimization problem known as a &lt;em&gt;second-order cone program&lt;/em&gt; (SOCP), and then solve it with an interior point method (IPM). The basic idea of an IPM is to start at some portfolio in the interior of the feasible region and iteratively update it in a way that is guaranteed to eventually approach the &lt;em&gt;optimal&lt;/em&gt; portfolio, which lies on the boundary of the feasible region, as depicted in Figure 1.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;http://localhost:1313/blog/20231113-aws/IPMnoaxesCompressed.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Figure 1: A toy example depicting how an Interior Point Method (IPM) solves portfolio optimization. The hexagonal region represents the feasible set of portfolios. The IPM generates a sequence of points (known as the central path) that follows a path from the interior of the region to the optimal point, which lies on the boundary. The color represents the value of the objective function, including barrier functions that implement constraints (darker is better). The black dots represent the central path, which is defined to be the optimal point of the objective function, including the barrier. As the barrier functions are relaxed, the objective function approaches the true objective function, and the central path approaches the optimal solution to the problem on the boundary of the feasible space.&lt;/p&gt;
&lt;p&gt;Performing a single iteration of the IPM requires solving a linear system of equations of size &lt;em&gt;L x L&lt;/em&gt;, where (in our formulation), &lt;em&gt;L ≈ 14n&lt;/em&gt; and &lt;em&gt;n&lt;/em&gt; is the number of stocks in which we could invest. The output of the IPM is a portfolio &lt;em&gt;w&lt;/em&gt; that obeys all the constraints and for which the objective function is within 𝜖 of the optimal value. Many flavors of IPM exist, and developing better classical IPMs is an active area of research.&lt;/p&gt;
&lt;p&gt;Here we focus on the IPMs that are most comparable with quantum algorithms. These classical IPMs solve the PO problem in time scaling as &lt;em&gt;n&lt;sup&gt;3.5&lt;/sup&gt; log(1/𝜖)&lt;/em&gt;. This runtime is efficient in theory (i.e., it scales polynomially with &lt;em&gt;n&lt;/em&gt;) and also in practice (IPMs are a leading method underlying many open-source and commercial solvers). Using IPM-based, off-the-shelf solvers, one can solve the PO problem on portfolios with thousands of stocks reasonably quickly on standard compute resources. However, the &lt;em&gt;n&lt;sup&gt;3.5&lt;/sup&gt;&lt;/em&gt; scaling is steep, and pushing the number of assets into the tens of thousands presents an opportunity for alternative solutions (such as quantum computing) to provide value. In the next section, we explore a proposal to do exactly that.&lt;/p&gt;
&lt;h2 id=&#34;quantum-interior-point-methods&#34;&gt;Quantum interior point methods&lt;/h2&gt;
&lt;p&gt;Quantum interior point methods (QIPMs) aim to accelerate classical IPMs by replacing certain algorithmic steps with quantum subroutines. The fact that IPMs are efficient both in theory and in practice gives hope that, if indeed these subroutines can be completed more quickly, QIPMs could satisfy the four criteria of a practical quantum algorithm and deliver practical utility in customer use cases.&lt;/p&gt;
&lt;p&gt;QIPMs preserve the main loop of the IPM (which iteratively generates a sequence of better and better portfolios), and replaces a subroutine in the IPM that solves linear systems of equations with a quantum algorithm for completing this task. To do this, QIPMs leverage three quantum ingredients — Quantum Random Access Memory, quantum linear system solvers, and quantum state tomography — in each iteration of the main loop. These ingredients perform the following roles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantum Random Access Memory (QRAM)&lt;/strong&gt; allows the quantum algorithm to access the data that defines the instance, namely u and 𝛴, in an efficient and quantum mechanical way. Theoretical analyses of QRAM-based algorithms often merely report the number of queries to the QRAM data structure and assume the query time is very fast. To account for criterion 2 (reasonable input model), a full end-to-end analysis must go further and compute the exact resources required to perform the QRAM queries. Specifically, our resource analysis for the QIPM utilizes the resource estimates for &lt;a href=&#34;https://aws.amazon.com/blogs/quantum-computing/goldman-sachs-and-aws-examine-efficient-ways-to-load-data-into-quantum-computers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementing a block-encoding of an arbitrary matrix&lt;/a&gt; — a primitive that is in some sense equivalent to QRAM — that we developed in separate, parallel work &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantum linear system solvers (QLSSs)&lt;/strong&gt;, as their name suggests, are quantum subroutines for solving linear systems of the form &lt;em&gt;Ax = b&lt;/em&gt;, where &lt;em&gt;A&lt;/em&gt; is an invertible &lt;em&gt;L x L&lt;/em&gt; matrix, and &lt;em&gt;x&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; are length-&lt;em&gt;L&lt;/em&gt; vectors. In the PO problem, &lt;em&gt;L ≈ 14n&lt;/em&gt;. The catch is that, unlike classical linear system solvers, QLSSs do not output the vector &lt;em&gt;x&lt;/em&gt;; rather, they output a quantum state &lt;em&gt;|x⟩&lt;/em&gt; whose amplitudes are proportional to the entries of &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Quantum state tomography&lt;/strong&gt; is precisely the subroutine that turns the quantum state &lt;em&gt;|x⟩&lt;/em&gt; into a classical description of the vector &lt;em&gt;x&lt;/em&gt;, which is then used to iterate the IPM. In essence, tomography is performed just by preparing many copies of &lt;em&gt;|x⟩&lt;/em&gt;, measuring them, and gathering statistics on the frequency of each measurement outcome to estimate the entries of &lt;em&gt;x&lt;/em&gt;. The need for tomography is related to criterion 1, above; the QLSS is efficient, but it outputs a quantum state rather than classical data, and this quantum state is not the object that is immediately useful to the rest of the algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full end-to-end analysis takes care to account for errors incurred by each of these primitive ingredients. The largest of these errors is incurred by tomography, due to the statistical noise of estimating an entry of &lt;em&gt;x&lt;/em&gt; from measurement outcomes of &lt;em&gt;|x⟩&lt;/em&gt;. An additional caveat is that these small errors prevent the QIPM from &lt;em&gt;exactly&lt;/em&gt; following the same path as the classical IPM, and they also cause the QIPM to leave the feasible region. Luckily, the IPM is designed to be self-correcting, in the sense that it partially fixes any errors it makes in future iterations.&lt;/p&gt;
&lt;p&gt;Ultimately, through this effect or other workarounds we explore in the full paper, the QIPM will solve the same problem as the classical IPM: it returns an ϵ-optimal solution in an amount of time that essentially scales as:&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.13_hu_c43f19cacf678da5.webp 400w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.13_hu_696a2dbef87479c9.webp 760w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.13_hu_186aea02a1796ca7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/CleanShot-2023-11-08-at-08.52.13_hu_c43f19cacf678da5.webp&#34;
               width=&#34;223&#34;
               height=&#34;58&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The appearance of new parameters &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;ξ&lt;/em&gt; represent additional technical complications, and need to be unpacked. The parameter &lt;em&gt;ⲕF&lt;/em&gt; is the “Frobenius condition number” — it represents the difficulty of inverting the relevant matrices with the QLSS. The parameter &lt;em&gt;ξ&lt;/em&gt; is the maximum error on quantum state tomography that does not break the algorithm (i.e., how poorly we can estimate the state &lt;em&gt;|x⟩&lt;/em&gt;) — small errors are okay as long as subsequent iterations of the IPM find portfolios that are close enough to those of an ideal IPM.&lt;/p&gt;
&lt;p&gt;These are &lt;em&gt;instance-specific&lt;/em&gt; parameters, meaning that two different instances of the PO problem with stocks will have different values for &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;ξ&lt;/em&gt;, which makes the magnitude and scaling of these parameters difficult to study. This is an issue, because it is clear that the effectiveness of the algorithm critically hinges on how large they are! If &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;1/ξ&lt;/em&gt; are small and constant, one is tempted to declare that the quantum algorithm has a large &lt;em&gt;n&lt;sup&gt;3.5&lt;/sup&gt; → n&lt;sup&gt;1.5&lt;/sup&gt;&lt;/em&gt; speedup over the classical IPM. However, it is expected that these parameters will also have an n dependence, cutting into this speedup. Work by Kerenidis, Prakash, and Szilágyi &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; studied one version of the QIPM for portfolio optimization and gave some preliminary numerical evidence that the QIPM still offers a speedup after accounting for the dependence on &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;ξ&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The expression above omits some additional logarithmic factors: we computed the full expression of the runtime to be proportional to:&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.54_hu_a6ac45fd225e2630.webp 400w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.54_hu_f97b1b5c5c2cfce2.webp 760w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.52.54_hu_11b6ed83f2886556.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/CleanShot-2023-11-08-at-08.52.54_hu_a6ac45fd225e2630.webp&#34;
               width=&#34;359&#34;
               height=&#34;57&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;These log factors are typically ignored in asymptotic analyses (such as that of Kerenidis, Prakash, and Szilágyi &lt;sup id=&#34;fnref1:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;), where the goal is to judge criterion 3 of a practical end-to-end algorithm, as these factors have a relatively small contribution in the limit where &lt;em&gt;n&lt;/em&gt; and other parameters are large. However, to judge criterion 4, it is essential to keep track of all contributions, as they can make a big difference on the runtime estimate for specific choices of &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;resource-estimate-for-the-qipm&#34;&gt;Resource estimate for the QIPM&lt;/h2&gt;
&lt;p&gt;To judge criterion 4, we need to know how many quantum resources the QIPM consumes for values of &lt;em&gt;n&lt;/em&gt; relevant to customers. We compute the number of logical qubits, the number of computationally expensive &lt;em&gt;T&lt;/em&gt; gates (i.e., the “&lt;em&gt;T&lt;/em&gt;-count”), and the number of layers of &lt;em&gt;T&lt;/em&gt; gates (i.e., “&lt;em&gt;T&lt;/em&gt;-depth”) required by the algorithm. A &lt;em&gt;T&lt;/em&gt; gate is a commonly studied quantum gate that, when coupled with other easy-to-implement quantum gates, equips quantum computers with their expressive power. We only count the &lt;em&gt;T&lt;/em&gt; gates because they are substantially more expensive than the other gates in popular approaches to fault-tolerant quantum computation, and they dominate the runtime of the algorithm. The number of logical qubits and the total number of &lt;em&gt;T&lt;/em&gt; gates are then used to determine the overall physical footprint of the quantum algorithm — this will also depend on details of the physical hardware, such as the physical error rate.&lt;/p&gt;
&lt;p&gt;Meanwhile, the &lt;em&gt;T&lt;/em&gt;-depth determines the runtime in an architecture where &lt;em&gt;T&lt;/em&gt;-gates on disjoint qubits can be implemented in parallel. These metrics have been computed for other quantum algorithms, allowing for reasonable comparisons.&lt;/p&gt;
&lt;p&gt;For a version of the PO problem like the one above, with a couple of additional, common constraints, we find that optimizing an &lt;em&gt;n&lt;/em&gt;-qubit portfolio to tolerance &lt;em&gt;ϵ&lt;/em&gt; requires&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.33_hu_c5c25bd91fc4340b.webp 400w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.33_hu_13160ccd36c57b03.webp 760w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.33_hu_f0363ef12b2978ec.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/CleanShot-2023-11-08-at-08.53.33_hu_c5c25bd91fc4340b.webp&#34;
               width=&#34;658&#34;
               height=&#34;102&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;where we omit subleading terms. Notably, the order-&lt;em&gt;n&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; overhead for the &lt;em&gt;T&lt;/em&gt;-count vs. the &lt;em&gt;T&lt;/em&gt;-depth, and the order-&lt;em&gt;n&lt;sup&gt;2&lt;/sup&gt;&lt;/em&gt; logical qubit dependence reflects the fact that QRAM can be implemented at shallow depth albeit, with large qubit and gate cost.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.51_hu_459b08875ff57a60.webp 400w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.51_hu_b6d9b619a4042c3f.webp 760w,
               /blog/20231113-aws/CleanShot-2023-11-08-at-08.53.51_hu_4eb47090033258bb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/CleanShot-2023-11-08-at-08.53.51_hu_459b08875ff57a60.webp&#34;
               width=&#34;651&#34;
               height=&#34;322&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Figure 2: Median value of the Frobenius condition number as a function of the number of stocks in the portfolio optimization instance, from numerical simulations. Error bars represent the 16th to 84th percentile of observed instances, and the dashed line is a power-law fit showing that the growth of the Frobenius condition number is nearly linear in &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The above expressions are not particularly illuminating, as they still depend on instance-specific parameters &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;ξ&lt;/em&gt;. To probe these parameters, we simulated random instances of portfolio optimization with &lt;em&gt;n&lt;/em&gt; varying from 10 to 120. For each instance, we chose a random set of &lt;em&gt;n&lt;/em&gt; stocks from the Dow Jones U.S. Total Stock Market Index (DWCF) and constructed the expected daily return vector &lt;em&gt;u&lt;/em&gt; and covariance matrix &lt;em&gt;𝛴&lt;/em&gt; using historical stock data collected over &lt;em&gt;2n&lt;/em&gt; time epochs. We found that both &lt;em&gt;ⲕF&lt;/em&gt; and &lt;em&gt;ξ&lt;sup&gt;-2&lt;/sup&gt;&lt;/em&gt; appear to grow with &lt;em&gt;n&lt;/em&gt;, on average, and were each typically on the order of 104 or 105 for &lt;em&gt;n&lt;/em&gt; = 100. Overall, we evaluated the average cost of the algorithm at &lt;em&gt;n&lt;/em&gt;=100 to be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of logical qubits: 8 million&lt;/li&gt;
&lt;li&gt;&lt;em&gt;T&lt;/em&gt;-depth: 2 x 1024&lt;/li&gt;
&lt;li&gt;&lt;em&gt;T&lt;/em&gt;-count: 7 x 1029&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even if layers of &lt;em&gt;T&lt;/em&gt;-gates are optimistically implemented at the GHz speeds of classical processors (in reality they are likely to be 2–4 orders of magnitude slower than that) these estimates suggest that the runtime of the quantum algorithm would still be millions of years, even for an instance size that is already classically tractable on a laptop.&lt;/p&gt;
&lt;p&gt;This outcome allows us to confidently say that the QIPM, in its current form, does not satisfy criterion 4 of the end-to-end practical algorithm. The reason this occurred was due to a confluence of several independent factors: a large constant pre-factor coming mainly from the QLSS, a large condition number &lt;em&gt;ⲕF&lt;/em&gt;, a significant number of samples needed for tomography, and even a non-negligible contribution from logarithmic factors (about a factor of 100,000 for the &lt;em&gt;T&lt;/em&gt;-depth) that are traditionally ignored in asymptotic analyses.&lt;/p&gt;
&lt;p&gt;These estimates also incorporate a few improvements we made to the algorithm, including an adaptive approach to tomography, and preconditioning the linear systems. However, there are several ways one could try to improve the algorithm further, including using newer, more advanced versions of quantum state tomography &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; with &lt;em&gt;ξ&lt;sup&gt;-1&lt;/sup&gt;&lt;/em&gt; rather than &lt;em&gt;ξ&lt;sup&gt;-2&lt;/sup&gt;&lt;/em&gt; dependence, additional preconditioning, or improvements to the QLSS. Nonetheless, the resource estimate we arrive at is so far from practicality that multiple improvements to various parts of the algorithm would be needed to make the algorithm practical.&lt;/p&gt;
&lt;p&gt;In retrospect, we can also consider whether the algorithm meets criterion 3, i.e., if it has an asymptotic speedup. The numerical simulations of Kerenidis, Prakash, and Szilágyi &lt;sup id=&#34;fnref2:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; suggested it does, but our numerical simulations fail to corroborate this, since we find that the scaling of the algorithmic runtime in the regime &lt;em&gt;10 ≤ n ≤ 120&lt;/em&gt; is roughly the same as the classical algorithm. However, this scaling does not have a robust trend, and cannot confidently extrapolate to larger industrially relevant choices of &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion-and-takeaways&#34;&gt;Conclusion and takeaways&lt;/h2&gt;
&lt;p&gt;Our end-to-end resource analysis of the QIPM for portfolio optimization is a revealing case study on the utility of quantum algorithms. Even when an algorithm presents signals of utility, closer inspection can reveal a drastically different picture when all factors are considered more fairly. This was the case for the QIPM—our analysis strongly suggests it will not be useful to customers, barring significant improvements to the underlying algorithm. However, the insights gleaned go beyond the QIPM; the core subroutines of the QIPM (QRAM, QLSS, tomography) are common to many other quantum algorithms as well. Parts of our detailed cost analysis could thus be re-utilized in those algorithms. Indeed, one specific lesson learned is the great cost of implementing quantum circuits for the data-access component of the quantum algorithm (QRAM).&lt;/p&gt;
&lt;p&gt;The QRAM accounts for most of the logical qubits in our accounting, and contributes a considerable factor to the gate depth and gate count. To improve the QIPM to the point of practicality, one ingredient which would likely be necessary is a dedicated QRAM hardware element that can leverage the specialized, non-universal nature of the QRAM operation to perform the operation more efficiently than our general analysis accounts for. This hardware element would also improve the practicality of many other quantum algorithms in many domains such as quantum machine learning, quantum chemistry, and quantum optimization.&lt;/p&gt;
&lt;p&gt;With our analysis revealing that QIPMs face significant challenges toward practicality, where does this leave portfolio optimization in the landscape of quantum computing applications? Fortunately, there are other proposed methods for solving portfolio optimization on quantum computers (e.g., &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;), and some of these are even amenable to near-term experimentation.&lt;/p&gt;
&lt;p&gt;These approaches operate by reformulating the convex problem above as a binary optimization problem and attacking them with variational quantum algorithms, such as the &lt;a href=&#34;https://github.com/aws-samples/amazon-braket-algorithm-library/blob/main/notebooks/textbook/Quantum_Approximate_Optimization_Algorithm.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quantum approximate optimization algorithm&lt;/a&gt; (QAOA), or using specialized hardware such as quantum annealers.&lt;/p&gt;
&lt;p&gt;The upshot is that some of these variational approaches can already be run on near-term quantum devices through Amazon Braket, but one loses the theoretical asymptotic success guarantees afforded by the QIPM. As such, it is unclear whether these near-term solutions satisfy the third criterion above (asymptotic speedup), and determining whether this is the case will require further empirical investigation using current hardware and future generations of devices.&lt;/p&gt;
&lt;p&gt;Finance remains a rich space for potential quantum algorithms, and perhaps some of the ideas in the QIPM will prove fruitful toward developing practical algorithms in the future. We hope that this example inspires researchers to search for the next generation of innovative, practical quantum algorithms in a systematic, end-to-end way.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;h2 id=&#34;alexander-dalzell&#34;&gt;Alexander Dalzell&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/dalzell_hu_a91676b1ae91e9fe.webp 400w,
               /blog/20231113-aws/dalzell_hu_f3d2cc0c1ade55d5.webp 760w,
               /blog/20231113-aws/dalzell_hu_baba73570f379fc7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/dalzell_hu_a91676b1ae91e9fe.webp&#34;
               width=&#34;637&#34;
               height=&#34;658&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Alex Dalzell is a Research Scientist at the AWS Center for Quantum Computing. Alex joined the team in 2021 after completing his PhD in Physics at Caltech, where he studied the complexity theory of quantum advantage experiments using noisy devices. His current research interests lie primarily in quantum algorithms and applications, but also quantum computation and quantum information more broadly.&lt;/p&gt;
&lt;h2 id=&#34;mario-berta&#34;&gt;Mario Berta&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/berta_hu_920755693375274a.webp 400w,
               /blog/20231113-aws/berta_hu_eb707dda60a3f249.webp 760w,
               /blog/20231113-aws/berta_hu_36b2fb90ad9ab6bd.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/berta_hu_920755693375274a.webp&#34;
               width=&#34;567&#34;
               height=&#34;718&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Mario Berta is a Professor of Physics at the Institute for Quantum Information RWTH Aachen University and holds a position as a Visiting Reader at the Department of Computing Imperial College London. He is working on the theory of quantum information science.&lt;/p&gt;
&lt;h2 id=&#34;dave-clader&#34;&gt;Dave Clader&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/clader_hu_dfba03d0ce2e0c5a.webp 400w,
               /blog/20231113-aws/clader_hu_ef4b6eed2f0551d4.webp 760w,
               /blog/20231113-aws/clader_hu_a880ec44a28bbbd.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/clader_hu_dfba03d0ce2e0c5a.webp&#34;
               width=&#34;713&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dave Clader is the founder of BQP Advisors, LLC providing quantum computing technical consulting services. His expertise includes quantum algorithms, error correction, and noise mitigation. Prior to founding BQP Advisors, he was a Vice President in the Quantum Research team at Goldman Sachs where he focused on financial applications of quantum computing. Prior to this, he was as a Principal Research Scientist at the Johns Hopkins University Applied Physics Lab. He completed his PhD in Physics at the University of Rochester.&lt;/p&gt;
&lt;h2 id=&#34;david-bader&#34;&gt;David Bader&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/bader_hu_f5d28cffce7ef39.webp 400w,
               /blog/20231113-aws/bader_hu_779dbabe99574196.webp 760w,
               /blog/20231113-aws/bader_hu_79e84d411680a49d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/bader_hu_f5d28cffce7ef39.webp&#34;
               width=&#34;750&#34;
               height=&#34;754&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;David A. Bader is a research scientist in Research &amp;amp; Development at Goldman Sachs; and a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. He is a Fellow of the IEEE, ACM, AAAS, and SIAM; a recipient of the IEEE Sidney Fernbach Award; and the 2022 Innovation Hall of Fame inductee of the University of Maryland’s A. James School of Engineering. In 1998, Bader built the first Linux supercomputer that led to a high-performance computing (HPC) revolution.&lt;/p&gt;
&lt;h2 id=&#34;helmut-katzgraber&#34;&gt;Helmut Katzgraber&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/katzgraber_hu_bb53d815ebb12415.webp 400w,
               /blog/20231113-aws/katzgraber_hu_744b3a6a4206c776.webp 760w,
               /blog/20231113-aws/katzgraber_hu_68984c4d114dd2df.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/katzgraber_hu_bb53d815ebb12415.webp&#34;
               width=&#34;507&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dr. Helmut Katzgraber earned a diploma in physics from ETH Zurich, and his master’s degree and PhD in physics at the University of California Santa Cruz. After postdoc positions at the University of California Davis and ETH Zurich, he was awarded a Swiss National Science Foundation professorship. In 2009, he joined Texas A&amp;amp;M University as an assistant professor, and became a full professor in 2015. Katzgraber joined Microsoft as a principal research manager in 2018 before joining Amazon in 2020. He was elected Fellow of the American Physical Society in 2021 and leads the Quantum Solutions Lab at AWS.&lt;/p&gt;
&lt;h2 id=&#34;cedric-lin&#34;&gt;Cedric Lin&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/lin_hu_8e784b7fbd9c7f82.webp 400w,
               /blog/20231113-aws/lin_hu_41ad73a4fa87b01.webp 760w,
               /blog/20231113-aws/lin_hu_81f3a456d7857e59.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/lin_hu_8e784b7fbd9c7f82.webp&#34;
               width=&#34;660&#34;
               height=&#34;660&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Cedric Lin is a Sr. Applied Scientist at Amazon Braket. He previously worked at Google as a software engineer, where he primarily designed and built data pipelines and algorithms for optimization. Cedric has a PhD in Physics from the Massachusetts Institute of Technology; he spent several years developing quantum algorithms, and understanding their limits through the tools of quantum complexity.&lt;/p&gt;
&lt;h2 id=&#34;martin-schuetz&#34;&gt;Martin Schuetz&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/schuetz_hu_c89d2b666e974394.webp 400w,
               /blog/20231113-aws/schuetz_hu_9d4384f009a2dda3.webp 760w,
               /blog/20231113-aws/schuetz_hu_d8e943d07f3d811c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/schuetz_hu_c89d2b666e974394.webp&#34;
               width=&#34;430&#34;
               height=&#34;586&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Martin Schuetz is a Principal Research Scientist at the Amazon Quantum Solutions Lab. Martin has worked several years as an academic researcher with a focus on quantum simulation and computing, at ETH Zurich, the Max-Planck-Institute for Quantum Optics and Harvard University. Today Martin is working with customers to help solve some of their hardest problems, designing and building quantum computing, machine learning and optimization solutions on AWS.&lt;/p&gt;
&lt;h2 id=&#34;nikitas-stamatopoulos&#34;&gt;Nikitas Stamatopoulos&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/stamatopoulos_hu_e44731a9feaf7dd0.webp 400w,
               /blog/20231113-aws/stamatopoulos_hu_5ac13e30e98a0d3a.webp 760w,
               /blog/20231113-aws/stamatopoulos_hu_a0e25dd8e1ddf5cb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/stamatopoulos_hu_e44731a9feaf7dd0.webp&#34;
               width=&#34;600&#34;
               height=&#34;750&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Nikitas Stamatopoulos is the Head of Quantum Computing Research at Goldman Sachs, with the goal of identifying and developing applications of quantum computing in finance. After working for the better part of a decade as a quantitative researcher with focus on classical HPC solutions for quantitative problems in derivative pricing/hedging and portfolio optimization, he began exploring how quantum computers could be harnessed to enhance computationally intensive problems typically encountered in finance. He completed his PhD in Physics (high energy - theory) at Dartmouth College.&lt;/p&gt;
&lt;h2 id=&#34;grant-salton&#34;&gt;Grant Salton&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/salton_hu_b02b52a4b201aea5.webp 400w,
               /blog/20231113-aws/salton_hu_ece45d9d1606c07c.webp 760w,
               /blog/20231113-aws/salton_hu_f49d3cc05d5606a3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/salton_hu_b02b52a4b201aea5.webp&#34;
               width=&#34;400&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Grant Salton is a senior research scientist in the Amazon Quantum Solutions Lab. Grant obtained his PhD from Stanford University, and his research interests include quantum information theory, quantum algorithms, quantum error correction, applications of near-term quantum devices, and the role of quantum information in other areas of fundamental physics (e.g., gravity). Prior to joining Amazon, Grant was a postdoctoral fellow at the IQIM, Caltech.&lt;/p&gt;
&lt;h2 id=&#34;william-zeng&#34;&gt;William Zeng&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231113-aws/zeng_hu_bcfa0618ff6e51d9.webp 400w,
               /blog/20231113-aws/zeng_hu_bc2d3b971f4800f4.webp 760w,
               /blog/20231113-aws/zeng_hu_62ea01d9b6f4a5b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231113-aws/zeng_hu_bcfa0618ff6e51d9.webp&#34;
               width=&#34;546&#34;
               height=&#34;554&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dr. William Zeng is a partner at Quantonation, investing in early stage quantum and deep physics technology companies. He is also founder and President of the Unitary Fund, a non-profit dedicated to developing the quantum ecosystem to benefit the most people. His research focuses on quantum computer architecture, algorithms and software. He previously led a quantum computing research group at Goldman Sachs and initial development of Rigetti Computing&amp;rsquo;s quantum cloud platform. He received his PhD in quantum algorithms from Oxford University and his BSc. in Physics from Yale University.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/quantum-computing/a-detailed-end-to-end-assessment-of-a-quantum-algorithm-for-portfolio-optimization-released-by-goldman-sachs-and-aws/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://aws.amazon.com/blogs/quantum-computing/a-detailed-end-to-end-assessment-of-a-quantum-algorithm-for-portfolio-optimization-released-by-goldman-sachs-and-aws/&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Alexander M. Dalzell, B. David Clader, Grant Salton, Mario Berta, Cedric Yen-Yu Lin, David A. Bader, Nikitas Stamatopoulos, Martin J. A. Schuetz, Fernando G. S. L. Brandão, Helmut G. Katzgraber, William J. Zeng. “End-to-end resource analysis for quantum interior point methods and portfolio optimization.” PRX Quantum 4, 040325 (2023). &lt;a href=&#34;https://doi.org/10.1103/PRXQuantum.4.040325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1103/PRXQuantum.4.040325&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Ewin Tang. “Quantum Principal Component Analysis Only Achieves an Exponential Speedup Because of Its State Preparation Assumptions” Phys. Rev. Lett. 127, 06050 (2021). &lt;a href=&#34;https://doi.org/10.1103/PhysRevLett.127.060503&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1103/PhysRevLett.127.060503&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;“MOSEK Portfolio Optimization Cookbook.“ Release 1.3.0 (2023) &lt;a href=&#34;https://docs.mosek.com/MOSEKPortfolioCookbook-a4paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.mosek.com/MOSEKPortfolioCookbook-a4paper.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Blog: “Goldman Sachs and AWS examine efficient ways to load data into quantum computers“
Paper: B. David Clader, Alexander M. Dalzell, Nikitas Stamatopoulos, Grant Salton, Mario Berta, William J. Zeng. “Quantum Resources Required to Block-Encode a Matrix of Classical Data.” IEEE Transactions on Quantum Engineering (Volume: 3) (2022) &lt;a href=&#34;https://doi.org/10.1109/TQE.2022.3231194&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TQE.2022.3231194&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Iordanis Kerenidis, Anupam Prakash, Dániel Szilágyi. “Quantum Algorithms for Portfolio Optimization.” Proceedings of the 1st ACM Conference on Advances in Financial Technologies. (2019) &lt;a href=&#34;https://doi.org/10.1145/3318041.3355465&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1145/3318041.3355465&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref2:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Joran van Apeldoorn, Arjan Cornelissen, András Gilyén, and Giacomo Nannicini. “Quantum tomography using state-preparation unitaries.” Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). &lt;a href=&#34;https://doi.org/10.1137/1.9781611977554.ch47&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1137/1.9781611977554.ch47&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Sebastian Brandhofer, Daniel Braun, Vanessa Dehn, Gerhard Hellstern, Matthias Hüls, Yanjun Ji, Ilia Polian, Amandeep Singh Bhatia, Thomas Wellens. “Benchmarking the performance of portfolio optimization with QAOA.” Quantum Inf. Process 22, 25 (2023) &lt;a href=&#34;https://doi.org/10.1007/s11128-022-03766-5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1007/s11128-022-03766-5&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AI&amp;HPC for Everything – the 32nd HPC Connection Workshop at SC23</title>
      <link>http://localhost:1313/blog/20231113-hpcwire/</link>
      <pubDate>Mon, 13 Nov 2023 07:12:16 -0700</pubDate>
      <guid>http://localhost:1313/blog/20231113-hpcwire/</guid>
      <description>&lt;p&gt;The 32nd HPC Connection Workshop will be held during SC23 in Denver, Co. on Nov 15, 2023. World renown experts and scholars will exchange on the latest artificial intelligence, supercomputing technologies, and applications. Topics include progress on the newest developments in AI and supercomputing applications, and recent optimization strategies for large-scale parallel computing.&lt;/p&gt;
&lt;p&gt;Featured speakers include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mauricio Araya Polo will present a set of algorithms ported to the Cerebras CS2. The implementations following this paradigm deliver disruptive level performance.
Anshumali Shrivastava will demonstrate the algorithmic progress that can exponentially reduce the compute and memory cost of pre-training, training, fine-tuning, as well as inference with LLMs.&lt;/li&gt;
&lt;li&gt;Yuval Bachar will share the blueprint for the next generation of sustainable computing infrastructure, ready to usher in a greener era of AI-driven advancements now.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt; will discuss the opportunities and challenges in massive data-intensive computing for applications in computational science and engineering.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;the-32nd-hpc-connection-workshop&#34;&gt;The 32nd HPC Connection Workshop&lt;/h2&gt;
&lt;p&gt;2:00-4:30pm Nov 15, 2023 the Magnolia Hotel, Denver, Co.&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Time&lt;/th&gt;
          &lt;th&gt;Agenda&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;2:00-2:30&lt;/td&gt;
          &lt;td&gt;Algorithms for Massive Scaling with Dataflow Paradigm&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Mauricio Araya Polo&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Senior R&amp;amp;D Manager on HPC and ML, TotalEnergies&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2:30-3:00&lt;/td&gt;
          &lt;td&gt;VASP Optimization for Large Scale Parallel Computing&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Liu Yu&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Senior Researcher at IEIT SYSTEMS&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3:00-3:30&lt;/td&gt;
          &lt;td&gt;How We Pre-Trained GPT/LLM Models from Scratch on a CPU-Only Cluster: Democratizing the GenAI Ecosystem with Algorithms and Dynamic Sparsity&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Anshumali Shrivastava&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Associate Professor, Rice University &amp;amp; Founder and CEO, ThirdAI Corp.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3:30-4:00&lt;/td&gt;
          &lt;td&gt;The ECL Fully Sustainable High-Performance Data Centers: Pioneering the Future of AI Workloads&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Yuval Bachar&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Founder &amp;amp; CEO, ECL (EdgeCloudLink) Inc.&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;4:00-4:30&lt;/td&gt;
          &lt;td&gt;Solving Global Grand Challenges with High Performance Data Analytics&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;David Bader&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;i&gt;&lt;/i&gt;&lt;/td&gt;
          &lt;td&gt;Distinguished Professor, Data Science Institute for Data Scienceat at New Jersey Institute of Technology&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Click here to register: &lt;a href=&#34;http://www.asc-events.net/HPCC/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.asc-events.net/HPCC/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The HPC Connection Workshop is an international High Performance Computing event organized by the Asia Supercomputer Community. This event takes place three times a year: during ASC in China, ISC in Germany, and SC in the USA. Top researchers and leading professionals from around the world gather at the workshops to discuss the latest developments and disruptive technologies in AI and supercomputing. Welcome to ASC’s website for videos of past highlights (&lt;a href=&#34;http://www.asc-events.org/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.asc-events.org/)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2023/11/13/aihpc-for-everything-the-32nd-hpc-connection-workshop-at-sc23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2023/11/13/aihpc-for-everything-the-32nd-hpc-connection-workshop-at-sc23/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer History Museum, Timeline of Computer History: Linux-based Supercomputing (1998)</title>
      <link>http://localhost:1313/blog/20231110-chm/</link>
      <pubDate>Sat, 11 Nov 2023 11:12:17 -0500</pubDate>
      <guid>http://localhost:1313/blog/20231110-chm/</guid>
      <description>

















&lt;figure  id=&#34;figure-linux-supercomputer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Linux Supercomputer&#34; srcset=&#34;
               /blog/20231110-chm/timeline_computers_1998.bader-linux-supercomputer-prototype_hu_edb7e769192a31d4.webp 400w,
               /blog/20231110-chm/timeline_computers_1998.bader-linux-supercomputer-prototype_hu_f9cbbb18f5d16c1b.webp 760w,
               /blog/20231110-chm/timeline_computers_1998.bader-linux-supercomputer-prototype_hu_7e35ed1ac2126bed.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231110-chm/timeline_computers_1998.bader-linux-supercomputer-prototype_hu_edb7e769192a31d4.webp&#34;
               width=&#34;760&#34;
               height=&#34;569&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Linux Supercomputer
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The first supercomputer using the Linux operating system, consumer, off-the shelf parts, and a high-speed, low-latency interconnection network, was developed by &lt;strong&gt;David A. Bader&lt;/strong&gt; while at the University of New Mexico. From this successful prototype design, Bader led the development of “RoadRunner”, the first Linux supercomputer for open use by the national science and engineering community via the National Science Foundation&amp;rsquo;s National Technology Grid. RoadRunner was put into production use in April 1999. Within a decade this design became the predominant architecture for all major supercomputers in the world.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerhistory.org/timeline/1998/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerhistory.org/timeline/1998/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hot Topics of Generative AI are the Focus of 2023 Data Science Summit</title>
      <link>http://localhost:1313/blog/20231109-njit/</link>
      <pubDate>Thu, 09 Nov 2023 18:02:46 -0500</pubDate>
      <guid>http://localhost:1313/blog/20231109-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-institute-for-data-science-advisory-board&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Institute for Data Science advisory board&#34; srcset=&#34;
               /blog/20231109-njit/PXL_20231106_165718580_0_hu_95f682c2852b5102.webp 400w,
               /blog/20231109-njit/PXL_20231106_165718580_0_hu_9dfce15032d9e241.webp 760w,
               /blog/20231109-njit/PXL_20231106_165718580_0_hu_913086ea6c7fbbad.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231109-njit/PXL_20231106_165718580_0_hu_95f682c2852b5102.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Institute for Data Science advisory board
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Artificial intelligence experts from ADP, Amazon and Maersk highlighted this year’s NJIT Data Science Summit, hosted by the university’s Institute for Data Science.&lt;/p&gt;
&lt;p&gt;The summit &lt;a href=&#34;https://news.njit.edu/experts-njits-data-science-summit-propose-new-paths-hardware-and-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;began in 2022&lt;/a&gt; with Google and IBM presenting about hardware and ethics. This year’s focus was on generative AI, large language models and how they’re used in the business world.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Data science is a new discipline that enables data-driven decisions across real-world problems in areas such as healthcare, security, retail and advertising, human resources, urban sustainability and entertainment,&amp;rdquo; said NJIT’s &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of the Institute for Data Science. &amp;ldquo;We were delighted to welcome the community to our NJIT @JerseyCity campus where we teach in our data science masters and certificate programs.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Amazon’s Sherry Marcus, director of applied science for generative AI, told the in-person and online attendees that generative AI has been happening for more than a decade. What is new, she explained, is access to open-source datasets, inexpensive GPUs (a type of computer processor ideal for crunching big data), and the 2017 public description of transformer software, which is a deep-learning architecture relying on parallel processing.&lt;/p&gt;
&lt;p&gt;“It’s the combination of these three things that as far as industry work is concerned, allow generative AI to take off,” Marcus stated.&lt;/p&gt;
&lt;p&gt;Everyone knows about large language models like ChatGPT, so Marcus instead discussed four other kinds of AI applications that she believes will change how businesses and consumers operate. They are contact centers, code generation, image generation and machine diagnostics. Meanwhile, she said prompt engineering is the easiest way to move a business into the AI age, while building one’s own language model would be the hardest way.&lt;/p&gt;
&lt;p&gt;“So right now, the industry is trying to reckon with all of this,” Marcus said. Amazon engineers and business leaders spend a lot of time thinking about AI explainability, fairness and robustness, she added. “As we move on with regulations, responsible AI is going to become more and more embedded within development, within training and within the whole lifecycle of applications. And that is something that we&amp;rsquo;re just very much at the beginning of.”&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20231109-njit/PXL_20231106_185445803_hu_bb8cc4b66f737fe3.webp 400w,
               /blog/20231109-njit/PXL_20231106_185445803_hu_39d06c43000e380.webp 760w,
               /blog/20231109-njit/PXL_20231106_185445803_hu_b7b70c76a1dc76a3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20231109-njit/PXL_20231106_185445803_hu_bb8cc4b66f737fe3.webp&#34;
               width=&#34;690&#34;
               height=&#34;492&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Erez Agmoni, global head of innovation for shipping giant Maersk, and Xiaojing Wang, distinguished engineer at ADP, both presented about their own companies’ uses of AI for practical matters. Agmoni focused on AI to help understand the data behind logistics, while Wang talked about user experiences in human resources.&lt;/p&gt;
&lt;p&gt;NJIT professors Mengnan Du, Usman Roshan and Mengjia Xu, all from Ying Wu College of Computing, presented research in AI reliability, medical diagnosis and human brain analysis, respectively.&lt;/p&gt;
&lt;p&gt;One of this year’s attendees was Jessica Horton, a spring 2023 NJIT @JerseyCity graduate from the online M.S. in data science program. She worked as a software engineer for actuarial and financial modeling and forecasting, before pivoting her career to data science.&lt;/p&gt;
&lt;p&gt;“I’m really excited about advances in AI with image recognition in medical imaging, for trying to recognize tumors and things,” Horton said, adding that Maersk’s Agmoni was her favorite speaker of the day. “It&amp;rsquo;s really interesting seeing the difference between research for pure research sake, and then how it&amp;rsquo;s applied in the real world with a company like Maersk and how they spent months just collecting data, not even looking for a specific answer, to see where the data could lead them.”&lt;/p&gt;
&lt;p&gt;Horton said she highly recommends the NJIT data science program because of the knowledge and connections gained from faculty. She cited Associate Professor Yiannis Koutis in particular. “The way he explained everything, it was just so easy to follow … with the examples that he gave, being able to dive in and do some coding.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/hot-topics-generative-ai-are-focus-2023-data-science-summit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/hot-topics-generative-ai-are-focus-2023-data-science-summit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Annual ECE &#39;Welcome Back&#39; Awards Ceremony and Reception</title>
      <link>http://localhost:1313/blog/20230927-maryland/</link>
      <pubDate>Wed, 27 Sep 2023 13:28:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230927-maryland/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230927-maryland/article15720.large_hu_1d1fc7d3b52ca093.webp 400w,
               /blog/20230927-maryland/article15720.large_hu_4b3186b7ce1e13ad.webp 760w,
               /blog/20230927-maryland/article15720.large_hu_bf0f30e381cc1a3b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230927-maryland/article15720.large_hu_1d1fc7d3b52ca093.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The ECE Department recently held its annual Fall ‘Welcome Back’ Awards Ceremony and Reception on Wednesday, September 20th.  ECE Chair Sennur Ulukus presented student, staff and faculty awards.  Here is a list of this year’s departmental awards:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Staff Service Awards&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alejandra Mercado         10 years&lt;br&gt;
Marion Devaney             25 years&lt;br&gt;
Ronald Jean                   30 years&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Department of Electrical and Computer Engineering Staff Award&lt;/strong&gt; in recognition of a staff member for outstanding service to faculty, staff and students:&lt;/p&gt;
&lt;p&gt;Crystal Umaru, Assistant Director, Finance and Business Services&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;George Corcoran Memorial Award&lt;/strong&gt; for a Graduate Student presented to two graduate teaching assistants in recognition of excellence in teaching:&lt;/p&gt;
&lt;p&gt;Sydney Overton and Faisal Hamman&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The David Bader Award&lt;/strong&gt; (formerly the Graduate Student Service Award) in recognition of graduate student who distinguishes themselves through exception service and leadership to the Department: Joseph Messou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The George Corcoran Memorial Award for Faculty&lt;/strong&gt;, presented annually to a young faculty member who has shown exemplary contributions to teaching and educations leadership: Professor Sahil Shah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jimmy H.C. Lin Awards&lt;/strong&gt; – awarded annually to students, staff and faculty who transform their ideas into innovations through invention and technology commercialization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jimmy H.C. Lin Award for Innovation and Invention&lt;/strong&gt;:  Professor Mario Dagenais, and Dr. Yang Zhang (Ph.D. ’19) for their patent, “High Power, Narrow Linewidth Semiconductor Laser System and Method of Fabrication.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Jimmy H.C. Lin Award for Invention&lt;/strong&gt;: Professor Edo Waks and his student, Uday Saha (Ph.D ’22), in recognition of their research, “Low Noise Quantum Frequency Conversion Scheme for Trapped Ion Quantum Network.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Department of Electrical and Computer Engineering Distinguished Dissertation Award&lt;/strong&gt;, in recognition of recent doctoral recipients who have already made unusually significant and original contributions to their fields.  This year’s recipients are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JinJing Han, advised by Professor Reza Ghodssi. Dissertation: Minimally Invasive Neurochemical Sensing System for Ex Vivo And In Vivo Investigation of Serotonergic Modulation&lt;/li&gt;
&lt;li&gt;Shoutik Mukherjee, advised by Professor Behtash Babadi. Dissertation: Statistical Models of Neural Computations and Network Interactions in High-Dimensional Neural Data&lt;/li&gt;
&lt;li&gt;Nilesh Suriyarachchi, advised by Professor John Baras. Dissertation: Cooperative Multi-agent Sensing, Planning, and Control for Connected Autonomous Vehicles&lt;/li&gt;
&lt;li&gt;Sajani Vithana, advised by Professor Sennur Ulukus. Dissertation: Achieving Information-Theoretic Privacy in Distributed Learning: Private Read-Update-Write (PRUW)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/annual-ece-welcome-back-awards-ceremony-and-reception&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/annual-ece-welcome-back-awards-ceremony-and-reception&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ying Wu College of Computing to Present Award-Winning Papers at IEEE HPEC Conference</title>
      <link>http://localhost:1313/blog/20230913-njit/</link>
      <pubDate>Tue, 26 Sep 2023 10:39:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230913-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230913-njit/Capture_hu_74b4fd8da7c26bd1.webp 400w,
               /blog/20230913-njit/Capture_hu_5084337bb25fae18.webp 760w,
               /blog/20230913-njit/Capture_hu_203efa5b42454bf4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230913-njit/Capture_hu_74b4fd8da7c26bd1.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A research group led by &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; has won the 2023 Student Innovation Award from the IEEE High Performance Extreme Computing (HPEC) conference for their paper “&lt;a href=&#34;https://davidbader.net/publication/2023-blgglrd/2023-blgglrd.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Triangle Counting Through Cover-Edges&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;The group features NJIT faculty and students, as well as a high school student. It is composed of Bader, Principal Senior Researcher Zhihui Du, doctoral candidates Fuhuan Li and Oliver Alvarado Rodriguez, Jason Lew ’23, former NJIT research intern Anya Ganeshan of Columbia University and Ahmet Gundogdu of Paramus High School.&lt;/p&gt;
&lt;p&gt;Bader also won the Innovation Award for his paper on “Fast Triangle Counting.”&lt;/p&gt;
&lt;p&gt;The winning papers will be featured as part of the IEEE HPEC conference, which is organized by MIT and sponsored by the IEEE Boston Section, and is the largest computing conference in New England with more than 125 talks. The conference will be presented virtually Sept. 19-23.&lt;/p&gt;
&lt;p&gt;Ying Wu College of Computing at NJIT will be further represented through accepted papers on “Parallel Longest Common SubSequence Analysis in Chapel” (co-authors: Professor Baruch Schieber, David Bader, Zhihui Du, and Ph.D. candidate Soroush Vahidi) and “Property Graphs in Arachne” (co-authors: David Bader, Zhihui Du and Ph.D. candidates Oliver Alvarado Rodriguez and Fernando Vera Buschmann).&lt;/p&gt;
&lt;p&gt;HPEC hosts leading-edge innovations on: AI, machine learning, big data, graph analytics, cloud computing, sensor processing, cyber, parallel computing, GPU computing, database technology and quantum computing, among others, along with tutorials and many distinguished invited speakers. See below for registration link and preliminary schedule link.&lt;/p&gt;
&lt;p&gt;External links:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieee-hpec.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieee-hpec.org/&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://ieee-hpec.org/index.php/ieee-hpec-2023-prelim-agenda/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieee-hpec.org/index.php/ieee-hpec-2023-prelim-agenda/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/ying-wu-college-computing-will-present-two-award-winning-papers-september-ieee-hpec-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/ying-wu-college-computing-will-present-two-award-winning-papers-september-ieee-hpec-conference&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>To The Point Cybersecurity Podcast: The Democratization of Data Science Tools with Dr. David Bader</title>
      <link>http://localhost:1313/blog/20230919-forcepoint/</link>
      <pubDate>Tue, 19 Sep 2023 11:38:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230919-forcepoint/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;E251_-_David_mixdown.mp3&#34;&gt;Listen&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;about-this-episode&#34;&gt;About This Episode&lt;/h2&gt;
&lt;p&gt;Joining us this week is Dr. David Bader, a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at the New Jersey Institute of Technology. He deep dives into the opportunity to democratize data science tools and the awesome free tool he and Mike Merrill spent the last several years building that can be found on the Bears-R-Us GitHub page open to the public.&lt;/p&gt;
&lt;p&gt;We also discuss the vulnerabilities in the open-source supply chain, what AI security teams should be concerned about, data poisoning, AI that is fair and equitable, and the discussion on regulation and self-regulation in AI. Key takeaway from the conversation, data science is indeed growing and it holds an exciting future for those that pursue it!&lt;/p&gt;
&lt;p&gt;1:02 Exploring the Power of Data Science Tools&lt;br&gt;
8:53 Advancing Cybersecurity Through Data Science Tools&lt;br&gt;
17:22 The Transformative Potential and Challenges of Data Science Tools&lt;br&gt;
24:53 Self-Regulation and AI Ethics in the Age of Data Science Tools&lt;br&gt;
32:45 Enabling Secure Information Sharing and Collaboration in a Connected World&lt;br&gt;
40:13 The Soaring Demand for Data Science Education in Today&amp;rsquo;s World&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230919-forcepoint/banner_hu_8a3bed16ffde9466.webp 400w,
               /blog/20230919-forcepoint/banner_hu_248a29cdfa0e090b.webp 760w,
               /blog/20230919-forcepoint/banner_hu_1489d6e413c3c022.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230919-forcepoint/banner_hu_8a3bed16ffde9466.webp&#34;
               width=&#34;760&#34;
               height=&#34;397&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Exploring the Power of Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rachael: I&amp;rsquo;m so excited today for our guest. Every guest we have, I learn something new and I absolutely know that that&amp;rsquo;s going to happen today. Please welcome to the podcast Dr. David Bader. He&amp;rsquo;s a distinguished professor and founder of the Department of Data Science in the Ying Wu College of Computing and director of the Institute for Data Science at the New Jersey Institute of Technology. He&amp;rsquo;s a leading expert in solving global grand challenges in science, engineering, computing, and data science. Welcome to the podcast, David.&lt;/p&gt;
&lt;p&gt;David: Thanks, Rachel. Good to be here today with you and Audra. Looking forward to talking about cybersecurity.&lt;/p&gt;
&lt;p&gt;Rachael: Love it. All right, Audra, I know you got the first question today.&lt;/p&gt;
&lt;p&gt;Audra: Could we jump off? I&amp;rsquo;d really like to start by talking about how open-source tools can support data analytics. And I know that you said on the side a little bit about how you can use some of the solutions to discover vulnerabilities in open source. So could you kick us off in that direction?&lt;/p&gt;
&lt;p&gt;David: So as you know, we&amp;rsquo;ve discovered in the last couple of years some major exploits dealing with open-source software and libraries included in many packages. This impacts our servers, it impacts our networks, and even our printers. It&amp;rsquo;s really scary to think about. And often open-source software is a supply chain of software that we use in many applications, even in firmware on our devices. And it&amp;rsquo;s very hard to detect malicious software being injected into the open source software. So within a single package, we&amp;rsquo;re very good now as a community for having tools that can scan individual packages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Uncovering Hidden Threats with Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: But it&amp;rsquo;s very hard to find exploits that may insert code across different packages where each piece of the code may not be that sensitive. But when brought all together and compiled together, linked together, then we find that we have a major exploit. We at the New Jersey Institute of Technology in partnership with Accenture, are working on tools and algorithms that are able to look at the open-source supply chain of software. And detect those vulnerabilities and be able to protect against those exploits. Very exciting project at hand.&lt;/p&gt;
&lt;p&gt;Audra: How are you actually linking up across multiple packages to actually be able to work out when something that looks harmless becomes evil?&lt;/p&gt;
&lt;p&gt;David: Great question. So we&amp;rsquo;re focused on a major source of open-source software, namely GitHub. And looking at tens of thousands of open-source packages that are used in everyday applications, commercial and business applications. And some of the most impactful projects that are often used by industry. We&amp;rsquo;re ingesting those packages and multiple versions of those packages and building graph connections of lines of software between those packages. Using the most sophisticated graph analytics at scale to detect those vulnerabilities.&lt;/p&gt;
&lt;p&gt;Audra: Now, I think this is amazing because I think the problem is we tend to look at what&amp;rsquo;s obvious and it&amp;rsquo;s like, oh. Well if we&amp;rsquo;re using this and it&amp;rsquo;s packaged by package, that&amp;rsquo;s where we&amp;rsquo;re safe. And the fact that we&amp;rsquo;re actually then finding where people are getting smart enough to put different bits of code across different packages. It makes a huge difference to be able to identify that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Needles in the Haystacks of Open Source Code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right. And it&amp;rsquo;s very hard to discover because it may be embedded in different versions of different libraries and packages and by building a graph. A graph is a very simple data structure where maybe vertices in the graph represent lines of source code and different packages. And edges represent connections between those lines of source code and other packages. Maybe where those subroutines are called. By building this massive graph, we&amp;rsquo;re able to discover those connections and those linkages that may be like finding a needle in a haystack without this type of tool.&lt;/p&gt;
&lt;p&gt;Audra: Exactly. The only question is where are you getting your actual test data from? Because when you want to find a needle in a haystack, you need to know what the needle might look like.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s correct. Well, we have decades worth of experience building some of the largest graph analytics at scale for the enterprise to discover not just one in a haystack. But many needles in haystacks are made of needles as well. So we&amp;rsquo;ve been working on these types of problems for a very long time. Building a lot of tools that have been transferred to industry and government. And we&amp;rsquo;re able to create better algorithms with fidelity to be able to find those exploits.&lt;/p&gt;
&lt;p&gt;We already know of some in the past in the past few years we&amp;rsquo;ve seen some of those exploits. So we can learn from those, but we&amp;rsquo;re also expecting that we may have new exploits that we haven&amp;rsquo;t seen before or have new ways. So we&amp;rsquo;re looking for patterns. Graphs are an excellent way to find those patterns of the types of malicious code that we may expect to see in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Empowering Cybersecurity with Open Source Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Audra: So moving on, but still on the topic of open source. How will open-source technology impact the democratization of data?&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s a great question, Audra. I&amp;rsquo;m very passionate about democratizing data science tools. So that anyone in any place on any system is able to perform the really complicated calculations that normally only large enterprises could do. So we are building out tools. Open source tools on GitHub that allow anyone with a Jupyter Notebook or running Python to be able to run their analytics as easily as they would toolkits like NumPy, Pandas, or NetworkX.&lt;/p&gt;
&lt;p&gt;But to be able to manipulate tens to hundreds of terabytes, massive amounts of data on backend supercomputers without having any knowledge or hero programming skills to program a supercomputer. Or understand how to interface and get access to those systems. So we&amp;rsquo;re trying to make data science accessible and provide the tools so that anyone anywhere can run their cybersecurity problems. And be able to ingest data sets, logs, and other information to protect themselves against cyber terrors and cyber hacks.&lt;/p&gt;
&lt;p&gt;Audra: Now, I think the thing is we&amp;rsquo;re learning from a work perspective that ML and AI can come into our lives and help us be more efficient. And if you&amp;rsquo;re actually starting to be able to make things more accessible to people who may be data scientists still. So they understand the methodologies and techniques to use it but don&amp;rsquo;t have access to the supercomputers and things like that. This is an amazing step forward.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advancing Cybersecurity Through Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right. As you see, learning large language models, for instance, ChatGPT, Bard, and others requires millions of dollars. And massive special purpose-built supercomputers and proprietary libraries to be able to create those models. What we would like to do is provide the tools so that the broad committee can create their own models. And not have to rely on having a budget of millions of dollars and access to new processor technologies to be able to do the same learning and the same capabilities. So we&amp;rsquo;re really passionate about open source and the democratization of these types of tools.&lt;/p&gt;
&lt;p&gt;Audra: So in terms of talking a bit more broadly. Maybe in the way from kind of chassis around open source particularly. But looking at how you apply data analytics to cybersecurity. Where are you seeing the areas where you can really make the biggest inroads?&lt;/p&gt;
&lt;p&gt;David: In cybersecurity, there are many types of data sets that we can employ. So for instance, I&amp;rsquo;ll talk to you about some of the problems that we&amp;rsquo;ve worked on in the past. One is understanding cyber threats to our organizations. Mostly what happens today is that something egregious is detected. We find that our IP has been exfiltrated. And we find that our perimeters have been encroached on.&lt;/p&gt;
&lt;p&gt;We find that there are bad actors that perform malicious damage to our corporate data sets and so on. We then try to discover how those actors got in, and what they see. What did they touch? What did they damage, what did they take? And this all happens after this egregious event. Now that&amp;rsquo;s great for CISO to try to discover, but what we would like to do is get out in front of the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Staying Ahead of Cyber Threats with Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: We want to be able to detect these types of activities before an egregious event happens. So rather than forensic analysis of our data sets. Where we would like to go is preventative analysis to be able to have situational awareness. The types of streaming analytics in near real-time that tell us that there&amp;rsquo;s an activity about to happen so that we can defend against it. And push those packets out of our networks. Prevent them from coming in before those egregious events happen.&lt;/p&gt;
&lt;p&gt;Audra: But again, so what are your common use cases that you&amp;rsquo;re focusing on? If you&amp;rsquo;re preventative, what are you looking for?&lt;/p&gt;
&lt;p&gt;David: We&amp;rsquo;re looking for malicious actors. Ones that may come in and sit on our networks for an extended amount of time. For instance, advanced persistent threat, APT, or other types of actors will burrow deep into our networks, hop around our networks, and lay in. Wait until they can touch and exfiltrate data. We don&amp;rsquo;t want them there in the first place.&lt;/p&gt;
&lt;p&gt;And what we try to do is build tools where we can detect those penetrations and also give attribution to where they&amp;rsquo;re coming from and be able to defend against those. And also to do it not with the old methods that they&amp;rsquo;ve been able to attack our networks on. But to be able to protect against methods that we may not have seen yet. So we&amp;rsquo;re trying to protect against behaviors where we may not know what that behavior looks like.&lt;/p&gt;
&lt;p&gt;Audra: Okay. Now that is very interesting. I&amp;rsquo;m very pro. We call it going left of the boom. So stopping preventative, stopping it before the boom happens, right?&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enhancing Cybersecurity Posture with Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Audra: So in terms of advice for organizations who are looking to use data analytics to improve their cybersecurity postures. What kind of recommendations can you make?&lt;/p&gt;
&lt;p&gt;David: So first I would look at the best practices of security and privacy within organizations.&lt;/p&gt;
&lt;p&gt;And that goes down to training individuals. Often the weakest link in many organizations is that the humans are workforces. I&amp;rsquo;m even guilty of getting an email and clicking on a link too fast before understanding where that link is coming from.&lt;/p&gt;
&lt;p&gt;And today&amp;rsquo;s spoofing and phishing emails are just, they&amp;rsquo;re terrifyingly good. In fact, with the advent of ChatGPT and other large language models, they are written and more sophisticated than ever. So we have to get better at automatically marking external emails on marking suspicious links. And train ourselves as humans not to just quickly click on the first thing we see in front of us.&lt;/p&gt;
&lt;p&gt;Audra: Exactly. What&amp;rsquo;s really interesting about that is previously, depending on language. There were lots and lots of spam emails and spoof emails and those sorts of things going out there. Now with the addition of AI tools. The level of spamming like in other languages like Arabic and things like that, has gone through the roof in the last few months.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right.&lt;/p&gt;
&lt;p&gt;Audra: They have that problem now.&lt;/p&gt;
&lt;p&gt;David: So just as they&amp;rsquo;re getting more capable. We need better tools to detect AI-generated messages and better tools to detect these types of attacks clearly.&lt;/p&gt;
&lt;p&gt;Audra: So can I ask in terms of what should organizations be concerned about around this area in terms of their posture, in terms of AI, and that sort of thing? What should they be concerned about?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Meeting the Challenge of AI-Powered Attacks with Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s a great question. I think what AI gives are methods for the attackers to evolve much faster than before in the types of attacks that they have. Rather than having to have individuals carefully craft single attacks against weak points in the organization, a network, and individuals. Now they can have a massive attack across every surface, every part of the surface of the organization, every individual. And it&amp;rsquo;s with that scale that now we have to also escalate our defenses. And we have to be much better at recognizing that it&amp;rsquo;s not just one directed email coming in.&lt;/p&gt;
&lt;p&gt;But now we have 10,000 directed emails coming in and we have to be able to protect against this higher rate. This higher onslaught of potential vulnerabilities within the organization. So I think it&amp;rsquo;s that scale and that detection where we&amp;rsquo;re going to have to increase our ability to look at things that look really innocuous. But maybe a Trojan force or a very sophisticated way for an attacker to gain our credentials.&lt;/p&gt;
&lt;p&gt;Audra: In terms of your predictions around this and where things are going. Because things are changing very rapidly. What do you actually predict for the future of data analytics in short, medium? How do you think it can be used to kind of enhance or take away either side, where do you see the future going?&lt;/p&gt;
&lt;p&gt;David: I think data analytics is off to a great start. This is going to be an area with rapid growth. We already saw the boom of generative AI with ChatGPT emerging last November. December publicly really changed the way we think about our workforce. It changes the way we think about hiring. It changes our cybersecurity posture, it changes our risk profile.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Transformative Potential and Challenges of Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: And I think we&amp;rsquo;re going to continue to see data science evolving rapidly. Like a Cambrian explosion of new ideas, algorithms, and techniques we&amp;rsquo;re really at the start of several years of this rapid change. So we&amp;rsquo;re going to have to hold on. We&amp;rsquo;re going to have to enjoy this ride. And know that we&amp;rsquo;re in a time of great transition and turbulence when it comes to data science and AI.&lt;/p&gt;
&lt;p&gt;Audra: Absolutely. But I think this is like the new internet, if you know what I mean because when the internet came out. It was created for particular reasons. They thought it would be very academic and that sort of thing. And it ended up being used for something very different. And I think AI will probably run that path as well.&lt;/p&gt;
&lt;p&gt;David: Correct. We are certainly in the hype stage of AI and generative AI right now. And I think comfortably in the next couple of years. We&amp;rsquo;re going to see where generative AI has some incredible uses. But also we&amp;rsquo;re going to see some places where it has incredible failures. We&amp;rsquo;ve already seen some, for instance, in the legal system as ChatGPT makes up references in cases argued to the Supreme Court. And as individuals get caught cheating on writings that use ChatGPT, that again takes liberty with the truth. These types of hallucinations, the bias within the results that we&amp;rsquo;re getting today. And the data pollution that can occur within training sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Navigating the Ethical Challenges of AI&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: These are areas where we need to be aware and understand. And always push toward AI that is explainable, that is fair, and equitable. That&amp;rsquo;s going to be a constant challenge to figure out how to do that and how to give these detailed explanations for how we arrived at the results that we&amp;rsquo;re seeing out of generative AI.&lt;/p&gt;
&lt;p&gt;Audra: So in the work that you do, well, we&amp;rsquo;ll go onto the positives. But I&amp;rsquo;m continuing on the negative side just at the moment in terms of data poisoning or the poisoning of data sets in order to flip labels. Doing all that kind of thing, how much are you seeing of that happening today?&lt;/p&gt;
&lt;p&gt;David: That is certainly happening today, and we see such a profound effect on businesses and competition. It is just natural to find those types of attacks taking place. For instance, there may be competitors, whether it&amp;rsquo;s companies within a particular sector or even if it&amp;rsquo;s countries or nation-states. We see this type of attack that is very hard to detect but can have a profound effect on our cybersecurity and our defenses. Whether it&amp;rsquo;s national security or organizational defense.&lt;/p&gt;
&lt;p&gt;Audra: Are you able to provide any examples of data poisoning attacks that you&amp;rsquo;re aware of? I was aware of the Google anti-spam where the dataset was poisoned. Therefore spammers were able to send out undetected emails that were definitely spam emails. Because they had actually changed the labeling in the dataset or polluted it to make it more noisy they just weren&amp;rsquo;t getting stopped.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Safeguarding AI Integrity&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: Well, one area that I&amp;rsquo;ve seen is in self-driving cars that use a lot of AI to be able to detect from their sensors what&amp;rsquo;s around them. There have been cases of stickers being placed on, for instance, stop signs or other street signs that cause the AI to mistake a stop sign for say a green traffic light. And so we have to be very careful with these technologies because they can have real-world consequences and affect all of us. So that&amp;rsquo;s an area where we&amp;rsquo;re going to see as we&amp;rsquo;ve witnessed before, malicious acts try to taint the learning in these data sets.&lt;/p&gt;
&lt;p&gt;Audra: Excellent. Is there any way in the work that you&amp;rsquo;re doing that you look at how you can protect against that?&lt;/p&gt;
&lt;p&gt;David: Yes. So one area is to include more explainability within our AI systems. I tend to think that we need to include both machine learning tools along with knowledge graphs and real-world data to really support the explainability behind the answers that we get out of AI.&lt;/p&gt;
&lt;p&gt;There may be other features that get incorporated into our neural necks to be able to give rational explanations that a human. That layperson can understand how it arrived at that decision. And I think that will go a long way to try to understand how models have been tweaked and how they&amp;rsquo;ve been changed. That could have disastrous effects. Also, there are models that are tweaked to try to make them more fair.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Science Tools and the Quest for Oversight&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: I just saw, as you probably did at Defcon. This meeting is where there&amp;rsquo;s hacking or trying to abuse ChatGPT to get it to say things that are a little bit controversial or incorrect. And these are very useful for having a nice press release. Here are the bad things it did. But also for open AI to go back and try to tweak their models to prevent some of these poor outcomes from happening.&lt;/p&gt;
&lt;p&gt;So I think again, we&amp;rsquo;re in the very early stages and we&amp;rsquo;re going to see more robust testing tools. And feedback to try to better train and guide these models as they give us inference to our most important problems that we ask of it.&lt;/p&gt;
&lt;p&gt;Rachael: I&amp;rsquo;m always interested in the regulation aspects. It&amp;rsquo;s how do you regulate the unknown basically? And particularly with AI when we don&amp;rsquo;t know all the applications to come. But it&amp;rsquo;s all driven by data. I mean, how do you regulate that in a meaningful way?&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right, and that&amp;rsquo;s a very hard challenge. As you know, leaders in the committee have come together to call for first a moratorium to understand the regulation. And these technologies are also being used in highly regulated industries such as healthcare and the financial sector and these areas. Areas where we really do need to understand how to regulate them.&lt;/p&gt;
&lt;p&gt;And this will be an ongoing conversation. It&amp;rsquo;s very hard to regulate technologies where we&amp;rsquo;re not even aware of the full extent of the capabilities of these technologies. And also we have to have conversations among individuals in this country and around the world on what is it that we allow for our data. How it should be used now or in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self-Regulation and AI Ethics in the Age of Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: There may be uses decades from now from data generated today that aren&amp;rsquo;t even imaginable. And we have to understand how we&amp;rsquo;re going to regulate today those potential uses five, 10 years down the line.&lt;/p&gt;
&lt;p&gt;Rachael: So is self-regulation an option here? I mean, we&amp;rsquo;ve talked about self-regulation in other areas of technology and how that went. I saw there&amp;rsquo;s an article, I think Google, CEO had made an AI pact with the EU about voluntary behavioral standards prior to them implementing the EU AI Act. And I just thought that was an interesting perspective as we kind of re-look at that whole process and how it could work.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right, but I think self-regulation for AI right now is going to be very challenging. Many companies are moving forward with AI in leaps and bounds. And one of the reasons is it&amp;rsquo;s an existential threat. If their competitors are doing it and they&amp;rsquo;re not, it could be the end of their company.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve had conversations with many CISOs who are looking at how to mitigate the risks while allowing the company to experiment and move forward. Understanding that the regulations may be put into place in the future and how to do things that will remain. Keep the company competitive while not creating that much of a risk in the future. But it is a challenge that we have and never before in the technology sense have we seen such a dramatic change in such a short period of time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Empowering Data Analytics with AI&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: So I believe that the next two, or three years will be a time when we come to terms with what is going to be the right best practices. What is the right amount of regulation by the government? What are the right types of self-regulation as we weather, not necessarily a storm? But this dramatic change and new excitement that we see from these new AI models.&lt;/p&gt;
&lt;p&gt;Rachael: Absolutely.&lt;/p&gt;
&lt;p&gt;Audra: So can we jump onto the positives of AI? Let&amp;rsquo;s see the positives now. So AI enabled data analytics. How are these technologies helping teams to understand more data and faster, more efficiently? There are sorts of things like what are examples of where it&amp;rsquo;s going to change. How we can absorb understand, and use data.&lt;/p&gt;
&lt;p&gt;Rachael: Can we put this into a little bit of context too? By the way, the latest data creation for 2023, I believe is 120 zettabytes, which is like a 23 zettabyte increase over the year before. Which is what 120 plus 21 zeros. I mean, it&amp;rsquo;s mind-blowing how much data there is. So I just want to put that in context with that huge question. That is Audra,&lt;/p&gt;
&lt;p&gt;David: Right? The amount of data is just unfathomable and it&amp;rsquo;s growing every single year. And that makes it very challenging for us to keep on top of tools that can manage these types of massive data sets. One area that I am working on is democratizing data science at this large scale. Creating open-source tools where anyone. Any organization anywhere around the world can readily ingest and process massive data sets beyond the capabilities of enterprise and commercial tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taming the Data Deluge with AI and Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: So imagine you have tens or a hundred terabytes today would be a large size. It&amp;rsquo;s not yet zettabytes. But we do have hundreds of terabytes that you&amp;rsquo;d like to be able to pick up and manipulate the process by analysts who speak in Python, the lingua de franca of data scientists. And maybe they know how to use NumPy, IMP pandas, and network X for graph analytics. But these data sizes would just overwhelm their tools.&lt;/p&gt;
&lt;p&gt;They would never finish loading their dataset yet ask any questions about their data. So I&amp;rsquo;m building out along with partners in the Department of Defense. A system open source called Aqua, which is the Greek word for bear, and this system is in GitHub. You can find it in the Bear’s repository on GitHub. We&amp;rsquo;ve been working on this for the last several years to allow individuals and organizations to be able to ingest. And basically replace MPA and PANDAS and network X with the actual tools and our specialty for graph analytics in a data repository or a data toolkit called Arachni, the Greek word for spider.&lt;/p&gt;
&lt;p&gt;So with Aqua and Arachni. We now have dozens of analysts ingesting tens of terabytes of data and being able to manipulate those data sets in near real-time just as easily as they could on their laptops. This really makes data science accessible. And we also use supercomputers and we use advanced computing techniques. We&amp;rsquo;re using an open-source compiler from HPE called Chapel that allows very productive coding by my research group and developers. And through our hard work, others are able to just turnkey use this solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bridging the Gap Between Data Scientists and Supercomputing with AI-Enabled Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: So we made supercomputing accessible and really democratized massive-scale data analytics so that anyone anywhere has the ability to look at these great data sets that we often find in cybersecurity problems.&lt;/p&gt;
&lt;p&gt;Audra: I would agree. The question is, what you&amp;rsquo;re enabling enables people to actually understand the outcomes? Is it step by step? At the moment, people have adopted things like ChatGPT but a lot of people don&amp;rsquo;t understand what&amp;rsquo;s under the bonnet or under the hood, so to speak, in America, I dunno what the engine is.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right. So we use Python or their Jupiter notebook as their productivity portal and just as easily as they&amp;rsquo;d write a few lines in Python, maybe using MPA and Pandas to ask questions about their data or to get results, they can do that just as easily in their same notebook but connected to a Cuda, the software framework that&amp;rsquo;s open source that we built out. So any data scientist who is very comfortable in their current working situation now can use a supercomputer with Acua and Rene.&lt;/p&gt;
&lt;p&gt;The challenge is not on them for that heroic programming that&amp;rsquo;s really for us to give them the illusion that they&amp;rsquo;re running on their desktop when the massive data set is really ingested on a backend supercomputer and we use new technology to be able to route their queries to this backend supercomputer.&lt;/p&gt;
&lt;p&gt;We do all the heavy lifting for them of paralyzing the approaches, being able to target supercomputers and massively parallel processors, GPUs, and all these types of technologies so that the results come back to them on their laptops and they don&amp;rsquo;t have to see everything happening in the cloud and on the backend where we&amp;rsquo;re doing that very heavy lifting for them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling Secure Information Sharing and Collaboration in a Connected World&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rachael: That seems like a really great means of information sharing too, right? I mean, as things come up then you&amp;rsquo;re able to share that information a little more broadly and help inform others what to look out for. This thing is emerging, all of that too.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right. People can be good citizens. So as they detect vulnerabilities and as they see exploits, certainly now they can share those results and it&amp;rsquo;s no longer left to the big Fortune 100 companies that have massive cybersecurity teams and large budgets and specialized systems. Now, anyone around the world at any time can ingest all of their system logs, can ingest all of their information, their data, and be able to run on this software framework to be able to ask the same types of queries and questions as large enterprises can do.&lt;/p&gt;
&lt;p&gt;Audra: Can that actually be run in kind of a walled garden manner so that the data that you&amp;rsquo;re uploading may not want to share wider and amongst the community or hash it or do something so it&amp;rsquo;s anonymized?&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s a great question. Audra, certainly you wouldn&amp;rsquo;t want to share all of your corporate and private data with the world. The toolkits and the chapel compiler from HPE are open-source. It&amp;rsquo;s on GitHub, and all of the software for Acua and Arachni are also open source on GitHub. So any organization can download it and run it in their corporate environment or in the cloud, and they&amp;rsquo;re free to encapsulate it in their own security at their own wall gardens, or they can run it publicly if they want.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Source Data Science Tools for All&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Audra: Excellent. Now that makes it very accessible and then that actually provides the protection that you need on top of it for your data. That&amp;rsquo;s fantastic.&lt;/p&gt;
&lt;p&gt;David: In fact, we built it with open source in mind so that others could use this in any way that they would like. And also contribute as well, new analytic routines, new performance optimizations, new algorithms. But by doing it in the open source, it&amp;rsquo;s become a trusted toolkit. Anyone can view the source code and understand what it&amp;rsquo;s doing. And we make it very accessible because our goal is really to democratize the ability to analyze these massive data sets.&lt;/p&gt;
&lt;p&gt;We think for cybersecurity to really take hold in the next few decades, everyone needs to be able to understand these tools and tooling. And as we have data created in every industry, in every sector, across every part of the world, from the edge to the cloud, we&amp;rsquo;re to need tools that we can deploy to really protect data. And as you know, data is the new air data is the new oil. We&amp;rsquo;re going to have to protect everything everywhere.&lt;/p&gt;
&lt;p&gt;Rachael: Yes. Just out of curiosity, how long does it take to build something like this, David? I mean, it sounds like a massive undertaking, but so beneficial to so many in providing this really invaluable resource. But I mean, is this like six months, a year, five years? I mean, how long have you been working on this?&lt;/p&gt;
&lt;p&gt;David: We&amp;rsquo;ve been working on this since about 2019, 2020. So it&amp;rsquo;s a labor of love for the last few years, and we have had support from the National Science Foundation to build out this software framework in the last couple of years.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Source Data Science Tools for a Legacy of Data Protection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: The project is joint with the Department of Defense, and the initial vision and contributions did come from collaborations with the DoD. I want to call out one of the visionaries that I&amp;rsquo;ve worked with in creating the system, Mike Merrill. Sadly, he passed away last year, about a year ago in November. And I really want to recognize him and his vision for building the system to make these data science tools available.&lt;/p&gt;
&lt;p&gt;In fact, Mike had a brief illness and up through his last days was really trying to make sure that our Cuda had a foothold and that we were carrying on this vision that we created together to continue developing these tools. So in Mike&amp;rsquo;s memory and for his passion, I really feel personally that Mike and I developed many tools together over decades in the past, and this was really something that we wanted to carry on and be a lasting legacy to his memory.&lt;/p&gt;
&lt;p&gt;Audra: And this is an amazing legacy to be leaving behind for the whole world. That&amp;rsquo;s incredible.&lt;/p&gt;
&lt;p&gt;Rachael: Truly, because these kinds of resources could be game changers for so many organizations that just wouldn&amp;rsquo;t have access, wouldn&amp;rsquo;t be able to do any of this, and would just kind of be finding their way through. So that&amp;rsquo;s fantastic.&lt;/p&gt;
&lt;p&gt;David: Right. So we&amp;rsquo;re very glad to be able to continue developing these tools. And again, we can&amp;rsquo;t do it alone. There are contributors to the project. There&amp;rsquo;s a team that we work with and continue to build out these tools.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From Niche to Necessity: The Meteoric Rise of Data Science and AI&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: Also, I should give a shout-out to H P E, they acquired the Supercomputing company Cray, where the chapel compiler was first formed and was developed under a DOPA program about 20 years ago to make these types of activities more productive. And the team at HPE led by Michelle Strout has been really fantastic in making chapel the best that we can achieve in terms of performance and capability.&lt;/p&gt;
&lt;p&gt;Audra: That&amp;rsquo;s exciting. I thought you just loved technology. So one last question for me, considering where the world was when you started working in this kind of area, looking at machine learning and AI and that sort of thing, how much has interest gone up in the last year in your career? It&amp;rsquo;s like, here, have our money build space.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s a fantastic question. So in 2019, I moved from Georgia Tech where I founded the School of Computational Science and Engineering to the New Jersey Institute of Technology because I wanted to be in the center of where data was happening in the New York City, Newark, New Jersey metropolitan region. It&amp;rsquo;s a hub for healthcare, finance, transportation, entertainment, security, and all of these areas. And I founded the Institute for Data Science at NJIT.&lt;/p&gt;
&lt;p&gt;This was an activity that brought together centers in cybersecurity in big data, medical informatics FinTech, and other areas, which is quite an exciting area. And then in the fall of 2021, I co-founded a department of data science at NJIT, a new academic degree program so that we now have a bachelor&amp;rsquo;s, a master&amp;rsquo;s, and a PhD in data science. We thought that this would grow by a few students per year that this was an exciting growth area, and it&amp;rsquo;s really phenomenal.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Soaring Demand for Data Science Education in Today&amp;rsquo;s World&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;David: We are just increasing the enrollments at our university this fall. The university is growing while many other universities are shrinking, and a big chunk of that growth is in data science. Our programs are being sought after left and right. The Wall Street Journal just named NJIT, the number two public school in terms of its ranking system. And we are really quite excited because data science is really at the core of many sectors, many areas, and it&amp;rsquo;s a growth right now.&lt;/p&gt;
&lt;p&gt;We are seeing many students demanding courses in data science, whether they&amp;rsquo;re a data science major or whether they&amp;rsquo;re an architect or a chemist or come from any other area. We see data sciences really being critical to that education and that thought process. So in the last six months, that&amp;rsquo;s increased by giving it jet fuel right behind it. With the advent of ChatGPT and the popularization of large language models, we see no end in sight for not only the demand but also the need for those who understand data science.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s the most needed job in this region. There are just thousands upon thousands of jobs open for those who know data science and also cybersecurity. So we&amp;rsquo;re really glad to be at the nexus of training the next generation of our workforce.&lt;/p&gt;
&lt;p&gt;Rachael: Thank you. That&amp;rsquo;s exciting. So how did you get into this? This is also one of our favorite questions. How did you get your start on this path, David?&lt;/p&gt;
&lt;p&gt;From Early Acoustic Modems to Cutting-Edge Data Science Tools
David: I&amp;rsquo;ve always been interested in some of the most capable computing technologies. Back when I was three, probably in 1972 was the first time I ever touched a computer. And we used to have a terminal at home, a 10-bot acoustic modem. I was able to learn to type and transform my dad&amp;rsquo;s RAT4 programming on programming cards between mainframe computers. And then I really got interested in parallel computing and high-performance computing technologies through the early 1980s. And my interest in graph algorithms grew from that.&lt;/p&gt;
&lt;p&gt;Over time, I want to be able to develop tools that would help our nation work in national security and be able to really look at some of the most pressing grand challenges that we have in this world and solve those real-world problems. Many of them included massive data sets where we needed supercomputers and parallel computers for just the large size of the problem.&lt;/p&gt;
&lt;p&gt;And also because often we need solutions in near real-time, we want to understand what&amp;rsquo;s happening before, as I mentioned earlier, something that we want to know that there&amp;rsquo;s a bomb here before it goes off or to be able to predict today&amp;rsquo;s weather and not take 30 days to predict what&amp;rsquo;s going to happen this afternoon. And so these really converged, and my passion for the last several decades has been in trying to solve cybersecurity problems.&lt;/p&gt;
&lt;p&gt;Problems with our population health, and human genomics problems related to software development and protecting our software supply chain problems related to cancer, genomics problems related to transportation and living within sustainable urban environments, and also with the existential threat of climate change. So these are all areas that require very similar types of techniques.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unleashing Imagination through Data Science Tools&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rachael: Wow, that&amp;rsquo;s exciting. It&amp;rsquo;s kind of like you&amp;rsquo;re only limited by your imagination and what you can accomplish with data science in a way that&amp;rsquo;s exciting.&lt;/p&gt;
&lt;p&gt;David: That&amp;rsquo;s right. And I always encourage students to start playing around with tools and look at problems that excite them today. For instance, one of the undergraduates that I mentor is very passionate about data science and just mentioned to me that she wants to work in related to ERAS related to marine science and oceanography.&lt;/p&gt;
&lt;p&gt;And has just reached out to look for interesting problems where maybe she&amp;rsquo;ll be able to solve the next big problem in marine science using the skills that she&amp;rsquo;s learned in data science. And that&amp;rsquo;s really what makes me happy, seeing students who put their passions together and solve important problems for our planet.&lt;/p&gt;
&lt;p&gt;Rachael: I love to think about this next generation and their access to technology and the opportunities that are in front of them today and what they&amp;rsquo;re going to be able to do with that. And I look forward to the next 20 years. I think it&amp;rsquo;s going to be an awesome time.&lt;/p&gt;
&lt;p&gt;David: Oh, it&amp;rsquo;s going to be a fun ride, and I look forward to it as well.&lt;/p&gt;
&lt;p&gt;Rachael: Awesome. Well, Dr. David Bader, thank you so much for joining us today. This has been such a fun conversation.&lt;/p&gt;
&lt;p&gt;David: Well, great to talk with you, Rachel, and Audra. Really enjoyed the conversation.&lt;/p&gt;
&lt;p&gt;Rachael: Awesome. So to all of our listeners, thanks again for joining us this week, another amazing guest. And don&amp;rsquo;t forget to subscribe because you can get a fresh episode every Tuesday right in your inbox. So until next time, everyone. Stay safe.&lt;/p&gt;
&lt;h2 id=&#34;about-our-guest&#34;&gt;About Our Guest&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230919-forcepoint/bader_hu_ce884fae67e82472.webp 400w,
               /blog/20230919-forcepoint/bader_hu_75a2eab5a3c0ea9a.webp 760w,
               /blog/20230919-forcepoint/bader_hu_8473c1ff8f664a56.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230919-forcepoint/bader_hu_ce884fae67e82472.webp&#34;
               width=&#34;512&#34;
               height=&#34;512&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at the New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. He is a Fellow of the IEEE, ACM, AAAS, and SIAM; a recipient of the IEEE Sidney Fernbach Award; and the 2022 Innovation Hall of Fame inductee of the University of Maryland’s A. James School of Engineering.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Voting Machine Approval Widely Panned</title>
      <link>http://localhost:1313/blog/20230822-smartelections/</link>
      <pubDate>Tue, 22 Aug 2023 10:58:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230822-smartelections/</guid>
      <description>&lt;p&gt;New York City - In an Albany Board meeting in early August that looked remarkably like the Board meeting in the hit film &amp;ldquo;Barbie&amp;rdquo; and seemed as out of step with the times, the New York State Board of Elections ended the guarantee that voters in New York  can vote with pen and paper, something they&amp;rsquo;ve come to rely on for over ten years.&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-ny-state-board-of-elections-meeting-and-the-hit-film-barbie-both-have-board-meetings-with-all-white-men&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The NY State Board of Elections meeting and the hit film &amp;#39;Barbie&amp;#39; both have Board meetings with all white men.&#34; srcset=&#34;
               /blog/20230822-smartelections/pic1_hu_ca7c587dc73aea53.webp 400w,
               /blog/20230822-smartelections/pic1_hu_d6d6a39c1c95ea54.webp 760w,
               /blog/20230822-smartelections/pic1_hu_46465cafc173021a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230822-smartelections/pic1_hu_ca7c587dc73aea53.webp&#34;
               width=&#34;468&#34;
               height=&#34;125&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The NY State Board of Elections meeting and the hit film &amp;lsquo;Barbie&amp;rsquo; both have Board meetings with all white men.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The Board voted to approve the ExpressVote XL, an ATM-style, touchscreen voting system from ES&amp;amp;S, that Bloomberg News called &amp;ldquo;expensive&amp;rdquo; and &amp;ldquo;glitchy&amp;rdquo;.(1) Due to New York statutes,(2) the ExpressVote XL would eliminate pen and paper voting wherever it is used in the state.&lt;/p&gt;
&lt;p&gt;The ExpressVote XL does not produce traditional ballots, but instead has a skinny plastic case where the voter is shown a summary card similar to a cash-register receipt. Research (3) shows that most voters do not look closely at computer-generated summary cards, so experts say there is no way of knowing if they accurately reflect voters&amp;rsquo; choices. Security and auditing experts have concluded that elections held with this type of computer-generated summary card &amp;ldquo;cannot be confirmed by audits.&amp;rdquo; (4)&lt;/p&gt;
&lt;p&gt;Another concern for both experts and advocates: the machine does not count the text that&amp;rsquo;s printed on the summary card. Instead, it counts a barcode for each candidate. &amp;ldquo;The barcode-based setup &amp;lsquo;makes a mockery of the notion that the ballot is &amp;lsquo;voter-verifiable,&amp;rsquo;&amp;rsquo; (5) said Duncan Buell, a computer science professor at the University of South Carolina, because &amp;lsquo;what the voter verifies is not what is tallied.&amp;rsquo;&amp;rsquo;&amp;rsquo; Colorado has banned (6) encoding votes in barcodes. A lawsuit (7) in Arkansas seeks to do the same and one Arkansas County is already making the switch (8) to pen and paper ballots.&lt;/p&gt;
&lt;p&gt;Princeton computer science professor Andrew Appel, who has testified before Congress (9) and been featured in Politico magazine (10) for his election security research, has demonstrated a design flaw (11) in the ExpressVote XL, and other all-in-one voting machines. He says, if hacked, the machines could  print &amp;ldquo;additional votes &amp;hellip; on to the ballot that the voter did not approve.&amp;rdquo; In previous research he called it &amp;ldquo;a bad voting machine&amp;rdquo;. (12) In a 2021 letter (13) to the New York legislature he wrote, &amp;ldquo;Not only are these machines dangerous—they don&amp;rsquo;t fill any real need. Other vendors are offering better voting machines&amp;hellip;&amp;rdquo; Two new voting machine vendors that do not have these issues, HART Intercivic and Clear Ballot, were also recently approved for use in New York.&lt;/p&gt;
&lt;p&gt;Close to 60 (14) good government organizations, civil rights groups and elections experts signed a letter opposing the machine. Among their top concerns: longer wait times for the XL could lead to voter suppression. &amp;ldquo;Voters require triple the time on average to navigate ES&amp;amp;S ballot-marking machines compared to filling out hand-marked ballots,&amp;rdquo; (15) according to Pennsylvania state certification documents. (16) Due to the hefty price tag of $10,000 per machine, experts say (17) counties will be inclined to underestimate how many machines they need, and this will exacerbate long lines. States that use all-touchscreen voting have had lines as long as seven (18) to ten hours to vote. (19) Counties could also experience sticker shock from the number of machines that are needed. One estimate (20) is that it takes over five times as many machines (21), if all voters are using a touchscreen device.&lt;/p&gt;
&lt;p&gt;New York elections are expected to factor heavily in the battle for control of Congress in 2024. Redistricting put a number of New York Congressional seats into play and six districts swung (22) from Democrat to Republican in the 2022 Midterms. How New York voters cast those ballots could be critical in whether all eligible voters get to vote and whether the results inspire national confidence.&lt;/p&gt;


















&lt;figure  id=&#34;figure-expressvote-xl-the-barcode-is-counted-not-the-text&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ExpressVote XL: The barcode is counted, not the text&#34; srcset=&#34;
               /blog/20230822-smartelections/pic2_hu_cbaf6436ca4f450f.webp 400w,
               /blog/20230822-smartelections/pic2_hu_a5e529806a122294.webp 760w,
               /blog/20230822-smartelections/pic2_hu_c01bf741d49e67ac.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230822-smartelections/pic2_hu_cbaf6436ca4f450f.webp&#34;
               width=&#34;446&#34;
               height=&#34;302&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ExpressVote XL: The barcode is counted, not the text
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The ExpressVote XL has a troubled track record in the field. In one Pennsylvania county it arrived 30% out of calibration, (23) miscounted tens of thousands of votes and declared the wrong candidate the winner. (24) The Northampton County Board of Elections gave it a 100% vote of no confidence, saying &amp;ldquo;We believe the problems the machines exhibited this year will make it virtually impossible to restore voters&amp;rsquo; confidence.&amp;rdquo;  The New Jersey attorney general requested an investigation (25) into an election in Monmouth County, New Jersey, where the ExpressVote XL also counted at least one race incorrectly, and the results had to be reversed. A lawsuit in Pennsylvania (26) is seeking to decertify the machine. Dr. Stephanie Singer, former Chair of the Philadelphia County Board of Elections, advises New York election officials, &amp;ldquo;not to waste taxpayers&amp;rsquo; money on an overpriced system with a track record of miscounting votes.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Around five thousand emails (27) and  many letters (28) opposing the ExpressVote XL were sent to New York State election officials. A flurry of media coverage just prior to the vote did not sway the commissioners. Both City &amp;amp; State New York (29) and Gothamist (30) called the ExpressVote XL &amp;ldquo;controversial.&amp;rdquo; A New York Daily News editorial (31) warned, &amp;ldquo;The New York State Board of Elections Must Reject the ES&amp;amp;S ExpressVote XL Machine.&amp;rdquo; An Albany Times Union editorial (32) called the decision, &amp;ldquo;mind boggling.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Read responses from experts and good government groups below.&lt;/p&gt;
&lt;p&gt;Center for Common Ground was founded to educate and empower under-represented voters in voter suppression states to engage in elections and advocate for their right to vote.&lt;/p&gt;
&lt;p&gt;SMART Elections is a nonpartisan 501(c)3. Our goal is for U.S. elections to be secure, fair, accurate, accessible, well-administered and publicly verifiable.&lt;/p&gt;
&lt;p&gt;FAQ on all-in-one, hybrid, and universal-use voting machines.&lt;/p&gt;
&lt;h2 id=&#34;response-from-experts--election-officials&#34;&gt;RESPONSE FROM EXPERTS &amp;amp; ELECTION OFFICIALS&lt;/h2&gt;
&lt;p&gt;All of the quotes in this document represent the individual&amp;rsquo;s opinion.&lt;/p&gt;
&lt;p&gt;Affiliations are listed for identification only&lt;/p&gt;
&lt;p&gt;&amp;ldquo;50 years ago, supermarkets invented the UPC barcode and I still can&amp;rsquo;t read them. Why would New York use similar barcodes to verify our votes?&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor, New Jersey Institute of Technology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;I am very disappointed that the Board of Elections of New York saw fit to make elections less transparent and more corruptible.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chris Bystroff, Professor of Biology and Computer Science, Rensselaer Polytechnic Institute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;In a time of unprecedented suspicion about the integrity of U.S. elections, it&amp;rsquo;s more important than ever that we avoid voting systems that reduce transparency. Bar codes are not transparent.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;David L. Dill, Professor, Emeritus, in the School of Engineering, Stanford University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;&amp;hellip;these all-in-one devices pose  extraordinary risks for the voters of New York and I urge you to reject their use in favor  of the secure hand-marked ballot systems currently used throughout the state.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Richard DeMillo, Professor, Charlotte B. and Roger C. Warren Chair of Computing, College of Computing, Georgia Tech (letter to the New York State Board of Elections)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1-YSa4JpWTTOY08j5B6bfu1PQEMzJl8ivuoYx9Jx60aY/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.google.com/document/d/1-YSa4JpWTTOY08j5B6bfu1PQEMzJl8ivuoYx9Jx60aY/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It makes no sense to spend ten thousand dollars on computerized ballot marking devices that do the work of 99-cent pens. In fact pens are better than BMDs for most voters because they can&amp;rsquo;t record votes that voters don&amp;rsquo;t intend.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;David Jefferson, Computer Scientist, Lawrence Livermore National Laboratory (retired)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even with the aid of a computer and the Internet, nothing on that ballot lets me (or the vast majority of New York voters) verify that the bar codes reflect my intended vote. This is worse than the old Voteomatic punch-card voting machine made famous by election 2000 in Florida.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Douglas W. Jones, Emeritus Associate Professor of Computer Science, University of Iowa&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Voters have the right to see their votes recorded accurately on their ballot.  NY should not be asking voters to trust that the vendor&amp;rsquo;s barcode software has accurately translated and represented their votes. Neither NY State nor the voters can discern if the votes are accurately recorded and tabulated, which is unacceptable.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Candice Hoke, Founding Director, Center for Cybersecurity &amp;amp; Privacy Protection, Cleveland State University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The safest, most secure and accurate system for recording votes is inserting hand-marked paper ballots into ballot scanners.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Douglas A. Kellner, Co-Chair, New York State Board of Elections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Counties should deploy anything other than this expensive, vulnerable ES&amp;amp;S system, which in NY requires several times as many machines per voter, and which thus requires far more effort to deploy, test before each election, deliver to polling places, and upgrade for the inevitable security fixes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Neal McBurnett, Independent Security and Election Auditing Consultant&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;As an intelligence specialist with four decades of experience, including 29 as an Army intelligence officer, I find myself in agreement with noted election security experts: Professors Appel, Halderman, Stark, and Jones when they say elections are most secure on hand-marked paper ballots. In my opinion, the use of barcodes in the tabulation process is an extreme vulnerability and the ballot marking devices cannot ensure the will of the voter. That&amp;rsquo;s why I condemn the NY governing body&amp;rsquo;s decision to force voters to use touchscreens over hand-marked paper ballots.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Colonel Conrad Reynolds (US Army RET.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Given the current political setting, what they have done is not only of questionable legality, it is contrary to the role they should be playing - building voter confidence in the integrity of the system.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Arthur Z. Schwartz, Esq., Principal Attorney Advocates for Justice Chartered Attorneys, Political Director- New York Progressive Action Network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Having written the election recount section of Goldfeder&amp;rsquo;s Modern Election Law, I cannot imagine having to recount an election where you don&amp;rsquo;t actually see the piece of paper touched by the voter.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Steven R. Schlesinger, Esq.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;[A] Ballot-marking device printout is a record of what the machines did, not what the voters did&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Philip Stark, Distinguished Professor, University of California, Berkeley&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;response-from-civil-rights-groups&#34;&gt;RESPONSE FROM CIVIL RIGHTS GROUPS &lt;/h2&gt;
&lt;p&gt;&amp;ldquo;Who do these voting machines benefit? Not people of color. Not the elderly waiting in line. Not the many individuals who struggle with this new technology. They will be disenfranchised yet again. We are moving forward, but when we move forward we cannot leave our beloved communities behind. That is not Justice. That is not Democracy.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reverend Dr. Cardes Brown, President, Justice Coalition USA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;We are deeply saddened and furious with the approval of the ExpressVote XL. From our perspective, the ExpressVote XL does not benefit voters and actually kills New York Elections. Our fight right now is to eradicate voter suppression by making voting and elections more inclusive. Research shows the ExpressVote XL does exactly the opposite of that. For far too long, Black communities and communities of color have had their voices rejected and neglected. This is our mantra, and we invite others to say it with us in solidarity: &amp;lsquo;The ExpressVote XL must not ever see the light of day! We must think about the generations of Black communities before us that have had numerous intimidation tactics used against them to stop their voices from being heard and this is no different.&amp;rsquo; Shame on New York Election Officials for the approval of the ExpressVote XL!&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Victoria Elias, Common Power&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;We seem to have learned nothing from the 2020 election. We must protect the vote. Not having a pen and paper backup to voting machines makes no sense. Shame on the NYSBOE. Voters of color beware!&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bertha Lewis, Founder &amp;amp; President, The Black Institute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;I am disappointed that the New York State Board of Election voted to move New York towards becoming a voter suppression state by approving ExpressVote XL.&amp;rdquo; - Andrea Miller, Founding Board Member &amp;amp; Executive Director, Center for Common Ground&lt;/p&gt;
&lt;h2 id=&#34;response-from-disability-advocates&#34;&gt;RESPONSE FROM DISABILITY ADVOCATES&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;We as disabled individuals STILL reserve the right to vote and SHOULD NOT be left out PLAIN AND SIMPLE!  We are still people whose votes must count.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mark M. Shaw, Psychotherapist; Niagara Falls Human Rights Commission&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;I&amp;rsquo;m concerned that people won&amp;rsquo;t be able to vote with a pen and paper. This will be overwhelming to a lot of people. Especially those with disabilities. Also, I don&amp;rsquo;t know anyone that can read barcode. How can we verify that our ballot is what we want it to be?&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Michael Ring&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;response-from-good-government--community-groups&#34;&gt;RESPONSE FROM GOOD GOVERNMENT &amp;amp; COMMUNITY GROUPS&lt;/h2&gt;
&lt;p&gt;New York&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I was an election inspector in Westchester for 20 years. Voting procedures and tools should be clear and simple, and give the voter confidence that their vote will be counted as they intended. ExpressVote XL does not meet this standard.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frank Brodhead, Concerned Families of Westchester&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a sad day in the future of NY elections &amp;hellip; It&amp;rsquo;s amazing that two Republican commissioners approved a flawed voting machine that in our opinion can negatively influence the chances of Republican candidates from winning future elections while, further eroding public trust&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Edwin De La Cruz, Northern Manhattan Republicans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;If the New York Board of Elections is so confident the ExpressVote XL can&amp;rsquo;t be hacked, will they donate one to DEFCON for independent security researchers to test?&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Laura Forman, Dutchess County Progressive Action Alliance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;I&amp;rsquo;m furious! When there are already so many other pressing issues, now we&amp;rsquo;ve got to organize and insist our local Boards of Elections don&amp;rsquo;t purchase these machines.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cari Gardner, Vice Chair NYPAN (New York Progressive Action Network)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The XL fails the essential element for New York elections: a verifiable paper ballot that serves as the backup for recounts.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jonathan Geballe, President, Village Independent Democrats&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The BOE must reverse this terrible decision so that the voting public can be sure our elections are fair, secure, and protected from malicious actors foreign and domestic. No democracy, no planet!&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Green Sanctuary Team, First Unitarian Universalist Society of Albany NY&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Free and honest elections are the very foundation of our republican form of government. Hence any attempt to defile the sanctity of the ballot cannot be viewed with equanimity.&amp;rdquo; United States v. Classic, 313 U.S. 299 (1941)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Marly Hornik, Executive Director, NY Citizens Audit&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Pay more, wait longer, and be more worried about whether your vote will count!&amp;rdquo; (New slogan from the NY State Board of Elections.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nada Khader, Executive Director, WESPAC Foundation, Inc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;In the current environment surrounding elections &amp;hellip; it is beyond comprehension that the NYS Board of Elections would approve a voting system that provides no assurance to New Yorkers that election results reflect the actual votes cast by voters. Voters have no ability to confirm that the ExpressVote XL&amp;rsquo;s barcode software has accurately recorded their votes. This is a fundamental flaw that should have disqualified these machines.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;-Alex Margolis, East End Action Network&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a shame that New York is downgrading to a very expensive machine that doesn&amp;rsquo;t use hand-marked paper ballots. For over ten years, we&amp;rsquo;ve had hand-marked paper ballots because they are the gold standard. We will pay the gold price, but won&amp;rsquo;t get the gold standard.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deborah Porder, Co-Chair, New York Democratic Lawyers Council Legislative Affairs Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The public cannot read bar codes such as those the ExpressVote XL uses to count votes. They are not transparent and, if used, expose our elections to questions. NY must do better.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jim Soper, Co-Chair, National Voting Rights Task Force&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Now it&amp;rsquo;s up to the county boards of elections to get well educated on the deficiencies of hybrid voting machines and choose other safer, less expensive and more efficient systems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mary Thorpe, Director, NYPAN (New York Progressive Action Network) of the Southern Finger Lakes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;response-from-good-government--community-groups-1&#34;&gt;RESPONSE FROM GOOD GOVERNMENT &amp;amp; COMMUNITY GROUPS&lt;/h2&gt;
&lt;p&gt;National&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We cannot find one good reason for New York to adopt the ExpressVoteXL device rather than use trusted, tested voting devices. We have to conclude that the people who approved this change either haven&amp;rsquo;t taken the time to fully investigate the decision, or arrogantly believe that New York&amp;rsquo;s systems are somehow not subject to the same serious pitfalls that others have experienced. We fear they are in for a rude awakening.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jan BenDor, Statewide Coordinator, Michigan Election Reform Alliance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Listen to the experts &amp;ndash; not the vendors.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stephanie Chaplin, Secure Elections Network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;We are profoundly disappointed that the New York State Board of Elections is choosing to ignore the laws of New York State, which require that voters have the opportunity to &amp;lsquo;verify votes selected.&amp;rsquo;  How can they verify a barcode?&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lulu Friesdat, Co-Founder &amp;amp; Executive Director, SMART Elections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;It is our belief that the Expressvote XL will further destroy confidence in election systems because of its security vulnerabilities, the loss of the ability to meaningfully audit, and the fact that the vote is encoded in a barcode and the voter cannot  verify what is being counted. We join in opposing the adoption of the Expressvote XL.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lori Gallagher, Executive Director, TallyTexas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;In Philadelphia we could have had a far more trustworthy voting system for $15 million less. We hope counties in New York don&amp;rsquo;t make the same mistake.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rich Garella, Co-founder, Protect Our Vote Philly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;With this decision, the NY State BOE has utterly failed in its responsibility to protect the public interest.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Emily Levy, Executive Director, Scrutineers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;These combo machines pose special hacking hazards, make verification by voters difficult, and make it impossible to know if voters were shown all the candidates on the ballot in their private session. All these issues do not exist with hand-marked paper ballots.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;-Ray Lutz, Citizens Oversight, Developer of AuditEngine&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Any New York county considering adopting these machines should first study Georgia&amp;rsquo;s 2019 disastrously expensive and roundly criticized move to touchscreen BMDs. This is simply the wrong direction for security and fiscal reasons.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Marilyn Marks, Executive Director, Coalition for Good Governance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Introducing a voting system that neither experts nor voters can trust sends the message that voters don&amp;rsquo;t matter.  Voters do matter. Their trust in you matters.  Now is the time to earn that trust again. We hope to see counties rejecting the ExpressVote XL.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jed Pauker for LACVAC LA County Voters Action Coalition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The ES&amp;amp;S ExpressVote XL discriminates against and excludes voters with cognitive disabilities and many other voters who find it difficult or impossible to use a computer, or have never used a computer. This issue is very personal to me because I have a brother who has a cognitive disability.  He can easily mark the ovals on a hand-marked paper ballot but he does not own or want a computer and there is no way he could ever use the ExpressVote XL. There is no justification for approving an in-person voting system that many voters cannot use. This is yet another reason why the ExpressVote XL should never be used in New York (or anywhere else).&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Susan Pynchon, Director, Florida Fair Elections Coalition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;The ExpressVote XL is estimated to cost $10,000 per machine. That is more than twice the cost of one of the competing ballot-markers. It&amp;rsquo;s a repulsive waste of taxpayer money.&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Michele Sutter, Co-Founder &amp;amp; Director, MOVI (Money Out Voters In)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;Democracy all over America is hanging on a precipice and these dereliction-of-duty actions could help push it over the edge. Shame on the New York State Board of Elections!&amp;rdquo;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jennifer Tanner, Director, Validate the Vote USA&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;FOOTNOTES&lt;/h2&gt;
&lt;p&gt;1 Kartikay Mehrotra and Margaret Newkirk, Expensive, Glitchy Voting Machines Expose 2020 Hacking Risks, Bloomberg News, Nov 8, 2019 &lt;a href=&#34;https://www.bnnbloomberg.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bnnbloomberg.ca/&lt;/a&gt; expensive-glitchy-voting- machines-expose-2020-hacking- risks-1.1345434&lt;/p&gt;
&lt;p&gt;2 Election Ballot § 7-104, 26. (p. 255), New York Election Law, &amp;ldquo;All paper ballots of the same kind for the same polling place shall be identical.&amp;rdquo; &lt;a href=&#34;https://www.elections.ny.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.elections.ny.gov/&lt;/a&gt; NYSBOE/download/law/ 2023ElectionLaw.pdf&lt;/p&gt;
&lt;p&gt;3 Matthew Bernhard, Allison McDonald, Henry Meng, Jensen Hwa, Nakul Bajaj, Kevin Chang, J. Alex Halderman, Can Voters Detect Malicious Manipulation of Ballot Marking Devices? University of Michigan and The Harker School &lt;a href=&#34;https://jhalderm.com/pub/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://jhalderm.com/pub/&lt;/a&gt; papers/bmd-verifiability-sp20. pdf&lt;/p&gt;
&lt;p&gt;4 Andrew W. Appel, Richard A. DeMillo, and Philip B. Stark, Ballot-Marking Devices Cannot Assure the Will of the Voters, Election Law Journal, vol. 19 no. 3, pp. 432-450, September 2020 &lt;a href=&#34;https://www.liebertpub.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.liebertpub.com/&lt;/a&gt; doi/10.1089/elj.2019.0619 Free version &lt;a href=&#34;https://www.cs.princeton.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.princeton.edu/&lt;/a&gt;~ appel/papers/bmd-insecure.pdf&lt;/p&gt;
&lt;p&gt;5 Eric Geller, State election officials opt for 2020 voting machines vulnerable to hacking, Politico, 03/1/2019 &lt;a href=&#34;https://www.politico.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.politico.com/&lt;/a&gt; story/2019/03/01/election- vulnerable-voting-machines- 1198780&lt;/p&gt;
&lt;p&gt;6 Kevin Collier, First on CNN: Colorado becomes first state to ban barcodes for counting votes over security concerns, CNN, 9/16/19 &lt;a href=&#34;https://www.cnn.com/2019/09/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnn.com/2019/09/&lt;/a&gt; 16/politics/colorado-qr-codes- votes/index.html&lt;/p&gt;
&lt;p&gt;7 Circuit court of Pulaski County, Arkansas, Arkansas Voter Integrity Initiative, Inc., Conrad Reynolds, and Donnie Scroggins Plaintiff vs. John Thurston, in his official capacity as Secretary of State, the State Board of Election Commissioners, in its official capacity, and Election Systems and Software, LLC, defendants, Case No: 60CV-23&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/&lt;/a&gt; d/ 1Pfgw0piY27smIp09puDqhqUMJI79e TGl/view?usp=drive_link&lt;/p&gt;
&lt;p&gt;8 Press Release, Arkansas Voter Integrity Initiative, 8/14/23 &lt;a href=&#34;https://us8.campaign&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://us8.campaign&lt;/a&gt;-archive. com/?u= ea2e12c9a40ed8d777be356d9&amp;amp;id= d14bf74abf&lt;/p&gt;
&lt;p&gt;9 House Oversight Committee - Election Cybersecurity, C-SPAN, 9/28/16 &lt;a href=&#34;https://www.c-span.org/video/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.c-span.org/video/&lt;/a&gt;? 415978-1/hse-oversight- election-cybersecurity-part&lt;/p&gt;
&lt;p&gt;10 Ben Wofford, How to Hack an Election in 7 Minutes, Politico, 8/5/16 &lt;a href=&#34;https://www.politico.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.politico.com/&lt;/a&gt; magazine/story/2016/08/2016- elections-russia-hack-how-to- hack-an-election-in-seven- minutes-214144/&lt;/p&gt;
&lt;p&gt;11 Andrew Appel, ExpressVote XL &amp;ldquo;fix&amp;rdquo; doesn&amp;rsquo;t fix anything, Freedom to Tinker, Research and commentary on digital technologies in public life, 7/28/23 &lt;a href=&#34;https://freedom-to-tinker.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://freedom-to-tinker.com/&lt;/a&gt; 2023/07/28/expressvote-xl-fix- doesnt-fix-anything/&lt;/p&gt;
&lt;p&gt;12 Andrew Appel, ESS voting machine company sends threats, Freedom to Tinker, Research and commentary on digital technologies in public life, 1/11/21https://freedom-to- tinker.com/2021/01/11/ess- voting-machine-company-sends- threats/&lt;/p&gt;
&lt;p&gt;13 Andrew Appel, Letter to the New York Legislature, 6/4/21 &lt;a href=&#34;https://docs.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://docs.google.com/&lt;/a&gt; document/d/ 1rd00PeXYgqMHGW0UcfnUtl839QQZm -eQbyRm20pMMt0/edit?usp= sharing&lt;/p&gt;
&lt;p&gt;14 Letter to the New York State Board of Elections, 7/31/23 &lt;a href=&#34;https://img1.wsimg.com/blobby/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://img1.wsimg.com/blobby/&lt;/a&gt; go/5275a097-faa2-4d46-8f25- 54b36ea675b1/Expert-Org% 20Letter%20Final%20We% 20Oppose%20Approval%20of%20. pdf&lt;/p&gt;
&lt;p&gt;15 Frank Bajak, Reliability of pricey new voting machines questioned, Associated Press via PBS, 2/3/2020 &lt;a href=&#34;https://www.pbs.org/newshour/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.pbs.org/newshour/&lt;/a&gt; politics/reliability-of- pricey-new-voting-machines- questioned&lt;/p&gt;
&lt;p&gt;16 Pennsylvania Department of State, Electronic Voting Systems, Voting Systems by County, &lt;a href=&#34;https://www.dos.pa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.dos.pa.gov/&lt;/a&gt; VotingElections/ OtherServicesEvents/Pages/ Voting-Systems.aspx&lt;/p&gt;
&lt;p&gt;View Election Systems &amp;amp; Software&amp;rsquo;s EVS 6.0.3.0 report 7/28/20: &amp;ldquo;The ExpressVote XL and ExpressVote 2.1 can accommodate 10-12 voters with disabilities per hour or 20-60 voters per hour when used as the primary voting system &amp;hellip; DS200 can serve 120-180 voters per hour.&amp;rdquo; (P. 34)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.dos.pa.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.dos.pa.gov/&lt;/a&gt; VotingElections/Documents/ Voting%20Systems/ESS% 20EVS6030/EVS%206030% 20Secretarys%20certification% 20report%20Final%20with% 20attachments%2008312020.pdf&lt;/p&gt;
&lt;p&gt;17 Charles Stewart III and Stephen Ansolabehere, Waiting in Line to Vote, U.S. Election Assistance Commission, (P.12) Published 7/28/13, Posted by EAC 2/24/17 &amp;quot; &amp;hellip; because of the high unit cost of DREs, compared to the unit cost of privacy booths to mark paper ballots, local election offices that use DREs may find it more expensive to expand capacity in response to an anticipated surge in turnout.&amp;rdquo; &lt;a href=&#34;https://drive.google.com/file/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/&lt;/a&gt; d/1EfbXlEZwB- xe29LNGua07bmCse1-DENT/view? usp=drive_link&lt;/p&gt;
&lt;p&gt;18 Alexa Ura, Harris County&amp;rsquo;s cascade of election day fumbles disproportionately affected communities of color, The Texas Tribune, 3/4/20 &lt;a href=&#34;https://www.texastribune.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.texastribune.org/&lt;/a&gt; 2020/03/04/harris-countys- texas-southern-university- voting-delays-what-happened/&lt;/p&gt;
&lt;p&gt;19 Sam Levine and agencies, More than 10-hour wait and long lines as early voting starts in Georgia, The Guardian, 10/12/20 &lt;a href=&#34;https://www.theguardian.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.theguardian.com/&lt;/a&gt; us-news/2020/oct/13/more-than- 10-hour-wait-and-long-lines- as-early-voting-starts-in- georgia&lt;/p&gt;
&lt;p&gt;20 Edward Perez, Georgia State Election Technology Acquisition, A Reality Check, OSET Institute, Inc., March 2019 &lt;a href=&#34;https://trustthevote.org/wp-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://trustthevote.org/wp-&lt;/a&gt; content/uploads/2019/03/ 06Mar19-OSETBriefing_ GeorgiaSystemsCostAnalysis.pdf&lt;/p&gt;
&lt;p&gt;21 Ibid&lt;/p&gt;
&lt;p&gt;22 Edward-Isaac Dovere, Hakeem Jeffries is staging a takeover of the New York Democrats. His hope to become Speaker may depend on it, CNN, 6/28/23 &lt;a href=&#34;https://www.cnn.com/2023/06/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnn.com/2023/06/&lt;/a&gt; 28/politics/hakeem-jeffries- takeover-new-york-democrats/ index.html&lt;/p&gt;
&lt;p&gt;23 Tom Shortell, No confidence: Northampton County election board &amp;rsquo;extremely disappointed in machines it selected, The Morning Call&lt;/p&gt;
&lt;p&gt;12/20/19 &lt;a href=&#34;https://www.mcall.com/2019/12/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.mcall.com/2019/12/&lt;/a&gt; 19/no-confidence-northampton- county-election-board- extremely-disappointed-in- machines-it-selected/&lt;/p&gt;
&lt;p&gt;Free version: &lt;a href=&#34;https://drive.google.com/file/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/&lt;/a&gt; d/ 1CuCBO8L9MPt9olGEfIq6ktxRgbl3X gej/view?usp=drive_link&lt;/p&gt;
&lt;p&gt;24 Nick Corasaniti, A Pennsylvania County&amp;rsquo;s Election Day Nightmare Underscores Voting Machine Concerns, How &amp;ldquo;everything went wrong&amp;rdquo; in Northampton County, The New York Times, 11/30/19 &lt;a href=&#34;https://www.nytimes.com/2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nytimes.com/2019/&lt;/a&gt; 11/30/us/politics/ pennsylvania-voting-machines. html&lt;/p&gt;
&lt;p&gt;25 Dan Radel, NJ investigates Monmouth County election after double counting may have flipped one race, Asbury Park Press, 1/24/23 &lt;a href=&#34;https://www.app.com/story/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.app.com/story/&lt;/a&gt; news/politics/elections/2023/ 01/24/nj-election-2022- monmouth-county-double-count- votes/69835977007/&lt;/p&gt;
&lt;p&gt;26 National Election Defense Coalition v. Boockvar, Commonwealth Court of Pennsylvania, Docket No.674 MD 2019, Status:Preliminary objections (motion to dismiss) denied https://freespeechforpeople. org/national-election-defense- coalition-v-boockvar/&lt;/p&gt;
&lt;p&gt;27 Stop ExpressVote XL Certification in New York, Action Network Campaign sponsored by SMART Elections &lt;a href=&#34;https://actionnetwork.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://actionnetwork.org/&lt;/a&gt; letters/stop-expressvote-xl- certification-in-new-york-3&lt;/p&gt;
&lt;p&gt;28 Letter to elected and election officials from SMART Elections, 7/31/23 &lt;a href=&#34;https://img1.wsimg.com/blobby/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://img1.wsimg.com/blobby/&lt;/a&gt; go/5275a097-faa2-4d46-8f25- 54b36ea675b1/SMART% 20Elections%20Against%20the% 20ExpressVote%20XL%20202.pdf&lt;/p&gt;
&lt;p&gt;Other letters opposing the approval of the ExpressVote XL were sent by good government groups: Verified Voting, Let NY Vote, Common Cause, Free Speech for People, and Public Citizen.&lt;/p&gt;
&lt;p&gt;Verified Voting letter: &lt;a href=&#34;https://verifiedvoting.org/wp-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://verifiedvoting.org/wp-&lt;/a&gt; content/uploads/2023/07/ Verified-Voting-Comments_ NYSBOE_ExpressVote-XL_July- 2023.pdf&lt;/p&gt;
&lt;p&gt;Let NY Vote letter: &lt;a href=&#34;https://www.commoncause.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.commoncause.org/&lt;/a&gt; new-york/wp-content/uploads/ sites/20/2023/07/Opposition- letter-from-LNYV.pdf&lt;/p&gt;
&lt;p&gt;Common Cause, Free Speech for People and Public Citizen letter: &lt;a href=&#34;https://www.commoncause.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.commoncause.org/&lt;/a&gt; new-york/wp-content/uploads/ sites/20/2023/07/Letter-of- Opposition-CCNY-and-National- Groups.pdf&lt;/p&gt;
&lt;p&gt;29 Rebecca C. Lewis and Shantel Destra, New York BOE expected to vote on the use of highly controversial electronic voting machines, Watchdogs have raised concerns about the machines&amp;rsquo; vulnerability to glitches and hacks and say paper ballots are still the most secure, City &amp;amp; State, New York, 7/31/23 &lt;a href=&#34;https://www.cityandstateny&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cityandstateny&lt;/a&gt;. com/policy/2023/07/new-york- boe-expected-vote-use-highly- controversial-electronic- voting-machines/388978/&lt;/p&gt;
&lt;p&gt;30 Brigid Bergin, New Yorkers could start voting on controversial touchscreen machines, as vote goes to board, Gothamist, A non-profit newsroom, powered by WNYC, 7/31/23 &lt;a href=&#34;https://gothamist.com/news/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gothamist.com/news/&lt;/a&gt; new-yorkers-could-start- voting-on-controversial- touchscreen-machines-as-vote- goes-to-board&lt;/p&gt;
&lt;p&gt;31 Daily News Editorial Board, Paper ballots are a must: The NYS Board of Elections must reject the ES&amp;amp;S ExpressVote XL machine, The Daily News, 8/2/23 &lt;a href=&#34;https://www.nydailynews.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nydailynews.com/&lt;/a&gt; opinion/ny-edit-paper-ballots- ess-expressvote-xl-20230802- rfuhtwd3cvaxdkjvjhzouff7ku- story.html&lt;/p&gt;
&lt;p&gt;Free version: &lt;a href=&#34;https://drive.google.com/file/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/&lt;/a&gt; d/1JuZt- qU3CrKCzx50EYz0pvbH4gGsXxKG/ view&lt;/p&gt;
&lt;p&gt;32 Times Union Editorial Board, Editorial: No vote of confidence on touchpad voting system, Albany Times Union, 8/4/23 &lt;a href=&#34;https://www.timesunion.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.timesunion.com/&lt;/a&gt; opinion/article/editorial-no- vote-confidence-18278049.php&lt;/p&gt;
&lt;p&gt;Free version: &lt;a href=&#34;https://drive.google.com/file/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://drive.google.com/file/&lt;/a&gt; d/1AqLZMjIMlouXKJjYyIhzD- kdkQ7JcVM2/view&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PBS State of Affairs with Steve Adubato</title>
      <link>http://localhost:1313/blog/20230819-pbs/</link>
      <pubDate>Sat, 19 Aug 2023 11:38:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230819-pbs/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/6Pd-q1-95sM?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, Distinguished Professor and Director of NJIT&amp;rsquo;s Institute for Data Science discusses the future of artificial intelligence and how this technology will impact the workforce, democracy, and our everyday lives.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njpbs.org/programs/one-on-one-with-steve-adubato/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.njpbs.org/programs/one-on-one-with-steve-adubato/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What CISOs need to know to mitigate quantum computing risks</title>
      <link>http://localhost:1313/blog/20230605-security/</link>
      <pubDate>Fri, 02 Jun 2023 15:32:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230605-security/</guid>
      <description>&lt;p&gt;&lt;strong&gt;By &lt;a href=&#34;https://www.securitymagazine.com/authors/4442-david-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-image-via-unsplash&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image via Unsplash&#34; srcset=&#34;
               /blog/20230605-security/anton-maksimov-5642-su-qM37iptlCNY-unsplash_hu_4108c4280092c72f.webp 400w,
               /blog/20230605-security/anton-maksimov-5642-su-qM37iptlCNY-unsplash_hu_5f1e08398fd8bed9.webp 760w,
               /blog/20230605-security/anton-maksimov-5642-su-qM37iptlCNY-unsplash_hu_710e2bcb75261aac.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230605-security/anton-maksimov-5642-su-qM37iptlCNY-unsplash_hu_4108c4280092c72f.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Image via Unsplash
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Quantum technologies harness the laws of quantum mechanics to solve complex problems beyond the capabilities of classical computers. Although quantum computing can one day lead to positive and transformative solutions for complex global issues, the development of these technologies also poses a significant and emerging threat to cybersecurity infrastructure for organizations.&lt;/p&gt;
&lt;p&gt;Chief Information Security Officers (CISOs) should not take the impending security threat of quantum computing lightly. To mitigate future risks of quantum attacks, CISOs will need to start identifying where to immediately shore up infrastructure vulnerabilities and developing a longer-term strategic plan to be cyber ready.&lt;/p&gt;
&lt;h2 id=&#34;quantum-computing-a-security-threat&#34;&gt;Quantum computing: A security threat&lt;/h2&gt;
&lt;p&gt;Most modern encryption security systems protect sensitive information by relying on the assumed difficulty of factoring a notably large integer (exceeding a thousand or more digits) into its prime factors. Although algorithms capable of finding solutions do exist, using classical computers to solve and identify the two large prime numbers proves to be extremely difficult. By classical computing standards and even with the most capable supercomputers, finding solutions requires exceedingly long running time and are intractable problems.&lt;/p&gt;
&lt;p&gt;However, quantum computers may be able to solve some of the problems that are now considered practically impossible due to processing time. While it is a type of parallel processing exhibited by classical supercomputing and accelerators such as GPUs and FPGAs, quantum computers hold the ability to work with data in different dimensions at once, performing many complex calculations simultaneously with increased processing power. There are immense consequences for data decryption, as quantum computing could accelerate reading and unencrypting data all at once, posing a potential threat to sensitive military communications, corporate trade secrets and intellectual property.&lt;/p&gt;
&lt;h2 id=&#34;preparing-for-q-day-post-quantum-cryptographic-standards&#34;&gt;Preparing for Q-Day: Post-quantum cryptographic standards&lt;/h2&gt;
&lt;p&gt;In the short-term, the security threat of quantum computing is real yet relatively low since the technologies have not yet been successfully developed at sufficiently large quantum machine sizes (in terms of qubits) to break today’s encryption. But these machines are coming: countries and organizations already understand the urgency to prepare in advance for Q-Day: the day when large quantum computers can successfully crack public key encryption systems.&lt;/p&gt;
&lt;p&gt;While the loss of any sensitive data is disastrous, the advancement of large-scale quantum computing puts even stolen encrypted data at risk of becoming exposed. Bad actors, primarily nations, can take a “Harvest Now, Decrypt Later” approach, harvesting enough encrypted data and waiting for a breakthrough in decryption technology.&lt;/p&gt;
&lt;p&gt;Since 2016, the National Institute of Standards and Technology (NIST) has been soliciting encryption algorithms that will be resistant to quantum computers. In 2022, NIST announced that four encryption algorithms were selected and will be incorporated in the agency’s post-quantum cryptographic standard, which is expected to be finalized around 2024.&lt;/p&gt;
&lt;p&gt;Although the general public won’t notice the shift in cryptographic standards, the transition from public key cryptography to post-quantum cryptography will become a massive undertaking by the U.S. government and lay the foundation for significant systematic change to private communications. Expect the transition to start with U.S. military adoption, as researchers start to understand the best practices and best-of-breed offerings for certification, deployment, as well as server- and client-side encryption and decryption.&lt;/p&gt;
&lt;h2 id=&#34;how-cisos-can-mitigate-future-risks&#34;&gt;How CISOs can mitigate future risks&lt;/h2&gt;
&lt;p&gt;Although the migration process to quantum-safe cryptographic standards will take time on all levels, it is crucial for CISOs to start developing a careful management plan now. This will require careful planning to deploy new encryption methods in a staged fashion and carefully transitioning both internal systems and external customers to these new methods without disrupting the business.&lt;/p&gt;
&lt;h3 id=&#34;1-survey-and-scrutinize-existing-infrastructure&#34;&gt;1. Survey and scrutinize existing infrastructure&lt;/h3&gt;
&lt;p&gt;CISOs should have commanding knowledge of their IT infrastructure, external APIs and cloud services to understand all of their organization’s internal and external data movements. To identify potential weak points and protect their data from external threats, they’ll need to survey their organizations to determine instances of public-key algorithm use in their networks.&lt;/p&gt;
&lt;p&gt;Be keenly aware of potential weak points in the infrastructure’s computer and communications hardware, operating systems, application programs, communications protocols, key infrastructures and access control mechanisms. As organizations became increasingly dependent on off-premise solutions, take stock of which critical services have shifted to cloud providers. Non-host devices on their networks such as routers, printers and even soda machines can also be weak points for attacks.&lt;/p&gt;
&lt;h3 id=&#34;2-create-or-update-security-policies-using-best-practices-of-the-industry&#34;&gt;2. Create or update security policies using best practices of the industry&lt;/h3&gt;
&lt;p&gt;Protecting an organization from future threats will require using best practices and taking defensive measures, making sure data is encrypted as it should be and having plans to prevent external threats from exfiltrating data. Best practices include: knowing the internal cyberinfrastructure, conducting regular security testing, training the workforce to be smart security personnel and formulating incident response plans.&lt;/p&gt;
&lt;h3 id=&#34;3-build-an-organizations-deployment-plan-to-be-quantum-cyber-ready&#34;&gt;3. Build an organization’s deployment plan to be quantum cyber-ready&lt;/h3&gt;
&lt;p&gt;CISOs are navigating uncharted territory abound with potential catastrophic mistakes in choosing encryption software. During this time, officers will need to prioritize deployment, to carefully evaluate the risks for a potential multi-year deployment and to learn emerging best practices. While an organization’s budget can be a limitation, it is not recommended to choose the cheapest solution to simply save costs — these may not be trusted yet.&lt;/p&gt;
&lt;p&gt;As the rest of world prepares for Q-day, it will also be important to follow best practices of the industry. Look for guidance from leading organizations and their implementation plans, including what solutions are acquired and how they are deployed, and take a collaborative learning approach with other industry stakeholders.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article originally ran in &lt;strong&gt;Today’s Cybersecurity Leader&lt;/strong&gt;, a monthly cybersecurity-focused eNewsletter for security end users, brought to you by &lt;strong&gt;Security&lt;/strong&gt; magazine.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.securitymagazine.com/articles/99434-what-cisos-need-to-know-to-mitigate-quantum-computing-risks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.securitymagazine.com/articles/99434-what-cisos-need-to-know-to-mitigate-quantum-computing-risks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>This New AI Brain Decoder Could Be A Privacy Nightmare, Experts Say</title>
      <link>http://localhost:1313/blog/20230504-lifewire/</link>
      <pubDate>Thu, 04 May 2023 21:30:25 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230504-lifewire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.lifewire.com/sascha-brodsky-5075267&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sascha Brodsky&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Artificial intelligence (AI) lets researchers interpret human thoughts, and the technology is sparking privacy concerns. The &lt;a href=&#34;https://cns.utexas.edu/news/podcast/brain-activity-decoder-can-reveal-stories-peoples-minds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new system&lt;/a&gt; can translate a person’s brain activity while listening to a story into a continuous stream of text. It’s meant to help people who can’t speak, such as those debilitated by strokes, to communicate. But there are concerns that the same techniques could one day be used to invade thoughts.&lt;/p&gt;
&lt;p&gt;“The time to think about how to appropriately protect mental privacy is now,” &lt;a href=&#34;https://medicine.yale.edu/profile/jennifer-e-miller/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jennifer Miller&lt;/a&gt;, a professor who studies ethics and medicine at the &lt;a href=&#34;https://medicine.yale.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yale School of Medicine&lt;/a&gt;, told Lifewire in an email interview. “It is best to engineer privacy protections into a technology at the outset rather than as an afterthought.”&lt;/p&gt;
&lt;h2 id=&#34;an-ai-view-of-brain-activity&#34;&gt;An AI View of Brain Activity&lt;/h2&gt;
&lt;p&gt;A recent study published in the journal &lt;a href=&#34;https://www.nature.com/articles/s41593-023-01304-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Nature Neuroscience&lt;/em&gt;&lt;/a&gt; described the new AI system called a semantic decoder. The work gets help from a transformer model similar to the ones that power Open AI’s ChatGPT and Google’s Bard.&lt;/p&gt;
&lt;p&gt;Unlike other similar systems, the new technique does not require subjects to have surgical implants. Brain activity is measured using an fMRI scanner after extensive training of the decoder, in which the individual listens to hours of podcasts in the scanner. Later, the participants listen to a new story or imagine telling a story that allows the machine to generate corresponding text from brain activity alone.&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a real leap forward compared to what’s been done before.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;“For a noninvasive method, this is a real leap forward compared to what’s been done before, which is typically single words or short sentences,” Alex Huth, an assistant professor of neuroscience and computer science at UT Austin, said in a &lt;a href=&#34;https://cns.utexas.edu/news/podcast/brain-activity-decoder-can-reveal-stories-peoples-minds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;news release&lt;/a&gt;. “We’re getting the model to decode continuous language for extended periods of time with complicated ideas.”&lt;/p&gt;
&lt;p&gt;Despite the promising research, no one will be reading your thoughts soon, &lt;a href=&#34;https://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;, the director of the &lt;a href=&#34;https://ds.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science at New Jersey Institute of Technology&lt;/a&gt;, said in an email interview. He said that in the best case, the semantic decoder correctly outputs the gist of what you&amp;rsquo;re thinking about half the time. And you can easily defeat the semantic decoder by simply thinking about different things.&lt;/p&gt;
&lt;p&gt;“This non-invasive procedure will require you to participate fully, and the results &amp;ndash; your semantic decoder &amp;ndash; will not work for anyone else, so there&amp;rsquo;s no risk that your thoughts will be read without your permission,” he added. “These fMRI machines cost upwards of $3 million and weigh 20 tons and can measure the microscopic blood flow in one&amp;rsquo;s brain, so we shouldn&amp;rsquo;t expect our thoughts to be read by a passer-by, our boss at the office, or a listening government ear.”&lt;/p&gt;
&lt;h2 id=&#34;the-ethics-of-brain-reading&#34;&gt;The Ethics of Brain Reading&lt;/h2&gt;
&lt;p&gt;Some observers say it’s not too soon to start thinking about how brain decoder technology might advance and ways to safeguard human thoughts.&lt;/p&gt;
&lt;p&gt;“These techniques pose an existential threat to something we never had to worry about – our brain privacy,” &lt;a href=&#34;https://sapl.ucalgary.ca/about/people/thomas-patrick-keenan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thomas P. Keenan&lt;/a&gt;, a professor at the &lt;a href=&#34;https://www.ucalgary.ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of Calgary&lt;/a&gt; who studies the implications of technology adoption, said via email. “Aside from cases of torture, the contents of our mind have always  been ours and ours alone.”&lt;/p&gt;
&lt;p&gt;Keenan pointed out that new research shows that, equipped with fMRI imaging and Artificial intelligence, much of our thinking can be decoded and verbalized.&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In many ways, we can already tell or predict what someone is thinking based on existing data sources.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;“It is hard to conceptualize “informed consent” when the reach of these technologies is, as of now, unknown and speculative,” he added. “Certainly, any forced use would be a form of “unreasonable search and seizure” and prohibited by law in most civilized countries.”&lt;/p&gt;
&lt;p&gt;Perhaps a whole new profession will arise of coaches who teach people, such as criminal suspects, how to throw these technologies off the scent of their real thoughts, Keenan suggested.&lt;/p&gt;
&lt;p&gt;“In many ways, we can already tell or predict what someone is thinking based on existing data sources, &amp;quot; Miller said. “Privacy, if not dead, is in the ICU. Thus, it becomes important not just to focus on privacy rights but also on specific protections from misuse of personal data and information. We want to make it hard for our thoughts to be used against us in unjust, predatory, or harmful ways.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lifewire.com/why-the-new-ai-brain-decoder-could-be-a-privacy-nightmare-7488772&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lifewire.com/why-the-new-ai-brain-decoder-could-be-a-privacy-nightmare-7488772&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Announcement: NJIT FY2024 Faculty Seed Grant Award Winners</title>
      <link>http://localhost:1313/blog/20230425-njit/</link>
      <pubDate>Tue, 25 Apr 2023 12:17:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230425-njit/</guid>
      <description>&lt;h2 id=&#34;memorandum&#34;&gt;Memorandum&lt;/h2&gt;
&lt;p&gt;Date:     April 25, 2023&lt;br&gt;
From:    Atam P Dhawan, Senior Vice Provost for Research&lt;br&gt;
RE:        Announcement: NJIT FY2024 Faculty Seed Grant Award Winners&lt;/p&gt;
&lt;p&gt;The faculty and student success are the strategic priorities in the NJIT Strategic Plan. Increasing external funding with collaborative synergy and partnerships for basic, applied and translational research is one of the strategic objectives to enable enhanced faculty and student success and professional development.  The purpose of the NJIT Faculty Seed Grant (FSG) initiative is to promote collaborative research in the core and interdisciplinary areas by providing seed funding to obtain preliminary results for developing future grant proposals for submission to external funding agencies. The FSG initiative specifically seeks seed funding proposals from faculty to launch new initiatives in core, collaborative and interdisciplinary emerging areas aligned with NJIT strategic tactics to develop critical research mass. It is expected that the FSG funds will be used to promote on-campus collaborative seed research with faculty and students.&lt;/p&gt;
&lt;p&gt;We are very pleased to announce 28 FY24 Faculty Seed Grant awards this year.&lt;/p&gt;
&lt;p&gt;Congratulations to all FY24 Faculty Seed Grant Winners!&lt;/p&gt;
&lt;h3 id=&#34;nce&#34;&gt;NCE:&lt;/h3&gt;
&lt;p&gt;Principal Investigator: Tara Alvarez&lt;br&gt;
Department: BME&lt;br&gt;
Project Title: Measures of cerebral blood flow in patients with post-concussion convergence insufficiency&lt;br&gt;
Co-Principal Investigator(s): Stephanie Iring-Sanchez&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Gennady Gor&lt;br&gt;
Department: CME&lt;br&gt;
Project Title: Assessing the State of Health of Li-Ion Coin Cells by Ultrasonic Testing&lt;br&gt;
Co-Principal Investigator(s): Alexei Khalizov (Chem)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Jin Huiran&lt;br&gt;
Department: SAET&lt;br&gt;
Project Title: Towards Closing the Education Gap and Workforce Development for 4IR in the AEC Industry&lt;br&gt;
Co-Principal Investigator(s): Laramie Potts (SAET)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Elisa Kallioniemi&lt;br&gt;
Department: BME&lt;br&gt;
Project Title: 3D printed sensing patch for measuring transcranial magnetic stimulation evoked electrical signals&lt;br&gt;
Co-Principal Investigator(s): Amir Miri (BME)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Eun Jung Lee&lt;br&gt;
Department: BME&lt;br&gt;
Project Title: The Effects of Microgravity and Hypoxia on the Heart&lt;br&gt;
Co-Principal Investigator(s):&lt;br&gt;
Funded Amount: $7,500&lt;/p&gt;
&lt;p&gt;Principal Investigator: Angelo Tafuni&lt;br&gt;
Department: SAET&lt;br&gt;
Project Title: Electric Capacitance Tomography for Cryogenic Flow&lt;br&gt;
Co-Principal Investigator(s): Samuel Lieber (SAET)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Ying Li&lt;br&gt;
Department: BME&lt;br&gt;
Project Title: Repetitive low-level blast TBI induces hyper-excitability of granule cells of hippocampus in rat&lt;br&gt;
Co-Principal Investigator(s): Bryan Pfister (BME)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Jay Meegoda&lt;br&gt;
Department: CEE&lt;br&gt;
Project Title: Stability of Rock Formations with Hydrogen Storage&lt;br&gt;
Co-Principal Investigator(s): Oladoyin Kolawole (CEE)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Roberto Rojas-Cessa&lt;br&gt;
Department: ECE&lt;br&gt;
Project Title: Drone-based Sensor Network for Water Quality Monitoring of Rushing Rivers and Analysis with Integrated Learning&lt;br&gt;
Co-Principal Investigator(s): Cong Wang (ECE), Wen Zhang (CEE)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Bo Shen&lt;br&gt;
Department: MIE&lt;br&gt;
Project Title: Physics-informed Modeling and Learning for Process-Structure-Property Relationships in Metal Additive Manufacturing&lt;br&gt;
Co-Principal Investigator(s): Yao Ma (CS)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;h3 id=&#34;csla&#34;&gt;CSLA:&lt;/h3&gt;
&lt;p&gt;Principal Investigator: Shahriar Afkhami&lt;br&gt;
Department: MATH&lt;br&gt;
Project Title: Computationally Fast and Robust Approximation of Differential Geometry Operators on Discrete Surfaces&lt;br&gt;
Co-Principal Investigator(s): &lt;strong&gt;David Bader&lt;/strong&gt; (CS)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Miriam Ascarelli&lt;br&gt;
Department: HSS&lt;br&gt;
Project Title: Mapping Urban Transformation: How Newark Became Newark&lt;br&gt;
Co-Principal Investigator(s): Laramie Potts (SAET)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Brooke Flammang&lt;br&gt;
Department: BIOL&lt;br&gt;
Project Title: Underwater Energy Harvesting to Extend Tag Life in Marine Mammal Biotelemetry Applications&lt;br&gt;
Co-Principal Investigator(s): Lin Dong (MIE)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Lou Kondic&lt;br&gt;
Department: MATH&lt;br&gt;
Project Title: Acoustofluidics for Cleaning Contaminated Fluids&lt;br&gt;
Co-Principal Investigator(s): Linda Cummings (MATH)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Enkeleida Lushi&lt;br&gt;
Department: MATH&lt;br&gt;
Project Title: Confined Collective Motion of Bristle-Bots: Modeling and Experiments&lt;br&gt;
Co-Principal Investigator(s): Externals&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Eliza Michalopoulou&lt;br&gt;
Department: MATH&lt;br&gt;
Project Title: Detection Theory, Simulations, and Experiments in Time-Dispersive Underwater Environments with a Multi-Sensor Architecture&lt;br&gt;
Co-Principal Investigator(s): Ali Abdi (ECE)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Yeldo Semizer&lt;br&gt;
Department: HSS&lt;br&gt;
Project Title: Breast Tissue Synthesis to Improve Cancer Detection&lt;br&gt;
Co-Principal Investigator(s): Externals&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Benjamin Thomas&lt;br&gt;
Department: PHYS&lt;br&gt;
Project Title: Monitoring Pollinator Abundance: A Novel Approach Based on Near-Infrared Optical Sensors&lt;br&gt;
Co-Principal Investigator(s): Gareth Russell (BIOL)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;h3 id=&#34;ywcc&#34;&gt;YWCC&lt;/h3&gt;
&lt;p&gt;Principal Investigator: Michael Houle&lt;br&gt;
Department: CS&lt;br&gt;
Project Title: Understanding and Managing Deep Learning through Local Intrinsic Dimensionality&lt;br&gt;
Co-Principal Investigator(s): Vincent Oria (CS)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Margarita Vinnikov&lt;br&gt;
Department: INFORM&lt;br&gt;
Project Title: Hidden in Plain Sight: Reconstructing Lost Histories in Istanbul and Rome&lt;br&gt;
Co-Principal Investigator(s): Burcak Ozludil (ADHC), Ersin Altin (HCAD), and Louis Hamilton (ADHC)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Lijing Wang&lt;br&gt;
Department: Data Science&lt;br&gt;
Project Title: Towards Improving the Generalization and Robustness of Large Pretrained Language Models&lt;br&gt;
Co-Principal Investigator(s): Mengnan Du (Data Science)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Zhi Wei&lt;br&gt;
Department: CS&lt;br&gt;
Project Title: Computational methods for selective inference with application to single-cell genomic data analysis&lt;br&gt;
Co-Principal Investigator(s): Wenge Guo (MATH)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;h3 id=&#34;hcad&#34;&gt;HCAD&lt;/h3&gt;
&lt;p&gt;Principal Investigator: Hyojin Kim&lt;br&gt;
Department: HCAD&lt;br&gt;
Project Title: Measurement and Verification of Energy and Indoor Environmental Quality Performance for Personalized Environmental Control Systems&lt;br&gt;
Co-Principal Investigator(s): Won Hee Ko&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;h3 id=&#34;mtsm-retdic&#34;&gt;MTSM-RETDIC&lt;/h3&gt;
&lt;p&gt;Principal Investigator: Rayan Assaad&lt;br&gt;
Department: CEE&lt;br&gt;
Project Title: An Intelligent Cloud-Based IoT System for Automated Water Quality Assessment and Leak Detection in Smart Real Estate Facilities&lt;br&gt;
Co-Principal Investigator(s): Aichih (Jasmine) Chang (MTSM)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Carrie Bobo Gibbs&lt;br&gt;
Department: HCAD&lt;br&gt;
Project Title: Commercial Conversions Investigating the Viability of Converting Large Floorplate Commercial Office Buildings into Mixed Rate Residential Buildings&lt;br&gt;
Co-Principal Investigator(s):&lt;br&gt;
Funded Amount: $7,500&lt;/p&gt;
&lt;p&gt;Principal Investigator: SangWoo Park&lt;br&gt;
Department: MIE&lt;br&gt;
Project Title: Electricity Consumption by Real Estate: Learning Heterogeneous Elasticity and Dynamic Pricing of Rewards for Demand Response&lt;br&gt;
Co-Principal Investigator(s): Jim Shi (MTSM)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Yao Sun&lt;br&gt;
Department: HSS&lt;br&gt;
Project Title: The Network Effect in Crowdfunding for FinTech and Real Estate Startups&lt;br&gt;
Co-Principal Investigator(s): Ajim Uddin (MTSM)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;p&gt;Principal Investigator: Ming Fang Taylor&lt;br&gt;
Department: MTSM&lt;br&gt;
Project Title: Exploring the Role of Real Estate Assets and other Moderators in Corporate Innovation&lt;br&gt;
Co-Principal Investigator(s): Shanthi Gopalakrishnan (MTSM)&lt;br&gt;
Funded Amount: $10,000&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Atam P Dhawan, PhD&lt;br&gt;
Senior Vice Provost for Research&lt;br&gt;
Distinguished Professor of Electrical and Computer Engineering&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lehigh University Alumni Bulletin Class Notes, Spring 2023</title>
      <link>http://localhost:1313/blog/20230415-lehigh/</link>
      <pubDate>Sat, 15 Apr 2023 11:40:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230415-lehigh/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230415-lehigh/page1_hu_bf992da960ced652.webp 400w,
               /blog/20230415-lehigh/page1_hu_19fcb5031a7d830b.webp 760w,
               /blog/20230415-lehigh/page1_hu_616312bb69f19ae6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230415-lehigh/page1_hu_bf992da960ced652.webp&#34;
               width=&#34;760&#34;
               height=&#34;744&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230415-lehigh/page2_hu_de7b396c1b22ba57.webp 400w,
               /blog/20230415-lehigh/page2_hu_3ba17dc87de5b5d.webp 760w,
               /blog/20230415-lehigh/page2_hu_ff858e053a504f77.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230415-lehigh/page2_hu_de7b396c1b22ba57.webp&#34;
               width=&#34;760&#34;
               height=&#34;521&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;class-of-90&#34;&gt;Class of &amp;lsquo;90&lt;/h2&gt;
&lt;p&gt;We also heard from &lt;strong&gt;David Bader&lt;/strong&gt;, who not only remains involved with Lehigh as a member of the advisory board of Lehigh&amp;rsquo;s Electrical and Computer Engineering Department, but also serves as the inaugural director of New Jersey Institute of Technology&amp;rsquo;s Institute for Data Science. This past year, he received one of computing&amp;rsquo;s highest honors, the IEEE Sidney Fernbach Award for inventing commodity-based supercomputing that can be traced back to his Lehigh days. David graduated with both bachelor&amp;rsquo;s and master&amp;rsquo;s degrees from Lehigh and now lives in Manhattan.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www2.lehigh.edu/news/publications/bulletin/spring-2023&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www2.lehigh.edu/news/publications/bulletin/spring-2023&lt;/a&gt;
&lt;a href=&#34;https://flippingbook.lehigh.edu/Bulletin-Spring23/64/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://flippingbook.lehigh.edu/Bulletin-Spring23/64/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Jersey Big Data Alliance launches Spring 2023 workshop series</title>
      <link>http://localhost:1313/blog/20230403-njbda/</link>
      <pubDate>Mon, 03 Apr 2023 18:16:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230403-njbda/</guid>
      <description>&lt;p&gt;The New Jersey Big Data Alliance (NJBDA) is partnering with several organizations to provide several free workshops and panel discussions for data practitioners and those interested in big data. This workshop series is part of NJBDA’s effort to build and leverage collaborations to increase competitiveness, generate a highly skilled workforce, drive innovation and catalyze data-driven economic growth for New Jersey. This robust series of workshop events complements “&lt;a href=&#34;https://njbda.org/2023symposium/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Big Data in FinTech&lt;/a&gt;” – the NJBDA’s tenth Annual Symposium on May 9 at Seton Hall University.&lt;/p&gt;
&lt;h2 id=&#34;big-data-workshops&#34;&gt;Big data workshops&lt;/h2&gt;
&lt;p&gt;The series begins tomorrow, April 4, with a virtual Smart Ports Workshop to discover the benefits and opportunities for innovations in the port systems. Members of government, faculty, students, port-related businesses and the general public are invited to attend this event. This workshop is hosted by NJBDA and the New Jersey Institute of Technology Institute for Data Science, and supported by the US Economic Development Administration (US EDA).&lt;/p&gt;
&lt;p&gt;Confirmed speakers include Beth Rooney, Port Director at the Port Authority of New York and New Jersey; Dennis Monts, Chief Operations Officer at Advent eModal; and Anne Strauss-Wieder, Director, Freight Planning, at the North Jersey Transportation Planning Authority. &lt;strong&gt;David Bader&lt;/strong&gt;, Director of the NJIT Institute for Data Science, will moderate, with an introduction by Peggy Brennan-Tonetta, NJBDA Co-Founder and Past-President, and Director, Resource and Economic Development, Rutgers New Jersey Agricultural Experiment Station, Rutgers University.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eventbrite.com/e/njbdaus-eda-smart-ports-workshop-tickets-565287709757&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for Smart Ports Workshop, April 4, 2023, 1:30 to 4:30 pm ET (virtual)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NJBDA workshop series continues on April 21 with a Garden State Open Data Index forum on Open Data and Artificial Intelligence. This event will include a discussion of the value creation potential of aligning open data with AI, especially generative AIs like ChatGPT and Bloom, with reflections on public policy.&lt;/p&gt;
&lt;p&gt;Panelists include Poonam Soans, Chief Data Officer &amp;amp; Director of Application Development at the State of New Jersey, Rakesh (Teddy) Kumar, PhD, Vice President, ICS, Center for Vision Technology at SRI International; Clint Andrews, PhD, Professor &amp;amp; Associate Dean at the Bloustein School, Rutgers University; and Rachel Rosenthal, editorial board member at Bloomberg. Jim Samuel, Executive Director at the Bloustein School Informatics Program will moderate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eventbrite.com/e/gsodi-forum-on-open-data-and-artificial-intelligence-tickets-569804208727&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for GSODI Forum on Open Data and AI, April 21, 2023, 11:00 am to 1:30 pm ET, at the Gov. James J. Florio Special Events Forum in New Brunswick&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The third event in the NJBDA spring workshop series is “Advanced Manufacturing Efficiencies through Technology” on April 28. Speakers include Donald Coakley, Principal at Plant Automation Group and Frank Catalana, Vice President of Operations at Cumberland Dairy. Nolan Lewin, Executive Director, The Food Innovation Center at Rutgers University will moderate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://rutgers.zoom.us/meeting/register/tJ0kd-CoqTgvE9P9pymFGIBrYhiN43hzxT2H&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for Advanced Manufacturing Efficiencies through Technology, April 28, 2023, 10:00 to 11:30 am ET (virtual)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“Big Data for Small Business: Leveraging Data for Business Growth” is the fourth event in the spring workshop series, on May 3. The NJBDA, the US Economic Development Administration, Edge and the Center for Business Analytics at Rider University’s Norm Brodsky College of Business invite small business owners to learn how to leverage big data.&lt;/p&gt;
&lt;p&gt;Two workshops and a networking lunch highlight this event. Panelists for “Challenges and opportunities in the adoption of technology and use of data by small businesses” include Laura Glotzbach of LGS Marketing Services; Joe Hudicka of Neurored; Ramesh Gudalur of Cognitive Operations; and Stuart Altschuler of Trenton Health Team. Panelists for “Role of ecosystem stakeholders in helping small businesses leverage big data and related technologies” include Elayne McClaine of the NJ Small Business Development Center at Rutgers, New Brunswick; Jeffrey A. Robinson, PhD of Rutgers University; Judith Sheft of NJ Commission on Science, Innovation and Technology; and Jasmine Ward of TechUnited. Panel moderators include Emre Yetgin, Director, Center for Business Analytics at Rider University; Forough Ghahramani, Assistant Vice President, Research, Innovation &amp;amp; Sponsored Programs at Edge; and Linda Lindner, Managing Editor at ROI-NJ.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eventbrite.com/e/big-data-for-small-business-tickets-579112510107&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for Big Data for Small Business, May 3, 2023, 9:00 am to 1:00 pm ET, at Rider University&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The “Cyberinfrastructure for Research Data Management” two-day workshop on May 23 and 24, 2023  is designed to help research computing professionals deploy next-gen cyberinfrastructure that can effectively support data-intensive science. Princeton University, Edge, Globus, and EPOC (TACC and ESnet, partners of EPOC) are hosting this in-person workshop at Princeton.&lt;/p&gt;
&lt;p&gt;The workshop will consist of two days of presentation material that focus on the concepts of the Science DMZ, Data Management using Globus, perfSONAR network measurement, and other affiliated Research &amp;amp; Education best common practices that are designed to operate modern research and education networks to support data intensive science. For the draft agenda and speakers, visit njedge.net.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://njedge.net/blog/cyberinfrastructure-for-research-data-management/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for Cyberinfrastructure for Research Data Management, May 23, 2023 (9:00 am to 8:00 pm ET) and May 24, 2023 (9:00 am to 4:00 pm ET) at Princeton University&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;njbda-annual-symposium&#34;&gt;NJBDA Annual Symposium&lt;/h2&gt;
&lt;p&gt;The 2023 NJBDA Symposium will be held at Seton Hall University on May 9, 2023. The Annual Symposium is New Jersey’s premier conference for big data &amp;amp; advanced computing, consistently attracting 200+ attendees from industry, government, and academia.&lt;/p&gt;
&lt;p&gt;Keynote speakers for the “Big Data in FinTech” symposium include George Calhoun, Professor and Founding Director of the Quantitative Finance Program at Stevens Institute of Technology; Kjersten Margaret Moody, Chief Data Officer at Prudential Financial; and Stephen Ward, Managing Director at Insight Partners.&lt;/p&gt;
&lt;p&gt;The symposium also includes several workshops and sessions on AI/Machine Learning for FinTech, Data Assets and Privacy, Entrepreneurship in FinTech, Workforce Development for FinTech, Cryptocurrencies and Risk Management, and Research Presentations in FinTech, as well as student research poster presentations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eventbrite.com/e/2023-njbda-symposium-tickets-558984296077?aff=ebdssbdestsearch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for NJBDA Annual Symposium, May 9, 2023, 8:30 am to 3:30 pm ET at Seton Hall University&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;more-spring-njbda-activities&#34;&gt;More spring NJBDA activities&lt;/h2&gt;
&lt;p&gt;The NJBDA is also partnering with the New Jersey Pathways to Career Opportunities Initiative to expand articulation agreements through convening New Jersey’s community colleges and four-year colleges and universities on April 18. This is an opportunity for the institutions of higher education to develop more collaborative relationships and increase the number of articulation agreements between them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.eventbrite.com/e/collaboration-for-articulation-agreements-tickets-574582560907&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for Collaboration for Articulation Agreements, April 18, 2023, 10:00 am to 1:00 pm ET at Middlesex College&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://njbda.org/news/new-jersey-big-data-alliance-launches-spring-2023-workshop-series/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njbda.org/news/new-jersey-big-data-alliance-launches-spring-2023-workshop-series/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPC User Forum Publishes Updated Agenda for April 2023 Meeting</title>
      <link>http://localhost:1313/blog/20230313-hpcwire/</link>
      <pubDate>Mon, 13 Mar 2023 17:21:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230313-hpcwire/</guid>
      <description>&lt;p&gt;ST PAUL, Minn., March 13, 2023 — The HPC User Forum has published an updated agenda spotlighting an impressive list of featured speakers for its upcoming meeting, April 18-19, 2023, in Princeton, New Jersey.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230313-hpcwire/Hyperion-Research-logo-300x51_hu_492c51cddab12118.webp 400w,
               /blog/20230313-hpcwire/Hyperion-Research-logo-300x51_hu_ca24bef4d6baf1d2.webp 760w,
               /blog/20230313-hpcwire/Hyperion-Research-logo-300x51_hu_92ee5fc34e8ccdfb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230313-hpcwire/Hyperion-Research-logo-300x51_hu_492c51cddab12118.webp&#34;
               width=&#34;300&#34;
               height=&#34;51&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The HPC User Forum is internationally recognized for its diverse user participation and facilitation of invaluable networking among users, vendors, government agencies and private industry. Each year, two full-membership meetings are held in the United States and two at international locations.&lt;/p&gt;
&lt;p&gt;At this event, Hyperion Research will provide an HPC market update, including a discussion on the health of the global HPC community with a spotlight on the high growth and low growth segments for 2023 and beyond.&lt;/p&gt;
&lt;p&gt;According to Earl Joseph, CEO of Hyperion Research, “The HPC User Forum membership is continuing to grow with both domestic and international representation. We are very excited to kick off the 2023 HPC User Forums with such a diverse and timely agenda, and at an incredible location such as the Princeton Marriott at Forrestal.”&lt;/p&gt;
&lt;p&gt;The following list is an updated summary of featured topics and speakers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Evolution of Robotics, AI/ML, and Nuclear Fusion: Past, Present, Future, &lt;em&gt;Paul Muzio, Chair, HPC User Forum&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Massive Scale Analytics for Real-World Applications, &lt;em&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor, Data Science, NJIT&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Magnetohydrodynamic Modeling at Various Scales: From the Turbulent Solar Corona to the Coupled Earth-Moon System, &lt;em&gt;Chuanfei Dong, Assistant Professor, Boston University&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Analyzing Streaming Data in Real-Time Without Central Control, &lt;em&gt;John Feo, Director of the Center for Adaptive Supercomputer Software PNNL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The Path to Exascale HPC, Quantum Computing and the Square Kilometre Array (SKA): An Update from Australia, &lt;em&gt;Mark Stickells, Executive Director, Pawsey Supercomputing Research Centre&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The Intersection of HPC and AI &amp;amp; Modern Magnetic Fusion Energy Development, &lt;em&gt;William Tang, Principal Research Physicist, Princeton Plasma Physics Laboratory (PPPL)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Princeton Plasma Physics Laboratory Update, &lt;em&gt;William Dorland, Associate Laboratory Director for Computational Science, Princeton Plasma Physics Laboratory (PPPL)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Simulations Towards an Aneutronic Fusion Pilot Plant, &lt;em&gt;Sean Dettrick, Director of Computational Sciences, TAE Technologies&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Radiation Transport Modeling, &lt;em&gt;Luca Longoni, HPC Technical Lead, Fed Data Technology Systems (FDTS)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Fusion Panel Discussion, Moderator: &lt;em&gt;David Martin, Manager, Industry Partnerships and Outreach, ANL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;LAMMPS, AI/ML Interatomic Potentials, And Leadership HPC Platforms: An Effective Approach for Modeling Fusion Materials, &lt;em&gt;Aidan Thompson, Distinguished Member of Technical Staff, SNL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Quantum Computing and AI, &lt;em&gt;Salvatore Mandrà, Research Scientist, Quantum AI, NASA&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Quantum Science Center Update, &lt;em&gt;Travis Humble, Director, Quantum Science Center, ORNL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Accelerating Scientific Machine Learning with Novel AI Systems, &lt;em&gt;Murali Krishna Emani, Assistant Computer Sciences, Data Sciences, ANL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;KAUST HPC Site Update, &lt;em&gt;Jysoo Lee, Facilities Director, Research Computing Core Labs, KAUST&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Site Update: NYU Langone HPC Medical Research Center, &lt;em&gt;Michael Costantino, Director, HPC, NYU Langone&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Exascale Machine Learning Technologies, &lt;em&gt;Sudip Seal, Senior Scientist, ORNL&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Panel Discussion: What Can the HPC User Forum do to Help advance the HPC Community, Technologies, and Markets? &lt;em&gt;Earl Joseph, CEO, Hyperion Research&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Update – Princeton University, &lt;em&gt;Curtis Hillegas, Associate CIO, Princeton University&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;HPC Site Update: Flatiron Institute, &lt;em&gt;Ian Fisk, Co-Director Scientific Computing, Flatiron Institute&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;New Directions in HPC Storage and Interconnects, &lt;em&gt;Mark Nossokoff, Research Director, Hyperion Research&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Using HPC for the Modeling of Galaxy Formation and Evolution, &lt;em&gt;Molly Peeples, Research Scientist, JHU STSI&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Special Dinner Event: Dinner Presentation: Volatiles Investigating Polar Exploration Rover, &lt;em&gt;Daniel Andrews, Project Manager, NASA&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The program also includes timely updates from the event sponsors Covalent, DDN, Dell Technologies, HPE, IBM, Intel, Lenovo, Panasis, Penguin Solutions, and VAST Data.&lt;/p&gt;
&lt;p&gt;The full agenda can be found &lt;a href=&#34;https://www.hpcuserforum.com/wp-content/uploads/2023/03/April-2023-Princeton-HPC-User-Forum-Agenda-3.10.23.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More information and the User Forum registration link can be found &lt;a href=&#34;https://www.hpcuserforum.com/event-page-hpc-user-forum-spring-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;about-the-hpc-user-forum&#34;&gt;About the HPC User Forum&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcuserforum.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The HPC User Forum&lt;/a&gt; was established in 1999 to promote the health of the global HPC industry and address issues of common concern to users. The organization is directed by a volunteer Steering Committee of users from government, industry and academia, and is operated for the users by market analyst firm Hyperion Research.&lt;/p&gt;
&lt;h2 id=&#34;about-hyperion-research&#34;&gt;About Hyperion Research&lt;/h2&gt;
&lt;p&gt;Hyperion Research is the premier industry analyst and market intelligence firm for high performance computing (HPC) and associated emerging markets. Hyperion Research analysts provide timely, in-depth mission-critical insight across a broad portfolio of advanced computing market segments, including High Performance Computing (HPC), Advanced Artificial Intelligence (AI), High-Performance Data Analysis (HPDA), Quantum Computing, Cloud and Edge Computing.&lt;/p&gt;
&lt;p&gt;For more than 25 years, the industry analysts at Hyperion Research have been at the forefront of helping private and public organizations and government agencies make intelligent, fact-based decisions related to business impact and technology direction in the complex and competitive landscape of advanced computing and emerging technologies.&lt;/p&gt;
&lt;p&gt;Hyperion Research provides data-driven research, analysis and recommendations for technologies, applications, and markets to help organizations worldwide make effective decisions and seize growth opportunities. Research includes market sizing and forecasting, share tracking, segmentation, technology, and related trend analysis, and both user and vendor analysis for multi-user technical server technology used for HPC and HPDA (high performance data analysis). The company provides thought leadership and practical guidance for users, vendors, and other members of the global HPC community by focusing on key market and technology trends across government, industry, commerce, and academia.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Source: Hyperion Research&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/hpc-user-forum-publishes-updated-agenda-april-2023/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/hpc-user-forum-publishes-updated-agenda-april-2023/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Common password mistakes you&#39;re making that could get you hacked</title>
      <link>http://localhost:1313/blog/20230303-cbs/</link>
      <pubDate>Fri, 03 Mar 2023 17:13:33 +0100</pubDate>
      <guid>http://localhost:1313/blog/20230303-cbs/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Jennifer Earl, CBS News&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s hard to memorize passwords as you juggle dozens of apps — whether you&amp;rsquo;re logging in to stream your &lt;a href=&#34;https://www.cbsnews.com/pictures/most-popular-tv-shows-america-ranked/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;favorite show&lt;/a&gt;, view your medical records, check your savings account balance or more, you&amp;rsquo;ll want to avoid unwanted prying eyes.&lt;/p&gt;
&lt;p&gt;You may be tempted to create the same easy password for every site, but that could leave you vulnerable to &lt;a href=&#34;https://www.cbsnews.com/news/how-to-prevent-iphone-hacks-apple-security/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;potential hacks&lt;/a&gt; which could end up draining your bank account.&lt;/p&gt;
&lt;p&gt;In 2022, consumers reported being cheated out of around $8.8 billion due to fraud — a 30% increase from 2021, according to newly released &lt;a href=&#34;https://www.ftc.gov/news-events/news/press-releases/2023/02/new-ftc-data-show-consumers-reported-losing-nearly-88-billion-scams-2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federal Trade Commission data&lt;/a&gt;. Roughly 2.4 million consumers reported cases of fraud to the FTC, with investment and imposter scams topping their list of complaints. The agency recently shared the &lt;a href=&#34;https://consumer.ftc.gov/consumer-alerts/2023/02/top-scams-2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;top scams of 2022&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;3-password-mistakes-to-avoid&#34;&gt;3 password mistakes to avoid&lt;/h3&gt;
&lt;p&gt;Creating strong passwords is one of the best ways to protect your accounts and keep hackers at bay. The first step toward protecting your digital footprint: reevaluating your passwords. Here are some common mistakes you may be making.&lt;/p&gt;
&lt;h2 id=&#34;1-setting-simple-passwords&#34;&gt;1. Setting simple passwords&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-password-123456-topped-a-recent-list-of-most-common-passwords-in-2022-getty-imagesistockphoto&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The password &amp;#39;123456&amp;#39; topped a recent list of most common passwords in 2022. GETTY IMAGES/ISTOCKPHOTO&#34; srcset=&#34;
               /blog/20230303-cbs/gettyimages-1089625090_hu_785380fcfb230470.webp 400w,
               /blog/20230303-cbs/gettyimages-1089625090_hu_72bacae6e407a78d.webp 760w,
               /blog/20230303-cbs/gettyimages-1089625090_hu_a2df870ebc77797b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230303-cbs/gettyimages-1089625090_hu_785380fcfb230470.webp&#34;
               width=&#34;620&#34;
               height=&#34;413&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The password &amp;lsquo;123456&amp;rsquo; topped a recent list of most common passwords in 2022. GETTY IMAGES/ISTOCKPHOTO
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Easy number password note stick on smartphone, keyboard.
The password &amp;ldquo;123456&amp;rdquo; topped a recent list of most common passwords in 2022.&lt;/p&gt;
&lt;p&gt;When it comes to keeping your online accounts safe, simplicity isn&amp;rsquo;t key.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;There are several common mistakes people make with their passwords. For example, using a simple or short password such as a word or name, a sequence of numbers, or combination of these, can be easily guessed by malicious attackers,&amp;rdquo; &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of the Institute for Data Science at the New Jersey Institute of Technology, told CBS News.&lt;/p&gt;
&lt;p&gt;Bader said one of the most common passwords is &amp;ldquo;abc123,&amp;rdquo; which is a prime example of a password you should never use. While it may be easy to remember, it&amp;rsquo;s also easy to guess.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s even more sophisticated compared to what password manager NordPass has found. In 2022, NordPass released its &lt;a href=&#34;https://nordpass.com/most-common-passwords-list/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;top 200 most common passwords&lt;/a&gt; list, crowning &amp;ldquo;password&amp;rdquo; as the top used. Numerical lists &amp;ldquo;123456&amp;rdquo; and &amp;ldquo;123456789&amp;rdquo; followed, along with &amp;ldquo;guest&amp;rdquo; and &amp;ldquo;qwerty.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This is why many sites now require setting passwords longer than a certain length such as eight or more characters, and using a combination of letters, numbers and special characters such as &amp;lsquo;!@#$%^&amp;amp;*()?,&amp;rsquo;&amp;rdquo; Bader explained.&lt;/p&gt;
&lt;h2 id=&#34;2-repeating-passwords&#34;&gt;2. Repeating passwords&lt;/h2&gt;


















&lt;figure  id=&#34;figure-cybersecurity-experts-warn-users-from-inserting-the-same-password-across-multiple-accounts-especially-if-its-been-flagged-in-a-security-breach-getty-imagesistockphoto&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cybersecurity experts warn users from inserting the same password across multiple accounts, especially if it&amp;#39;s been flagged in a security breach. GETTY IMAGES/ISTOCKPHOTO&#34; srcset=&#34;
               /blog/20230303-cbs/gettyimages-503701087_hu_7d8fc940cf881b1.webp 400w,
               /blog/20230303-cbs/gettyimages-503701087_hu_ace050f702728f49.webp 760w,
               /blog/20230303-cbs/gettyimages-503701087_hu_ca1eb25be9f66e9c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230303-cbs/gettyimages-503701087_hu_7d8fc940cf881b1.webp&#34;
               width=&#34;620&#34;
               height=&#34;413&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cybersecurity experts warn users from inserting the same password across multiple accounts, especially if it&amp;rsquo;s been flagged in a security breach. GETTY IMAGES/ISTOCKPHOTO
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Cybersecurity experts warn users from inserting the same password across multiple accounts, especially if it&amp;rsquo;s been flagged in a security breach.&lt;/p&gt;
&lt;p&gt;Repeatedly using a simple password is bad, but regurgitating that same simple password across multiple apps and sites is even worse.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This is like putting the same lock on every door in your neighborhood. If one is compromised, then the entire group is compromised,&amp;rdquo; Bader cautioned.&lt;/p&gt;
&lt;p&gt;An estimated 64% of people have reused a password that had been compromised in a breach, computer security service SpyCloud stated in its &lt;a href=&#34;https://f.hubspotusercontent20.net/hubfs/3791228/2022%20SpyCloud%20Annual%20Identity%20Exposure%20Report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022 annual identity exposure report&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If a site has you change to a new password, do not reuse any previous passwords as they may have already been stolen,&amp;rdquo; Bader said, encouraging people to update their passwords at least every 90 days.&lt;/p&gt;
&lt;h2 id=&#34;3-sharing-passwords&#34;&gt;3. Sharing passwords&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cbsnews.com/video/netflix-cracks-down-on-password-sharing-what-does-that-mean-for-users/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix cracks down on password sharing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Password sharing has become increasingly popular among streamers. Netflix estimates more than 100 million households are &lt;a href=&#34;https://www.cbsnews.com/news/netflix-password-sharing-limits-crackdown-how-it-could-work-faq/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sharing Netflix passwords&lt;/a&gt;. By the end of March, Netflix will start to use a customer&amp;rsquo;s geographic location — based on their connected IP address and other signals — to determine the primary household and help curb outside use.&lt;/p&gt;
&lt;p&gt;While it may seem harmless to swap passwords with friends and family, it&amp;rsquo;s risky.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Never email or share your passwords with anyone. No legitimate organization will ever call you up and ask for your password either. So if you receive a call from tech support claiming to need this information for one of your accounts, simply hang up the phone,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;h3 id=&#34;how-to-keep-your-passwords-secure&#34;&gt;How to keep your passwords secure&lt;/h3&gt;
&lt;p&gt;Diversifying passwords, creating more sophisticated combinations and keeping them private are solid ways to keep your accounts secure. Additionally, you can enable backup security measures like two-factor authentication, prompting you to enter a second code and your password before gaining access to an app.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Two-factor authentication for Apple ID is a must, the second factor should be a separate trusted device (like an iPad, a Mac, or an Apple Watch),&amp;rdquo; Vitaly Shmatikov, a professor of computer science at Cornell University and Cornell Tech, told CBS News.&lt;/p&gt;
&lt;p&gt;Just don&amp;rsquo;t use SMS text messages as your backup, Shmatikov suggested. &amp;ldquo;Instead, use an authenticator app (like Google Authenticator, Microsoft Authenticator, Duo, Okta Verify, etc.) and turn on biometric protection — require Face ID or Touch ID — in the authenticator app. Then a thief who steals your phone won&amp;rsquo;t be able to get authentication codes and log into financial sites as you.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;You may also want to consider using a &lt;a href=&#34;https://www.cbsnews.com/news/in-wake-of-lastpass-hack-how-safe-are-password-managers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password manager&lt;/a&gt; or password vault, which can recommend and store passwords for you, though even those tools occasionally flag &lt;a href=&#34;https://blog.lastpass.com/2023/03/security-incident-update-recommended-actions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;security incidents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I recommend using a secure password vault to store potentially hundreds of passwords for the sites you use, and many password vaults available today will also suggest strong passwords that would be hard for an attacker to guess,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cbsnews.com/news/common-password-mistakes-people-expert-advice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cbsnews.com/news/common-password-mistakes-people-expert-advice/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Should organizations swear off open-source software altogether?</title>
      <link>http://localhost:1313/blog/20230227-venturebeat/</link>
      <pubDate>Mon, 27 Feb 2023 16:33:53 +0100</pubDate>
      <guid>http://localhost:1313/blog/20230227-venturebeat/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by &lt;a href=&#34;https://venturebeat.com/author/timkeary/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tim Keary&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-image-credit-shutterstock&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Image Credit: Shutterstock&#34; srcset=&#34;
               /blog/20230227-venturebeat/VB_hu_9938c6211659ceef.webp 400w,
               /blog/20230227-venturebeat/VB_hu_89f047f301968d50.webp 760w,
               /blog/20230227-venturebeat/VB_hu_7fda93d22a1ce2e8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230227-venturebeat/VB_hu_9938c6211659ceef.webp&#34;
               width=&#34;750&#34;
               height=&#34;500&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Image Credit: Shutterstock
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Open-source software is a nightmare for &lt;a href=&#34;https://venturebeat.com/security/what-is-cybersecurity-definition-importance-threats-and-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;data security&lt;/a&gt;. According to &lt;a href=&#34;https://www.darkreading.com/ics-ot/half-apps-high-risk-vulnerabilities-open-source&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Synopsys&lt;/a&gt;, while 96% of software programs contain some kind of open-source software component, 84% of codebases contain at least one vulnerability.&lt;/p&gt;
&lt;p&gt;These vulnerabilities are not only present in internal software, but also in third-party apps and services scattered across on-premises and cloud environments.&lt;/p&gt;
&lt;p&gt;Awareness over the &lt;a href=&#34;https://venturebeat.com/security/the-software-supply-chain-new-threats-call-for-new-security-measures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software supply chain&lt;/a&gt; threats has been growing over the past few years, with President Biden releasing an &lt;a href=&#34;https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Executive Order&lt;/a&gt; in May 2021 calling for federal government agencies to create a &lt;a href=&#34;https://venturebeat.com/security/sboms-what-they-are-and-why-organizations-need-them/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software bill of materials (SBOM)&lt;/a&gt;, to produce an inventory of software components used throughout their environments.&lt;/p&gt;
&lt;p&gt;Likewise, the revelation that the &lt;a href=&#34;https://venturebeat.com/security/failing-log4j/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Log4j&lt;/a&gt; vulnerability &lt;a href=&#34;https://www.veracode.com/blog/security-news/58-orgs-are-using-vulnerable-version-log4j&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;impacted&lt;/a&gt; 58% of organizations showed that organizations needed to be doing more to vet the software they use in their environments.&lt;/p&gt;
&lt;p&gt;While the ubiquitous use of open-source software means that organizations can’t swear off these tools altogether, there are some steps organizations can take to start mitigating the risk of exposing critical data assets.&lt;/p&gt;
&lt;h2 id=&#34;what-risks-are-facing-open-source-software&#34;&gt;What risks are facing open-source software?&lt;/h2&gt;
&lt;p&gt;One of the biggest threats facing open-source software is supply chain attacks. In a supply chain attack, a cybercriminal or state-sponsored threat actor will target the maintainer of an open-source project so they can embed malicious code into an open-source library and ship it to any downstream organizations that download it.&lt;/p&gt;
&lt;p&gt;This style of attack is becoming increasingly common to the point where &lt;a href=&#34;https://www.sonatype.com/state-of-the-software-supply-chain/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research&lt;/a&gt; suggests that there has been a 742% average annual increase in software supply chain attacks over the past three years, with &lt;a href=&#34;https://www.sonatype.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sonatype&lt;/a&gt; discovering 106,872 malicious packages available online.&lt;/p&gt;
&lt;p&gt;“From a supply chain perspective, it’s increasingly common to see malicious code introduced into open source — and that can be accomplished by compromising a legitimate project, or via a malicious project meant to confuse users into downloading counterfeit code that resembles a common project,” said Dale Gardner, Gartner Sr. director analyst.&lt;/p&gt;
&lt;p&gt;Gardner suggests that organizations reliant on open-source software need to evaluate the risk presented by each project.&lt;/p&gt;
&lt;p&gt;“For example, does the project have a good track record for responding to problems, are the appropriate security controls in place, is the code up to date, and so on. And from a supply chain perspective, it’s not just open source with which we should be concerned — we’ve seen a number of cases where commercial code has been compromised,” Gardner said.&lt;/p&gt;
&lt;p&gt;Frameworks such as the secure software development framework (SSDF) and Supply-chain Levels for Software Artifacts (SLSA) are one way that organizations can evaluate software suppliers for potential weaknesses, to evaluate the risk of software they use to build their own applications.&lt;/p&gt;
&lt;h2 id=&#34;defining-acceptable-risk-in-the-open-source-supply-chain&#34;&gt;Defining acceptable risk in the open-source supply chain&lt;/h2&gt;
&lt;p&gt;Another way to manage risk when implementing open-source software is to define acceptable risk. This comes down to deciding whether the vulnerabilities presented by a particular application present an acceptable and controllable level of risk.&lt;/p&gt;
&lt;p&gt;“Organizations that utilize open-source software, which today is every digitized business, benefit from developing and socializing an open-source strategy. A strategy provides guidelines on when open source can be utilized, what approval is required and what is acceptable risk to the business,” said Janet Worthington, Forrester senior analyst.&lt;/p&gt;
&lt;p&gt;“Have a plan in place in the event a high-impacting security vulnerability is disclosed. Your development team may have to back-port a fix to the version of the open-source library that your organization depends on,” Worthington said.&lt;/p&gt;
&lt;p&gt;Worthington highlights that organizations can start to codify and measure risk by creating an SBOM and maintaining an inventory of all software they acquire and download. In addition, security leaders should also ask suppliers to provide a description of their secure software development practices.&lt;/p&gt;
&lt;p&gt;When it comes to open-source libraries, Worthington suggests that organizations should first look for an SBOM; if there isn’t one, then scanning it with a software composition analysis (SCA) tool can help to reveal vulnerabilities in the code. You can then see if updates or patches are available to mitigate it.&lt;/p&gt;
&lt;p&gt;However, if you do choose to use an SCA to scan open-source components, it’s important to note that tools that use package managers to identify and scan packages are susceptible to missing &lt;a href=&#34;https://www.prnewswire.com/news-releases/rezilion-research-discovers-hidden-vulnerabilities-in-hundreds-of-docker-container-images-301753477.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;software packages&lt;/a&gt; and vulnerabilities.&lt;/p&gt;
&lt;h2 id=&#34;moving-beyond-scas-and-sboms&#34;&gt;Moving beyond SCAs and SBOMs&lt;/h2&gt;
&lt;p&gt;One of the core challenges of securing open-source software components in the enterprise is that they’re not static. Third parties can make changes to open-source software that, at a minimum, create new vulnerabilities, and at worse create actively malicious threats.&lt;/p&gt;
&lt;p&gt;While Lisa O’Connor, global lead of security research at &lt;a href=&#34;https://www.accenture.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accenture&lt;/a&gt;, notes the importance of static application security testing and SBOMs, she warns “we need to go much deeper to understand the risks.”&lt;/p&gt;
&lt;p&gt;“Researchers from Accenture’s Security Research and Development Labs are currently working on next-generation SBOM traceability to bring the sophistication needed to not only identify security threats, but to understand the downstream effects of vulnerability open-source functions on an organization’s actual installed codebase,” O’Connor said.&lt;/p&gt;
&lt;p&gt;The organization’s Security Research and Development Labs are currently working alongside Professor David Bader from the New Jersey Institute of Technology (&lt;a href=&#34;https://news.njit.edu/accenture-working-institute-data-science-combat-software-supply-chain-attacks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT&lt;/a&gt;), an expert in knowledge graphs and analytics, to help improve how organizations identify and isolate vulnerable open-source components.&lt;/p&gt;
&lt;p&gt;Understanding risk as the software supply chain evolves and moves is the key to mitigating open-source risk. Dynamic risks require an equally flexible mitigation strategy.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://venturebeat.com/security/should-organizations-swear-off-open-source-software-altogether/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://venturebeat.com/security/should-organizations-swear-off-open-source-software-altogether/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Launches PhD Program in Data Science</title>
      <link>http://localhost:1313/blog/20230217-njit/</link>
      <pubDate>Fri, 17 Feb 2023 07:20:06 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230217-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230217-njit/Arctic-Report-Card-2022_increased_shipping_map_graph_0_hu_c9f266cbbb77adf8.webp 400w,
               /blog/20230217-njit/Arctic-Report-Card-2022_increased_shipping_map_graph_0_hu_fb03c9d8b6c36a7f.webp 760w,
               /blog/20230217-njit/Arctic-Report-Card-2022_increased_shipping_map_graph_0_hu_4211ac5565834e01.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230217-njit/Arctic-Report-Card-2022_increased_shipping_map_graph_0_hu_c9f266cbbb77adf8.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Today’s world is driven by data – and data science is what powers the engine in this rapidly expanding global ecosystem. To address the need for talent and knowledge in this emerging field, NJIT’s Departments of &lt;a href=&#34;https://ds.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science&lt;/a&gt; and &lt;a href=&#34;https://math.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mathematical Sciences&lt;/a&gt; have launched a new &lt;a href=&#34;https://ds.njit.edu/phd-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ph.D. in Data Science&lt;/a&gt; program, dedicated to growing the field and generating top-notch data scientists.&lt;/p&gt;
&lt;p&gt;The program offers a computing option and statistics option to accommodate different interest profiles, with some overlap between the two. The computing option, offered by the data science department, deals with the core algorithmic and software technologies. The statistics option, offered by the mathematics department, deals with the fundamental mathematical and statistical underpinnings of the field.&lt;/p&gt;
&lt;p&gt;Professor Jim Geller, interim chair of the Department of Data Science in the Ying Wu College of Computing, notes how the data science and artificial intelligence (AI) revolutions are profoundly changing the way people will live in the near future: how we work, communicate, entertain ourselves, manage money, maintain our health and, perhaps sooner than later, how self-driving cars will move us.&lt;/p&gt;
&lt;p&gt;Geller predicts that data science will rival computer science in its impact on society, and is soon poised to become the most important and impactful branch of computing.&lt;/p&gt;
&lt;p&gt;In defining data science, Assistant Professor Aritra Dasgupta, director of &lt;a href=&#34;https://niiv.njitvis.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT’s Intelligible Information Visualization Lab&lt;/a&gt;, abides by what he calls the 3Cs of data science: computation, comprehension and communication. This implies that as a discipline, practitioners combine ideas from multiple areas, such as mathematics and statistics, big data, machine learning, visualization and human-data interaction.&lt;/p&gt;
&lt;p&gt;He believes that “while the strength of the industry is in scaling innovative solutions, academia will continue to be at the forefront of harnessing these innovations to address socio-economic problems, such as inequity, climate change, transparent automation, ethical AI and diversifying STEM education.”&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;”In God we trust. All others must bring data.” -W. Edwards Deming&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;As in other doctoral programs, NJIT’s Ph.D. in Data Science provides candidates one-to-one mentorship by faculty to pursue their passion in research areas of their choice. The results of this research are shared with the community by publication in professional and peer-reviewed journals, and presentation at leading scientific conferences. More enterprising students can commercialize their research at a later stage, resulting in real-world impact.&lt;/p&gt;
&lt;p&gt;Oliver Alvarado Rodriguez, a Ph.D. candidate, states that he is having the time of his life, thanks to the guidance of his supervisor, &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;, who encouraged him to “take a deeper dive” and go beyond his comfort zone. With Bader’s support, Rodriguez delivered the student keynote presentation at the &lt;a href=&#34;https://www.linkedin.com/feed/update/urn:li:activity:6908161968583421952/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spring 2022 Academic Data Science Alliance Annual Meeting&lt;/a&gt; in Irvine, California. “I never thought I would ever do something like this and was quite nervous, but Dr. Bader convinced me that it was within my grasp,” he said.&lt;/p&gt;
&lt;p&gt;Alvarado Rodriguez completed his undergraduate degree in computer science with a concentration in security at another university. He was surprised and honored when Bader, a prominent national figure in the field of data science who is also credited with building the first Linux-based supercomputer, approached him about joining his research team with no significant data science background. He now realizes that this is where his true passion lies and is glad he decided to “dive in.”&lt;/p&gt;
&lt;p&gt;NJIT’s &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt;, of which Bader is founder and director, provided the research opportunities for Rodriguez and other Ph.D. candidates.&lt;/p&gt;
&lt;p&gt;“I’m collaborating on research, co-authoring papers, and writing grant funding proposals. NJIT and the College of Computing have really empowered me to be a leader through this program in so many ways,” Alvarado Rodriguez added.&lt;/p&gt;
&lt;p&gt;Learn more about the Ph.D. in Data Science at &lt;a href=&#34;https://ds.njit.edu/phd-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ds.njit.edu/phd-data-science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-launches-phd-program-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-launches-phd-program-data-science&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>13 best antivirus software platforms, per a cybersecurity expert</title>
      <link>http://localhost:1313/blog/20230213-newyorkpost/</link>
      <pubDate>Mon, 13 Feb 2023 13:20:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20230213-newyorkpost/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Victoria Giardina&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Much like the best password protector services out there, investing in antivirus software isn’t as expensive as you would think — and it provides a safety net for your online data.&lt;/p&gt;
&lt;p&gt;Not to mention, when investing in quality &lt;a href=&#34;https://nypost.com/article/best-smart-home-devices-on-amazon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;smart home devices&lt;/a&gt; — like your treasured laptops and desktops — you want to make sure that hefty purchase isn’t being compromised.&lt;/p&gt;
&lt;p&gt;Luckily, one of the 13 best antivirus software programs of 2023 can help.&lt;/p&gt;
&lt;p&gt;“A computer virus is a malicious piece of software that spreads between computers and can harm your computer or steal your information,” &lt;strong&gt;&lt;a href=&#34;https://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader, PhD&lt;/a&gt;&lt;/strong&gt;, distinguished professor of data science at New Jersey Institute of Technology (NJIT) told the New York Post. “For instance, viruses can delete your files, watch your keystrokes (and grab your login information for your online bank account), or turn your computer into a weapon to attack corporate or government websites.”&lt;/p&gt;
&lt;p&gt;For unmatched protection, check out the best antivirus software programs, as vetted by our cybersecurity expert. For more on what these entail — including what malware, spyware and adware actually mean — check out our FAQ section below.&lt;/p&gt;
&lt;h2 id=&#34;1-mcafee-antivirus-35-for-the-first-year&#34;&gt;1. McAfee Antivirus, $35 for the first year&lt;/h2&gt;
&lt;p&gt;For the past 35 years, McAfee has held a premium spot as one of the earliest and best-known companies offering antivirus software. It offers antivirus software for all major platforms (Windows, macOS, Android, iOS). “McAfee has a 30-day free trial with “all-in-one” protection that includes a VPN for privacy when using public WiFi networks, web protection to avoid phishing scams, identity monitoring, and more,” Bader adds.&lt;/p&gt;
&lt;p&gt;For new customers, McAfee’s basic edition protects up to five devices and costs $35 for the first year and $85 per year after that. For an additional $45 per year, the premium edition adds parental controls and protects up to ten devices, and for another $30 per year, McAfee’s ultimate edition protects an unlimited number of devices and includes $1 million of identity theft coverage and identity restoration assistance.&lt;/p&gt;
&lt;h2 id=&#34;2-norton-antivirus-plus-10-for-the-first-year&#34;&gt;2. Norton Antivirus Plus, $10 for the first year&lt;/h2&gt;
&lt;p&gt;As one of the names synonymous with computer security, per Bader, Norton has been offering antivirus software for just over thirty years. From its AntiVirus Plus to its 360 Deluxe version, it has a slew of digital products for you to shop.&lt;/p&gt;
&lt;p&gt;The Antivirus Plus offers protection for a single PC or Mac at $10 for the first year, and $60 a year thereafter. This service includes Anti-Spyware, 2GB of cloud backup, a smart firewall, a password manager and an online privacy monitor, too.&lt;/p&gt;
&lt;p&gt;If you’d like more protection on your smartphone and tablet, its 360 bundle is $85 per year (now on sale for just $30!), upping the cloud backup to 10GB and including a VPN for public WiFi use. For more protection including for smartphones and tablets, Norton offers its Norton 360 bundles with the standard version ($85/year) increasing the cloud backup to 10GB, and incorporating a VPN for public WiFi use.&lt;/p&gt;
&lt;h2 id=&#34;3-avg-ultimate-60-for-the-first-year&#34;&gt;3. AVG Ultimate, $60 for the first year&lt;/h2&gt;
&lt;p&gt;AVG Ultimate offers antivirus protection for PC, Mac, Android, and iOIS, and costs $60 for the first year, and $128 per year after that, for protecting up to 10 devices. In addition to antivirus protection, AVG Ultimate also has an array of nifty features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Capabilities to manage battery profiles:&lt;/strong&gt; This enables you to maximize your computer, smartphone, or tablet’s performance while minimizing battery use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Device lock:&lt;/strong&gt; This stops unwanted visitors from accessing your phone.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Startup optimizer:&lt;/strong&gt; This stops software you don’t use or care about from slowing down your device’s time to boot up.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Payment protection:&lt;/strong&gt; This encrypts and keeps safe all of your online payments from the prying eyes of hackers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smart photo cleaner:&lt;/strong&gt; This finds duplicate and poor-quality photos on your device and deletes them to free up space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Browser and disk cleaner:&lt;/strong&gt; This finds and erases junk files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitive data shield:&lt;/strong&gt; This secures your most sensitive files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPS tracking:&lt;/strong&gt; This helps to find a lost device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“AVG Ultimate includes this whole bundle of security-related tools and services without having to buy additional packages,” Bader adds.&lt;/p&gt;
&lt;h2 id=&#34;4-bitdefender-antivirus-plus-30-for-the-first-year&#34;&gt;4. Bitdefender Antivirus Plus, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Bitdefender Antivirus Plus, impressively, protects up to three Windows devices from viruses including malware, spyware and adware. It’s $30 for the first year and $60 per year thereafter.&lt;/p&gt;
&lt;p&gt;Bader calls this a “no-frills product,” with the next plan level, Bitdefender Internet Security is $80 per year and includes a firewall and parental controls. “The ultimate plan, Bitdefender Total Security, is $90 per year, includes protection for up to five devices and is the only plan that also protects every OS (Windows, macOS, Android, and iOS) as well as including a device optimizer that places your device in game, movie, and work modes to save the battery while maximizing performance.”&lt;/p&gt;
&lt;h2 id=&#34;5-webroot-secureanywhere-antivirus-30-for-the-first-year&#34;&gt;5. Webroot SecureAnywhere Antivirus, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Providing baseline antivirus protection, Webroot SecureAnywhere Antivirus secures your data on one Windows or Mac device at $30 for the first year and $40 each year after. “Its claim is that it runs 60x faster than the scan time of other tested competitor antivirus products and takes 20 seconds to check your computer for malware by being fully cloud-based,” Bader said.&lt;/p&gt;
&lt;p&gt;What’s more, Webroot offers a VPN for WiFi Security for up to three devices along with antivirus software at $110 a year.&lt;/p&gt;
&lt;h2 id=&#34;6-cylance-smart-antivirus-45-per-block-of-data-protection&#34;&gt;6. Cylance Smart Antivirus, $45 per block of data protection&lt;/h2&gt;
&lt;p&gt;“Cylance technology is now incorporated into BlackBerry Cybersecurity and being incorporated into BlackBerry’s unified endpoint security solution,” Bader explains.&lt;/p&gt;
&lt;p&gt;This integrated threat prevention solution combines artificial intelligence (AI) to help “block malware infections with additional security controls that safeguard against script-based, fileless, memory, and external device-based attacks,” per its site. After taking a consultation, you’ll receive a quote, which is usually $45 per block of data protection.&lt;/p&gt;
&lt;h2 id=&#34;7-emsisoft-anti-malware-30-for-the-first-year&#34;&gt;7. Emsisoft Anti-Malware, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Only available for Windows, Emsisoft Anti-Malware has a 30-day free trial and then costs $30 per year for each device. “The software also includes extensive protections for finding and removing malware from your system and malware removal assistance,” Bader notes.&lt;/p&gt;
&lt;h2 id=&#34;8-eset-nod32-antivirus&#34;&gt;8. ESET NOD32 Antivirus&lt;/h2&gt;
&lt;p&gt;For just $40 a year, ESET NOD32 Antivirus secures each Windows and Mac device while including malware, ransomware, and phishing protection. At $50 per year, ESET Internet Security “also protects Android devices and includes two additional protection features for privacy and banking and for network and smart devices,” Bader explains.&lt;/p&gt;
&lt;p&gt;The ultimate protection, however, is ESET Smart Security Premium: $60 per year per device with an included password manager, sensitive data encryption and protection against new threats. “It’s disappointing, though, that a customer must buy this higher cost package to be protected against never-before-seen threats,” Bader commented.&lt;/p&gt;
&lt;h2 id=&#34;9-f-secure-anti-virus&#34;&gt;9. F-Secure Anti-Virus&lt;/h2&gt;
&lt;p&gt;As one of the oldest antivirus companies — founded in 1988 — F-Secure “has a strong international presence in Europe, North America, and Asia Pacific regions,” per Bader. It’s available for Windows only and costs $36 per year to protect one computer or $40 for up to three computers.&lt;/p&gt;
&lt;p&gt;“F-Secure has additional security product add-ons including F-Secure SAFE for internet security ($70 for up to three devices per year), F-Secure FREEDOME VPN for secure and private browsing ($55 for up to three devices per year), and F-Secure ID PROTECTION for secure passwords and online identity ($60 for up to five devices per year),” Bader said. The premium package from F-Secure, TOTAL, provides full online protection at $90 a year for up to three devices.&lt;/p&gt;
&lt;h2 id=&#34;10-g-data-antivirus&#34;&gt;10. G Data Antivirus&lt;/h2&gt;
&lt;p&gt;G DATA Software is a German cybersecurity company that “claims to have made the first antivirus software in 1987,” Bader notes. The G DATA Antivirus protection for a single Windows PC costs $30 for the first year and $40 per year thereafter, while the Mac version is a bit more expensive at $40 for the first year and $55 each year after.&lt;/p&gt;
&lt;p&gt;“G DATA also offers an Internet Security package with a firewall and parental monitoring at $55 per year per device, and a Total Security bundle for $70 per year per device that includes a password manager, encrypted backups, access control, and an integrated tuner for performance and security,” Bader adds.&lt;/p&gt;
&lt;h2 id=&#34;11-malwarebytes-premium&#34;&gt;11. Malwarebytes Premium&lt;/h2&gt;
&lt;p&gt;If you frequent the Internet often, you’ve likely heard of Malwarebytes Premium: an antivirus software for Windows, Mac, Android, iOS, and Chrome, costing $40 per year for one device or $80 per year for up to five devices. It includes both antivirus protection and a browser guard, too.&lt;/p&gt;
&lt;p&gt;“For $100 a year for up to five devices, Malwarebytes Premium + Privacy includes a VPN for safe WiFi-only protection,” Bader highlights. “A free Windows version can be used to clean up an infected computer and limited trials to for the other antivirus protections.”&lt;/p&gt;
&lt;h2 id=&#34;12-sophos-home-premium&#34;&gt;12. Sophos Home Premium&lt;/h2&gt;
&lt;p&gt;Sophos Home Premium costs $45 for all of your Windows and Mac devices and is “basic antivirus software that scans and cleans malware from your system, protects your privacy online, and has parental web filtering,” per Bader. Conveniently, there are no complicated add-ons and upgraded packages.&lt;/p&gt;
&lt;p&gt;“Sophos Home Premium keeps the package simple to buy and use, with everything included,” he adds.&lt;/p&gt;
&lt;h2 id=&#34;13-trend-micro-antivirussecurity&#34;&gt;13. Trend Micro Antivirus+Security&lt;/h2&gt;
&lt;p&gt;“Trend Micro’s Internet Security software for Windows only includes online privacy controls and fixes and optimizes systems, and costs $80 per year for up to three devices,” Bader said.&lt;/p&gt;
&lt;p&gt;Its basic package, Antivirus+Security, protects a single Windows PC from viruses and ransomware and costs $20 for the first year and $40 for each year thereafter. Uniquely, Trend Micro’s Maximum Security bundle is the only product from Trend Micro for Windows, Mac, Android, iOS, and Chromebooks, that “includes a password manager and secures mobile devices along with the antivirus protection” and costs $90 per year for up to five devices,” Bader said.&lt;/p&gt;
&lt;h2 id=&#34;an-faq-on-antivirus-software&#34;&gt;An FAQ on Antivirus Software&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-computer-virus&#34;&gt;What is a computer virus?&lt;/h3&gt;
&lt;p&gt;Nobody likes them, but it’s important to know how these pesky tech inconveniences and breaches begin.&lt;/p&gt;
&lt;p&gt;“Viruses can propagate in many ways such as through email attachments, unwitting downloads from infected websites, and through shared USB memory sticks,” Bader explains. “Antivirus software exists for all popular computing platforms (Windows, Mac, Android, iOS) and scans your system, memory, and files (including downloads) for known viruses.”&lt;/p&gt;
&lt;p&gt;When the antivirus software detects a virus, it may take several actions including cleaning the virus from the system and placing infected files in quarantine.&lt;/p&gt;
&lt;h3 id=&#34;what-benefits-come-with-some-antivirus-software-besides-protection&#34;&gt;What benefits come with some antivirus software, besides protection?&lt;/h3&gt;
&lt;p&gt;Aside from giving you (and your files) peace of mind, Bader highlights other add-ons some programs include.&lt;/p&gt;
&lt;p&gt;“Antivirus software often comes bundled with other security features such as secure password keepers, protection against clicking on malicious links to websites and scans of the dark web to find if your information has been compromised and potentially sold to hackers,” he notes.&lt;/p&gt;
&lt;h3 id=&#34;what-does-malware-mean&#34;&gt;What does malware mean?&lt;/h3&gt;
&lt;p&gt;Malware is malicious software that you may receive through an email attachment or other file transfer that can take over your computer if you run it.  “Be cautious when clicking on links in the body of emails from unknown or faked senders, or opening any file sent to you that you weren’t expecting,” Bader tips off. “These are the main ways hackers send you malware.”&lt;/p&gt;
&lt;h3 id=&#34;what-does-spyware-mean&#34;&gt;What does spyware mean?&lt;/h3&gt;
&lt;p&gt;More niche, spyware is a type of malware that watches everything you do on the computer and steals information. “For instance, spyware can monitor your keystrokes for all of your passwords, including ones for your bank accounts, social media, and email accounts,” Bader explains. “Spyware gathers information that can be used against your or your business and sends it to third parties who may sell the private information on the dark web or craft even more personalized attacks against you and your friends.”&lt;/p&gt;
&lt;h3 id=&#34;what-does-adware-mean&#34;&gt;What does adware mean?&lt;/h3&gt;
&lt;p&gt;According to Bader, adware is different from spyware and is generally unwanted software, usually connected to your web browser, that watches your activity and puts annoying ads up on your screen. Some antivirus software packages can scan and remove adware from your computer.&lt;/p&gt;
&lt;h3 id=&#34;is-antivirus-software-necessary&#34;&gt;Is antivirus software necessary?&lt;/h3&gt;
&lt;p&gt;Like your car and home insurance, investing about $20 to $50 yearly is surely worth it. Not to mention, the service pairs well with a &lt;a href=&#34;https://nypost.com/article/best-password-managers-per-experts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password protector&lt;/a&gt;, too.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nypost.com/article/best-antivirus-software/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nypost.com/article/best-antivirus-software/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Will computing advances mean the end of digital privacy?</title>
      <link>http://localhost:1313/blog/20230213-njbiz/</link>
      <pubDate>Mon, 13 Feb 2023 11:15:39 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230213-njbiz/</guid>
      <description>&lt;p&gt;&lt;em&gt;By: &lt;a href=&#34;mailto:jperry&amp;#43;marty27@njbiz.com&#34;&gt;Martin Daks&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An emerging technology harnesses the laws of quantum mechanics to solve problems too complex for classical computers. Quantum computing is expected to shatter barriers and turbocharge processes, from drug discovery to financial portfolio management. But this revolutionary new approach may also give hackers the ability to crack open just about any kind of digital “safe,” giving them access to trade secrets, sensitive communications and other mission-critical data. Last year, the threat prompted President Joe Biden to sign a national security memorandum, “Promoting United States Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems,” directing federal agencies to migrate vulnerable cryptographic systems to quantum-resistant cryptography. We spoke with some cybersecurity experts to find out what’s ahead.&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader-a-distinguished-professor-at-new-jersey-institute-of-technology-and-a-founder-of-its-institute-of-data-science&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader, a distinguished professor at New Jersey Institute of Technology and a founder of its Institute of Data Science&#34; srcset=&#34;
               /blog/20230213-njbiz/David-A.-Bader_Distinguished-Professor-at-NJIT_courtesy-NJIT-120x150_hu_c457475ef08c9c33.webp 400w,
               /blog/20230213-njbiz/David-A.-Bader_Distinguished-Professor-at-NJIT_courtesy-NJIT-120x150_hu_520795ef470b03bf.webp 760w,
               /blog/20230213-njbiz/David-A.-Bader_Distinguished-Professor-at-NJIT_courtesy-NJIT-120x150_hu_c72e55f1ca201899.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230213-njbiz/David-A.-Bader_Distinguished-Professor-at-NJIT_courtesy-NJIT-120x150_hu_c457475ef08c9c33.webp&#34;
               width=&#34;120&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader, a distinguished professor at New Jersey Institute of Technology and a founder of its Institute of Data Science
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;According to &lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor at &lt;a href=&#34;https://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Jersey Institute of Technology&lt;/a&gt; and a founder of its Institute of Data Science, the quantum hacking threat isn’t so much about getting into systems and extracting data — “cyber hackers are already doing a lot of that with current devices,” he noted. Instead, it will enhance their ability to unlock encrypted data after it’s lifted.&lt;/p&gt;
&lt;p&gt;“Right now, when sensitive financial or other information is sent online, the data is typically automatically encrypted, or coded with the RSA [Rivest-Shamir-Adleman] public-private key algorithm,” Bader explained. “Basically, as the data is transmitted, the sender devices will use the recipient’s ‘public key’ – which may be safely shared among multiple parties – to encrypt it using high-level math problems that are pretty opaque to even the fastest conventional computers. The recipient device’s ‘private’ key, which is generally not shared, is then used to decode it, so anyone intercepting the message will only have access to meaningless symbols that could take decades or longer to decode. But a quantum computer may enable a hacker to decrypt it within minutes at the most.”&lt;/p&gt;
&lt;p&gt;All of this takes place “under the hood,” without any action on the part of the sender or receiver. For the most part, the data encryption approaches currently used were thought to be safe against all but the most sophisticated nation-state hackers, since it could take typical computers thousands of years, if not more, to crack. But that’s because traditional computers are generally limited to using bits and bytes, or 0 and 1 symbols, to solve problems.&lt;/p&gt;


















&lt;figure  id=&#34;figure-in-2019-google-reported-that-its-quantum-processor-took-just-over-three-minutes-to-solve-a-problem-that-the-equivalent-state-of-the-art-classical-supercomputer-would-take-approximately-10000-years-to-crack&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;In 2019, Google reported that its quantum processor took just over three minutes to solve a problem that the equivalent “state-of-the-art classical supercomputer would take approximately 10,000 years” to crack.&#34; srcset=&#34;
               /blog/20230213-njbiz/Computer-stock-art-385x275_hu_4e67f387271cfce4.webp 400w,
               /blog/20230213-njbiz/Computer-stock-art-385x275_hu_2020d72d063eef4f.webp 760w,
               /blog/20230213-njbiz/Computer-stock-art-385x275_hu_7f4bbfabe2b01120.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230213-njbiz/Computer-stock-art-385x275_hu_4e67f387271cfce4.webp&#34;
               width=&#34;385&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      In 2019, Google reported that its quantum processor took just over three minutes to solve a problem that the equivalent “state-of-the-art classical supercomputer would take approximately 10,000 years” to crack.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In contrast, quantum computers – which are still being developed and refined – use multiple probability states to attack a problem, which can result in exponentially faster processing ability. In 2019, for example, Google reported that its quantum processor took just over three minutes to solve a problem that the equivalent “state-of-the-art classical supercomputer would take approximately 10,000 years” to crack.&lt;/p&gt;
&lt;p&gt;There is some good news for the “good actors,” noted Bader. “Since 2016, NIST [the federal National Institute of Standards and Technology] has been soliciting encryption algorithms (instructions for solving a problem or performing a computation) that will be resistant to quantum computers. In 2022, NIST announced that four encryption algorithms were selected and will be incorporated in the agency’s post-quantum cryptographic standard, which is expected to be finalized around 2024.”&lt;/p&gt;
&lt;p&gt;Once the standard is fully developed, it will likely initially be released to military and intelligence agencies before gradually trickling down for general public and commercial use, he said. “So small- and medium-sized businesses won’t need a Pentagon-sized budget to utilize quantum-resistant e-commerce, email and other digital assets. Instead, their banks, and other providers will bake the upgraded encryption standards into their products, similar to e-commerce and other protections they currently embed. Over time, all providers of apps, log-ons and web interfaces will shift to the new algorithms. Of course the bad actors will continue to innovate, but NJIT and other institutions will continue to research ways to improve cyber defenses.”&lt;/p&gt;
&lt;h2 id=&#34;if-not-now-when&#34;&gt;If not now, when?&lt;/h2&gt;
&lt;p&gt;The security threat presented by quantum computers is something to be concerned about, “But we don’t need to worry immediately about it,” according to Jaideep Vaidya, a management science and information systems professor at &lt;a href=&#34;https://www.business.rutgers.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rutgers Business School–Newark&lt;/a&gt; and New Brunswick.&lt;/p&gt;


















&lt;figure  id=&#34;figure-jaideep-vaidya-management-science-and-information-systems-professor-at-rutgers-business-schoolnewark-and-new-brunswick&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Jaideep Vaidya, management science and information systems professor at Rutgers Business School–Newark and New Brunswick&#34; srcset=&#34;
               /blog/20230213-njbiz/JAIDEE_1-120x150_hu_396b47fe21d7fa4.webp 400w,
               /blog/20230213-njbiz/JAIDEE_1-120x150_hu_17e819683c96bd6e.webp 760w,
               /blog/20230213-njbiz/JAIDEE_1-120x150_hu_f2cb7e94f9b38619.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230213-njbiz/JAIDEE_1-120x150_hu_396b47fe21d7fa4.webp&#34;
               width=&#34;120&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Jaideep Vaidya, management science and information systems professor at Rutgers Business School–Newark and New Brunswick
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;“Cryptography, or taking data and converting it to something that looks like gibberish, underlies all secure communication,” he said. “So everything sensitive we do on the internet such as sending credit-card information to purchase goods is protected, as indicated by the ‘https’ [Hypertext Transfer Protocol Secure] that prefaces a web address. We are not quite at the point where quantum computers can crack these encryptions, but it is important to keep government and other computers secure even when large quantum computers pose a threat to RSA and other encryption. However, that won’t happen until at least a few more years.”&lt;/p&gt;
&lt;p&gt;NIST plans to have new post-quantum cryptographic standards developed by 2024, he added, “so the main objective then will be to move the existing infrastructure, like browsers and the underlying public-key technology, to the new standard. For the most part, businesses will not need to make major changes, but they should be aware of NIST’s guidance and follow any recommendations about managing their activity once the new standards are issued and deployed.”&lt;/p&gt;


















&lt;figure  id=&#34;figure-emazzanti-technologies-president-carl-mazzanti&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;eMazzanti Technologies President Carl Mazzanti&#34; srcset=&#34;
               /blog/20230213-njbiz/Carl-Mazzanti_headshot-120x150_hu_3f1d3de5ff555db4.webp 400w,
               /blog/20230213-njbiz/Carl-Mazzanti_headshot-120x150_hu_7e1edcf64a7db7c4.webp 760w,
               /blog/20230213-njbiz/Carl-Mazzanti_headshot-120x150_hu_71d3bde01e2f9d04.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230213-njbiz/Carl-Mazzanti_headshot-120x150_hu_3f1d3de5ff555db4.webp&#34;
               width=&#34;120&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      eMazzanti Technologies President Carl Mazzanti
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The opportunities and challenges of quantum computing are just “one more example of the way that things which were ‘theoretical’ at the turn of this century are already real and available for purchase,” according to &lt;a href=&#34;https://njbiz.com/tech-intelligence-application-denied/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carl Mazzanti&lt;/a&gt;, president of &lt;a href=&#34;https://www.emazzanti.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eMazzanti Technologies&lt;/a&gt;. “Computing power is growing so fast that it forces cybersecurity professionals to keep on changing their own strategies. Consider that a smartphone today is exponentially more powerful than the guidance computer that NASA used for the 1969 Apollo 11 mission.”&lt;/p&gt;
&lt;p&gt;The swift changes – and the security challenges that quantum computing is expected to bring – mean that users have to be prepared to modify their behavior, he added. “The useful life of a password is getting shorter and shorter, so the days of ‘set them and forget them’ are over,” he explained. “Now, the thing you forget to change can be your biggest vulnerability.”&lt;/p&gt;
&lt;p&gt;Mazzanti suggested a layered defense, “&lt;a href=&#34;https://njbiz.com/closing-entry-where-it-all-begins/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;beginning with a stronger password&lt;/a&gt; of at least 14 characters. That should be supplemented by commitments to use MFA [multifactor authentication, a secondary identification challenge in addition to entering a password] and to update software by periodically downloading security ‘patches’ from developers and providers.”&lt;/p&gt;
&lt;p&gt;He said business owners and others should also consider deploying monitoring solutions, like Security Event and Incident Management systems, which are cybersecurity products and services that provide real time analysis, monitoring, and alerts. “The MFA of a client malfunctioned, and our SIEM [security information and event management] instantly alerted us, so their password was changed – and another seven characters were added to it – within five minutes, before anything could be compromised. Businesses have information that hackers want, so business owners need to keep up with IT and cybersecurity advances so they can continue to protect their information.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://njbiz.com/will-computing-advances-mean-the-end-of-digital-privacy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njbiz.com/will-computing-advances-mean-the-end-of-digital-privacy/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accenture Working with Institute for Data Science to Combat Software Supply Chain Attacks</title>
      <link>http://localhost:1313/blog/20230208-njit/</link>
      <pubDate>Wed, 08 Feb 2023 18:17:45 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230208-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20230208-njit/PXL_20220729_132613811_hu_efd1a6bcc7417351.webp 400w,
               /blog/20230208-njit/PXL_20220729_132613811_hu_4b33ee97c0c99c27.webp 760w,
               /blog/20230208-njit/PXL_20220729_132613811_hu_e1f5087733d5688.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230208-njit/PXL_20220729_132613811_hu_efd1a6bcc7417351.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Accenture, a leader in information consulting services, is collaborating with NJIT’s Institute for Data Science, led by &lt;strong&gt;Distinguished Prof. David Bader&lt;/strong&gt;, to develop methods to mitigate risks arising in the use of open-source components in the software supply chain.&lt;/p&gt;
&lt;p&gt;Modern cloud-based software is incredibly complex and often uses open-source code, which is cost-free and provides developers with countless libraries of prewritten functions. But that openness is also a risk, because anyone can change the code and it&amp;rsquo;s not always clear who made the changes or what motivated them. &lt;a href=&#34;https://www.techtarget.com/searchsecurity/news/252527814/Tenable-72-of-organizations-remain-vulnerable-to-Log4Shell&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A recent malicious exploit&lt;/a&gt; in open-source code grabbed headlines in 2021, and continues to pose a risk today.&lt;/p&gt;
&lt;p&gt;Lisa O’Connor, global leader of cybersecurity research and development for Accenture, shared the importance of this collaboration. “Understanding supply chain cyber risk is a business resilience imperative. It’s not enough to know the application or service, it’s essential that we understand the code components that make up the application or service,” O’Connor said.&lt;/p&gt;
&lt;p&gt;Traditional software bill-of-materials (SBOM) applications — inventories of all the components of a software program, such as containers, licenses, microservices, security patches and versions — may no longer be sufficient to keep track of the software supply chain for security purposes.&lt;/p&gt;
&lt;p&gt;Bader and his team, along with peers at Accenture, aim to explore the benefits of next-generation software bills of materials and the traceability of the software supply chain to identify security threats. They are applying structures known as knowledge graphs to model the connections between the software components.&lt;/p&gt;


















&lt;figure  id=&#34;figure-a-knowledge-graph-visually-maps-data-and-represents-relationships-edges-between-entities-nodes-credit-benevolent-ai&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34; A knowledge graph visually maps data and represents relationships (edges) between entities (nodes). Credit: Benevolent AI&#34; srcset=&#34;
               /blog/20230208-njit/benevolent%20AI_hu_291baab93429a3d1.webp 400w,
               /blog/20230208-njit/benevolent%20AI_hu_9a68ec3e9392360.webp 760w,
               /blog/20230208-njit/benevolent%20AI_hu_95a398db583c1239.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230208-njit/benevolent%20AI_hu_291baab93429a3d1.webp&#34;
               width=&#34;690&#34;
               height=&#34;502&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A knowledge graph visually maps data and represents relationships (edges) between entities (nodes). Credit: Benevolent AI
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;By examining these graphs using Bader’s graph analytical tools, software developers can identify and isolate vulnerable components that may put their software at risk. Finally, the technology makes suggestions about which components to include and which to be removed or strengthened.&lt;/p&gt;
&lt;p&gt;“Protecting the open-source software supply chain has tremendous impact to the security of businesses and government. Combining our expertise, the Institute’s real-world graph analytics and Accenture’s cybersecurity expertise, yields a world-class team for addressing this critical supply chain problem,” Bader stated.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re excited about this collaboration with the Accenture team and the results we hope to achieve together,&amp;rdquo; Bader added. &amp;ldquo;We plan to share our results in the months to come.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/accenture-working-institute-data-science-combat-software-supply-chain-attacks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/accenture-working-institute-data-science-combat-software-supply-chain-attacks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>M.S. Degrees in Security, Data Science Ranked No. 4 in Nation by Fortune</title>
      <link>http://localhost:1313/blog/20230131-njit/</link>
      <pubDate>Tue, 31 Jan 2023 15:37:45 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230131-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-at-njit-researchers-put-ai-and-data-science-into-action&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;At NJIT, researchers put AI and data science into action&#34; srcset=&#34;
               /blog/20230131-njit/iStock-675938062_hu_c17fedea9a069a38.webp 400w,
               /blog/20230131-njit/iStock-675938062_hu_aea1cdd99ea6c1a4.webp 760w,
               /blog/20230131-njit/iStock-675938062_hu_f4b6a4f3d4d0d3ed.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230131-njit/iStock-675938062_hu_c17fedea9a069a38.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      At NJIT, researchers put AI and data science into action
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey Institute of Technology is now ranked fourth in the nation for its &lt;a href=&#34;https://fortune.com/education/information-technology/best-masters-in-cybersecurity/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M.S. in Cybersecurity&lt;/a&gt; and &lt;a href=&#34;https://fortune.com/education/information-technology/best-online-masters-in-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online M.S. in Data Science&lt;/a&gt;, according to Fortune magazine.&lt;/p&gt;
&lt;p&gt;Researchers in both fields were quite busy in 2022, working at the Newark main campus and NJIT@JerseyCity, and through Ying Wu College of Computing, Cybersecurity Research Center, Institute for Data Science. There is also collaboration with Israel&amp;rsquo;s Ben Guiron University of the Negev through the mutually operated Institute for Future Technologies.&lt;/p&gt;
&lt;p&gt;Data science — along with its cousins in artificial intelligence and machine learning — pervades many fields beyond computing, such as humanities, mathematics and physics. The year 2022 kicked off with &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;, director of Institute for Data Science, being named a &lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fellow of the ACM&lt;/a&gt;. Associate Professor Przemyslaw Musialski figured out how to make 2D designs that &lt;a href=&#34;https://news.njit.edu/new-research-shows-how-2d-layouts-can-be-popped-3d-structures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;convert into&lt;/a&gt; 3D structures. Machine learning helped &lt;a href=&#34;https://news.njit.edu/njit-machine-learning-expert-pan-xu-combats-covid-vaccine-inequity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;combat COVID vaccine inequity&lt;/a&gt;, students &lt;a href=&#34;https://news.njit.edu/revitalized-data-science-club-eyes-insightful-statistics-njit-athletics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;formed a data science club&lt;/a&gt;, while data tools were used to help local journalists &lt;a href=&#34;https://news.njit.edu/revitalized-data-science-club-eyes-insightful-statistics-njit-athletics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;understand public information&lt;/a&gt; and to help self-driving cars &lt;a href=&#34;https://news.njit.edu/njit-expert-evaluating-self-driving-car-behavior-yellow-lights&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;understand traffic signals&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In security research for 2022, NJIT&amp;rsquo;s Association for Computing Machinery chapter hosted its annual security event called JerseyCTF, which reached new levels of &lt;a href=&#34;https://news.njit.edu/jerseyctf-competition-shatters-record-highest-participation-around-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;worldwide participation&lt;/a&gt;. Professor Reza Curtmolo and peers &lt;a href=&#34;https://news.njit.edu/browser-bug-exposes-user-data-top-websites-njit-researchers-find&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;discovered a critical browser bug&lt;/a&gt; and developed a software cure. Students form &lt;a href=&#34;https://news.njit.edu/students-will-design-hacking-apps-hardware-exploits-gain-experience&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a pair of new security clubs&lt;/a&gt;, including one focused on hardware hacking. In addition, NJIT hosted a &lt;a href=&#34;https://news.njit.edu/njit-hosts-big-data-alliance-symposium-focus-education&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Big Data Alliance Symposium&lt;/a&gt;, students developed a &lt;a href=&#34;https://news.njit.edu/computing-senior-makes-fake-tweets-detector-satire-checker-next&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fake Tweet detector&lt;/a&gt; and a graduate student applied artificial intelligence to &lt;a href=&#34;https://news.njit.edu/phd-students-research-reduce-wind-turbine-costs-15m-annually&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;offshore windmill maintenance&lt;/a&gt;. AI was further applied to &lt;a href=&#34;https://news.njit.edu/artificial-intelligence-pushes-internet-device-cpus-outside-box&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;new CPU architectures&lt;/a&gt; and &lt;a href=&#34;https://news.njit.edu/artificial-intelligence-pushes-internet-device-cpus-outside-box&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;supply chain research&lt;/a&gt;. There was a &lt;a href=&#34;https://news.njit.edu/experts-njits-data-science-summit-propose-new-paths-hardware-and-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Summit&lt;/a&gt;, while &lt;a href=&#34;https://news.njit.edu/turing-laureate-compiler-pioneer-jeffrey-ullman-visits-njit-data-science-lecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Turing Award winner Jeffrey Ullman&lt;/a&gt; gave his own insights.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are very pleased to note that our graduate programs in cybersecurity and data science are highly ranked and among the best in the nation,&amp;rdquo; said NJIT Interim Provost Atam Dhawan. &amp;ldquo;Computing, data science and information systems are the priority strategic areas for NJIT for developing prominence and future growth. Congratulations to NJIT faculty, students and staff in Ying Wu College of Computing for this outstanding recognition of their handwork, dedication, synergy and vision.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/ms-degrees-security-data-science-ranked-no-4-nation-fortune&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/ms-degrees-security-data-science-ranked-no-4-nation-fortune&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT College of Computing Dean Craig Gotsman Named ACM Fellow</title>
      <link>http://localhost:1313/blog/20230118-njit/</link>
      <pubDate>Wed, 18 Jan 2023 09:56:21 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230118-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-ywcc-dean-craig-gotsman-left&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;YWCC Dean Craig Gotsman, left&#34; srcset=&#34;
               /blog/20230118-njit/deangotsman-3-cropped_hu_9e4098d7e6f3146e.webp 400w,
               /blog/20230118-njit/deangotsman-3-cropped_hu_e4f31f2eafa69e3c.webp 760w,
               /blog/20230118-njit/deangotsman-3-cropped_hu_40662ece2210d0c0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20230118-njit/deangotsman-3-cropped_hu_9e4098d7e6f3146e.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      YWCC Dean Craig Gotsman, left
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;NJIT’s dean of Ying Wu College of Computing, Craig Gotsman, is among 57 researchers worldwide named as a &lt;a href=&#34;https://www.acm.org/media-center/2023/january/fellows-2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022 Fellow of the Association for Computing Machinery&lt;/a&gt;, the leading global organization for computing research.&lt;/p&gt;
&lt;p&gt;The ACM Fellows program, &lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_fellows_of_the_Association_for_Computing_Machinery#1994&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;since 1993&lt;/a&gt;, recognizes the top 1% of ACM members annually for their outstanding accomplishments in computing and information technology, as well as or service to ACM and the larger computing community. These individuals have made significant contributions in topics across the spectrum of computing, including algorithms, data science, graphics, cybersecurity, mobile and networked systems and other technological fields in daily use.&lt;/p&gt;
&lt;p&gt;Gotsman, a Distinguished Professor at NJIT, was cited for his contributions to computer graphics, geometry processing and visual computing. His research focuses on 3D computer graphics, geometry processing, animation and computational geometry. He made influential contributions, both theoretical and applied, in 3D compression, mesh parameterization, shape deformation methods and video processing.&lt;/p&gt;
&lt;p&gt;Publishing more than 170 research papers, he won eight best paper awards at leading conferences and mentored more than 50 postgraduate students at the masters, doctoral and postdoctoral levels. Among his contributions are 3D coding algorithms incorporated into the MPEG-4 standard.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I am honored to be named an ACM Fellow,&amp;rdquo; Gotsman said. &amp;ldquo;Coming from the leading professional organization in my field, this is a significant distinction, and it is extremely satisfying to have my work recognized in this way.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Computing’s most important advances are often the result of a collection of many individual contributions, which build upon and complement each other,&amp;rdquo; explained ACM President Yannis Ioannidis. &amp;ldquo;But each individual contribution is an essential link in the chain. The ACM Fellows program is a way to recognize the women and men whose hard work and creativity happens inconspicuously but drives our field.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Gotsman holds multiple US patents, is a fellow of the National Academy of Inventors and a member of the Academy of Europe (Academia Europae). He is also the co-founder of three startup companies, two based on his academic research, the most recent developing 3D video processing technologies which was  acquired by Apple in 2017.&lt;/p&gt;
&lt;p&gt;An induction ceremony will take place at the ACM annual meeting in June in San Francisco. Gotsman is NJIT&amp;rsquo;s second inductee, following &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; &lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;last year&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-college-computing-dean-craig-gotsman-named-acm-fellow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-college-computing-dean-craig-gotsman-named-acm-fellow&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader selected as Sigma Xi Distinguished Lecturer 2023–2024</title>
      <link>http://localhost:1313/blog/20230101-sigmaxi/</link>
      <pubDate>Sat, 31 Dec 2022 15:40:35 -0500</pubDate>
      <guid>http://localhost:1313/blog/20230101-sigmaxi/</guid>
      <description>&lt;h2 id=&#34;sigma-xi-distinguished-lecturers-20232024&#34;&gt;Sigma Xi Distinguished Lecturers 2023&amp;ndash;2024&lt;/h2&gt;
&lt;p&gt;For the 85th year, Sigma Xi presents its panel of Distinguished
Lecturers as an opportunity for chapters to host visits from
outstanding individuals who are at the leading edge of science. These
visitors communicate their insights and excitement on a broad range of
topics.&lt;/p&gt;
&lt;p&gt;The Distinguished Lecturers are available from July 1, 2023, to June
30, 2024. Each speaker has consented to a modest honorarium together
with full payment of travel costs and subsistence.&lt;/p&gt;
&lt;p&gt;Local chapters may apply for subsidies to support expenses related to
hosting a Distinguished Lecturer. Applications must be submitted
online by March 1, 2023, for funds to be available the next fiscal
year.&lt;/p&gt;
&lt;p&gt;Additional support for the program comes from the American
Meteorological Society. Lecturer biographies, contact information, and
additional details can be found online at sigmaxi.org/lectureships or
by sending an email to &lt;a href=&#34;mailto:lectureships@sigmaxi.org&#34;&gt;lectureships@sigmaxi.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Marc Imhoff, &lt;em&gt;Chair&lt;/em&gt;&lt;br&gt;
&lt;em&gt;Committee on Lectureships&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Distinguished Professor and Director of the Institute for Data Science, New Jersey Institute of Technology&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Solving Global Grand Challenges with High Performance Data Analytics&lt;/em&gt; (P, G, S)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Predictive Analysis from Massive Knowledge Graphs&lt;/em&gt; (P, G, S)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Interactive Data Science at Scale&lt;/em&gt; (P, G, S)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Rise of A.I. - The Series</title>
      <link>http://localhost:1313/blog/20221222-techtalk/</link>
      <pubDate>Thu, 22 Dec 2022 09:52:48 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221222-techtalk/</guid>
      <description>&lt;p&gt;Discover a world of artificial intelligence all around us, listening, seeing, hearing and soon it could overtake us! The Rise of A.I takes you deep inside the experts and creators behind artificial intelligence of our generation.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221222-techtalk/10_hu_7199eb1bbf0cda0b.webp 400w,
               /blog/20221222-techtalk/10_hu_af1253b58d67c3e0.webp 760w,
               /blog/20221222-techtalk/10_hu_b3093d896b327762.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221222-techtalk/10_hu_7199eb1bbf0cda0b.webp&#34;
               width=&#34;760&#34;
               height=&#34;325&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/MLybkx9mVdo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2 id=&#34;discover-what-our-future-has-in-store&#34;&gt;Discover what Our Future has in store!&lt;/h2&gt;
&lt;p&gt;We are already developing a new generation of intelligent and super powered beings. With our tiny understanding of our own human abilities, who knows what will become of these bionic, self-learning super creatures. And we passed the point of no return? The rise of AI, artificial intelligence, will take you to see the vast evolution robotics, machine learning and artificial programming. A doc-series designed to educate and inform the masses on how A.I. and machine learning is being integrated into our daily lives, and what the future holds. We also meet with some of the top experts from brands such as Samsung, Spotify, Google and others to hear how artificial intelligence shapes their business and our future.&lt;/p&gt;
&lt;p&gt;Streaming on &lt;a href=&#34;https://www.amazon.com/gp/video/detail/B0B6P66MNP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Amazon Prime Video&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.techtalkmedia.tv/projects/the-rise-of-ai-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.techtalkmedia.tv/projects/the-rise-of-ai-series.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracing the Roots of a Supercomputer</title>
      <link>http://localhost:1313/blog/20221201-maryland/</link>
      <pubDate>Sun, 11 Dec 2022 12:58:40 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221201-maryland/</guid>
      <description>&lt;p&gt;Within the clark school’s
Innovation Hall of Fame (IHOF),
&lt;strong&gt;David Bader (Ph.D. ’96)&lt;/strong&gt;—
the hall’s 2022 inductee—
joins a distinguished community of inventors who use
their knowledge, perseverance,
innovation, and ingenuity to
change the world.
Bader “democratized” supercomputing by designing the first Linux Supercomputer RoadRunner for open use by the national science and engineering
community from a prototype he built in 1998 using commodity off-the-shelf parts. His computer was first used in April
1999; the top 500 supercomputers operating in the world
today have roots in Bader’s early work.
In a Q&amp;amp;A, Engineering at Maryland asks Bader about
his path to IHOF:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You learned to program before you could read or write.
How did this shape the direction of your future work?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My father was a physical chemist and an early computational scientist and my mother is an inorganic chemist,
so they both get a lot of credit for my appreciation of the
sciences and engineering.
In the 1970s, “programming” really meant the thoughtful use of resources, because every
computer was limited in terms of
memory size, cost of operations,
etc. It set me in a direction of
thinking about algorithms and how
we can accelerate algorithms to run
even faster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your work on supercomputers
and high-performance computing
began in the 90s. What drove you?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At the time, I had a couple areas
of focus that I was very passionate
about; one was thinking about our
nation’s security and how we spread
peace to everyone on the planet.
For me, understanding large
data sets was important. Exploring
data sets from remote sensing of the
Earth, biological sequences, and social networks led the way to
what we see today with big data and the massive scale of data
science, high performance data analytics, and the area that I’ve
pioneered in massive graph analytics.
As we have access to more and more data sets, we can
now think about the power of combining these together to
solve today’s global grand challenges. To me, that’s just a
tremendous opportunity to improve the world and to make
it a better place for everyone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why was “democratizing” supercomputers important to you?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I grew up in a first-generation American family. My
mother was a child Holocaust survivor from Europe, and
my dad’s parents immigrated to the United States. In the
late 1970s when I was in 5th grade, I started delivering newspapers to earn enough money to buy my own computer. In
the early 1980s I discovered parallel computing, but to get
into supercomputing you needed millions of dollars to buy
a unique commercial system.
I had a passion to harness multiple processors together to
solve important and computationally demanding problems,
and I recognized early that this would require harnessing the
economics of commodity-based systems to make supercomputing ubiquitous.
To me, democratizing supercomputers means making
them accessible to anyone in the world who wishes to work
on these kinds of big problems. Solutions to real-world grand
challenges can come from anyone, not just from people who
can afford million-dollar supercomputers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What do you hope for the future?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I would like to see the continued leveraging of commodity
technologies and miniaturization of technologies so that we
could deploy supercomputers ubiquitously. These systems
must integrate new architectures that combine scientific computing capabilities with new technologies for supporting data
science, machine learning, and artificial intelligence.
There are so many phenomenal uses that if everyone
had access to a supercomputer—maybe we will see them in
our next generation of smartphones—then we would see a
tremendous amount of change.
That’s really my dream: to make supercomputing access
as simple as driving a car or reading a book or carrying your
smartphone, so that everyone with a question can solve
real-world grand challenge problems quite easily&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://view.publitas.com/umdclarkschool/fall-2022-engineering-at-maryland-magazine/page/26-27&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://view.publitas.com/umdclarkschool/fall-2022-engineering-at-maryland-magazine/page/26-27&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NESA Members in Action: David Bader</title>
      <link>http://localhost:1313/blog/20221129-nesa/</link>
      <pubDate>Tue, 29 Nov 2022 10:23:42 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221129-nesa/</guid>
      <description>&lt;p&gt;National Eagle Scout Association (NESA)&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader-1985&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader (1985)&#34; srcset=&#34;
               /blog/20221129-nesa/2022-09-16_Bader_David_hu_79fc838e11ee937d.webp 400w,
               /blog/20221129-nesa/2022-09-16_Bader_David_hu_d09ec8508328d3ff.webp 760w,
               /blog/20221129-nesa/2022-09-16_Bader_David_hu_1b2791a1854c91f0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221129-nesa/2022-09-16_Bader_David_hu_79fc838e11ee937d.webp&#34;
               width=&#34;500&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader (1985)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Eagle Scout &lt;strong&gt;David Bader&lt;/strong&gt; received one of the highest awards in computing for inventing commodity-based supercomputing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nesa.org/faces_places/david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nesa.org/faces_places/david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic Data Science Alliance Picks Up Steam</title>
      <link>http://localhost:1313/blog/20221122-datanami/</link>
      <pubDate>Tue, 22 Nov 2022 21:03:35 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221122-datanami/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Alex Woodie&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-matej-kastelicshutterstock&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;(Matej Kastelic/Shutterstock)&#34; srcset=&#34;
               /blog/20221122-datanami/college_shutterstock_Matej-Kastelic-300x200_hu_ba708a07e8a37c51.webp 400w,
               /blog/20221122-datanami/college_shutterstock_Matej-Kastelic-300x200_hu_4f3407b4f420ce40.webp 760w,
               /blog/20221122-datanami/college_shutterstock_Matej-Kastelic-300x200_hu_bdd4e5849537d9bb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221122-datanami/college_shutterstock_Matej-Kastelic-300x200_hu_ba708a07e8a37c51.webp&#34;
               width=&#34;300&#34;
               height=&#34;200&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      (Matej Kastelic/Shutterstock)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Universities looking for resources to build their data science curriculums and degree programs have a new resource at their disposal in the form of the Academic Data Science Alliance. Founded just prior to the pandemic, the ADSA survived COVID and now it’s working to foster a community of data science leaders at universities across North America and Europe.&lt;/p&gt;
&lt;p&gt;The roots of &lt;a href=&#34;https://academicdatascience.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic Data Science Alliance&lt;/a&gt; go back to 2014 at the University of Washington, one of three schools selected by the &lt;a href=&#34;https://sloan.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Alfred P Sloan Foundation&lt;/a&gt; and the &lt;a href=&#34;https://www.moore.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gordon and Betty Moore Foundation&lt;/a&gt; to foster and enhance data-intensive discovery in university settings. UC Berkeley and NYU were the other schools to participate in the Moore-Sloan Data Science Environments.&lt;/p&gt;
&lt;p&gt;As an executive director of University of Washington’s &lt;a href=&#34;https://escience.washington.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eScience Institute&lt;/a&gt;, Micaela Parker worked with the foundations on this academic data science experiment. An oceanographer by training, Parker struggled with big data and statistics early in her academic career and realized there was a large need to use data science concepts and techniques across different academic disciplines.&lt;/p&gt;
&lt;p&gt;After the fifth and final year of the MSDSEs, it was clear the program was a success at the eScience Institute and other universities. In addition to achieving critical mass and becoming self-sustaining, the faculty and students benefited from the “rich intellectual” environment of the institute and kickstarted a data science career track, &lt;a href=&#34;https://escience.washington.edu/reflections-on-five-years-of-the-moore-sloan-data-science-environments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;among other findings&lt;/a&gt; of the program at the eScience Institute.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221122-datanami/academic-data-science-alliance_logo-300x144_hu_5d86d75ae747c0b9.webp 400w,
               /blog/20221122-datanami/academic-data-science-alliance_logo-300x144_hu_a3825e015cc040b.webp 760w,
               /blog/20221122-datanami/academic-data-science-alliance_logo-300x144_hu_884a978631b794fe.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221122-datanami/academic-data-science-alliance_logo-300x144_hu_5d86d75ae747c0b9.webp&#34;
               width=&#34;300&#34;
               height=&#34;144&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;But that success triggered another problem, Parker says. “The two foundations recognized they couldn’t afford to invest to that extent in all of the universities nationwide,” she tells &lt;em&gt;Datanami&lt;/em&gt; in an interview. “So they approached me…and asked me to put together a proposal for a national organization that would do some of the same things.”&lt;/p&gt;
&lt;h2 id=&#34;data-science-education&#34;&gt;Data Science Education&lt;/h2&gt;
&lt;p&gt;Thus was the beginning of the ADSA. Backed by the same two foundations, Parker founded the organization in the spring of 2019, and she has led the program as its executive director ever since.&lt;/p&gt;
&lt;p&gt;The group’s charter has several items. First and foremost, it serves as a clearinghouse of information for heads of data science departments or university-wide institutes  and deans of related colleges at universities around the country.&lt;/p&gt;


















&lt;figure  id=&#34;figure-micaela-parker-founder-and-executive-director-of-the-academic-data-science-alliance&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Micaela Parker, founder and executive director of the Academic Data Science Alliance&#34; srcset=&#34;
               /blog/20221122-datanami/micaela-parker_hu_9255592716b4bf6a.webp 400w,
               /blog/20221122-datanami/micaela-parker_hu_1a6f17b97fd90c0.webp 760w,
               /blog/20221122-datanami/micaela-parker_hu_8299e2fd66cebaa6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221122-datanami/micaela-parker_hu_9255592716b4bf6a.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Micaela Parker, founder and executive director of the Academic Data Science Alliance
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Micaela Parker, founder and executive director of the Academic Data Science Alliance&lt;/p&gt;
&lt;p&gt;In addition to helping to establish relevant data science curricula for bachelors, Master’s and PhD programs, the ADSA helps universities think about how to attract professors and build a staff for their data science programs, which is a major challenge.&lt;/p&gt;
&lt;p&gt;“It’s like the chicken and the egg,” Parker says. “There hasn’t been a PhD in data science to hire into your faculty for data science. You have to kind of say, okay, what fields would I hire from, what qualifications am I looking for? And then on top of that, everybody is hiring.”&lt;/p&gt;
&lt;p&gt;The ADSA also hosts a variety of events and workshops, including an annual meeting and the &lt;a href=&#34;https://academicdatascience.org/adsa-meetings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Leadership Summit&lt;/a&gt;, which is held every spring (next year’s summit is scheduled for May 8-10 at Boston University).&lt;/p&gt;
&lt;p&gt;The group also caters to data science students, and posts ads for data science jobs, both in industry and in academia. It doesn’t have a formal journal, but it does publish the ADSA Monthly newsletter as well as the &lt;a href=&#34;https://academicdatascience.org/resources/newsletter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data Science Community Newsletter&lt;/a&gt;, which was originally started at NYU and has nearly 9,000 readers. It has an active Slack channel with more than 900 members. In general, it seeks to act as the glue keeping the academic data science community together.&lt;/p&gt;
&lt;p&gt;“Our mission is to connect any data science researchers, educators, or practitioners in academia to help them to do better data science,” Parker says.&lt;/p&gt;
&lt;h2 id=&#34;adsa-finds-growth&#34;&gt;ADSA Finds Growth&lt;/h2&gt;
&lt;p&gt;Parker had just finished standing up the ADSA and hosting the first round of events in early 2020 when COVID hit. Like other groups, the ADSA pivoted to virtual conferences and Zoom meetings. Parker and her staff used the downtime to build up the group’s resources, and after the lockdowns ended, the group went back to hosting in-person events, which are a critical aspect of its charter.&lt;/p&gt;
&lt;p&gt;ADSA has more than 45 paying academic members (Image courtesy ADSA)&lt;/p&gt;


















&lt;figure  id=&#34;figure-adsa-has-more-than-45-paying-academic-members-image-courtesy-adsa&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ADSA has more than 45 paying academic members (Image courtesy ADSA)&#34; srcset=&#34;
               /blog/20221122-datanami/ADSA_Institution-members-725x410_hu_8761eff1097d1db7.webp 400w,
               /blog/20221122-datanami/ADSA_Institution-members-725x410_hu_49bb55e769e24613.webp 760w,
               /blog/20221122-datanami/ADSA_Institution-members-725x410_hu_43a264b293f53b9f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221122-datanami/ADSA_Institution-members-725x410_hu_8761eff1097d1db7.webp&#34;
               width=&#34;725&#34;
               height=&#34;410&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ADSA has more than 45 paying academic members (Image courtesy ADSA)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;So far, the group has attracted 45 universities to its ranks of dues-paying members, although it has many more universities participating in programs. The organization is approaching critical mass, and is on a path to becoming self-sustaining, just like the eScience Institute did before it.&lt;/p&gt;
&lt;p&gt;The ADSA has filled a need for many heads of data science programs, including &lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor and the inaugural director of the Institute for Data Science at the New Jersey Institute of Technology (NJIT).&lt;/p&gt;
&lt;p&gt;“It’s a fantastic group,” Bader says. “The Academic Data Science Alliance is a gathering valuable point for schools that are in the process of creating data science programs, departments, centers, and institutes.”&lt;/p&gt;
&lt;p&gt;Bader, who was recruited to NJIT in 2019 as the founding director of the &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt;, tapped the collective knowledge of the ADSA earlier this year when his department created two new degree programs, one for a bachelor’s and another for PhD in data science. (NJIT has had a MS in data science running since 2017.) The ADSA was instrumental in providing guidance for developing new degree programs.&lt;/p&gt;
&lt;p&gt;“We needed outside experts to review our curriculum, to give feedback to our university leadership and the state, and we were able to rely on members of the Academic Data Science Alliance to help us improve our offerings,” Bader tells &lt;em&gt;Datanami&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-professional-society&#34;&gt;A Professional Society?&lt;/h2&gt;
&lt;p&gt;While Parker initially tried to keep ADSA a more informal group with a close, grass-roots connection to its members, the ADSA has gradually come to resemble a professional society.&lt;/p&gt;
&lt;p&gt;According to Bader, the ADSA serves the need for a professional data science society in the same way that the &lt;a href=&#34;https://www.acm.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Association for Computing Machinery&lt;/a&gt; (ACM) serves the computer science community.&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader-distinguished-professor-and-founding-director-of-the-institute-for-data-science-at-the-new-jersey-institute-for-technology&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader, distinguished professor and founding director of the Institute for Data Science at the New Jersey Institute for Technology&#34; srcset=&#34;
               /blog/20221122-datanami/Bader-2022-square-300x300_hu_62be3b8db4440b63.webp 400w,
               /blog/20221122-datanami/Bader-2022-square-300x300_hu_8c76cb056e198305.webp 760w,
               /blog/20221122-datanami/Bader-2022-square-300x300_hu_aed10b295e923710.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221122-datanami/Bader-2022-square-300x300_hu_62be3b8db4440b63.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader, distinguished professor and founding director of the Institute for Data Science at the New Jersey Institute for Technology
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;David Bader, distinguished professor and founding director of the Institute for Data Science at the New Jersey Institute for Technology&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;Computing&lt;/em&gt; has about 70 years on us,” Bader says. “But data science is somewhat unique. For instance, in computer science, most of the members come from computer science and computer engineering departments. For data science we have a far greater breadth. In some universities, data science is part of library and information science. In other places, it’s part of computer science or computing, as it is at NJIT, and other places it comes out of the math department or business school.”&lt;/p&gt;
&lt;p&gt;At NJIT, the Institute of Data Science touches four different colleges and involves 40 faculty members. Bader sees the ADSA as being instrumental in helping fledgling data science programs chart a path through these issues and identify which faculty members and topics belong in their program and which ones to avoid.&lt;/p&gt;
&lt;p&gt;“It’s very important that we have a professional society for discussing many of the issues with creating a new academic discipline, that we think about accreditation for academic degree programs, that we think about the university hiring and professional development tracks for promotion, for instance, for research data scientists at the university and then for sharing the best practices for everything data science between these top universities,” Bader says. “There are many types of data scientists and the Academic Data Science Alliance is recognizing that diversity and allowing a meeting point that really brings together these various constituents across many academic disciplines.”&lt;/p&gt;
&lt;p&gt;From her early struggles with bioinformatics as an oceanographer, it was clear that data science touches many different academic disciplines, which is one of the reasons that data science needs its own group. While the ACM and the &lt;a href=&#34;https://www.amstat.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;American Statistical Association&lt;/a&gt; (ASA) did some work in the field of data science, Parker realized that data science deserved its own organization.&lt;/p&gt;
&lt;p&gt;“The debate had been raging, especially in those early days, is data science just applied statistics? And computer science is saying, no it’s an arm of computer science,” Parker says. “So they each tried to create their footprint on data science by creating working groups, and that I think is maybe part of the reason that no one entity emerged until I started this.”&lt;/p&gt;
&lt;p&gt;Parker initially resisted it, but she says that the ADSA is becoming more of a professional association, like the ACM. That’s a good thing for the data science community.&lt;/p&gt;
&lt;p&gt;“I’ve started to realize that that is kind of what we’re becoming, an emerging professional society,” she says. “But we’re still very much focused on those relationships that people have. That’s how we build trust, and that’s how we move the field forward.”&lt;/p&gt;
&lt;h3 id=&#34;related-items&#34;&gt;Related Items:&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2021/08/27/shrinking-the-education-gap-in-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shrinking the Education Gap in Data Science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2021/08/27/shrinking-the-education-gap-in-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In Search of Data Science Talent with Dr. Kirk Borne&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2018/08/23/universities-get-creative-with-data-science-education/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Universities Get Creative with Data Science Education&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2022/11/22/academic-data-science-alliance-picks-up-steam/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/2022/11/22/academic-data-science-alliance-picks-up-steam/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experts at NJIT&#39;s Data Science Summit Propose New Paths in Hardware and AI</title>
      <link>http://localhost:1313/blog/20221114-njit/</link>
      <pubDate>Mon, 14 Nov 2022 16:10:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221114-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221114-njit/Data-Science-Summit-2022-4680_hu_2e084a658525fc5b.webp 400w,
               /blog/20221114-njit/Data-Science-Summit-2022-4680_hu_b862b12f0684b7c8.webp 760w,
               /blog/20221114-njit/Data-Science-Summit-2022-4680_hu_beca200bd34c14a6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221114-njit/Data-Science-Summit-2022-4680_hu_2e084a658525fc5b.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;New kinds of unconventional computer hardware, along with new ways of considering software responsibility, are both necessary if the next wave of data science will do anything more useful for the world than increase corporate profits.&lt;/p&gt;
&lt;p&gt;Such were two key messages expressed by experts from IBM and Google at the Data Science Summit this month, hosted by New Jersey Institute of Technology’s Institute for Data Science, in the NJIT @  JerseyCity location which also houses the university&amp;rsquo;s Institute for Future Technologies and various graduate-level courses from Ying Wu College of Computing.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Computer system performance is far shy of where it needs to be to solve some important problems,&amp;rdquo; said IBM&amp;rsquo;s Manoj Kumar, program director for analytics systems at the T.J. Watson Research Center.&lt;/p&gt;
&lt;p&gt;He said the biggest problem is computers just aren&amp;rsquo;t fast enough to keep up with data intake. This happens in fields such as healthcare, manufacturing, retail and throughout the federal government. Kumar noted that in the Department of Homeland Security, computer systems must digest several petabytes of data per hour. The majority of it can&amp;rsquo;t be processed fast enough, and sometimes that means threats can occur.&lt;/p&gt;
&lt;p&gt;Kumar said new kinds of hardware such as non-volatile memory, in-memory computing and custom chip fabrication are all examples that could help. These developments happen industry-wide and NJIT experts are in the game, such as in computer engineering professor &lt;a href=&#34;https://news.njit.edu/artificial-intelligence-pushes-internet-device-cpus-outside-box&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shaahin Angizi&amp;rsquo;s laboratory&lt;/a&gt; and through the &lt;a href=&#34;https://news.njit.edu/njit-open-state-art-facility-fabricate-nanoelectronic-devices-and-sensors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microfabrication Innovation Center&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On the software side, we must remember that artificial intelligence is still more artificial than intelligent, explained Google&amp;rsquo;s Kathy Meier-Hellstern, principal engineer for responsible AI. &amp;ldquo;These modes have the power for great good or to do great harm,&amp;rdquo; she stated.&lt;/p&gt;
&lt;p&gt;Meier-Hellstern added that human responsibility is needed at every stage of building AI models, which includes gathering the data, training the model, adapting it into a useful piece of software and applying the program to real-world issues. Leaving too many decisions to the bits-and-bytes can and often does cause irresponsibility. She said a mildly offensive example would be software that automatically shows a white male when asked for images of doctors, but that the offenses can run much deeper in technology such as deep fakes or propaganda.&lt;/p&gt;
&lt;p&gt;Google uses so-called guardrail technology, which are classifying filters built into its AI software, that err on the side of false positives when removing poor AI decisions. NJIT researchers are thinking along similar lines, in studying ways to make AI software more transparent about how it arrives at decisions. The Center for Artificial Intelligence Research &lt;a href=&#34;https://news.njit.edu/experts-njits-new-ai-research-center-study-theory-and-applications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;opened in 2021&lt;/a&gt; is led by Distinguished Professor Grace Wang.&lt;/p&gt;
&lt;p&gt;Wang also spoke at the event. She emphasized data science challenges such as human factors, arbitrage trading and inherent randomness, which she said can be balanced by employing tools like natural language processing sentiment analysis, the analysis of voting-by-dollars and generative adversarial networks.&lt;/p&gt;
&lt;p&gt;NJIT professors Senjuti Basu Roy and Reza Curtmola rounded out the day, speaking from their expertise in AI and computer security.&lt;/p&gt;
&lt;p&gt;Azadeh Naderi, a second-year doctoral student in informatics, said Meier-Hellstern was her favorite speaker of the day.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Her field of research was very close to mine and also, I really enjoyed how she presented her work strongly, especially as a woman,&amp;rdquo; said Naderi, who is from Iran and now lives in Newark. Her own work uses quantitative and statistical methods, focusing on digital patronage and social-virtual reality.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;From the large turnout of attendees at our Data Science Summit, ranging from undergraduate and graduate students to industrial researchers, it’s clear the presented topics from cybersecurity to responsible AI have societal importance,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, director of the Institute for Data Science.&lt;/p&gt;
&lt;p&gt;Bader is also a distinguished professor in Ying Wu College of Computing. &amp;ldquo;The 2022 Data Science Summit was a huge success,&amp;rdquo; he said. &amp;ldquo;It was a pleasure spending the day making new connections between our attendees.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/experts-njits-data-science-summit-propose-new-paths-hardware-and-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/experts-njits-data-science-summit-propose-new-paths-hardware-and-ai&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‘Weaponised app’: Is Egypt spying on COP27 delegates’ phones?</title>
      <link>http://localhost:1313/blog/20221112-aljazeera/</link>
      <pubDate>Sat, 12 Nov 2022 10:30:03 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221112-aljazeera/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221112-aljazeera/cop27_hu_5a44b26e3e718d4c.webp 400w,
               /blog/20221112-aljazeera/cop27_hu_950221d9e7a39f88.webp 760w,
               /blog/20221112-aljazeera/cop27_hu_6827a4497e7b290a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221112-aljazeera/cop27_hu_5a44b26e3e718d4c.webp&#34;
               width=&#34;760&#34;
               height=&#34;506&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.aljazeera.com/author/zemelyteb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Beatrice Zemelyte&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cybersecurity concerns have been raised at the United Nations’ COP27 climate talks over an official smartphone app that reportedly has carte blanche to monitor locations, private conversations and photographs.&lt;/p&gt;
&lt;p&gt;About 35,000 people are expected to attend the two-week climate conference in Egypt, and the app has been downloaded more than 10,000 times on Google Play, including by officials from France, Germany and Canada.&lt;/p&gt;
&lt;p&gt;Egypt’s Ministry of Communications and Information Technology developed the app for the summit’s delegates.&lt;/p&gt;
&lt;p&gt;It is meant to assist attendees in smoothly navigating the conference, but “the government of Egypt may have weaponised the app and now has the ability to surveil all of the summit attendees”, &lt;strong&gt;David Bader&lt;/strong&gt;, an expert in data science and cybersecurity, told Al Jazeera.&lt;/p&gt;
&lt;p&gt;Analysts warn the COP27 app can extensively monitor the user’s movement and communications, and is able to read users’ email and encrypted messages, record phone conversations, and even scan the entire device for sensitive information.&lt;/p&gt;
&lt;p&gt;Bader noted while the developer states the app does not collect data: “Surprisingly the app does have the strange ability to access the user’s name, phone number and email address, all of the user’s email – with the ridiculous explanation for ‘app functionality’ and one’s photos for ‘account management’.&lt;/p&gt;
&lt;p&gt;“Would you want a stranger accessing your private photos, let alone a foreign government?” Bader said, warning there could be more clandestinely going on with the app.&lt;/p&gt;
&lt;h2 id=&#34;no-smoking-gun-on-data-collection&#34;&gt;No ‘smoking gun’ on data collection&lt;/h2&gt;
&lt;p&gt;The majority of apps ask permission to access various aspects of a smartphone, including location for GPS functions or cameras for social media, but users need to be cautious, said Kevin Curran, professor of cybersecurity at Ulster University.&lt;/p&gt;
&lt;p&gt;“One has to ask whether each of these permissions are necessary,” Curran said, describing the COP27 app as “highly intrusive”.&lt;/p&gt;
&lt;p&gt;“In this case, it is difficult to identify a smoking gun. What we cannot ascertain is whether the Egyptian government is using this for data collection,” Curran told Al Jazeera.&lt;/p&gt;
&lt;p&gt;He noted, however, the app could continue to provide information on users even after the climate conference ends on November 18.&lt;/p&gt;
&lt;h2 id=&#34;refuted-completely&#34;&gt;‘Refuted completely’&lt;/h2&gt;
&lt;p&gt;According to &lt;a href=&#34;https://www.politico.eu/article/cop-27-climate-change-app-cybersecurity-weapon-risks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an analysis&lt;/a&gt; of the app by American media group Politico, it can monitor communications even when the device is in sleep mode.&lt;/p&gt;
&lt;p&gt;Egypt’s COP27 ambassador Wael Aboulmagd denounced the speculation, telling reporters a cybersecurity assessment was completed and, “I was told how unlikely, or physically or technically impossible” it would be to use the app so intrusively.&lt;/p&gt;
&lt;p&gt;Since it is available on Google Play and the Apple Store, those companies “would never allow that” because of security protocols, he added.&lt;/p&gt;
&lt;p&gt;“There has been a cybersecurity assessment done and it refuted that completely,” said Aboulmagd.&lt;/p&gt;
&lt;p&gt;But Bader warned delegates with the app on their phones remain vulnerable. “Intelligence may be gathered not just about their positions on climate change, but also on trade negotiations, political activities and military operations,” he said.&lt;/p&gt;
&lt;p&gt;Some rights activists have criticised the decision for Egypt to host COP27, citing a long track record of cracking down on political dissent. &lt;a href=&#34;https://www.aljazeera.com/news/2019/1/4/egypt-demands-cbs-should-not-air-interview-with-president-sisi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tens of thousands of people&lt;/a&gt; are estimated to have been jailed.&lt;/p&gt;
&lt;p&gt;A number of attendees have &lt;a href=&#34;https://www.theguardian.com/environment/2022/nov/06/egypt-cop27-climate-surveillance-cybersecurity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shared&lt;/a&gt; that the WiFi at the climate conference blocks access to websites such as Human Rights Watch and Egypt’s independent outlet Mada Masr, &lt;a href=&#34;https://www.aljazeera.com/news/2022/9/14/egypt-decides-to-release-al-jazeera-journalist-lawyer-says&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;as well as Al Jazeera&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For those concerned about the COP27 app, cybersecurity experts recommend using a “burner phone”, or secondary device, while being aware their conversations and other communications could be monitored.&lt;/p&gt;
&lt;p&gt;Those who already have the app should uninstall it as a first step, they say.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aljazeera.com/news/2022/11/12/expert-warn-egypts-cop27-app-could-be-used-for-surveilliance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.aljazeera.com/news/2022/11/12/expert-warn-egypts-cop27-app-could-be-used-for-surveilliance&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumnus David A. Bader to be Inducted into Clark School Innovation Hall of Fame</title>
      <link>http://localhost:1313/blog/20221108-maryland/</link>
      <pubDate>Tue, 08 Nov 2022 05:59:08 -0500</pubDate>
      <guid>http://localhost:1313/blog/20221108-maryland/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221108-maryland/article15168.large_hu_b9c7049e0dd0b4b6.webp 400w,
               /blog/20221108-maryland/article15168.large_hu_5ce258be3e404597.webp 760w,
               /blog/20221108-maryland/article15168.large_hu_4c3f8880e1a80cb9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221108-maryland/article15168.large_hu_b9c7049e0dd0b4b6.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221108-maryland/IMG_2812_hu_37311b9d0cb315ea.webp 400w,
               /blog/20221108-maryland/IMG_2812_hu_917812ba08040966.webp 760w,
               /blog/20221108-maryland/IMG_2812_hu_982cdb9517f99be6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221108-maryland/IMG_2812_hu_37311b9d0cb315ea.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In recognition for his role in creating the Linux supercomputer, Department of Computer and Electrical Engineering (ECE) alumnus &lt;strong&gt;David A. Bader&lt;/strong&gt; (Ph.D., ‘96, electrical engineering) will be inducted into the A. James Clark School of Engineering’s &lt;a href=&#34;https://eng.umd.edu/ihof&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Innovation Hall of Fame&lt;/a&gt; (IHOF) on Wednesday, November 9—joining a small but distinguished community of inventors who use their knowledge, perseverance, innovation, and ingenuity to change how we do things in the world.&lt;/p&gt;
&lt;p&gt;Bader is currently a Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and the inaugural director of the Institute for Data Science at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;As an Assistant Professor at the University of New Mexico in 1998, and pioneering ideas he developed during his time at the University of Maryland, Bader began building one of the first supercomputers that used readily available commercial-off-the-shelf (COTS) parts and a high-speed, low-latency interconnection network.&lt;/p&gt;
&lt;p&gt;“While I was at Maryland, we built the first high-performance cluster of DEC AlphaServer multiprocessor computers, essentially a commodity-based supercomputer that broke the mold of traditional monolithic supercomputers,” said Bader.&lt;/p&gt;
&lt;p&gt;His prototype using Intel processors and a high-speed interconnection network led to the development of the first Linux supercomputer, named “RoadRunner.” This breakthrough opened the door for the “democratization of supercomputers” by enabling users to assemble a supercomputer at a fraction of the cost, which created accessibility to these systems to a wider audience, allowing anyone to design and program their own high-performance computers.&lt;/p&gt;
&lt;p&gt;Bader goes on to explain that previously, users would have to write code specifically for that one supercomputer, adding that his work ultimately helped bring supercomputing capabilities to a much wider range of users. “Our work provided the first programming paradigm for what is ubiquitous today on today’s parallel computers with multicore processors and accelerators.”&lt;/p&gt;
&lt;p&gt;“It was very important to me to understand that we needed harness the economics of commodity-based systems in order to make supercomputing ubiquitous rather than in the hands of just a couple organizations,” explained Bader. “We needed to find technologies that would be cost effective, affordable and supply the high performance that traditional supercomputers could. Democratizing it meant that any person around the planet who had access to electricity would be able to work on important problems that mattered.”&lt;/p&gt;
&lt;p&gt;When the Roadrunner supercomputer was put into widespread use on the National Technology Grid in 1999, it was ranked one of the 100 fastest supercomputers in the world. A recent study by Hyperion Research calculated the value of Linux supercomputing developed by Bader has had an economic worth of over $100 trillion over the last 25 years.&lt;/p&gt;
&lt;p&gt;“Dr. Bader was always passionate about high performance computing since his early days at the University of Maryland and how to build commodity-based systems that can be afforded and effectively used by scientists and engineers” says Professor Joseph JaJa, Bader’s former faculty advisor. JaJa adds “Dr. Bader has shown a special knack at co-designing hardware and software systems built from inexpensive components to create very powerful supercomputers that were competitive with the most expensive mainframes. He showed incredible innovation and drive to achieve this goal.”&lt;/p&gt;
&lt;p&gt;Considered a leading authority in the areas of science, engineering, computing and data science, Bader has led an extensive career. He has served as lead scientist with DARPA and as an advisor to the White House, mostly recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE). He is a Fellow of the IEEE, ACM, AAAS, and SIAM, and a recipient of the IEEE Sidney Fernbach Award, one of the highest awards in computing. He has co-authored over 300 scholarly papers and has received best paper awards from ISC, IEEE HPEC and IEEE/ACM SC. He is currently Editor-in-Chief of the ACM Transactions on Parallel Computing. His research interests center on data science, high performance computing and real-world analytics.&lt;/p&gt;
&lt;p&gt;As an alum, Bader maintains his relationship with the Department of Electrical and Computer Engineering, and most recently established the David A. Bader Endowed Graduate Student Program Support Fund in Electrical and Computer Engineering.&lt;/p&gt;
&lt;p&gt;As a Ph.D. student, Bader began organizing social and academic activities for graduate students as a way of building community for students beyond the classrooms and labs, resulting in the formation of the ECE Graduate Student Association (ECE GSA). With this recent GSA endowment, he hopes to provide graduate students with the support they need to not only succeed in earning their degrees and advancing their careers, but establishing a community of colleagues that can support one another beyond their time at Maryland.&lt;/p&gt;
&lt;p&gt;Reflecting on his time at Maryland, Bader said, “I can attribute the success of my career to the department, to engineering, and also to the university. My gift also recognizes my Ph.D. advisor, Joseph JaJa, to whom I am grateful. From him, I learned not just about engineering, algorithms, and computing, but about thoughtful leadership and professionalism as part of an academic community. That, along with all his support, helped advance my career.”&lt;/p&gt;
&lt;p&gt;Fostering the creation of networks and communities from computers to people has been a connective thread in Bader’s life and career.&lt;/p&gt;
&lt;p&gt;“We’re often focused on technologies and solutions, which is a fantastic endeavor, but we often forget about people and the problems that we face together in the world that I think are really important,” said Bader, adding “That has been a core element of everything that I think about every day in terms of democratizing computing and data science, and solving global grand challenges. It&amp;rsquo;s really to make this world better for the generations to come, making it better and safer in every way.”&lt;/p&gt;
&lt;p&gt;To learn more about Bader’s work in developing commodity-based supercomputers, read “&lt;a href=&#34;https://ieeexplore.ieee.org/document/9546947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linux and Supercomputing: How my passion for building COTS systems led to an HPC revolution&lt;/a&gt;.”&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221108-maryland/20221109-Maryland_hu_a3a341215133ac54.webp 400w,
               /blog/20221108-maryland/20221109-Maryland_hu_72ffef4461d81859.webp 760w,
               /blog/20221108-maryland/20221109-Maryland_hu_eb483482a440ac3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221108-maryland/20221109-Maryland_hu_a3a341215133ac54.webp&#34;
               width=&#34;631&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://eng.umd.edu/news/story/alumnus-david-a-bader-to-be-inducted-into-clark-school-innovation-hall-of-fame&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eng.umd.edu/news/story/alumnus-david-a-bader-to-be-inducted-into-clark-school-innovation-hall-of-fame&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Futurology Podcast #213 Solving the challenges of our times with massive graph analytics with Dr. David A Bader, Distinguished Professor at the New Jersey Institute of Technology</title>
      <link>http://localhost:1313/blog/20221101-datafuturology/</link>
      <pubDate>Tue, 01 Nov 2022 15:52:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20221101-datafuturology/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/hBkP_2zM0tA?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;This week on the Data Futurology podcast, we have the special privilege to host &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, a Distinguished Professor at the New Jersey Institute of Technology, and the inaugural director of the Institute for Data Science there.&lt;/p&gt;
&lt;p&gt;Bader joins us on the podcast to discuss massive graph analytics, a topic that he is a recognised expert in and has &lt;a href=&#34;https://www.routledge.com/Massive-Graph-Analytics/Bader/p/book/9780367464127?gclid=Cj0KCQjwwfiaBhC7ARIsAGvcPe5NZ201x-A_QHJwXYW2ukXOyVJRw2qL_ErMrZY1Zrc_o0iZyajOQBcaAg_1EALw_wcB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recently published a book on&lt;/a&gt;. He and his team are currently working on a project that will allow anyone, via the Jupyter Notebook and Python, to leverage their data science framework, running on “tens of terabytes” of data. “It is quite exciting to democratise data science – and especially graph analytics – so that anyone with a problem that knows Python can work with some of the largest data sets,” he said.&lt;/p&gt;
&lt;p&gt;According to Bader, graphs are now a mainstream part of data science and a way to solve the most challenging and complex problems in the enterprise. “A graph abstracts relationships between objects, and any problem that we can abstract where we have relationships between objects, we could use graph analytics to solve,” he said.&lt;/p&gt;
&lt;p&gt;Much of Bader’s work – including through his book – is focused on helping organisations grapple with the exponential growth in data, and the impact that this has on their ability to dedicate adequate resources to work at scale. As he said, being able to do that is going to be fundamental to humanity’s ability to respond to the many real challenges that it faces ahead.&lt;/p&gt;
&lt;p&gt;“I want equitable access for everyone to be able to work on these problems, and to find new discoveries that are important, and help solve global grand challenges,” he said. “I think that we have many issues in the world today. And if we give more capabilities to those with data, and let them empower the data will make the world a much better place.”&lt;/p&gt;
&lt;p&gt;For more deep insights on the importance and value of massive graph analytics, tune in to our conversation with Dr. David A. Bader.&lt;/p&gt;
&lt;p&gt;Enjoy the show!&lt;/p&gt;
&lt;p&gt;Thank you to our sponsor, Talent Insights Group!&lt;/p&gt;
&lt;p&gt;Join us in Sydney for Ops World: &lt;a href=&#34;https://www.datafuturology.com/opsworld&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datafuturology.com/opsworld&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Join our Slack Community: &lt;a href=&#34;https://join.slack.com/t/datafuturologycircle/shared_invite/zt-z19cq4eq-ET6O49o2uySgvQWjM6a5ng&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://join.slack.com/t/datafuturologycircle/shared_invite/zt-z19cq4eq-ET6O49o2uySgvQWjM6a5ng&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“We’re creating a productivity front end so that anyone from their Jupiter Notebook and Python can call our framework. But in the back end, you can have a supercomputer running on 10s of terabytes of data. For me, this is quite exciting to democratise data science, and especially graph analytics, so that anyone with a problem who knows Python will be able to work on some of the largest and most challenging data sets.” — Dr. David A Bader, Distinguished Professor at the New Jersey Institute of Technology&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;what-we-discussed&#34;&gt;WHAT WE DISCUSSED&lt;/h2&gt;
&lt;p&gt;00:00 Introduction&lt;/p&gt;
&lt;p&gt;03:51 How did you get started in the graphs and graph space?&lt;/p&gt;
&lt;p&gt;10:26  What would you say to people looking to get started in in applying this capability to the problems that they might be facing in their organisations?&lt;/p&gt;
&lt;p&gt;15:13 How do you increase the fidelity of the algorithms?&lt;/p&gt;
&lt;p&gt;21:57 What was the some of the aims on what you were wanting to bring to the world with the book?&lt;/p&gt;
&lt;p&gt;27:40 Could you tell us a little bit more about our CUDA, how&amp;rsquo;s the trajectory been so far and where is it at now? What&amp;rsquo;s coming up next?&lt;/p&gt;
&lt;p&gt;32:39 What is your vision for the future of Cuda?&lt;/p&gt;
&lt;h2 id=&#34;episode-highlights&#34;&gt;EPISODE HIGHLIGHTS&lt;/h2&gt;
&lt;p&gt;Very early on, I was very interested in looking at large graphs, and graphs that abstract from the real world, whether it&amp;rsquo;s a transportation network, or whether it is trying to understand network security, or whether it&amp;rsquo;s a friendship network on a social, a social network platform. For me, graphs have gone from being a niche to now being mainstream, where most organisations have graph problems and look for graph analytics that can solve their problems in the enterprise.&lt;/p&gt;
&lt;p&gt;Once we can map a problem into the graph space, we think about what are we trying to find? Are we looking for a path, for instance, this path between friends from me to you in the graph? Are we looking for communities? So is there some emerging community of interest? Are we looking for influencers? And so these are the types of questions that we often have once we&amp;rsquo;re in the graph domain. And what I like to do is design new scalable algorithms that are able to work on these ever-increasing large size graphs to solve the analytic that we&amp;rsquo;re looking for.&lt;/p&gt;
&lt;p&gt;We can also use graphs to understand for instance, patients and electronic health records, to understand better treatments, and even personalised medicine through graphs. And we can also use graphs to understand our electric power grid, so to shore up and make more resilience, the power that runs our telecommunications, our food production, our transportation, and more.&lt;/p&gt;
&lt;p&gt;So anyone who thinks they have a small graph problem today, probably tomorrow will have a massive graph analytic problem. So we&amp;rsquo;d like to think about what do you do once the problem no longer fits on your laptop? Once you have to figure out how do I solve this problem? When I need more than my laptop, and my Python routine is taking too long? This has to run instantaneously, and it&amp;rsquo;s taking me hours, what do I do now? So this is a solution book for anyone who is thinking about graphs in the enterprise and moving towards large scale.&lt;/p&gt;
&lt;p&gt;I think that we have many issues in the world today. And if we give more capabilities to those with data, and let them empower the data will make the world a much better place. So that&amp;rsquo;s really my goal and my vision, and I really hope others will join in and help with this work.&lt;/p&gt;
&lt;p&gt;Thank you so much. It&amp;rsquo;s great to talk with you. And I encourage everyone out there to look at the world as a graph. And this is just a fantastic time to do so.&lt;/p&gt;
&lt;p&gt;At Data Futurology, we are always working to bring you use cases, new approaches and everything related to the most relevant topics in data science to help you get the most value out of these technologies! Check out our upcoming events for more amazing content.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datafuturology.com/podcast/213-solving-the-challenges-of-our-times-with-massive-graph-analytics-with-dr-david-a-bader-distinguished-professor-at-the-new-jersey-institute-of-technology-e1q3ope&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datafuturology.com/podcast/213-solving-the-challenges-of-our-times-with-massive-graph-analytics-with-dr-david-a-bader-distinguished-professor-at-the-new-jersey-institute-of-technology-e1q3ope&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Department Celebrates Achievements During Annual ‘Welcome Back’ Luncheon and Awards Ceremony</title>
      <link>http://localhost:1313/blog/20221025-maryland/</link>
      <pubDate>Tue, 25 Oct 2022 10:45:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20221025-maryland/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20221025-maryland/article15139.large_hu_a8677b4baff44ec.webp 400w,
               /blog/20221025-maryland/article15139.large_hu_ae4710325ee00cc7.webp 760w,
               /blog/20221025-maryland/article15139.large_hu_fb81f3f206af10f6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221025-maryland/article15139.large_hu_a8677b4baff44ec.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Department of Electrical and Computer Engineering recently held its annual Fall ‘Welcome Back’ Luncheon and Awards Ceremony, where student, staff and faculty were recognized for their accomplishments.&lt;/p&gt;
&lt;p&gt;This year’s departmental awards include:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Department of Electrical and Computer Engineering Staff Award:&lt;/strong&gt;
Kathryn Weiland, Director of Undergraduate Studies&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;George Corcoran Memorial Award for a Graduate Student&lt;/strong&gt; in recognition of excellence in teaching: Niloy Acharjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The David Bader Award&lt;/strong&gt; (formerly the Graduate Student Service Award) in recognition of graduate student who distinguishes themselves through exception service and leadership to the Department: Arafat Hasnain&lt;/p&gt;
&lt;p&gt;This award was recently re-named for &lt;strong&gt;Dr. David A. Bader (Ph.D. ’96)&lt;/strong&gt; in honor of his support of the electrical and computer engineering graduate student program as an alumnus and as the founder of the Electrical and Computer Engineering Graduate Student Association (ECEGSA).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The George Corcoran Memorial Award for Faculty&lt;/strong&gt;, presented annually to a young faculty member who has shown exemplary contributions to teaching and educations leadership:  Professor Cheng Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jimmy H.C. Lin Awards&lt;/strong&gt; – awarded annually to students, staff and faculty who transform their ideas into innovations through invention and technology commercialization.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Jimmy H.C. Lin Award for Innovation and Invention&lt;/strong&gt;:  Professor Mohammad Hafezi and his former student, Venkata Vikram Orre (EE Ph.D. 2019) for their patent “Tunable Robust Topological Source of Indistinguishable Correlated Photon Pairs.”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Jimmy H.C. Lin Award for Invention&lt;/strong&gt;: Professor Cheng Gong, in recognition of his research “2D Materials-Based Nanosensors for Rapid Monitoring of Meat Freshness.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Department of Electrical and Computer Engineering Distinguished Dissertation Award&lt;/strong&gt;, in recognition of recent doctoral recipients who have already made unusually significant and original contributions to their fields.  This year’s recipients are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Uday Saha&lt;/strong&gt;, advised by Prof. Edo Waks&lt;br&gt;
Dissertation: &lt;em&gt;Quantum Modem and Router for Quantum Internet&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nadee Seneviratne&lt;/strong&gt;, advised by Prof. Carol Espy-Wilson
Dissertation: &lt;em&gt;Generalizable Depression Detection and Severity Prediction Using Articulatory Representations of Speech&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Justin Stine&lt;/strong&gt;, advised by Prof. Reza Ghodssi&lt;br&gt;
Dissertation: &lt;em&gt;Mesoscale Embedded Sensor-Integrated Systems for Localized Biomedical Monitoring&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Jiahao Zhan&lt;/strong&gt;, advised by Prof. Mario Dagenais and Prof. Sylvain Veilleux (Astronomy)&lt;br&gt;
Dissertation: &lt;em&gt;Silicon Nitride Integrated Photonic Devices and Their Applications in Astronomy and Quantum Physics&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Congratulations to all of this year&amp;rsquo;s awardees!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/department-celebrates-achievements-during-annual-lsquowelcome-backrsquo-luncheon-and-awards-ceremony&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/department-celebrates-achievements-during-annual-lsquowelcome-backrsquo-luncheon-and-awards-ceremony&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hub &amp; Spoken Podcast Episode 151: Building massive scale analytics</title>
      <link>http://localhost:1313/blog/20221006-hubandspoken/</link>
      <pubDate>Thu, 06 Oct 2022 13:41:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20221006-hubandspoken/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/P1Si3E7qXTU?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2 id=&#34;whats-in-this-podcast&#34;&gt;What&amp;rsquo;s in this podcast?&lt;/h2&gt;
&lt;p&gt;In this episode, Jason talks to &lt;strong&gt;David Bader&lt;/strong&gt;, a Distinguished Professor in the Department of Data Science at the New Jersey Institute of Technology about large scale analytics.&lt;/p&gt;
&lt;p&gt;Listen to this episode on Spotify, iTunes, and Stitcher. You can also catch up on the previous episodes of the Hub &amp;amp; Spoken podcast when you subscribe.&lt;/p&gt;
&lt;p&gt;What are your thoughts on this topic? We’d love to hear from you; join the #HubandSpoken discussion and let us know on Twitter and LinkedIn.&lt;/p&gt;
&lt;p&gt;For more on data, take a look at the webinars and events that we have lined up for you.&lt;/p&gt;
&lt;h2 id=&#34;one-big-message&#34;&gt;One big message&lt;/h2&gt;
&lt;p&gt;Information is gradually becoming easier and easier to gather and utilise, causing data sets to expand exponentially. This expansion has been a huge driver for change in the way that these massive data sets are stored and analysed. Frameworks and infrastructures that are being developed for large data need to be able to bridge the gap along a full vertical stack of knowledge and skills to make it easier for everyone to use.&lt;/p&gt;
&lt;p&gt;[00:54] David’s path from electrical engineering, to founding the Data Department at New Jersey Institute of Technology&lt;/p&gt;
&lt;p&gt;[02:30] Classifying large scale data and what has been driving the trend to collect and analyse more data&lt;/p&gt;
&lt;p&gt;[04:39] Looking at instances where scale is important and how to determine when you need to scale&lt;/p&gt;
&lt;p&gt;[10:40] Comparing large scale data analysis locally vs. in the cloud&lt;/p&gt;
&lt;p&gt;[13:29] The capabilities and skills your data department requires to run large scale data projects&lt;/p&gt;
&lt;p&gt;[15:03] The ‘secret sauce’ that helps David’s team handle large scale data&lt;/p&gt;
&lt;p&gt;[17:14] Breaking down what hardware you need for large amounts of data&lt;/p&gt;
&lt;p&gt;[20:17] What experience is needed to build the entire infrastructure that can hold and analyse large scale data&lt;/p&gt;
&lt;p&gt;[24:25] Exploring use cases&lt;/p&gt;
&lt;h2 id=&#34;the-rise-of-large-scale-data&#34;&gt;The rise of large scale data&lt;/h2&gt;
&lt;p&gt;Over the past decade, we have seen a dramatic increase in the amount of data that is being generated. This has been driven by a number of factors, including the growth of social media, the proliferation of connected devices, and the rise of big data analytics. As a result, organisations are now able to collect and store large amounts of data more efficiently than ever before.&lt;/p&gt;
&lt;h2 id=&#34;but-what-exactly-is-large-scale-data&#34;&gt;But what exactly is large scale data?&lt;/h2&gt;
&lt;p&gt;From a data scientist’s perspective, it’s usually classed as an amount of data that cannot typically fit on a single laptop as that is often where the data scientist is playing. But large scale data isn’t always about how many bytes the raw data takes up on a hard drive. Often large scale data is about sophisticated data that generates more complex scenarios and data structures.&lt;/p&gt;
&lt;h2 id=&#34;when-do-you-need-to-scale-data&#34;&gt;When do you need to scale data?&lt;/h2&gt;
&lt;p&gt;The data analytics landscape is constantly evolving, and with new tools and techniques emerging all the time, it can be hard to know when to scale your data operation.&lt;/p&gt;
&lt;p&gt;When determining when to scale your data, often it is the easiest to work with the smallest amount of data that can provide the solution to your problem. Smaller sets of data are not just easier to work with, but can also be analysed faster and leave less room for error.&lt;/p&gt;
&lt;p&gt;Sometimes you may be dealing with increasing data volumes, expanding user requirements, or simply looking to future-proof your operation as your organisation grows over time. In these scenarios you don’t need to go big straight away but having a system in place that can be scaled at any time will help you become more flexible with your capabilities.&lt;/p&gt;
&lt;h2 id=&#34;bridging-the-gap-between-architecture-and-algorithms&#34;&gt;Bridging the gap between architecture and algorithms&lt;/h2&gt;
&lt;p&gt;There’s a lot of talk these days about the divide between data architecture and algorithms. On one side, you have the data experts who understand the inner workings of databases and know how to extract insights from large data sets. On the other side, you have the algorithm specialists who can develop sophisticated models to solve complex problems.&lt;/p&gt;
&lt;p&gt;When it comes to large scale data, you need experts who are able to bridge the vertical knowledge between the two. This requires people who have much broader skill sets and oftentimes more experience in the industry.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The past few years have seen a dramatic increase in the amount of data being generated. This is largely due to the proliferation of connected devices and sensors, which are constantly collecting and transmitting information. Along with this, there has been a corresponding increase in the ability to store and process large amounts of data. As a result, organisations are now able to leverage data on a scale that was previously unimaginable by creating backend infrastructure and frameworks that make it easier to analyse and pull insights from.&lt;/p&gt;
&lt;p&gt;This newfound capability is driving innovation across all sectors, as businesses look to gain insights that can give them a competitive edge. In many cases, those who are able to effectively utilise data are rewriting the rules of their respective industries. It is clear that data is becoming increasingly important in today’s economy, and its impact will only continue to grow in the years ahead.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cynozure: &lt;a href=&#34;https://www.cynozure.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cynozure.com/&lt;/a&gt;&lt;br&gt;
Twitter: &lt;a href=&#34;https://twitter.com/cynozuregroup/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/cynozuregroup/&lt;/a&gt;&lt;br&gt;
LinkedIn: &lt;a href=&#34;https://linkedin.com/company/cynozure/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://linkedin.com/company/cynozure/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hub&amp;amp;Spoken: &lt;a href=&#34;http://www.hubandspoken.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hubandspoken.com&lt;/a&gt;&lt;br&gt;
Twitter: &lt;a href=&#34;https://twitter.com/HubandSpoken/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/HubandSpoken/&lt;/a&gt;&lt;br&gt;
LinkedIn: &lt;a href=&#34;https://www.linkedin.com/showcase/hubandspoken&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/showcase/hubandspoken&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;CDO Hub: &lt;a href=&#34;http://www.cdohub.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cdohub.com&lt;/a&gt;&lt;br&gt;
Twitter: &lt;a href=&#34;https://twitter.com/cdohub1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/cdohub1/&lt;/a&gt;&lt;br&gt;
LinkedIn: &lt;a href=&#34;https://www.linkedin.com/showcase/cdo-hub/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/showcase/cdo-hub/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cynozure.com/hub-spoken/building-massive-scale-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cynozure.com/hub-spoken/building-massive-scale-analytics/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GraphBLAS and GraphChallenge Advance Network Frontiers</title>
      <link>http://localhost:1313/blog/20221003-siamnews/</link>
      <pubDate>Mon, 03 Oct 2022 21:43:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20221003-siamnews/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jeremy Kepner, &lt;strong&gt;David A. Bader&lt;/strong&gt;, Tim Davis, Roger Pearce, and Michael M. Wolf&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Many factors inspire interest in networks and graphs. The Internet is just as important to modern-day civilization as land, sea, air, and space; it connects billions of humans and is heading towards trillions of devices. Deep neural networks (DNNs)—which are also graphs—are key to artificial intelligence, and biological networks underpin life on Earth. In addition, graph algorithms have served as a foundation of computer science since its inception [3]. One can represent and operate on graphs in many different ways. A particularly attractive approach exploits the well-known duality between a graph as a collection of vertices and edges and its representation as a sparse adjacency matrix.&lt;/p&gt;
&lt;p&gt;Graph Algorithms in the Language of Linear Algebra, which was published by SIAM in 2011 [6], provides an applied mathematical introduction to graphs by addressing the foundations of graph/matrix duality (see Figure 1). This fundamental connection between the core operations of graph algorithms and matrix mathematics is quite powerful and represents a primary viewpoint for DNNs. Yet despite its widespread use in graph analysis, basic graph/matrix duality is still only a starting point. For instance, the final chapter of Graph Algorithms in the Language of Linear Algebra posed several fundamental questions about the analysis of large graphs in ontology/data modeling; time evolution (or streaming); detection theory (or graph modeling in general); and algorithm scaling [6]. These questions—along with the emergence of important applications in privacy, health, and cyber contexts—set the stage for the subsequent decade of work.&lt;/p&gt;
&lt;p&gt;Since 2011, researchers have written thousands of papers that explore the aforementioned topics from a graph/matrix perspective. Interestingly, previous prototyping efforts that began in the mid-2000s recognized that existing computer architectures were not a good match for a variety of graph and sparse matrix problems [10]. The prototypes introduced several innovations: high-bandwidth three-dimensional networks, cacheless memory, accelerator-based processors, custom low-power circuits, and—perhaps most importantly—a sparse matrix-based graph instruction set. Today, many of these innovations are present in commercially available systems like Cerebras, Graphcore, Lucata, and NVIDIA.&lt;/p&gt;


















&lt;figure  id=&#34;figure-figure-1-graphmatrix-duality-as-depicted-via-a-breadth-first-search-from-starting-point-alice-to-neighbors-bob-and-carl-and-its-adjacency-matrix-multiplication-equivalent-here-aij0-implies-an-edge-between-vertices-i-and-j-figure-courtesy-of-jeremy-kepner&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1. Graph/matrix duality as depicted via a breadth-first search from starting point Alice to neighbors Bob and Carl and its adjacency matrix multiplication equivalent. Here, **A**(*i*,*j*)&amp;gt;0 implies an edge between vertices *i* and *j*. Figure courtesy of Jeremy Kepner.&#34; srcset=&#34;
               /blog/20221003-siamnews/11028Figure1_hu_1a2023395fe0fd9b.webp 400w,
               /blog/20221003-siamnews/11028Figure1_hu_f0f1c59c09afd5db.webp 760w,
               /blog/20221003-siamnews/11028Figure1_hu_5273038e005e2971.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221003-siamnews/11028Figure1_hu_1a2023395fe0fd9b.webp&#34;
               width=&#34;600&#34;
               height=&#34;229&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1. Graph/matrix duality as depicted via a breadth-first search from starting point Alice to neighbors Bob and Carl and its adjacency matrix multiplication equivalent. Here, &lt;strong&gt;A&lt;/strong&gt;(&lt;em&gt;i&lt;/em&gt;,&lt;em&gt;j&lt;/em&gt;)&amp;gt;0 implies an edge between vertices &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt;. Figure courtesy of Jeremy Kepner.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The challenges associated with graph algorithm scaling led multiple scientists to identify the need for an abstraction layer that would allow algorithm specialists to write high-performance, matrix-based graph algorithms that hardware specialists could then design to without having to manage the complexities of every type of graph algorithm. With this philosophy in mind, a number of researchers (including two Turing Award winners) came together and proposed the idea that “the state of the art in constructing a large collection of graph algorithms in terms of linear algebraic operations is mature enough to support the emergence of a standard set of primitive building blocks” [8]. The centerpiece of this abstraction is the extension of traditional matrix multiplication to semirings&lt;br&gt;
&lt;strong&gt;C&lt;/strong&gt;=&lt;strong&gt;AB&lt;/strong&gt;=&lt;strong&gt;A&lt;/strong&gt;⊕.⊗&lt;strong&gt;B&lt;/strong&gt;&lt;br&gt;
where &lt;strong&gt;A&lt;/strong&gt;, &lt;strong&gt;B&lt;/strong&gt;, and &lt;strong&gt;C&lt;/strong&gt; are (usually sparse) matrices over a semiring with corresponding scalar addition ⊕ and scalar multiplication ⊗. Particularly interesting combinations include standard matrices over real or complex numbers (+.x), tropical algebras (max.+) that are important for neural networks, and set operations (∪.∩) that form the foundation of relational databases like SQL. One can build countless graph algorithms with these combinations of operations, and the Graph Basic Linear Algebra Subprograms (GraphBLAS) mathematical specification, C specification, and high-performance implementation subsequently emerged [1, 4, 5]. These programs are now part of some of the world’s most popular mathematical software environments.&lt;/p&gt;
&lt;p&gt;Many innovations in graph processing occurred during this time, inspiring new venues to highlight these developments. MIT, DARPA, Amazon, IEEE, and SIAM collaborated to establish &lt;a href=&#34;https://graphchallenge.mit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GraphChallenge&lt;/a&gt;, which consists of several hundred data sets and well-defined mathematical graph problems in the areas of triangle counting, clustering of streaming graphs, and sparse DNNs. Since its debut in 2017, GraphChallenge has seen an abundance of submissions — contestants have even integrated parts of their work into a wide range of research programs and system procurements.&lt;/p&gt;
&lt;p&gt;GraphChallenge has revealed that due to resulting improvements in graph analysis systems, many graph problems are now fundamentally bound by computer memory bandwidth (as opposed to processor speed or memory latency) [9]. It has also provided clear targets for those who are trying to advance computing systems scaling for the solution of graph problems. Additionally, innovations to improve the performance of graph algorithms have fed back into sparse linear algebra libraries to benefit scientific computing applications like Kokkos Kernels [11].&lt;/p&gt;
&lt;p&gt;Questions in ontology/data modeling pertain to the way in which researchers handle more diverse data. The data that we want to manage with graphs involve more than just simple vertices and edges; they often include a large variety of very diverse metadata that are stored in SQL and NoSQL databases. Unsurprisingly, many folks in the database community have also been working on graph databases. To mathematically encompass these concepts, we must generalize the idea of a matrix into something called an associative array. For example, one can view a matrix as a mapping&lt;br&gt;
&lt;strong&gt;A&lt;/strong&gt;:&lt;em&gt;I&lt;/em&gt;×&lt;em&gt;J&lt;/em&gt;→&lt;em&gt;V&lt;/em&gt;&lt;br&gt;
where &lt;em&gt;I&lt;/em&gt;={1,&amp;hellip;,&lt;em&gt;M&lt;/em&gt;}, &lt;em&gt;J&lt;/em&gt;={1,&amp;hellip;,&lt;em&gt;N&lt;/em&gt;}, and &lt;em&gt;V&lt;/em&gt; is complex. In an associative array, &lt;em&gt;I&lt;/em&gt; and &lt;em&gt;J&lt;/em&gt; are now any strict totally ordered set (e.g., a set of strings) and &lt;em&gt;V&lt;/em&gt; is a semiring [7]. This concept was first implemented in the D4M software system, which links matrix mathematics and databases. It is now present in a number of database systems that utilize GraphBLAS as their underlying mathematical engine [2].&lt;/p&gt;


















&lt;figure  id=&#34;figure-figure-2-streaming-multi-hyper-edge-graph-edge-or-incidence-matrices-assign-a-row-to-each-edge-and-naturally-handle-the-dynamic-addition-of-identical-edges-multi-edges-and-edges-that-connect-more-than-two-vertices-hyper-edges-eoutei0-and-einej0-imply-an-edge-e-between-vertex-i-and-vertex-j-figure-courtesy-of-jeremy-kepner&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 2. Streaming multi-hyper-edge graph. Edge (or incidence) matrices assign a row to each edge and naturally handle the dynamic addition of identical edges (multi-edges) and edges that connect more than two vertices (hyper-edges). Eout(*e*,*i*)&amp;gt;0 and Ein(*e*,*j*)&amp;gt;0 imply an edge e between vertex *i* and vertex *j*. Figure courtesy of Jeremy Kepner.&#34; srcset=&#34;
               /blog/20221003-siamnews/11029Figure2_hu_1ef49addbc52a834.webp 400w,
               /blog/20221003-siamnews/11029Figure2_hu_ec4e5671014b713c.webp 760w,
               /blog/20221003-siamnews/11029Figure2_hu_f2792ea5b5559bcc.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20221003-siamnews/11029Figure2_hu_1ef49addbc52a834.webp&#34;
               width=&#34;649&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2. Streaming multi-hyper-edge graph. Edge (or incidence) matrices assign a row to each edge and naturally handle the dynamic addition of identical edges (multi-edges) and edges that connect more than two vertices (hyper-edges). E&lt;sub&gt;out&lt;/sub&gt;(&lt;em&gt;e&lt;/em&gt;,&lt;em&gt;i&lt;/em&gt;)&amp;gt;0 and E&lt;sub&gt;in&lt;/sub&gt;(&lt;em&gt;e&lt;/em&gt;,&lt;em&gt;j&lt;/em&gt;)&amp;gt;0 imply an edge e between vertex &lt;em&gt;i&lt;/em&gt; and vertex &lt;em&gt;j&lt;/em&gt;. Figure courtesy of Jeremy Kepner.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Time-evolving or streaming graphs have become one of the most important problems in graph analysis, and GraphBLAS has a natural way of addressing streaming graphs with diverse data via edge (or incidence) matrices (see Figure 2). Traditional adjacency matrices are limited in the types of graphs that they can represent. Adjacency matrices typically represent directed weighted graphs—which are a very important class of problems—but real data tend to be much more dynamic and diverse, with multiple edges and hyper-edges (edges that are connected to multiple vertices). In an edge matrix representation, one can easily adjust for this type of graph by simply adding rows to the end of the matrix. Furthermore, researchers can compute the corresponding adjacency matrix via&lt;br&gt;
&lt;strong&gt;A&lt;/strong&gt;=&lt;strong&gt;E&lt;/strong&gt;&lt;sub&gt;out&lt;/sub&gt;&lt;sup&gt;T&lt;/sup&gt;&lt;strong&gt;E&lt;/strong&gt;&lt;sub&gt;in&lt;/sub&gt;&lt;br&gt;
where &lt;sup&gt;T&lt;/sup&gt; denotes the matrix transpose.&lt;/p&gt;
&lt;p&gt;Ultimately, the aforementioned capabilities—enabled by GraphBLAS along with other graph innovations that are highlighted by GraphChallenge—have yielded new tools for tackling some of the most difficult and important problems in health data, privacy-preserving analytics, cybersecurity, and DNNs.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1] Buluç, A., Mattson, T., McMillan, S., Moreira, J., &amp;amp; Yang, C. (2017). Design of the GraphBLAS API for C. In 2017 IEEE international parallel and distributed processing symposium workshops (IPDPSW). Lake Buena Vista, FL: IEEE.&lt;br&gt;
[2] Cailliau, P., Davis, T., Gadepally, V., Kepner, J., Lipman, R., Lovitz, J., &amp;amp; Ouaknine, K. (2019). RedisGraph GraphBLAS enabled graph database. In 2019 IEEE international parallel and distributed processing symposium workshops (IPDPSW). Rio de Janeiro, Brazil: IEEE.&lt;br&gt;
[3] Cormen, T.H., Leiserson, C.E., Rivest, R.L., &amp;amp; Stein, C. (2022). Introduction to algorithms (4th ed). Cambridge, MA: MIT Press.&lt;br&gt;
[4] Davis, T.A. (2019). Algorithm 1000: SuiteSparse:GraphBLAS: Graph algorithms in the language of sparse linear algebra. ACM Transact. Math. Software, 45(4), 1-25.&lt;br&gt;
[5] Kepner, J., Aaltonen, P., Bader, D., Buluç, A., Franchetti, F., Gilbert, J., … Moreira, J. (2016). Mathematical foundations of the GraphBLAS. In 2016 IEEE high performance extreme computing conference (HPEC). Waltham, MA: IEEE.&lt;br&gt;
[6] Kepner, J., &amp;amp; Gilbert, J. (Eds.) (2011). Graph algorithms in the language of linear algebra. Philadelphia, PA: Society for Industrial and Applied Mathematics.&lt;br&gt;
[7] Kepner, J., &amp;amp; Jananthan, H. (2018). Mathematics of big data: Spreadsheets, databases, matrices, and graphs. In MIT Lincoln Laboratory Series. Cambridge, MA: MIT Press.&lt;br&gt;
[8] Mattson, T., Bader, D., Berry, J., Buluç, A., Dongarra, J., Faloutsos, C., … Yoo, A. (2013). Standards for graph algorithm primitives. In 2013 IEEE high performance extreme computing conference (HPEC). Waltham, MA: IEEE.&lt;br&gt;
[9] Samsi, S., Kepner, J., Gadepally, V., Hurley, M., Jones, M., Kao, E., … Monticciolo, P. (2020). GraphChallenge.org triangle counting performance. In 2020 IEEE high performance extreme computing conference (HPEC). Waltham, MA: IEEE.&lt;br&gt;
[10] Song, W.S., Kepner, J., Nguyen, H.T., Kramer, J.I., Gleyzer, V., Mann, J.R., … Mullen, J. (2010). 3-D graph processor. Presented at the fourteenth annual high performance embedded computing workshop (HPEC 2010). Lexington, MA: MIT Lincoln Laboratory.&lt;br&gt;
[11] Trott, C., Berger-Vergiat, L., Poliakof, D., Rajamanickam, S., Lebrun-Grandie, D., Madsen, J., … Womeldorff, G. (2021). The Kokkos EcoSystem: Comprehensive performance portability for high performance computing. Comput. Sci. Eng., 23(5), 10-18.&lt;/p&gt;
&lt;p&gt;Jeremy Kepner is head and founder of the Massachusetts Institute of Technology (MIT) Lincoln Laboratory’s Supercomputing Center, a founder of the MIT-Air Force AI Accelerator, and a founder of the Massachusetts Green High Performance Computing Center, with appointments in MIT’s Department of Mathematics and MIT Connection Science. He is a SIAM Fellow. &lt;strong&gt;David A. Bader&lt;/strong&gt; is a distinguished professor and founder of the Department of Data Science in the Ying Wu College of Computing, as well as director of the Institute for Data Science at the New Jersey Institute of Technology. He is a SIAM Fellow. Tim Davis is a professor of computer science and engineering at Texas A&amp;amp;M University. He is also a SIAM Fellow. Roger Pearce is a computer scientist in the Center for Applied Scientific Computing at Lawrence Livermore National Laboratory and an adjunct Computer Science and Engineering Associate Professor of Practice at Texas A&amp;amp;M University. Michael M. Wolf manages the Scalable Algorithms Department at Sandia National Laboratories.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sinews.siam.org/Details-Page/graphblas-and-graphchallenge-advance-network-frontiers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sinews.siam.org/Details-Page/graphblas-and-graphchallenge-advance-network-frontiers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presentation of the Inaugural 2022 David A. Bader Award</title>
      <link>http://localhost:1313/blog/20220913-maryland/</link>
      <pubDate>Tue, 13 Sep 2022 09:20:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220913-maryland/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/OqqMjYx4LA8?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;On 13 September 2022, Prof. Joseph JaJa, interim chair of the Department of Electrical and Computer Engineering at The University of Maryland, presented the inagural 2022 David A. Bader Award.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dr-david-a-bader-arafat-hasnain-and-dr-joseph-f-jaja&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dr. David A. Bader, Arafat Hasnain, and Dr. Joseph F. JaJa&#34; srcset=&#34;
               /blog/20220913-maryland/PXL_20220913_173535861~2_hu_d7a5a621105e4f61.webp 400w,
               /blog/20220913-maryland/PXL_20220913_173535861~2_hu_8ae5ad251beddcb.webp 760w,
               /blog/20220913-maryland/PXL_20220913_173535861~2_hu_8478779cb7c46cd0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220913-maryland/PXL_20220913_173535861~2_hu_d7a5a621105e4f61.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dr. David A. Bader, Arafat Hasnain, and Dr. Joseph F. JaJa
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;The David A. Bader Award&lt;/strong&gt;: this award recognizes gradudate students who distinguish themselves through exceptional service and leadership to the Department. The award is named for &lt;strong&gt;Dr. David A. Bader (Ph.D. &amp;lsquo;96)&lt;/strong&gt; in honor of his tremendous support of the electrical and computer engineering graduate student program as an alumnus and as the founder of the Electrical and Computer Engineering Graduate Student Association (ECEGSA).&lt;/p&gt;
&lt;p&gt;The 2022 David A. Bader Award is presented to Arafat Hasnain.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220913-maryland/PXL_20220913_173712669~2_hu_988718fb1f5f7bab.webp 400w,
               /blog/20220913-maryland/PXL_20220913_173712669~2_hu_8c0ff89526e87a07.webp 760w,
               /blog/20220913-maryland/PXL_20220913_173712669~2_hu_f9bad6ff457e4e83.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220913-maryland/PXL_20220913_173712669~2_hu_988718fb1f5f7bab.webp&#34;
               width=&#34;564&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Who is the strongest computer in the world? Why is China leading the world in supercomputers, but why is it secretive?</title>
      <link>http://localhost:1313/blog/20220909-voa/</link>
      <pubDate>Fri, 09 Sep 2022 09:13:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220909-voa/</guid>
      <description>&lt;p&gt;&lt;em&gt;Translated by Google.&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-chinese-supercomputer-sunway-taihulight-once-topped-the-top500-evaluation-list&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Chinese supercomputer &amp;#39;Sunway TaihuLight&amp;#39; once topped the Top500 evaluation list&#34; srcset=&#34;
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_f4e9ac5cd2697651.webp 400w,
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_b6677ec8de755b24.webp 760w,
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_40aa7b6ba6008202.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_f4e9ac5cd2697651.webp&#34;
               width=&#34;650&#34;
               height=&#34;366&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Chinese supercomputer &amp;lsquo;Sunway TaihuLight&amp;rsquo; once topped the Top500 evaluation list
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;WASHINGTON —
The US supercomputer Frontier topped the global supercomputing speed test earlier this year. However, there are reports that at least two Chinese supercomputers have surpassed their American counterparts in speed, but refused to participate in the world competition. Some analysts say that China does not want to disclose the details of its supercomputing system to the public, perhaps because it is afraid that it will be sanctioned by the United States. In addition, whether Chinese supercomputers use TSMC chips has also become a matter of concern to the industry.&lt;/p&gt;
&lt;h2 id=&#34;the-us-frontier-supercomputing-list-tops-the-list--but-china-already-has-a-secret-weapon&#34;&gt;The US &amp;ldquo;frontier&amp;rdquo; supercomputing list tops the list , but China already has a secret weapon?&lt;/h2&gt;
&lt;p&gt;The super speed of 10 billion billion calculations per second has always been the goal of the supercomputer field. Supercomputers that can reach this threshold of computing power are called &amp;ldquo;exascale&amp;rdquo; supercomputers. On May 30 this year, the American supercomputer system &amp;ldquo;Frontier&amp;rdquo; topped the &amp;ldquo;TOP500&amp;rdquo; list of the world&amp;rsquo;s top 500 supercomputers with a speed of 11.02 billion operations per second. &amp;ldquo;TOP500&amp;rdquo; is regarded by the industry as one of the authoritative evaluations of supercomputing computing power.&lt;/p&gt;
&lt;p&gt;The Top500 list was first published in 1993. Jack J. Dongarra, Distinguished Professor of Electrical Engineering and Computer Science at the University of Tennessee, is the designer of the Top500 benchmark. &amp;ldquo;In a sense, the top 500 people have acquired a kind of proud capital. Now, this proud capital is obtained by the cutting-edge supercomputer at Oak Ridge National Laboratory,&amp;rdquo; Dongarra told VOA.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Countries with the fastest supercomputers can in a sense do the best science and simulations, leading to scientific discoveries and discoveries of new things,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;China has been catching up in the field of supercomputers in recent decades. From 2010 to 2019, China&amp;rsquo;s supercomputer ranked first in the 20th TOP500 list 11 times. Although the speed champion in recent years has been taken by American computers, Chinese products account for the largest share in terms of the number of supercomputers that have entered the top 500.&lt;/p&gt;


















&lt;figure  id=&#34;figure-frontier-supercomputer-operated-by-oak-ridge-national-laboratory&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Frontier supercomputer operated by Oak Ridge National Laboratory&#34; srcset=&#34;
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_22977e42b5438a20.webp 400w,
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_4df8a006994703.webp 760w,
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_ba6644a9eb6bfb08.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_22977e42b5438a20.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Frontier supercomputer operated by Oak Ridge National Laboratory
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Although &amp;ldquo;Frontier&amp;rdquo; is the fastest supercomputer in the official Top500 evaluation, some industry insiders believe that China has designed at least two E-class supercomputers, and the computing power even exceeds that of &amp;ldquo;Frontier&amp;rdquo; - &amp;ldquo;Tianhe-3&amp;rdquo; and &amp;ldquo;Shenwei&amp;rdquo; &amp;ldquo;The system&amp;rsquo;s new product OceanLight (&amp;ldquo;Ocean Light&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;The American technology media The Next Platform was the first to report that China is running two E-class supercomputers. Among them, &amp;ldquo;Sunway Ocean Light&amp;rdquo; inherits the &amp;ldquo;Sunway Taihu Light&amp;rdquo; system, with a peak performance of 1.3 billion per second. 100 million times, higher than the top American &amp;ldquo;Frontier&amp;rdquo; in this Top500 list.&lt;/p&gt;
&lt;p&gt;The designer of the Top500 list, Tangalla, said that the designers of &amp;ldquo;Light of the Sea&amp;rdquo; and &amp;ldquo;Tianhe No. 3&amp;rdquo; chose not to participate in the evaluation, which made the outside world feel &amp;ldquo;a bit mysterious&amp;rdquo;. He said it may have something to do with the designer&amp;rsquo;s desire to conceal the origin of the chips for the two supercomputers.&lt;/p&gt;
&lt;p&gt;He said: &amp;ldquo;There is speculation that they are not on the list, maybe because it may reveal some information about where these chips are made. So, we believe that these chips may be made by TSMC in Taiwan.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;“The U.S. makes chips in Taiwan, and many countries make computer chips there. My guess is that the Chinese also use TSMC to make chips — chips designed in China but made in Taiwan,” he said. “If China publishes these The real performance of supercomputers, the U.S. may take some action against TSMC to prevent them from producing chips for China.”&lt;/p&gt;
&lt;h2 id=&#34;is-it-counterproductive-to-restrict-chinas-supercomputing-by-the-us-entity-list&#34;&gt;Is it counterproductive to restrict China&amp;rsquo;s supercomputing by the US entity list?&lt;/h2&gt;
&lt;p&gt;China used to rely on U.S. companies such as Intel and Nvidia to design and manufacture supercomputers, but starting in 2015, the path has narrowed due to U.S. sanctions.&lt;/p&gt;
&lt;p&gt;The U.S. added China&amp;rsquo;s National University of Defense Technology, Changsha National Supercomputing Center, Guangzhou National Supercomputing Center, and Tianjin National Supercomputing Center (the designer of the &amp;ldquo;Tianhe System&amp;rdquo;) to the Entity List in 2015. In June 2019, the United States included Sugon, Haiguang, Chengdu Haiguang, Chengdu Haiguang Microelectronics Technology and Jiangnan Institute of Computing Technology on the entity list; April 8, 2021, including Tianjin Feiteng Information Technology Company, Shanghai Integrated Circuit Technology Chinese supercomputing research institutions, including the Industry Promotion Center, Sunway Microelectronics, the National Supercomputing Jinan Center, the National Supercomputing Shenzhen Center, the National Supercomputing Wuxi Center, and the National Supercomputing Zhengzhou Center, were also blacklisted by the United States. Chinese entities on the Entity List must be chartered by the U.S. government to do business with U.S. companies.&lt;/p&gt;
&lt;p&gt;The Biden administration of the United States introduced new restrictions at the end of August this year, restricting Nvidia and Advanced Micro Devices (AMD) from exporting high-end GPU chips to China and Russia. Such chips are widely used in supercomputers. Nvidia said the new restrictions affect the existing product A100, as well as the H100, which is expected to be available later this year.&lt;/p&gt;
&lt;p&gt;Douglas Fuller, an associate professor at Copenhagen Business School, told VOA that U.S. sanctions against China are becoming increasingly targeted.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;They (export controls) are fairly narrow and well-targeted&amp;hellip;because (the U.S. government) doesn&amp;rsquo;t want AI chips going into the military,&amp;rdquo; he said in an email.&lt;/p&gt;
&lt;p&gt;U.S. technology export controls have slowed the development of certain supercomputing systems in China to a certain extent, but some analysts believe that such a chip control strategy is only temporary in restricting the development of Chinese supercomputers.&lt;/p&gt;
&lt;p&gt;Dongarra, a distinguished professor at the University of Tennessee, said China&amp;rsquo;s lack of access to U.S. chip hardware has created software challenges for supercomputing designers.&lt;/p&gt;
&lt;p&gt;He said: &amp;ldquo;One of the disturbances (for China) to feel is that, for example, the applications that were running on the previous generation of Chinese high-performance computers based on Western technology had to be rewritten based on the software running on the new machines. Writing software can be a huge challenge.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It may take a lot of time, but I think the Chinese are very capable of doing this conversion, converting algorithms into software that can run on Chinese machines.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor of New Jersey Institute of Technology and Director of the Institute of Data Science, gave an example of &amp;ldquo;Dawning&amp;rdquo;, one of China&amp;rsquo;s three major supercomputing systems, that China chose to independently adjust the technical integration of domestic processors after the US sanctions. and succeed.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The U.S. restrictions definitely had some impact on Dawning,&amp;rdquo; Bader told VOA: &amp;ldquo;However, instead of using Intel, they used Hygon x86 processors and Hygon DCU accelerators instead of Nvidia GPUs, So they have been using technology to consider integrating domestic CPUs and GPUs. It can be said that this underlying technology is still based on American technology, such as the x86 core, but now China can manufacture its own chips independently of American companies and chipsets.”&lt;/p&gt;
&lt;p&gt;&amp;ldquo;So, in the past, U.S. sanctions have accelerated China&amp;rsquo;s development of its own supercomputing technology&amp;hellip; So I believe that today&amp;rsquo;s restrictions on&amp;hellip; Nvidia and Supermicro GPU accelerators, targeting China and Russia, etc., will lead to China&amp;rsquo;s Further development of new capabilities beyond current technology.”&lt;/p&gt;
&lt;h2 id=&#34;what-exactly-is-a-supercomputer-useful-for&#34;&gt;What exactly is a supercomputer useful for?&lt;/h2&gt;
&lt;p&gt;Experts say high-performance computing (HPC)-based simulation experiments are a direct driver of modern scientific research. &amp;ldquo;We often think that science is done by performing physical experiments on theoretical results. Today, we augment scientific research with computational simulations that we develop. Simulations are helping us understand things that were previously incomprehensible,&amp;rdquo; Dongarra said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s often said that the people who have the fastest supercomputers can do the best science,&amp;rdquo; he said. &amp;ldquo;It&amp;rsquo;s because with these fast supercomputers, you can get more accuracy in computational analysis, and you can compute faster. quick.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Take the &amp;ldquo;Frontier&amp;rdquo; supercomputer in the United States as an example. According to the &amp;ldquo;Wall Street Journal&amp;rdquo;, &amp;ldquo;Frontier&amp;rdquo; was used to analyze 11 million genomes to better distinguish and predict virus mutations.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It gives us clues to develop the next generation of vaccines and therapies to not only treat and prevent Covid-19, but other types of disease,&amp;rdquo; Oak Ridge National Laboratory director Thomas Zacharia told The Wall Street Journal.&lt;/p&gt;
&lt;p&gt;According to the report, Oak Ridge National Laboratory allows scientists from around the world to use &amp;ldquo;leading edge&amp;rdquo; supercomputers based on the value of the research, and requires scientists who use the supercomputer to request the publication of their research results.&lt;/p&gt;
&lt;p&gt;China says scientists from other countries are welcome to use Tianhe-3 for research, but so far the Chinese supercomputer has only been used by Chinese government-backed scientific institutions.&lt;/p&gt;
&lt;p&gt;Supercomputers can be used for weapons development and intelligence gathering. The New York Times said that some of China&amp;rsquo;s giant computer systems are believed to be used to monitor Muslim minority groups such as the Uighurs in Xinjiang, and artificial intelligence technology is increasingly being used in facial recognition to enhance social surveillance.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s important to be the leading country, because, as people say, &amp;rsquo;to outcompute is to outcompete,&amp;rsquo;&amp;rdquo; Bader said. &amp;ldquo;So for national competitiveness, the U.S. dominates the calculations and data science fundamentals, which is very important.”&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;谁是世界最强电脑中国超算世界领先却为何遮遮掩掩&#34;&gt;谁是世界最强电脑？中国超算世界领先却为何遮遮掩掩&lt;/h1&gt;
&lt;p&gt;2022年9月9日 09:13&lt;br&gt;
许宁&lt;/p&gt;


















&lt;figure  id=&#34;figure-中国超级计算机神威太湖之光曾登上top500评测榜首&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;中国超级计算机“神威·太湖之光”曾登上Top500评测榜首&#34; srcset=&#34;
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_f4e9ac5cd2697651.webp 400w,
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_b6677ec8de755b24.webp 760w,
               /blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_40aa7b6ba6008202.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220909-voa/0CF8BA27-953B-488A-A353-813A86A2D862_w650_r1_s_hu_f4e9ac5cd2697651.webp&#34;
               width=&#34;650&#34;
               height=&#34;366&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      中国超级计算机“神威·太湖之光”曾登上Top500评测榜首
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;华盛顿 &amp;ndash; 美国超级计算机“前沿”（Frontier）今年早些时候登上全球超算速度评测榜首。不过，有报道说，中国至少有两台超级计算机在速度上已经超过美国对手，但拒绝参与世界评比。有分析说，中国不想将其超算系统细节公之于众，可能是怕“树大招风”再招美国制裁。另外，中国超算是否使用了台积电芯片也成为业界关注的问题。&lt;/p&gt;
&lt;h2 id=&#34;美国前沿超算榜单拔头筹-但中国早有秘密武器&#34;&gt;美国“前沿”超算榜单拔头筹 但中国早有秘密武器？&lt;/h2&gt;
&lt;p&gt;每秒钟进行一百亿亿次计算的超级速度，一直是超级计算机领域试图突破的目标。能达到这个运算能力门槛的超级计算机被称为“E级”（exascale）超算。今年5月30日，美国巨型计算机系统 “前沿”（Frontier） 以每秒110.2亿亿次的运算速度登上全球超级计算机500强榜单“TOP500”之首。“TOP500”被业界认为是超算计算能力的权威评测之一。&lt;/p&gt;
&lt;p&gt;Top500榜单1993年首次公布排名。美国田纳西大学电气工程和计算机科学杰出教授杰克·唐加拉（Jack J. Dongarra）是Top500评测标准的设计者。唐加拉对美国之音说，“从某种意义上说，Top500排名第一的人获得了一种自豪的资本。现在，这个自豪的资本由橡树岭国家实验室的前沿超级计算机获得。”&lt;/p&gt;
&lt;p&gt;他说：“拥有最快超级计算机的国家在某种意义上可以从事最好的科学和模拟，从而致使科学发现和新事物的发现。”&lt;/p&gt;
&lt;p&gt;中国近几十年在超级计算机领域奋起直追。2010年到2019年，中国超算在20届TOP500榜单评选上11次排名第一。近几年的速度冠军虽然由美国计算机摘取，但从进入前500名超级计算机的数量来看，中国产品所占最多。&lt;/p&gt;


















&lt;figure  id=&#34;figure-美国橡树岭国家实验室运行的前沿frontier超级计算机&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;美国橡树岭国家实验室运行的“前沿”（Frontier）超级计算机&#34; srcset=&#34;
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_22977e42b5438a20.webp 400w,
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_4df8a006994703.webp 760w,
               /blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_ba6644a9eb6bfb08.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220909-voa/f5528e81-4109-4b27-838d-5e876c76f1f2_w1023_n_r1_st_s_hu_22977e42b5438a20.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      美国橡树岭国家实验室运行的“前沿”（Frontier）超级计算机
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;虽然“前沿”是Top500官方评测的最快超算，但一些业界人士认为，中国已经设计出至少两款E级超级计算机，运算能力甚至超过了“前沿”——“天河三号”和“神威”系统的新产品OceanLight（“海洋之光”）。&lt;/p&gt;
&lt;p&gt;美国科技媒体The Next Platform此前率先报道说，中国正在运行这两台E级超算，其中 ，“神威·海洋之光”继承了“神威·太湖之光”系统，峰值性能达到每秒1.3百亿亿次，高于本次Top500榜单排名第一的美国“前沿”。&lt;/p&gt;
&lt;p&gt;Top500榜单设计者唐加拉说，“海洋之光”和“天河三号”的设计者选择不参加评测，让外界感到“有点神秘”。他说，这可能与设计者希望隐瞒这两款超算的芯片产地有关。&lt;/p&gt;
&lt;p&gt;他说：“有人猜测，它们不在榜单上，可能因为这样可能会透露一些关于这些芯片制造地的信息。所以，我们相信这些芯片可能是台积电在台湾制造的。”&lt;/p&gt;
&lt;p&gt;“美国在台湾制造芯片，许多国家在那里制造计算机芯片。我的猜测是，中国人也用台积电来制造芯片——芯片在中国设计，但在台湾制造。”他说：“如果中国公布了这些超级计算机的真实性能，美国可能会对台积电采取一些行动，阻止他们为中国生产芯片。”&lt;/p&gt;
&lt;h2 id=&#34;美国实体清单制约中国超算适得其反&#34;&gt;美国实体清单制约中国超算适得其反？##&lt;/h2&gt;
&lt;p&gt;中国过去依赖美国英特尔（Intel）和英伟达（Nvidia）等公司设计和制造超级计算机，但从2015年开始，由于美国制裁，这条路变得越来越窄。&lt;/p&gt;
&lt;p&gt;美国于2015年将中国国防科技大学、长沙国家超级计算中心、广州国家超级计算中心和天津国家超级计算中心（“天河系统”的设计者）列入实体清单。2019年6月，美国将中科曙光、海光、成都海光、成都海光微电子技术和江南计算技术研究所列入实体清单；2021年4月8日，包括天津飞腾信息技术公司、上海集成电路技术与产业促进中心、Sunway Microelectronics、国家超级计算济南中心、国家超级计算深圳中心、国家超级计算无锡中心以及国家超级计算郑州中心在内的中国超算研究机构也被美国列入黑名单。实体清单上的中国机构必须获得美国政府的特许才能与美国公司开展业务。&lt;/p&gt;
&lt;p&gt;美国拜登政府今年8月底出台新的限制，限制英伟达和超微半导体公司（AMD）向中俄两国出口GPU高端芯片。这类芯片被广泛用于超级计算机中。英伟达方面表示，新的限制令影响了现有产品A100，以及预计将于今年晚些时候上市的产品H100。&lt;/p&gt;
&lt;p&gt;哥本哈根商学院副教授傅道格（Douglas Fuller）对美国之音说，美国针对中国的制裁正越来越具有针对性。&lt;/p&gt;
&lt;p&gt;他通过一封电子邮件中说：“它们（出口管制）范围相当狭窄，目标明确……因为（美国政府）不希望人工智能芯片进入军方。”&lt;/p&gt;
&lt;p&gt;美国的技术出口管制一定程度上放缓了中国某些超算系统的发展，但一些分析人士认为，这样的芯片管制策略对制约中国超级计算机的发展只是暂时性的。&lt;/p&gt;
&lt;p&gt;田纳西大学杰出教授唐加拉说，中国无法获取美国的芯片硬件，这给超算设计者带来了软件方面的难题。&lt;/p&gt;
&lt;p&gt;他说：“（让中国）感受到的一种干扰是，比方说，上一代基于西方技术的中国高性能计算机上运行的应用程序，必须根据新机器上运行的软件重新编写这些应用程序。重新编写软件可能是一个巨大的挑战。”&lt;/p&gt;
&lt;p&gt;“这可能需要很多时间，但我觉得中国人很有能力进行这种转换，将算法转换成到中国机器上可以运行的软件。”&lt;/p&gt;
&lt;p&gt;新泽西理工学院杰出教授、数据科学研究所主任戴维·巴德（David Bader）以中国三大超算系统之一“曙光”举例说，中国在美国制裁后选择自主调整国产处理器的技术整合，并取得成功。&lt;/p&gt;
&lt;p&gt;“美国的限制肯定对‘曙光’产生了一定影响。”巴德对美国之音说：“不过，他们不使用英特尔，而是使用了海光x86处理器和海光DCU加速器，而不是英伟达的GPU，因此他们一直在利用技术，来考虑整合国产CPU和GPU。可以说，这一底层技术仍基于美国技术，例如x86核心，但现在中国能够独立于美国企业和芯片组，制造自己的芯片。”&lt;/p&gt;
&lt;p&gt;他说：“因此，在过去，美国的制裁加速了中国发展自己的超级计算技术……因此，我相信，今天对……英伟达和超微GPU加速器的限制，针对中国和俄罗斯等，将导致中国进一步发展超越当前技术的新能力。”&lt;/p&gt;
&lt;h2 id=&#34;超级计算机究竟有什么用&#34;&gt;超级计算机究竟有什么用？&lt;/h2&gt;
&lt;p&gt;专家说，基于而高性能计算（HPC）的模拟实验是现代科技研究的直接驱动力。唐加拉说：“我们常常以为科学是通过对理论结果进行物理实验来完成的。今天，我们用我们开发的计算模拟来增强科研。模拟正在帮助我们理解以前无法理解的东西。&lt;/p&gt;
&lt;p&gt;他说：“人们常说，拥有最快超级计算机的人可以做最好的科学。这是因为有了这些快速的超级计算机，你可以在计算解析中获得更高的准确性，并且计算速度更快。”&lt;/p&gt;
&lt;p&gt;以美国“前沿”超算为例，据《华尔街日报》报道，“前沿”被用于分析1100万个基因组，以更好地区分和预测病毒变异。&lt;/p&gt;
&lt;p&gt;橡树岭国家实验室总监托马斯·扎卡利亚（Thomas Zacharia） 对《华尔街日报》说：“它为我们提供了开发下一代疫苗和疗法的线索，不仅可以治疗和预防新冠，还可以治疗其他类型的疾病。”&lt;/p&gt;
&lt;p&gt;报道说，橡树岭国家实验室根据研究的价值，允许来自世界各地的科学家使用“前沿”超级计算机，并要求使用这台超算的科学家要求发表他们的研究结果。&lt;/p&gt;
&lt;p&gt;中国表示，欢迎其他国家的科学家使用“天河三号”进行研究，但迄今为止，这台中国超算仅由中国政府支持的科学机构使用。&lt;/p&gt;
&lt;p&gt;超级计算机可被应用于武器开发和情报收集等方面。《纽约时报》说，中国的一些巨型计算机系统据信用于监视新疆维吾尔人等穆斯林少数民族群体，人工智能技术也越来越多地应用于人脸识别，强化社会监视。&lt;/p&gt;
&lt;p&gt;巴德说：“成为领先的国家十分重要。因为正如人们所说的，‘要算过对手才能竞争过对手’（to outcompute is to outcompete）。因此，对于国家竞争力来说，美国要主导计算和数据科学基础，这非常重要。”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.voachinese.com/a/us-china-supercomputer-20220908/6737627.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.voachinese.com/a/us-china-supercomputer-20220908/6737627.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ECE Launches 14th Year of Booz Allen Hamilton Colloquium Series</title>
      <link>http://localhost:1313/blog/20220901-maryland/</link>
      <pubDate>Thu, 01 Sep 2022 10:55:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220901-maryland/</guid>
      <description>&lt;p&gt;The Fall 2022 Booz Allen Hamilton Colloquium Series kicked off its 14th year last Friday.  The first talk of the semester was given by Dr. Tim O’Shea, Chief Technology Officer of &lt;a href=&#34;https://www.deepsig.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepSig&lt;/a&gt;, and hosted by Professor &lt;a href=&#34;https://ece.umd.edu/clark/faculty/490/Sennur-Ulukus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sennur Ulukus&lt;/a&gt;.  O’Shea’s talk was titled &amp;ldquo;Deep Learning in the Physical Layer: Building AI-Native Sensing and Communications Systems&amp;rdquo;.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dr-tim-oshea-from-deepsig-and-professor-sennur-ulukus-with-colloquium-attendees&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dr Tim O&amp;#39;Shea from DeepSig and Professor Sennur Ulukus with Colloquium Attendees&#34; srcset=&#34;
               /blog/20220901-maryland/article15054.large_hu_33607e777c41bff0.webp 400w,
               /blog/20220901-maryland/article15054.large_hu_2086f73aba2a038b.webp 760w,
               /blog/20220901-maryland/article15054.large_hu_d76a397a3cf887db.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220901-maryland/article15054.large_hu_33607e777c41bff0.webp&#34;
               width=&#34;233&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dr Tim O&amp;rsquo;Shea from DeepSig and Professor Sennur Ulukus with Colloquium Attendees
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The ECE Colloquium Series, sponsored by Booz Allen Hamilton, is held on select Fridays throughout the Fall and Spring semesters.  Started in 2014, the series brings to campus a variety of leaders in academia, industry and government.  Talks are held from 3:30 to 4:30 in the Zupnik Lecture Hall (Room 1110) in the Kim Engineering Building and are open to all students, faculty and staff.&lt;/p&gt;
&lt;p&gt;Upcoming talks include:&lt;/p&gt;
&lt;h3 id=&#34;friday-september-9-2022&#34;&gt;Friday, September 9, 2022&lt;/h3&gt;
&lt;p&gt;Jay Patel, Vice President, Northrup Grumman Space Systems&lt;/p&gt;
&lt;p&gt;Talk Title: &amp;ldquo;Changing Industry Trends and the Surprising Tools needed To Best Prepare&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;friday-september-16-2022&#34;&gt;Friday, September 16, 2022&lt;/h3&gt;
&lt;p&gt;Laura Fisk, Head of Mixed Signal Circuits Research, Mythic AI&lt;/p&gt;
&lt;p&gt;Talk Title: &amp;ldquo;Compute-in-Memory Processing for Energy Efficient Algorithms&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;friday-september-23-2022&#34;&gt;Friday, September 23, 2022&lt;/h3&gt;
&lt;p&gt;Kostas Daniilidis, Professor, University of Pennsylvania&lt;/p&gt;
&lt;h3 id=&#34;friday-september-30-2022&#34;&gt;Friday, September 30, 2022&lt;/h3&gt;
&lt;p&gt;Yiorgos Makris. Professor, The University of Texas at Dallas&lt;/p&gt;
&lt;p&gt;Talk Title: &amp;ldquo;Malicious Hardware-Induced Covert Channels in Wireless Networks: Risks and Remedies&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;friday-october-21-2022&#34;&gt;Friday, October 21, 2022&lt;/h3&gt;
&lt;p&gt;Michael Lundberg, Solutions Architect, Booz Allen Hamilton&lt;/p&gt;
&lt;h3 id=&#34;friday-november-11-2022&#34;&gt;Friday, November 11, 2022&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Director of Institute for Data Science&lt;/p&gt;
&lt;p&gt;New Jersey Institute of Technology&lt;/p&gt;
&lt;p&gt;Further information on the series can be found at &lt;a href=&#34;https://ece.umd.edu/events/distinguished-colloquium-series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/events/distinguished-colloquium-series&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/ece-launches-14th-year-of-booz-allen-hamilton-colloquium-series&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/ece-launches-14th-year-of-booz-allen-hamilton-colloquium-series&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Talk Club podcast: Leading Data Research with David Bader</title>
      <link>http://localhost:1313/blog/20220830-datatalkclub/</link>
      <pubDate>Tue, 30 Aug 2022 14:59:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220830-datatalkclub/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/vZLlpsUlchQ?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;: &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://davidbader.net/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NJIT Institute for Data Science: &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datascience.njit.edu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Arkouda: &lt;a href=&#34;https://github.com/Bears-R-Us/arkouda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Bears-R-Us/arkouda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NJIT Data Science YouTube Channel: &lt;a href=&#34;https://www.youtube.com/c/NJITInstituteforDataScience&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/c/NJITInstituteforDataScience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ML Zoomcamp: &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&lt;/a&gt;&lt;br&gt;
Join DataTalks.Club: &lt;a href=&#34;https://datatalks.club/slack.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datatalks.club/slack.html&lt;/a&gt;&lt;br&gt;
Our events: &lt;a href=&#34;https://datatalks.club/events.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datatalks.club/events.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datatalks.club/podcast/s10e08-leading-data-research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datatalks.club/podcast/s10e08-leading-data-research.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AWS Takes the Short and Long View of Quantum Computing</title>
      <link>http://localhost:1313/blog/20220830-hpcwire/</link>
      <pubDate>Tue, 30 Aug 2022 12:58:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220830-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Russell&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-a-microwave-package-encloses-the-aws-quantum-processor-the-packaging-is-designed-to-shield-the-qubits-from-environmental-noise-while-enabling-communication-with-the-quantum-computers-control-systems-source-aws&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A microwave package encloses the AWS quantum processor. The packaging is designed to shield the qubits from environmental noise while enabling communication with the quantum computer’s control systems. Source: AWS&#34; srcset=&#34;
               /blog/20220830-hpcwire/AWS-packaged-quantum-processor-675x380_hu_125ef5fc9124d098.webp 400w,
               /blog/20220830-hpcwire/AWS-packaged-quantum-processor-675x380_hu_e7b44ddc27102b70.webp 760w,
               /blog/20220830-hpcwire/AWS-packaged-quantum-processor-675x380_hu_417261cb4a339e74.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220830-hpcwire/AWS-packaged-quantum-processor-675x380_hu_125ef5fc9124d098.webp&#34;
               width=&#34;675&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A microwave package encloses the AWS quantum processor. The packaging is designed to shield the qubits from environmental noise while enabling communication with the quantum computer’s control systems. Source: AWS
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;It is perhaps not surprising that the big cloud providers – a poor term really – have jumped into quantum computing. Amazon, Microsoft Azure, Google, and their like have steadily transformed into major technology developers, no doubt in service of their large cloud services offerings. The same is true internationally. You may not know, for example, that China’s cloud giants – Baidu, Alibaba, and Tencent –  also all have significant quantum development initiatives.&lt;/p&gt;
&lt;p&gt;The global cloud crowd tends to leave no technology stone unturned and quantum was no different. Now the big players are all-in. At Amazon, most of the public attention has centered on Braket, its managed quantum services offering that provides tools for learning and access to a variety of quantum computers. Less well-known are Amazon’s Quantum Solutions Lab, Center for Quantum Computing, and Center for Quantum Networking, the last just launched in June. These four initiatives capture the scope of AWS’s wide-ranging quantum ambitions, which include building a fault-tolerant quantum computer.&lt;/p&gt;


















&lt;figure  id=&#34;figure-simone-severini-aws-quantum&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Simone Severini, AWS Quantum&#34; srcset=&#34;
               /blog/20220830-hpcwire/IMG_0393-150x150_hu_fbf86a56a0f3c675.webp 400w,
               /blog/20220830-hpcwire/IMG_0393-150x150_hu_d0e56ec585c0856c.webp 760w,
               /blog/20220830-hpcwire/IMG_0393-150x150_hu_51a9f35ba1db24d1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220830-hpcwire/IMG_0393-150x150_hu_fbf86a56a0f3c675.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Simone Severini, AWS Quantum
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;HPCwire&lt;/em&gt; recently talked with Simone Severini, director, quantum computing, AWS, about its efforts. A quantum physicist by training, &lt;a href=&#34;https://www.linkedin.com/in/simoneseverini/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Severini&lt;/a&gt; has been with AWS for ~ four years. He reports to AWS’s overall engineering chief, Bill Vass. Noting that there’s “not much evidence” that NISQ era systems will provide decisive business value soon, Severini emphasized quantum computing is a long-term bet. Now is the time for watching, learning, and kicking the tires on early systems.&lt;/p&gt;
&lt;p&gt;“Amazon Braket provides a huge opportunity for doing that. Customers can keep an eye on the dynamics of the evolution of this technology. We believe there’s really not a single path to quantum computing. It’s very, very early, right. This is a point that I like to stress,” said Severini. “I come from academia and have been exposed to quantum computing, one way or another, for over two decades. It’s amazing to see the interest in the space. But we also need to be willing to set the right expectations. It’s definitely very, very early still in quantum computing.”&lt;/p&gt;
&lt;p&gt;Launched in 2019, AWS describes Braket as a “fully managed quantum computing service designed to help speed up scientific research and software development for quantum computing.” This is not unlike what most big quantum computer makers, such D-Wave, IBM and Rigetti also provide.&lt;/p&gt;
&lt;p&gt;The premise is to provide all the quantum tools and hardware infrastructure required for new and more experienced quantum explorers to use on a pay-as-you-go basis. Indeed, in the NISQ era, many believe such portal offerings are the only realistic way to deliver quantum computing. Cloud providers (and other “concierge-like” service providers such Strangeworks, for example) have the advantage of being able to provide access to several different systems.&lt;/p&gt;
&lt;p&gt;With Braket, said Severini, “Users don’t have to sign contracts. Just go there, and you have everything you need to see what’s going on [in quantum computing], to program or to simulate, and to use quantum computers directly. We have multiple devices with different [qubit] technologies on the service. The hope is that on one side, customers can indeed keep an eye on the technology on the other side, researchers can run experiments and hopefully contribute to knowledge as well contribute to science.”&lt;/p&gt;
&lt;p&gt;Braket currently offers access to quantum computers based on superconducting, trapped ion, photonic, and quantum annealers. Presumably other qubit technologies, cold atoms for example, will be added over time.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220830-hpcwire/AWS-Quantum-Hardware_hu_36ef32be88a4e6ca.webp 400w,
               /blog/20220830-hpcwire/AWS-Quantum-Hardware_hu_81691d2e0e8ab1be.webp 760w,
               /blog/20220830-hpcwire/AWS-Quantum-Hardware_hu_ff938515ec37fd1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220830-hpcwire/AWS-Quantum-Hardware_hu_36ef32be88a4e6ca.webp&#34;
               width=&#34;760&#34;
               height=&#34;397&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Interestingly, Braket is also a learning tool for AWS. “It’s an important exercise for us as well, because in this way, we can envision how quantum computers one day, would really feed a complex, cloud based infrastructure. Today, the workloads on Braket are all experimental, but for us, it’s important to learn things like security or operator usability, and the management of resources that we do for customers,” said Severini. “This is quite interesting, because in the fullness of time, a quantum computer could be used together with a lot of other classical resources, including HPC.”&lt;/p&gt;
&lt;p&gt;On the latter point, there is growing belief that much of quantum computing may indeed become a hybrid effort with some pieces of applications best run on quantum computers and other parts best run on classical resources. We’ll see. While it is still early days for the pursuit of hybrid classical-quantum computing, AWS launched Amazon Braket Hybrid late year. Here’s an excerpt of AWS’s description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Amazon Braket Hybrid Jobs enables you to easily run hybrid quantum-classical algorithms such as the Variational Quantum Eigensolver (VQE) and the Quantum Approximate Optimization Algorithm (QAOA), that combine classical compute resources with quantum computing devices to optimize the performance of today’s quantum systems. With this new feature, you only have to provide your algorithm script and choose a target device — a quantum processing unit (QPU) or quantum circuit simulator. Amazon Braket Hybrid Jobs is designed to spin up the requested classical resources when your target quantum device is available, run your algorithm, and release the instances after completion so you only pay for what you use. Braket Hybrid Jobs can provide live insights into algorithm metrics to monitor your algorithm as it progresses, enabling you to make adjustments more quickly. Most importantly, your jobs have priority access to the selected QPU for the duration of your experiment, putting you in control, and helping to provide faster and more predictable execution.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“To run a job with Braket Hybrid Jobs, you need to first define your algorithm using either the Amazon Braket SDK or PennyLane. You can also use TensorFlow and PyTorch or create a custom Docker container image. Next, you create a job via the Amazon Braket API or console, where you provide your algorithm script (or custom container), select your target quantum device, and choose from a variety of optional settings including the choice of classical resources, hyper-parameter values, and data locations. If your target device is a simulator, Braket Hybrid Jobs is designed to start executing right away. If your target device is a QPU, your job will run when the device is available and your job is first in the queue. You can define custom metrics as part of your algorithm, which can be automatically reported to Amazon CloudWatch and displayed in real time in the Amazon Braket console. Upon completion, Braket Hybrid Jobs writes your results to Amazon S3 and releases your resources.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;The second initiative, Amazon Quantum Solution Lab, is aimed at collaborative research programs; it is, in essence, Amazon’s professional quantum services group.&lt;/p&gt;
&lt;p&gt;“They engage in research project with customers. For example, they recently wrote a paper with &lt;strong&gt;a team of researchers at Goldman Sachs&lt;/strong&gt;. They run a very interesting initiative together with BMW Group, something called the BMW Group quantum computing challenge. BMW proposed four areas related to their interests, like logistic, manufacturing, some stuff that related to automotive engineering, and there was a call for a proposal to crowdsource solutions that use quantum computers to address these problems,” said Severini.&lt;/p&gt;
&lt;p&gt;“There were 70 teams, globally, that submitted solutions. I think this is very interesting because [it’s still early days] and the fact is that quantum computers are not useful in business problems today. They can’t [yet] be more impactful than classical computing today. An initiative of this type can really help bridge the real world with the theory. We have several such initiatives,” he said.&lt;/p&gt;
&lt;h2 id=&#34;building-a-fault-tolerant-computer&#34;&gt;Building a Fault-Tolerant Computer&lt;/h2&gt;
&lt;p&gt;Amazon’s efforts to build a fault-tolerant quantum are based at the AWS Center for Quantum Computing, located in Pasadena, Calif., and run in conjunction with Caltech. “We launched this initiative in 2019 but last year, in 2021, we opened a building that we built inside the campus of Caltech,” said Severini. “It’s a state of the art research facility and we are doing research to build an error-corrected, fault tolerant computer,” he said.&lt;/p&gt;


















&lt;figure  id=&#34;figure-new-aws-center-for-quantum-computing-at-caltech&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New AWS Center for Quantum Computing at Caltech&#34; srcset=&#34;
               /blog/20220830-hpcwire/AWS-Center-for-Quantum-Computing_hu_a85ea774794f541e.webp 400w,
               /blog/20220830-hpcwire/AWS-Center-for-Quantum-Computing_hu_79138a34fe5091ad.webp 760w,
               /blog/20220830-hpcwire/AWS-Center-for-Quantum-Computing_hu_1a2b066a440920a2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220830-hpcwire/AWS-Center-for-Quantum-Computing_hu_a85ea774794f541e.webp&#34;
               width=&#34;760&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New AWS Center for Quantum Computing at Caltech
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;AWS has settled on semiconductor-based superconducting qubit technology, citing the deep industry knowledge of semiconductor manufacturing techniques and scalability. The challenge, of course, is achieving fault-tolerance. Today’s NISQ systems are noisy and error-prone and require near-zero Kelvin temperatures. Severini said simply, “There is a lot of scientific challenges still and there’s a lot of engineering to be done.”&lt;/p&gt;
&lt;p&gt;“We believe strongly that there are two things that need to be done at this stage. One is improving error rates at the physical level and to invest in material science to really understand on a fundamental level how to build components that have an improvement in with respect to error rates. The second point is [to develop] new qubit architectures for protecting qubits from errors,” he said.&lt;/p&gt;
&lt;p&gt;“This facility includes everything [to do] that. We are doing the full stack. We’re building everything ourselves from software to the architecture to the qubits, and the wiring. These are long-term investments,” said Severini.&lt;/p&gt;
&lt;p&gt;AWS has been relatively quiet in promoting its quantum computer building effort. It has vigorously embraced competing qubit technologies on Braket, and Severini noted that it’s still unclear how progress will unfold. Some approaches may work well for a particular application but not for others. AWS is tracking all of them, and is including some prominent quantum researchers. For example, &lt;a href=&#34;https://www.amazon.science/blog/amazon-scholar-john-preskill-on-the-aws-quantum-computing-effort&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Preskill&lt;/a&gt;, the Caltech researcher who coined the term NISQ, is an Amazon Scholar. (Preskill, of course, is fittingly the Richard P. Feynman Professor of Theoretical Physics at the California Institute of Technology.)&lt;/p&gt;
&lt;p&gt;Last February, AWS published a &lt;a href=&#34;https://journals.aps.org/prxquantum/abstract/10.1103/PRXQuantum.3.010329&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; in PRX Quantum (Building a fault-tolerant quantum computer using concatenated cat codes) which outlines directional thinking. The abstract is excerpted below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“We present a comprehensive architectural analysis for a proposed fault-tolerant quantum computer based on cat codes concatenated with outer quantum error-correcting codes. For the physical hardware, we propose a system of acoustic resonators coupled to superconducting circuits with a two-dimensional layout. Using estimated physical parameters for the hardware, we perform a detailed error analysis of measurements and gates, including cnot and Toffoli gates. Having built a realistic noise model, we numerically simulate quantum error correction when the outer code is either a repetition code or a thin rectangular surface code.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Our next step toward universal fault-tolerant quantum computation is a protocol for fault-tolerant Toffoli magic state preparation that significantly improves upon the fidelity of physical Toffoli gates at very low qubit cost. To achieve even lower overheads, we devise a new magic state distillation protocol for Toffoli states. Combining these results together, we obtain realistic full-resource estimates of the physical error rates and overheads needed to run useful fault-tolerant quantum algorithms. We find that with around 1000 superconducting circuit components, one could construct a fault-tolerant quantum computer that can run circuits, which are currently intractable for classical computers. Hardware with 18 000 superconducting circuit components, in turn, could simulate the Hubbard model in a regime beyond the reach of classical computing.”&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;The latest &lt;a href=&#34;https://aws.amazon.com/blogs/quantum-computing/announcing-the-aws-center-for-quantum-networking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;big piece&lt;/a&gt; of Amazon’s quantum puzzle is the AWS Center for Quantum Networking, located in Boston. AWS says major news about the new center is forthcoming soon. The quantum networking center, said Severini, is focused on hardware, software, commercial and scientific applications. That sounds like a lot and is perhaps in keeping with Amazon’s ambitious quantum programs overall.&lt;/p&gt;
&lt;p&gt;The proof of all these efforts, as the saying goes, will be in the pudding.&lt;/p&gt;
&lt;p&gt;Stay tuned.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2022/08/30/aws-takes-the-short-and-long-view-of-quantum-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2022/08/30/aws-takes-the-short-and-long-view-of-quantum-computing/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Online Graduate Computing Student Travels a Route Less Taken: The Thesis Option</title>
      <link>http://localhost:1313/blog/20220816-njit/</link>
      <pubDate>Tue, 16 Aug 2022 17:22:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220816-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220816-njit/Patchett_hu_c8732de1d60b6474.webp 400w,
               /blog/20220816-njit/Patchett_hu_e3c0e0d37bc1f373.webp 760w,
               /blog/20220816-njit/Patchett_hu_dcdec9df5111664.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220816-njit/Patchett_hu_c8732de1d60b6474.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Joseph Patchett, a third year M.S. in Computer Science student, thrives on individuality. Whether pursuing his NJIT graduate degree as one of the few online students to actually visit campus or choosing the unprecedented option of completing his degree with a research thesis, he welcomes challenges on the road less taken.&lt;/p&gt;
&lt;p&gt;A master’s candidate in NJIT’s Ying Wu College of Computing (YWCC) may satisfy their degree requirements either completely through coursework, a mix of class credits and final project, or, as is the case with Patchett, with a research thesis. Few students choose the more challenging thesis option. For an online student, Patchett’s decision is a first.&lt;/p&gt;
&lt;p&gt;“I wanted to do something unique and independent. Research has always been a desire of mine, and this is an opportunity to identify and formulate solutions on my own,” he said.&lt;/p&gt;
&lt;p&gt;His thesis titled &amp;ldquo;Efficient and Scalable Triangle Centrality Algorithms in an Arkouda Framework&amp;rdquo; explores the implementation of different triangle counting methods for graphs using parallel computing. Triangle centrality is a method of ranking how central a vertex is in a graph using connectedness as the measure of importance. Arkouda is a distributed framework that lowers the barrier for access to supercomputing and empowers personal computers to manipulate massive datasets. Running the algorithms on this computing framework allows the user to quickly analyze how connected different vertices are in a given graph and rank those most connected.&lt;/p&gt;
&lt;p&gt;Though Patchett graduated from Penn State with a bachelor’s in Material Science &amp;amp; Engineering, computing had long been an interest, and the computer-oriented elements of engineering inspired him to make a career transition full-time.&lt;/p&gt;
&lt;p&gt;“I love driving insights and finding patterns. Computing is like Lego. You can build almost anything with it,” he said.&lt;/p&gt;
&lt;p&gt;YWCC’s online M.S. in Computer Science makes it easy for students to transition into computing while earning a graduate-level degree. But, in the meantime, while pursuing his new career path, Patchett still needed to earn a living. He chose NJIT because it was the only online graduate program that was flexible enough to accommodate the need to frequently travel for work at the last minute. NJIT also featured two key people whom he credits for making his experience that much more manageable and rewarding – and one relationship was the conduit for the second.&lt;/p&gt;
&lt;p&gt;“Anything I achieved at NJIT is because of [Tim Hart],” he said. Hart is the enrollment services manager for YWCC who kept in constant touch with Patchett to coordinate and guide his application process and onboarding while he was traveling (on several other roads) across the country for Infosys. Hart also introduced him to &lt;strong&gt;&lt;a href=&#34;http://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;professor David Bader&lt;/a&gt;&lt;/strong&gt; of the Department of Data Science, who became very influential in Patchett’s future.&lt;/p&gt;
&lt;p&gt;Bader, a distinguished data scientist, became Patchett’s thesis advisor and introduced him to Arkouda, which is supported by the Department of Defense. Bader’s research group works closely with the Arkouda developers, which enabled Patchett to get first-hand support from the Arkouda team. His collaborators on this project are NJIT researcher Zhihui Du and Ph.D. students Fuhuan Li and Oliver Alvarado Rodriguez.&lt;/p&gt;
&lt;p&gt;Patchett also recently presented his research on Scalable K-Truss Implementation in Arkouda at the New Jersey Big Data Alliance 2022 Symposium held at NJIT.&lt;/p&gt;
&lt;p&gt;When asked what was next on his journey ahead, he responded, “I’ll start figuring out my next move when I have more free time!”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/online-graduate-computing-student-travels-route-less-taken-thesis-option&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/online-graduate-computing-student-travels-route-less-taken-thesis-option&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Engineering Podcast: Interactive Exploratory Data Analysis On Petabyte Scale Data Sets With Arkouda</title>
      <link>http://localhost:1313/blog/20220731-dataengineeringpodcast/</link>
      <pubDate>Sun, 31 Jul 2022 19:44:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220731-dataengineeringpodcast/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;Episode-311-Arkouda.mp3&#34;&gt;Listen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Exploratory data analysis works best when the feedback loop is fast and iterative. This is easy to achieve when you are working on small datasets, but as they scale up beyond what can fit on a single machine those short iterations quickly become long and tedious. The Arkouda project is a Python interface built on top of the Chapel compiler to bring back those interactive speeds for exploratory analysis on horizontally scalable compute that parallelizes operations on large volumes of data. In this episode &lt;strong&gt;David Bader&lt;/strong&gt; explains how the framework operates, the algorithms that are built into it to support complex analyses, and how you can start using it today.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Host: Tobias Macey&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.podchaser.com/podcasts/data-engineering-podcast-244123/episodes/interactive-exploratory-data-a-146056977&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.podchaser.com/podcasts/data-engineering-podcast-244123/episodes/interactive-exploratory-data-a-146056977&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People of ACM - David A. Bader</title>
      <link>http://localhost:1313/blog/20220707-acm/</link>
      <pubDate>Thu, 07 Jul 2022 16:29:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220707-acm/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/hd1APIbv8c0?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Why did you decide to pursue a career in computer engineering, and specifically in parallel computing?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I first used a computer when I was three years old in 1972, when my father—a college chemistry professor and early computer scientist who developed numerical methods and plotting packages—let us enter programs into a Digital Equipment Corporation PDP 11/45 mainframe computer. In the college library, I browsed through computing journals and was inspired first by a 1981 paper by H.J. Siegel et al., focused on parallel computing architectures for image processing. This led me to pursuing degrees in electrical and computer engineering and my doctoral research under Joseph JaJa at University of Maryland, who had authored the leading textbook on parallel algorithms. Parallel computing was a path for me to pursue solutions to some of the most challenging real-world problems of global significance such as those in Earth sciences, biology and genomics, and national security. These problems require the use of parallel computing due to the data volumes, computational complexity, and time-to-solution requirements.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;You have been recognized as a pioneer in using the Linux operating system along with commodity parts in the development of high-performance computers. Will you tell us about your initial efforts in this area? Why has Linux been such an effective framework for supercomputers?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yes, last year I received the IEEE Sidney Fernbach Award for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications. According to Hyperion Research’s HPC Qview tracking of servers and the broader HPC ecosystem, the total economic impact of my invention of Linux supercomputing has been over $100 trillion in the past 25 years. In the early 1980’s I was fascinated with parallel computing and passionate to build my own system. I recognized that to democratize supercomputers and improve their affordability, one needs to leverage commodity technologies and make programming and system software portable. In 1989, as an undergraduate student at Lehigh University, I built my first parallel computer, using several Commodore Amiga 1000 personal computers that the company had donated to Lehigh. They had been collecting dust in a closet when a friend and I networked them together. In January 1992, I joined the University of Maryland as an electrical and computer engineering doctoral student and visited the NASA Goddard Space Flight Center in search of fellowships in parallel computing. In August, I received the NASA Graduate Student Researcher Fellowship and built my first parallel computer using Ethernet-connected, Intel-based PCs and the FreeBSD operating system in 1993, prior to the Beowulf project. After receiving my PhD in May 1996, over the next eighteen months I was a postdoc at the university and a National Science Foundation (NSF) research associate at its Institute for Advanced Computer Studies (UMIACS). In this role, I built an experimental computing cluster comprising 10 DEC AlphaServer nodes, each with four DEC Alpha RISC processors and a DEC PCI card connected to a DEC Gigaswitch ATM switch.&lt;/p&gt;
&lt;p&gt;In January 1998, I moved to the University of New Mexico and the Albuquerque High Performance Computing Center (AHPCC). There, I had the opportunity to build and deploy, to my knowledge, the first bona fide Linux supercomputer while continuing to develop clusters of COTS processors into systems with the speed, performance, and services of a traditional supercomputer. I came to UNM with the idea of building the first x86 Linux supercomputer as a teaching tool for advanced computer design. My system design took a revolutionary new direction that differed significantly from Beowulf and the HPC research community’s cluster efforts. From my experience with real applications, I knew that Beowulf did not have the capabilities to run the broad set of scientific computing tasks on contemporary supercomputers, and more engineering was necessary to create a Linux-based system that would displace traditional supercomputers. By spring 1998 I had built the first working Intel/Linux supercomputer with 8 dual-Intel Pentium II nodes and a Myrinet interconnection network. I pitched the idea of including a Linux supercomputer on the NSF-supported National Technology Grid. NCSA’s director Larry Smarr made a high risk/high payoff bet on my vision of the first Linux supercomputer widely available to national science communities by allocating $400,000. I assembled a team to build a 128-node dual-Intel Pentium II process system Roadrunner with Myrinet. Roadrunner was among the 100 fastest supercomputers in the world when it went online. More historical information can be found in: D.A. Bader, “&lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9546947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linux and Supercomputing: How my passion for building COTS systems led to an HPC revolution&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;In December 2021, it was announced that you will be leading an &lt;a href=&#34;https://davidbader.net/post/20211216-njit/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;international project to develop streaming algorithms for graph data&lt;/a&gt;. What are the goals of this project? How are new innovations in scalable graph analytics impacting areas including cybersecurity and healthcare?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Graphs are an abstraction that are useful for solving real-world problems such as those in cybersecurity and healthcare, where the vertices of the graph represent the objects and the edges connecting vertices represent the relationships between these objects. Graphs require combinatorial algorithms that are particularly difficult to parallelize due to their irregular data structures, lack of locality, and unpredictable memory traces. In the over three decades since I designed and implemented my first parallel graph algorithm, I’ve produced multiple “firsts” in algorithm design and implementation, including the first evaluation of parallel single-source shortest path algorithms on large and real-world graphs, the first scalable parallel betweenness centrality calculation, the first scalable parallel community detection algorithm (winner of the 10th DIMACS Challenge&amp;rsquo;s mix challenge), the first parallel streaming community maintenance algorithm, and many of the best-performing graph algorithms for GPUs. My open-source STINGER streaming graph analysis framework (winner of IEEE HPEC Best Paper) is the first of its kind, leading research and applications in graph analysis as the graph evolves. My recent NSF awards include support for open-source frameworks StreamWare (with D. Brooks, S.R. Kuppannagari, S. Basu Roy, V. Prasanna, X. Qian, and Z. Du) to enable users to develop streaming data-science applications, and Arkouda, which combines the productivity of Python with the performance of HPE/Cray Chapel for manipulation multi-terabyte data sets and graphs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Why are graph analytics effective in understanding the dynamics of large social networks? Where is research heading in this area?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Graphs are of tremendous importance in understanding the dynamics of large social networks. The graph abstraction allows natural modeling of people, devices, social media posts (as vertices), and interactions and information flow (as edges). With the availability of new open-source tools, such as STINGER, StreamWare, and Arkouda, as mentioned earlier, we’re able to discover emerging trends, find new communities, identify campaigns of misinformation, and make social networks safer by removing malicious actors. I’m proud of my team of collaborators and students whose research and algorithms are used in practice today. While parallel graph algorithms and supercomputers have helped analyze massive data sets, they are often performing forensic analysis after an egregious event. With new algorithmic and architectural developments that enable high-performance streaming analytics, future research will solve predictive analytics—actionable intelligence that understands events as they unfold and allows for real-time actions within the network.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;David A. Bader is a Distinguished Professor and a founder of the Department of Data Science and Director of the Institute for Data Science at New Jersey Institute of Technology. His research interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. Bader has co-authored over 300 scholarly papers and received best paper awards at various conferences. He is the editor of the books &lt;em&gt;Massive Graph Analytics&lt;/em&gt;, &lt;em&gt;Graph Partitioning and Graph Clustering&lt;/em&gt;, &lt;em&gt;Scientific Computing with Multicore and Accelerators&lt;/em&gt;, and &lt;em&gt;Petascale Computing&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bader serves as Editor-in-Chief of &lt;a href=&#34;https://dl.acm.org/journal/topc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;ACM Transactions on Parallel Computing&lt;/em&gt;&lt;/a&gt; (TOPC). Bader was selected as an ACM Fellow for contributions to high-performance computing systems, graph analytics, and technical leadership in parallel computing. He is also a Fellow of the Society of Industrial and Applied Mathematics (SIAM), the American Association for the Advancement of Science (AAAS), and the Institute of Electrical and Electronics Engineers (IEEE). Bader received the 2021 IEEE Sidney Fernbach Award for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220707-acm/david-bader_hu_4ca753815d43728d.webp 400w,
               /blog/20220707-acm/david-bader_hu_44951060fd286222.webp 760w,
               /blog/20220707-acm/david-bader_hu_95f68d8f38cbef2a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220707-acm/david-bader_hu_4ca753815d43728d.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.acm.org/articles/people-of-acm/2022/david-a-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.acm.org/articles/people-of-acm/2022/david-a-bader&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Establishes the David A. Bader Endowed Graduate Student Program Support Fund at U of Maryland</title>
      <link>http://localhost:1313/blog/20220617-hpcwire/</link>
      <pubDate>Fri, 17 Jun 2022 13:10:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220617-hpcwire/</guid>
      <description>&lt;p&gt;David A. Bader, Distinguished Professor and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology, recently established the David A. Bader Endowed Graduate Student Program Support Fund in the Department of Electrical &amp;amp; Computer Engineering, A. James Clark School of Engineering, University of Maryland, College Park.&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader&#34; srcset=&#34;
               /blog/20220617-hpcwire/David-Bader-239x300_hu_6c37db97ecf89d65.webp 400w,
               /blog/20220617-hpcwire/David-Bader-239x300_hu_210fed2f1227b26a.webp 760w,
               /blog/20220617-hpcwire/David-Bader-239x300_hu_7ffaa368233f3e1a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220617-hpcwire/David-Bader-239x300_hu_6c37db97ecf89d65.webp&#34;
               width=&#34;239&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Bader gave the gift to his alma mater (Ph.D. 1996) on the 30 year anniversary of the start of his doctoral program under the direction of Prof. Joseph JaJa, and Bader’s founding of the department’s graduate student association. In recognition of his gift, the department renamed the Electrical and Computer Engineering Graduate Student Service Award the David A. Bader Award. While at University of Maryland, Bader built an early high-performance cluster of DEC AlphaServers, prior to his creation of the first Linux supercomputer with widely available commodity components a few years later — a computing solution that’s estimated to have had economic impact of more than $100 trillion dollars over the last 25 years — and earned him the 2021 IEEE Sidney Fernbach Award.&lt;/p&gt;
&lt;p&gt;In 1992, Bader arrived at the University of Maryland to both pursue his Ph.D. in electrical engineering and explore his interest in the potential of parallel computing. While he landed a prestigious NASA Fellowship that supported his research, a top-notch faculty advisor in Professor Joseph JaJa, and an office suite in the newly opened A.V. Williams building on campus, Bader felt something was missing from his student experience.&lt;/p&gt;
&lt;p&gt;“Once I received the (NASA) award, I felt my academic life was nirvana with supercomputing and high-performance computing,” explained Bader. “We had access to several experimental parallel computing systems and were working on real-world applications in areas such as Earth sciences, biology, and national security. But I felt isolated. Other than brief interactions next to the shared printer outside of my office, I would rarely speak with any other students. Graduate students would go into their labs and shut the doors, and all the doors remained closed.”&lt;/p&gt;
&lt;p&gt;At that time, there were no activities for graduate students in the department, so in an effort to open those closed doors and meet other students, Bader approached the department about organizing activities specifically for graduate students.&lt;/p&gt;
&lt;p&gt;“I was able to organize graduate students and acquire significant support from the department to initiate a whole set of activities that augmented the academic programs,” said Bader. He brought together a cluster of students who cared about advancing graduate student life and led the way in several new initiatives, such as the graduate student coffee hour, the graduate student electronic newsgroup, the graduate student fellowship book, the graduate student job bank, and the graduate student handbook. What started as a series of community-building activities would ultimately become the Electrical &amp;amp; Computer Engineering Graduate Student Association (ECEGSA).&lt;/p&gt;
&lt;p&gt;“Under my leadership, we created study groups for qualifying exams and selected graduate students to serve as representatives on appropriate department committees. These activities taken as a whole unified the graduate students into a more inclusive community,” he said.&lt;/p&gt;
&lt;p&gt;Bader’s desire to foster community ran beyond academic and professional networking.&lt;/p&gt;
&lt;p&gt;“My parents would take my siblings and me to diverse activities when we were growing up, and they instilled in me that it was always important to realize that we live in a global world,” explained Bader. Bader’s father’s family escaped anti-Semitic persecution in Poland and came to the United States in the early 20th century. His mother, born in Austria, survived the Holocaust as a child through the Kindertransport, an organized effort to rescue children from Nazi-controlled territory just before World War II broke out. Bader noted that many of his fellow students were international and potentially far from home for the first time. “Coming into such a culturally diverse department, the ECEGSA was a way for people to get to know one another, launch their careers and become lifelong friends in addition to classmates.”&lt;/p&gt;
&lt;p&gt;In the nearly 30 years since Bader established the ECEGSA—an effort for which he received university recognition for improving the quality and experience of graduate life—the organization has continued to foster community and serve as a voice for the department’s graduate students. It is an enduring legacy that Bader not only started, but that he aims to support for the next 30 years and beyond through the creation of the David A. Bader Endowed Graduate Student Program Support Fund in Electrical and Computer Engineering.&lt;/p&gt;
&lt;p&gt;“Wishing to sustain the success of the ECEGSA sparked my idea for this gift,” said Bader. “I wanted to purposefully support the continued building of graduate student culture in the department, whether it is through the GSA or other activities that aren’t typically supported through academic scholarships.”&lt;/p&gt;
&lt;p&gt;Bader acknowledged that he wanted to give back to the campus in a way that was both meaningful to him and a tribute to the positive doctoral experience he had at Maryland.&lt;/p&gt;
&lt;p&gt;“I can attribute the success of my career to the department, to engineering, and also to the university,” said Bader. “This gift also recognizes my Ph.D. advisor, Joseph JaJa, to whom I am grateful. From him, I learned not just about engineering, algorithms, and computing, but about thoughtful leadership and professionalism as part of an academic community. That, along with all his support, helped advance my career.”&lt;/p&gt;
&lt;p&gt;It is a fitting tribute to an academic and professional relationship that has spanned nearly 30 years.&lt;/p&gt;
&lt;p&gt;“Since I started working with David, I felt that he was a unique student in terms of his broad interests and leadership skills,” said JaJa. “In addition to the rare combination of his analytical skills and the ease of which he can build systems, he had excellent organizational skills that extended to a wide range of social activities well beyond courses and research. He was a fantastic team player and a visionary researcher.”&lt;/p&gt;
&lt;p&gt;Bader has gone on to a career with a remarkable number of milestones, not the least of which is his role in creating the first Linux supercomputer using widely available commodity components—a computing solution that’s estimated to have had economic impact of more than $100 trillion dollars over the last 25 years. The roots of this watershed research were developed during his Ph.D. and post-doc days at Maryland.&lt;/p&gt;
&lt;p&gt;“While I was at Maryland, we built the first high-performance cluster of DEC AlphaServer multiprocessor computers, essentially a commodity-based supercomputer that broke the mold of traditional monolithic supercomputers,” said Bader. “Our work provided the first programming paradigm for what is ubiquitous today, but at the time, was a very novel type of architecture.”&lt;/p&gt;
&lt;p&gt;As Bader explained, until he helped bring about the era of commodity-based supercomputing, every high-end machine was unique in terms of the processor, the operating system, and the design.&lt;/p&gt;
&lt;p&gt;“You would have to write code specifically for that one supercomputer,” he said, adding that his work ultimately helped ‘democratize supercomputing.’ “We made it accessible where anyone could buy a machine and basically have a supercomputer in their office. As long as you used some standards in how you programmed it, you could pick and choose the parts that you wanted, and because they’re all commodity, you could build your own supercomputer much cheaper than having to pay one manufacturer for one big supercomputer.”&lt;/p&gt;
&lt;p&gt;Creating solutions where previously none existed has been another hallmark of Bader’s career. While at The Georgia Institute of Technology, he founded the School of Computational Science and Engineering, and in his current role as Distinguished Professor and Director of the Institute for Data Science at the New Jersey Institute of Technology, he established their Department of Data Science.&lt;/p&gt;
&lt;p&gt;In reflecting on both his career, and his establishment of the ECEGSA, Bader said, “The world is always changing, and everyone has the ability to make an impact. Rather than accepting the status quo, we can create new things that better prepare us for the future.”&lt;/p&gt;
&lt;p&gt;He hopes his example inspires and empowers others to be creative as they graduate, head out into the world and the workforce, and invariably confront problems that need to be solved. To encourage and support those efforts at Maryland, and in recognition of his gift, the department renamed the Electrical and Computer Engineering Graduate Student Service Award the David A. Bader Award.&lt;/p&gt;
&lt;p&gt;“The world is changing rapidly, and I can’t imagine what the needs will be in another 30 years,” he said. “To be able to give back to the next generation and potentially help any student who could benefit by having the support system they need to make it through their master’s or Ph.D. program, fulfill their potential, and significantly impact society, that’s important to me.”&lt;/p&gt;
&lt;p&gt;Bader has been recognized with numerous awards for his work, including most recently the 2021 IEEE Sidney Fernbach Award and recognition as an Association for Computing Machinery (ACM) Fellow. He is also a Fellow with IEEE, AAAS, and SIAM. In 2012, Bader was the inaugural recipient of the Electrical and Computer Engineering department’s Distinguished Alumni Award. He has served as an expert panelist on several White House committees addressing supercomputing and big data and been involved in numerous public and private sector research partnerships.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/building-stronger-networks-to-help-grad-students-excel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source&lt;/a&gt;: University of Maryland&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/david-bader-establishes-the-david-a-bader-endowed-graduate-student-program-support-fund-at-u-of-maryland/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/david-bader-establishes-the-david-a-bader-endowed-graduate-student-program-support-fund-at-u-of-maryland/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Stronger Networks to Help Grad Students Excel</title>
      <link>http://localhost:1313/blog/20220615-maryland/</link>
      <pubDate>Wed, 15 Jun 2022 20:52:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220615-maryland/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220615-maryland/article14934.large_hu_b9c7049e0dd0b4b6.webp 400w,
               /blog/20220615-maryland/article14934.large_hu_5ce258be3e404597.webp 760w,
               /blog/20220615-maryland/article14934.large_hu_4c3f8880e1a80cb9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220615-maryland/article14934.large_hu_b9c7049e0dd0b4b6.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In 1992, alumnus David Bader (Ph.D. ‘96) arrived at the University of Maryland to both pursue his Ph.D. in electrical engineering and explore his interest in the potential of parallel computing. While he landed a prestigious NASA Fellowship that supported his research, a top-notch faculty advisor in Professor Joseph JaJa, and an office suite in the newly opened A.V. Williams building on campus, Bader felt something was missing from his student experience.&lt;/p&gt;
&lt;p&gt;“Once I received the (NASA) award, I felt my academic life was nirvana with supercomputing and high-performance computing,” explained Bader. “We had access to several experimental parallel computing systems and were working on real-world applications in areas such as Earth sciences, biology, and national security. But I felt isolated. Other than brief interactions next to the shared printer outside of my office, I would rarely speak with any other students. Graduate students would go into their labs and shut the doors, and all the doors remained closed.”&lt;/p&gt;
&lt;p&gt;At that time, there were no activities for graduate students in the department, so in an effort to open those closed doors and meet other students, Bader approached the department about organizing activities specifically for graduate students.&lt;/p&gt;
&lt;p&gt;“I was able to organize graduate students and acquire significant support from the department to initiate a whole set of activities that augmented the academic programs,” said Bader. He brought together a cluster of students who cared about advancing graduate student life and led the way in several new initiatives, such as the graduate student coffee hour, the graduate student electronic newsgroup, the graduate student fellowship book, the graduate student job bank, and the graduate student handbook. What started as a series of community-building activities would ultimately become the Electrical &amp;amp; Computer Engineering Graduate Student Association (ECEGSA).&lt;/p&gt;
&lt;p&gt;“Under my leadership, we created study groups for qualifying exams and selected graduate students to serve as representatives on appropriate department committees. These activities taken as a whole unified the graduate students into a more inclusive community,” he said.&lt;/p&gt;
&lt;p&gt;Bader’s desire to foster community ran beyond academic and professional networking.&lt;/p&gt;
&lt;p&gt;“My parents would take my siblings and me to diverse activities when we were growing up, and they instilled in me that it was always important to realize that we live in a global world,” explained Bader. Bader’s father’s family escaped anti-Semitic persecution in Poland and came to the United States in the early 20th century. His mother, born in Austria, survived the Holocaust as a child through the Kindertransport, an organized effort to rescue children from Nazi-controlled territory just before World War II broke out. Bader noted that many of his fellow students were international and potentially far from home for the first time. “Coming into such a culturally diverse department, the ECEGSA was a way for people to get to know one another, launch their careers and become lifelong friends in addition to classmates.”&lt;/p&gt;
&lt;p&gt;In the nearly 30 years since Bader established the ECEGSA—an effort for which he received university recognition for improving the quality and experience of graduate life—the organization has continued to foster community and serve as a voice for the department’s graduate students. It is an enduring legacy that Bader not only started, but that he aims to support for the next 30 years and beyond through the creation of the David A. Bader Endowed Graduate Student Program Support Fund in Electrical and Computer Engineering.&lt;/p&gt;
&lt;p&gt;“Wishing to sustain the success of the ECEGSA sparked my idea for this gift,” said Bader. “I wanted to purposefully support the continued building of graduate student culture in the department, whether it is through the GSA or other activities that aren’t typically supported through academic scholarships.”&lt;/p&gt;
&lt;p&gt;Bader acknowledged that he wanted to give back to the campus in a way that was both meaningful to him and a tribute to the positive doctoral experience he had at Maryland.&lt;/p&gt;
&lt;p&gt;“I can attribute the success of my career to the department, to engineering, and also to the university,” said Bader. “This gift also recognizes my Ph.D. advisor, Joseph JaJa, to whom I am grateful. From him, I learned not just about engineering, algorithms, and computing, but about thoughtful leadership and professionalism as part of an academic community. That, along with all his support, helped advance my career.”&lt;/p&gt;
&lt;p&gt;It is a fitting tribute to an academic and professional relationship that has spanned nearly 30 years.&lt;/p&gt;
&lt;p&gt;“Since I started working with David, I felt that he was a unique student in terms of his broad interests and leadership skills,” said JaJa. “In addition to the rare combination of his analytical skills and the ease of which he can build systems, he had excellent organizational skills that extended to a wide range of social activities well beyond courses and research. He was a fantastic team player and a visionary researcher.”&lt;/p&gt;
&lt;p&gt;Bader has gone on to a career with a remarkable number of milestones, not the least of which is his role in creating the first Linux supercomputer using widely available commodity components—a computing solution that’s estimated to have had economic impact of more than $100 trillion dollars over the last 25 years. The roots of this watershed research were developed during his Ph.D. and post-doc days at Maryland.&lt;/p&gt;
&lt;p&gt;“While I was at Maryland, we built the first high-performance cluster of DEC AlphaServer multiprocessor computers, essentially a commodity-based supercomputer that broke the mold of traditional monolithic supercomputers,” said Bader. “Our work provided the first programming paradigm for what is ubiquitous today, but at the time, was a very novel type of architecture.”&lt;/p&gt;
&lt;p&gt;As Bader explained, until he helped bring about the era of commodity-based supercomputing, every high-end machine was unique in terms of the processor, the operating system, and the design.&lt;/p&gt;
&lt;p&gt;“You would have to write code specifically for that one supercomputer,” he said, adding that his work ultimately helped ‘democratize supercomputing.’ “We made it accessible where anyone could buy a machine and basically have a supercomputer in their office. As long as you used some standards in how you programmed it, you could pick and choose the parts that you wanted, and because they&amp;rsquo;re all commodity, you could build your own supercomputer much cheaper than having to pay one manufacturer for one big supercomputer.”&lt;/p&gt;
&lt;p&gt;Creating solutions where previously none existed has been another hallmark of Bader’s career. While at The Georgia Institute of Technology, he founded the School of Computational Science and Engineering, and in his current role as Distinguished Professor and Director of the Institute for Data Science at the New Jersey Institute of Technology, he established their Department of Data Science.&lt;/p&gt;
&lt;p&gt;In reflecting on both his career, and his establishment of the ECEGSA, Bader said, “The world is always changing, and everyone has the ability to make an impact. Rather than accepting the status quo, we can create new things that better prepare us for the future.”&lt;/p&gt;
&lt;p&gt;He hopes his example inspires and empowers others to be creative as they graduate, head out into the world and the workforce, and invariably confront problems that need to be solved. To encourage and support those efforts at Maryland, and in recognition of his gift, the department renamed the Electrical and Computer Engineering Graduate Student Service Award the David A. Bader Award.&lt;/p&gt;
&lt;p&gt;“The world is changing rapidly, and I can’t imagine what the needs will be in another 30 years,” he said. “To be able to give back to the next generation and potentially help any student who could benefit by having the support system they need to make it through their master’s or Ph.D. program, fulfill their potential, and significantly impact society, that’s important to me.”&lt;/p&gt;
&lt;p&gt;Bader has been recognized with numerous awards for his work, including most recently the 2021 IEEE Sidney Fernbach Award and recognition as an Association for Computing Machinery (ACM) Fellow. He is also a Fellow with IEEE, AAAS, and SIAM. In 2012, Bader was the inaugural recipient of the Electrical and Computer Engineering department’s Distinguished Alumni Award. He has served as an expert panelist on several White House committees addressing supercomputing and big data and been involved in numerous public and private sector research partnerships.&lt;/p&gt;
&lt;p&gt;To learn more about Bader’s work in developing Linux-based supercomputers, read “&lt;a href=&#34;https://doi.org/10.1109/MAHC.2021.3101415&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linux and Supercomputing: How my passion for building COTS systems led to an HPC revolution&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/building-stronger-networks-to-help-grad-students-excel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/building-stronger-networks-to-help-grad-students-excel&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader receives ACM Fellow at ACM Awards Banquet</title>
      <link>http://localhost:1313/blog/20220611-acm/</link>
      <pubDate>Sat, 11 Jun 2022 21:23:19 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220611-acm/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220611-acm/20220611-ACM-Fellow_hu_dc70bc43bbe10e56.webp 400w,
               /blog/20220611-acm/20220611-ACM-Fellow_hu_6f6ede0da59a3933.webp 760w,
               /blog/20220611-acm/20220611-ACM-Fellow_hu_abb7da8f7814e759.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220611-acm/20220611-ACM-Fellow_hu_dc70bc43bbe10e56.webp&#34;
               width=&#34;760&#34;
               height=&#34;569&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-david-a-bader-with-2021-am-turing-award-recipient-jack-dongarra&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David A. Bader with 2021 A.M. Turing Award recipient Jack Dongarra.&#34; srcset=&#34;
               /blog/20220611-acm/pic_hu_11ca18c94cd11678.webp 400w,
               /blog/20220611-acm/pic_hu_e4371d65d6a3155f.webp 760w,
               /blog/20220611-acm/pic_hu_843bda5acf85256c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220611-acm/pic_hu_11ca18c94cd11678.webp&#34;
               width=&#34;573&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David A. Bader with 2021 A.M. Turing Award recipient Jack Dongarra.
    &lt;/figcaption&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>NJIT Hosts Big Data Alliance Symposium with Focus on Education</title>
      <link>http://localhost:1313/blog/20220518-njit/</link>
      <pubDate>Wed, 18 May 2022 21:43:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220518-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-florence-hudson-pitched-the-national-student-data-corps&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Florence Hudson pitched the National Student Data Corps&#34; srcset=&#34;
               /blog/20220518-njit/20220509_140732_hu_55b7aabee2bfc8cf.webp 400w,
               /blog/20220518-njit/20220509_140732_hu_b6c633bee78a3c5e.webp 760w,
               /blog/20220518-njit/20220509_140732_hu_e103d6557bb9e722.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220518-njit/20220509_140732_hu_55b7aabee2bfc8cf.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Florence Hudson pitched the National Student Data Corps
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;NJIT hosted the ninth New Jersey Big Data Alliance Symposium, bringing together nearly 200 experts from academia, government and industry to share ideas about the present state and future trends of their field.&lt;/p&gt;
&lt;p&gt;Attendees represented a gamut of careers, not just programmers, which the panelists said indicates that the importance of artificial intelligence, data science and machine learning applies to nearly all aspects of life in the 21st century. Many students were also present.&lt;/p&gt;
&lt;p&gt;Distinguished Professor &lt;strong&gt;David Bader&lt;/strong&gt;, director of NJIT’s Institute for Data Science, chaired the symposium. Atam Dhawan, senior vice provost for research, said in a welcome session that data science is a major enabler of future technology innovation and its wider role in society. &amp;ldquo;I&amp;rsquo;m a very strong believer in the enablers of technology … addressing not only the global challenges but addressing the needs of the society,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Keynote speakers Stephen Ezell, vice president of global information policy for the Information Technology and Innovation Foundation, and Florence Hudson, executive director of the Northeast Big Data Innovation Hub, both drove home the point.&lt;/p&gt;
&lt;p&gt;Ezell talked about the state of teaching computer science. He said digital skills aren&amp;rsquo;t being taught equitably across the United States, and that it&amp;rsquo;s wrong to assume artificially intelligent systems will take over jobs. He recommended that educators continue pushing for more opportunities going to females, computer science should count as a high school science credit, more STEM charter schools are needed and tax credits should be provided for employer tuition payments.&lt;/p&gt;
&lt;p&gt;Hudson spotlighted student resources available from her organization. There&amp;rsquo;s the National Student Data Corps, which she said is growing even internationally. Several colleges formed chapters and Hudson indicated that she&amp;rsquo;d like to work with the existing &lt;a href=&#34;https://news.njit.edu/revitalized-data-science-club-eyes-insightful-statistics-njit-athletics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT Data Science Club&lt;/a&gt; as a possible new chapter. Her group also provides Slack-based student mentoring and even data science comic books for K-12 students. The group also strives to keep minorities and people of color as 50% of its leadership.&lt;/p&gt;
&lt;p&gt;In an afternoon panel discussion, Loralyn Mears of soft skills academy SteerUs said that data science can be used to help evaluate social and personal issues, not just technical ones. Mukesh Dalal, chief AI officer at Stanley Black &amp;amp; Decker, assured attendees that the positives of artificial intelligence and machine learning outweigh the negatives, even if the negatives get more attention.&lt;/p&gt;
&lt;p&gt;Research track sessions focused on using data science to address topics such as climate change risks, gender bias in natural language processing and stock markets in developing nations.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Data science is an exciting, new discipline focused on solving real-world problems from protecting business from novel cyberattacks to predicting solar activity that could disrupt our communication systems,&amp;rdquo; &lt;strong&gt;Bader&lt;/strong&gt; said, noting the success of the university&amp;rsquo;s newly created data science department within Ying Wu College of Computing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-hosts-big-data-alliance-symposium-focus-education&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-hosts-big-data-alliance-symposium-focus-education&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hyperion Study Tracks Rise and Impact of Linux Supercomputers</title>
      <link>http://localhost:1313/blog/20220517-hpcwire/</link>
      <pubDate>Tue, 17 May 2022 13:32:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220517-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Russell&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220517-hpcwire/Aurora_System_v3_09-16-20_7560x3744_environment-3-675x380_hu_b816ab843c0b6d5b.webp 400w,
               /blog/20220517-hpcwire/Aurora_System_v3_09-16-20_7560x3744_environment-3-675x380_hu_8a934b3681edd6ac.webp 760w,
               /blog/20220517-hpcwire/Aurora_System_v3_09-16-20_7560x3744_environment-3-675x380_hu_572a609fefd0810d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220517-hpcwire/Aurora_System_v3_09-16-20_7560x3744_environment-3-675x380_hu_b816ab843c0b6d5b.webp&#34;
               width=&#34;675&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;That supercomputers produce impactful, lasting value is a basic tenet among the HPC community. To make the point more formally, Hyperion Research has issued a new &lt;a href=&#34;https://davidbader.net/publication/2022-hyperionresearch/2022-HyperionResearch.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;report&lt;/a&gt;, &lt;em&gt;The Economic and Societal Benefits of Linux Supercomputers&lt;/em&gt;. Inclusion of Linux is fundamental here. The powerful, open source operating system was embraced early by the HPC world and helped spawn a huge HPC application ecosystem that makes these systems so broadly useful.&lt;/p&gt;
&lt;p&gt;(Thank you, Linux developer Linus Torvalds and &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor, New Jersey Institute of Technology, one of the leaders pushing Linux into early supercomputing.)&lt;/p&gt;
&lt;p&gt;“Supercomputers have made cars and planes much safer, more fuel efficient and environmentally friendly. They are crucial aids in discovering and extracting new sources of oil and gas, and for developing alternative energy sources. They have enabled the weather community to create more accurate predictions of severe storms that can devastate lives and property. They are heavily relied on by industries ranging from financial services to medicine and health care, entertainment, consumer products, and more recently by Internet companies. And most recently, supercomputers were instrumental in addressing the COVID-19 pandemic,” asserts the report.&lt;/p&gt;
&lt;p&gt;That’s an impressive list. While the subject matter isn’t new, the report is a good reminder and snapshot of the power (and profitability) of HPC writ large. &lt;a href=&#34;https://hyperionresearch.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyperion&lt;/a&gt; briefly traces the history of Linux and the vast application universe it helped spawn, and how when combined in supercomputers, not only do good things happen, but also big things can happen.&lt;/p&gt;
&lt;p&gt;Here’s an excerpt from the introduction:&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“This special study explores the early history of how Linux supercomputers were developed and the overall returns from supercomputing in both: 1) the economic value from building and supporting supercomputers; and 2) the value from using supercomputers. Additionally, this report presents some interesting examples of how supercomputers provide returns.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Over $300 billion in revenue has been generated from selling supercomputers. This represents a sizable economic gain, especially since the use of these systems generated research valued at least ten times over the purchase price. While it is difficult to fully measure the value that supercomputers have generated, even looking at just automotives, aircraft, and pharmaceuticals supercomputers have contributed to products valued at more than $100 trillion over the last 25 years. And this doesn’t count the tremendous value to new scientific discoveries in almost all disciplines.”&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;Based on the consistent high-growth rates of HPC purchases to address some of the most challenging national issues and to increase corporate competitiveness, says the report, the next five years are expected to produce even high revenues.&lt;/p&gt;
&lt;p&gt;“Looking just in 2022, the direct economic returns are projected to exceed $16 billion in servers and around $30 billion including the supporting infrastructure. The direct economic returns from selling Linux computers in 2022-2026 are projected to exceed $90 billion in servers and an additional $90 billion for the supporting infrastructure. This results in nearly $200 billion in revenue generated from selling Linux supercomputers over just a five-year period. This represents a sizable amount of economic gain, especially since the use of these Linux systems generates research valued at least ten times over the purchase price,” according to the study.&lt;/p&gt;
&lt;p&gt;The study documents Linux’s spectacular rise noting: “A clear step change in multiple developmental facets of the technology, open-source Linux OS-based supercomputers quickly gained popularity between the years of 2002-2010, representing over 90% of the Top500 in that year, and representing 82% of all technical servers in 2010. While it has since become the only OS represented on Top500 list, there [is] a wealth of diversity for those with different requirements or limitations. Needs for specific hardware, application, data, or user types can and are met with one of a plethora of Linux-based operating systems market offerings (as shown in Figure 3). Figure 3 shows (below) that a number of companies have been created to sell and support Linux operating systems.”&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220517-hpcwire/Hyperion-Report_OS-shares-600x365_hu_ae49e6a282d41f8f.webp 400w,
               /blog/20220517-hpcwire/Hyperion-Report_OS-shares-600x365_hu_e81c542bb1baf21.webp 760w,
               /blog/20220517-hpcwire/Hyperion-Report_OS-shares-600x365_hu_7876c4d298e2f972.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220517-hpcwire/Hyperion-Report_OS-shares-600x365_hu_ae49e6a282d41f8f.webp&#34;
               width=&#34;600&#34;
               height=&#34;365&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220517-hpcwire/Hyperion-Report_OS-Revenue-600x333_hu_218c8aaf3d03d383.webp 400w,
               /blog/20220517-hpcwire/Hyperion-Report_OS-Revenue-600x333_hu_307007215177016e.webp 760w,
               /blog/20220517-hpcwire/Hyperion-Report_OS-Revenue-600x333_hu_1467efd5b83c91f0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220517-hpcwire/Hyperion-Report_OS-Revenue-600x333_hu_218c8aaf3d03d383.webp&#34;
               width=&#34;600&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Hyperion report also calls out ROI for use of Linux supercomputers based on average financial impacts by sector for 175 projects. “The average financial ROI was $509.3 dollars in revenues/sales per dollar invested in HPC, and $47.2 dollars in profits or cost savings per dollar invested in HPC. In addition, there were 2,335 new jobs created from these projects,” declares Hyperion.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220517-hpcwire/Hyperion-Report_ROI_Avg-600x171_hu_dc06d23af3e06ff4.webp 400w,
               /blog/20220517-hpcwire/Hyperion-Report_ROI_Avg-600x171_hu_b9a3ae5f3e2f2735.webp 760w,
               /blog/20220517-hpcwire/Hyperion-Report_ROI_Avg-600x171_hu_511e77d06e1c9974.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220517-hpcwire/Hyperion-Report_ROI_Avg-600x171_hu_dc06d23af3e06ff4.webp&#34;
               width=&#34;600&#34;
               height=&#34;171&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220517-hpcwire/Hyperion-Report_ROI-by-Industry-600x289_hu_fe36abdadc1a27f2.webp 400w,
               /blog/20220517-hpcwire/Hyperion-Report_ROI-by-Industry-600x289_hu_27e14d0e5f529e60.webp 760w,
               /blog/20220517-hpcwire/Hyperion-Report_ROI-by-Industry-600x289_hu_b42fb5d052e3a54e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220517-hpcwire/Hyperion-Report_ROI-by-Industry-600x289_hu_fe36abdadc1a27f2.webp&#34;
               width=&#34;600&#34;
               height=&#34;289&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;On balance the report is quick read covering a bit of history and pulling together data points to showcase the rise and impact of supercomputing.&lt;/p&gt;
&lt;p&gt;Link to study, &lt;a href=&#34;https://davidbader.net/publication/2022-hyperionresearch/2022-HyperionResearch.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://davidbader.net/publication/2022-hyperionresearch/2022-HyperionResearch.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Feature image&lt;/em&gt;: The Aurora exascale supercomputer &lt;a href=&#34;https://www.hpcwire.com/2022/05/10/aurora-installation-underway-now-open-for-reservations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;now being installed&lt;/a&gt; at Argonne National Laboratory.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hyperion Report authors&lt;/em&gt;: Earl Joseph (CEO), Melissa Riddle (associate analyst), Tom Sorensen (associate analyst), and Steve Conway (senior advisor)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2022/05/17/hyperion-study-tracks-rise-and-impact-of-linux-supercomputers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2022/05/17/hyperion-study-tracks-rise-and-impact-of-linux-supercomputers/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meet Dashlane, one of the best password protectors on the market</title>
      <link>http://localhost:1313/blog/20220505-nypost/</link>
      <pubDate>Thu, 05 May 2022 18:11:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220505-nypost/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Victoria Giardina&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-new-york-post-composite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New York Post Composite&#34; srcset=&#34;
               /blog/20220505-nypost/Dashlane-1_hu_7e4c921d02c48e01.webp 400w,
               /blog/20220505-nypost/Dashlane-1_hu_c58a72204d66faf9.webp 760w,
               /blog/20220505-nypost/Dashlane-1_hu_d5dd421189b1440b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220505-nypost/Dashlane-1_hu_7e4c921d02c48e01.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New York Post Composite
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;With online data, you have to be extremely careful about how you go about saving it. From sensitive files to a string of letters and numbers we call passwords, it can begin to seem like a full-time job managing it all.&lt;/p&gt;
&lt;p&gt;We’re here to tell you that it doesn’t have to be. As one of the &lt;a href=&#34;https://nypost.com/article/best-password-managers-per-experts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;best password protectors&lt;/a&gt; (per a cybersecurity expert), &lt;a href=&#34;https://www.dashlane.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dashlane&lt;/a&gt; deserves much acclaim.&lt;/p&gt;
&lt;p&gt;For one, the service has made it simpler to manage your data. It’s essentially a platform to aggregate all your top-secret, personal information in one place, from the hundreds of online accounts you use on occasion to the access code of your parent’s apartment.&lt;/p&gt;
&lt;p&gt;“Dashlane software has been available for a decade, and runs either as a browser extension on Chrome, Edge, Firefox, and Safari, or on a mobile device such as iOS and Android,” &lt;strong&gt;David Bader&lt;/strong&gt;, PhD, distinguished professor of data science at New Jersey Institute of Technology (NJIT), told the New York Post. “Similar with 1Password and Keeper, Dashlane uses PBKDF2 on your device and industry-standard 256-bit AES encryption to protect your passwords in the secure vault.”&lt;/p&gt;
&lt;p&gt;What’s more, Dashlane is used by more than 15 million people and more than 20,000 companies worldwide. The company also has a free version that can store up to 50 passwords from one device.&lt;/p&gt;
&lt;p&gt;“For users who purchase a license, Dashlane has also partnered with HotSpot Shield to provide a VPN, which adds a layer of protection for your browsing and data when you’re connecting to the Internet on public WiFi,” Bader adds. Plus, it’s $90 a year for a family of six.&lt;/p&gt;
&lt;h2 id=&#34;dashlane-password-manager-60-a-year&#34;&gt;Dashlane Password Manager, $60 a year&lt;/h2&gt;


















&lt;figure  id=&#34;figure-this-interface-is-what-business-users-would-see-upon-using-dashlane-dashane&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;This interface is what business users would see upon using Dashlane. (Dashane)&#34; srcset=&#34;
               /blog/20220505-nypost/Dashlane_hu_613d651c909b303c.webp 400w,
               /blog/20220505-nypost/Dashlane_hu_a06a80a6bc71a204.webp 760w,
               /blog/20220505-nypost/Dashlane_hu_1882dc4455a1e35a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220505-nypost/Dashlane_hu_613d651c909b303c.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      This interface is what business users would see upon using Dashlane. (Dashane)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Oh, and you don’t have to manage a Fortune 500 company to benefit from Dashlane’s services, either. The platform offers pricing for both personal and business plans, along with offering a concrete option for families.&lt;/p&gt;
&lt;p&gt;In other words, Dashlane wants your entire household to be protected (no more asking your parents or children “what’s the password” any longer)!&lt;/p&gt;
&lt;p&gt;Also, you’ll be glad to know that in the 10+ years Dashlane has been in operation, it has &lt;strong&gt;never&lt;/strong&gt; had a security breach. Plus, there are more than 125,000 five-star ratings in the Apple App Store and Google Play Store.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nypost.com/2022/05/05/dashlane-why-the-password-protector-is-worth-the-buy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nypost.com/2022/05/05/dashlane-why-the-password-protector-is-worth-the-buy/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your Hard Drive May One Day Use Diamonds for Storage</title>
      <link>http://localhost:1313/blog/20220503-lifewire/</link>
      <pubDate>Tue, 03 May 2022 13:28:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220503-lifewire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Sascha Brodsky, Fact checked by Jerri Ledford&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Diamonds could one day be used to store vast amounts of information.&lt;/li&gt;
&lt;li&gt;Researchers are trying to use the strange effects of quantum mechanics to hold information.&lt;/li&gt;
&lt;li&gt;However, experts say don’t expect a quantum hard drive in your PC anytime soon.&lt;/li&gt;
&lt;/ul&gt;


















&lt;figure  id=&#34;figure-manley099--getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;manley099 / Getty Images&#34; srcset=&#34;
               /blog/20220503-lifewire/GettyImages-173948340_hu_a09762037cccf19.webp 400w,
               /blog/20220503-lifewire/GettyImages-173948340_hu_79dec5e309cfa65b.webp 760w,
               /blog/20220503-lifewire/GettyImages-173948340_hu_5fe1e7af9329f47.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220503-lifewire/GettyImages-173948340_hu_a09762037cccf19.webp&#34;
               width=&#34;650&#34;
               height=&#34;433&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      manley099 / Getty Images
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Diamonds may be the key to storing vast quantities of data.&lt;/p&gt;
&lt;p&gt;Researchers in Japan have created &lt;a href=&#34;https://www.ad-na.com/magazine_en/archives/1401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a pure and light diamond&lt;/a&gt; for use in quantum computing in a move that could lead to new kinds of hard drives. It’s part of an ongoing effort to use the strange effects of quantum mechanics to hold information.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Unlike our classical computers that operate on binary digits (or &amp;lsquo;bits&amp;rsquo;), that is, 0&amp;rsquo;s and 1&amp;rsquo;s, quantum computers use &amp;lsquo;qubits&amp;rsquo; that can be in a linear combination of two states,&amp;rdquo; &lt;strong&gt;David Bader&lt;/strong&gt;, a computer science professor at the New Jersey Institute of Technology who studies quantum memory, told Lifewire in an email interview. &amp;ldquo;Storing qubits is more challenging than storing classic bits since qubits cannot be cloned, are error-prone, and have a brief lifetime of a fraction of a second.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;quantum-memories&#34;&gt;Quantum Memories&lt;/h2&gt;
&lt;p&gt;Researchers have long hypothesized that diamonds could be used as a quantum storage medium. The crystalline structures can be used to store data as qubits if they can be made nearly free of nitrogen. However, the manufacturing process is complex, and up until now, the diamonds that have been created are too small for practical purposes.&lt;/p&gt;


















&lt;figure  id=&#34;figure-saga-university&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Saga University&#34; srcset=&#34;
               /blog/20220503-lifewire/20220421diamond_hu_7ce2e4e61f3f3d13.webp 400w,
               /blog/20220503-lifewire/20220421diamond_hu_b7983c0b56d9e641.webp 760w,
               /blog/20220503-lifewire/20220421diamond_hu_d7e581feae52fbb3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220503-lifewire/20220421diamond_hu_7ce2e4e61f3f3d13.webp&#34;
               width=&#34;635&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Saga University
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ad-na.com/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adamant Namiki Precision Jewelry Company&lt;/a&gt; and researchers from Saga University claim to have developed a new manufacturing process that can produce diamond wafers that are two inches in size and pure enough for practical applications.  &amp;ldquo;A 2-inch diamond wafer theoretically enables enough quantum memory to record 1 billion Blu-ray discs,&amp;rdquo; the company wrote in the news release. &amp;ldquo;This is equivalent to all the mobile data distributed in the world in one day.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader said this diamond memory approach relies on storing the qubit as a nuclear spin. &amp;ldquo;For example, physicists have demonstrated storing a qubit in the spin of a nitrogen atom embedded in a diamond,&amp;rdquo; he added.&lt;/p&gt;
&lt;h2 id=&#34;promising-research&#34;&gt;Promising Research&lt;/h2&gt;
&lt;p&gt;Diamonds are only one way in which quantum computers could store data. Scientists are pursuing two directions for building quantum memories, one using transmission of light and the other using physical materials, Bader said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Qubits can be represented by the amplitude and phase of light,&amp;rdquo; Bader added. &amp;ldquo;Light is also used in quantum computing&amp;rsquo;s gradient echo memory where the states of light are mapped into the excitation of clouds of atoms, and the light can be &amp;lsquo;un-absorbed&amp;rsquo; later. Unfortunately, though, it is impossible to measure both the amplitude and phase without interfering with the light. So we can think about light as a way to transport qubits—much like a classical computer network.&amp;rdquo;&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/spUJ1vIEMOo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Even more exotic materials than diamonds are being considered. Earlier this year, scientists &lt;a href=&#34;https://www.sciencedaily.com/releases/2022/02/220217141249.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;used a qubit&lt;/a&gt; made from an ion of the rare earth element, ytterbium, which is also used in lasers, and embedded this ion in a transparent crystal of yttrium orthovanadate. &amp;ldquo;The quantum states were then manipulated using optical and microwave fields,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;Quantum memory could potentially sidestep problems producing large enough hard drives. Bader pointed out that classical computer storage systems of the kind that are in PCs grow linearly in the amount of information stored by classical bits. For example, if you double your hard drive from 512GB to 1TB, you&amp;rsquo;ve doubled the amount of information you can store, he said.&lt;/p&gt;
&lt;p&gt;Qubits are &amp;ldquo;phenomenal&amp;rdquo; for storing information, and the amount of information represented grows exponentially in the number of qubits. &amp;ldquo;For instance, adding just one more qubit to a system doubles the number of states,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;Vasili Perebeinos, a professor at The State University of New York Buffalo who works on a quantum memory, told Lifewire in an email interview that researchers are trying to identify solid-state materials that could be useful for quantum data storage.&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Storing qubits is more challenging than storing classic bits since qubits cannot be cloned, are error-prone, and have a brief lifetime of a fraction of a second.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;&amp;ldquo;The advantage of solid-state quantum memory is in the ability to miniaturize and scale the quantum network device components,&amp;rdquo; Perebeinos said.&lt;/p&gt;
&lt;p&gt;However, don’t expect a quantum hard drive in your PC anytime soon. Bader said that &amp;ldquo;it will take years, and possibly even decades, to build large enough quantum computers with sufficient numbers of qubits for solving real-world applications.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.lifewire.com/your-hard-drive-may-one-day-use-diamonds-for-storage-5270809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lifewire.com/your-hard-drive-may-one-day-use-diamonds-for-storage-5270809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mobitool.net/en/your-hard-drive-may-one-day-use-diamonds-for-storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mobitool.net/en/your-hard-drive-may-one-day-use-diamonds-for-storage/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.viknews.com/your-hard-drive-may-one-day-use-diamonds-for-storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.viknews.com/your-hard-drive-may-one-day-use-diamonds-for-storage/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vozz.vn/en/your-hard-drive-may-one-day-use-diamonds-for-storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vozz.vn/en/your-hard-drive-may-one-day-use-diamonds-for-storage/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ROI Influencers: Technology 2022 — Thought Leaders &amp; Academics</title>
      <link>http://localhost:1313/blog/20220421-roi-nj/</link>
      <pubDate>Thu, 21 Apr 2022 19:33:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220421-roi-nj/</guid>
      <description>&lt;p&gt;Innovating: If your company isn’t doing it, it probably will not be around for very long. We all get that. The question is: Who is doing it best?&lt;/p&gt;
&lt;p&gt;That’s what we set out to tackle in our second annual ROI Influencers: Technology list.&lt;/p&gt;
&lt;p&gt;Our goal was to select those who are having the most impact in the technology and innovation sectors.&lt;/p&gt;
&lt;p&gt;Categorizing the list wasn’t easy.&lt;/p&gt;
&lt;p&gt;It starts with the founders — those who have created the cool and innovative companies that dot the state. But it wouldn’t be complete without the investors that helped many of them grow, the academics who are mentoring and inspiring the next generation, the executives of tech companies that give the state such a rich ecosystem and the thought leaders that help bring everyone together.&lt;/p&gt;
&lt;p&gt;This year, we’ve added the key service providers who are helping these companies grow — and the chief technology and chief innovation officers, the ones that are the heads of so much internal innovation.&lt;/p&gt;
&lt;p&gt;We’re overwhelmed with the quality of honorees on the list — and humbled to know that there are many others out there deserving of the recognition. We’re confident this list will only grow each year.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20220421-roi-nj/Bader-David-768x500_hu_9cdb435d87a2e00e.webp 400w,
               /blog/20220421-roi-nj/Bader-David-768x500_hu_d9a2130aa76186dc.webp 760w,
               /blog/20220421-roi-nj/Bader-David-768x500_hu_d24b7d3795b937f1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220421-roi-nj/Bader-David-768x500_hu_9cdb435d87a2e00e.webp&#34;
               width=&#34;760&#34;
               height=&#34;495&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Distinguished Professor, New Jersey Institute of Technology&lt;/p&gt;
&lt;h2 id=&#34;technology-2022-thought-leaders--academics&#34;&gt;Technology 2022: Thought Leaders &amp;amp; Academics&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Director, Institute for Data Science&lt;br&gt;
New Jersey Institute of Technology&lt;/p&gt;
&lt;p&gt;An expert in massive-scale analytics and computational genomics, he is the founder of the Department of Data Science. Was recently named a fellow by the Association for Computing Machinery.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.roi-nj.com/2022/04/21/roi-influencers/technology/2022-technology/roi-influencers-technology-2022-thought-leaders-academics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.roi-nj.com/2022/04/21/roi-influencers/technology/2022-technology/roi-influencers-technology-2022-thought-leaders-academics/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.roi-nj.com/2022/04/20/roi-influencers/technology/2022-technology/thought-leaders-academics/roi-influencers-technology-2022-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.roi-nj.com/2022/04/20/roi-influencers/technology/2022-technology/thought-leaders-academics/roi-influencers-technology-2022-david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random but Memorable Podcast: Minority Report Super Computer with David Bader</title>
      <link>http://localhost:1313/blog/20220405-1password/</link>
      <pubDate>Tue, 05 Apr 2022 13:54:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220405-1password/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;episode-9-2-v5.mp3&#34;&gt;Listen&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;episode-summary&#34;&gt;EPISODE SUMMARY&lt;/h2&gt;
&lt;p&gt;This week we discover the real-world capabilities of supercomputers in cybersecurity and how data analysis can uncover insider threats with &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;. We also wind back the clock and look at how far computing has come, from David&amp;rsquo;s work building the first ever Linux supercomputer to the revolutionary chip inside the PlayStation 3.&lt;/p&gt;
&lt;p&gt;Plus, in Watchtower Weekly, we discuss the recent Okta data breach and the new password sharing fee Netflix is testing. Finally, if Roo&amp;rsquo;s mind wasn&amp;rsquo;t blown enough, we play the Random but Memorable meta edition of Ridiculous Requirements!&lt;/p&gt;
&lt;h2 id=&#34;guest-interview---david-bader&#34;&gt;Guest Interview –  David Bader&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Visit &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;davidbader.net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Follow @Prof_DavidBader on Twitter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://randombutmemorable.simplecast.com/episodes/minority-report-super-computer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://randombutmemorable.simplecast.com/episodes/minority-report-super-computer&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>13 best antivirus software platforms, per a cybersecurity expert</title>
      <link>http://localhost:1313/blog/20220404-newyorkpost/</link>
      <pubDate>Mon, 04 Apr 2022 13:20:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220404-newyorkpost/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Victoria Giardina&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Much like the best password protector services out there, investing in antivirus software isn’t as expensive as you would think — and it provides a safety net for your online data.&lt;/p&gt;
&lt;p&gt;Not to mention, when investing in quality &lt;a href=&#34;https://nypost.com/article/best-smart-home-devices-on-amazon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;smart home devices&lt;/a&gt; — like your treasured laptops and desktops — you want to make sure that hefty purchase isn’t being compromised.&lt;/p&gt;
&lt;p&gt;Luckily, one of the 13 best antivirus software programs of 2022 can help.&lt;/p&gt;
&lt;p&gt;“A computer virus is a malicious piece of software that spreads between computers and can harm your computer or steal your information,” &lt;strong&gt;&lt;a href=&#34;https://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader, PhD&lt;/a&gt;&lt;/strong&gt;, distinguished professor of data science at New Jersey Institute of Technology (NJIT) told the New York Post. “For instance, viruses can delete your files, watch your keystrokes (and grab your login information for your online bank account), or turn your computer into a weapon to attack corporate or government websites.”&lt;/p&gt;
&lt;p&gt;For unmatched protection, check out the best antivirus software programs, as vetted by our cybersecurity expert. For more on what these entail — including what malware, spyware and adware actually mean — check out our FAQ section below.&lt;/p&gt;
&lt;h2 id=&#34;1-mcafee-antivirus-35-for-the-first-year&#34;&gt;1. McAfee Antivirus, $35 for the first year&lt;/h2&gt;
&lt;p&gt;For the past 35 years, McAfee has held a premium spot as one of the earliest and best-known companies offering antivirus software. It offers antivirus software for all major platforms (Windows, macOS, Android, iOS). “McAfee has a 30-day free trial with “all-in-one” protection that includes a VPN for privacy when using public WiFi networks, web protection to avoid phishing scams, identity monitoring, and more,” Bader adds.&lt;/p&gt;
&lt;p&gt;For new customers, McAfee’s basic edition protects up to five devices and costs $35 for the first year and $85 per year after that. For an additional $45 per year, the premium edition adds parental controls and protects up to ten devices, and for another $30 per year, McAfee’s ultimate edition protects an unlimited number of devices and includes $1 million of identity theft coverage and identity restoration assistance.&lt;/p&gt;
&lt;h2 id=&#34;2-norton-antivirus-plus-10-for-the-first-year&#34;&gt;2. Norton Antivirus Plus, $10 for the first year&lt;/h2&gt;
&lt;p&gt;As one of the names synonymous with computer security, per Bader, Norton has been offering antivirus software for just over thirty years. From its AntiVirus Plus to its 360 Deluxe version, it has a slew of digital products for you to shop.&lt;/p&gt;
&lt;p&gt;The Antivirus Plus offers protection for a single PC or Mac at $10 for the first year, and $60 a year thereafter. This service includes Anti-Spyware, 2GB of cloud backup, a smart firewall, a password manager and an online privacy monitor, too.&lt;/p&gt;
&lt;p&gt;If you’d like more protection on your smartphone and tablet, its 360 bundle is $85 per year (now on sale for just $30!), upping the cloud backup to 10GB and including a VPN for public WiFi use. For more protection including for smartphones and tablets, Norton offers its Norton 360 bundles with the standard version ($85/year) increasing the cloud backup to 10GB, and incorporating a VPN for public WiFi use.&lt;/p&gt;
&lt;h2 id=&#34;3-avg-ultimate-60-for-the-first-year&#34;&gt;3. AVG Ultimate, $60 for the first year&lt;/h2&gt;
&lt;p&gt;AVG Ultimate offers antivirus protection for PC, Mac, Android, and iOIS, and costs $60 for the first year, and $128 per year after that, for protecting up to 10 devices. In addition to antivirus protection, AVG Ultimate also has an array of nifty features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Capabilities to manage battery profiles:&lt;/strong&gt; This enables you to maximize your computer, smartphone, or tablet’s performance while minimizing battery use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Device lock:&lt;/strong&gt; This stops unwanted visitors from accessing your phone.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Startup optimizer:&lt;/strong&gt; This stops software you don’t use or care about from slowing down your device’s time to boot up.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Payment protection:&lt;/strong&gt; This encrypts and keeps safe all of your online payments from the prying eyes of hackers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Smart photo cleaner:&lt;/strong&gt; This finds duplicate and poor-quality photos on your device and deletes them to free up space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Browser and disk cleaner:&lt;/strong&gt; This finds and erases junk files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sensitive data shield:&lt;/strong&gt; This secures your most sensitive files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPS tracking:&lt;/strong&gt; This helps to find a lost device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“AVG Ultimate includes this whole bundle of security-related tools and services without having to buy additional packages,” Bader adds.&lt;/p&gt;
&lt;h2 id=&#34;4-bitdefender-antivirus-plus-30-for-the-first-year&#34;&gt;4. Bitdefender Antivirus Plus, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Bitdefender Antivirus Plus, impressively, protects up to three Windows devices from viruses including malware, spyware and adware. It’s $30 for the first year and $60 per year thereafter.&lt;/p&gt;
&lt;p&gt;Bader calls this a “no-frills product,” with the next plan level, Bitdefender Internet Security is $80 per year and includes a firewall and parental controls. “The ultimate plan, Bitdefender Total Security, is $90 per year, includes protection for up to five devices and is the only plan that also protects every OS (Windows, macOS, Android, and iOS) as well as including a device optimizer that places your device in game, movie, and work modes to save the battery while maximizing performance.”&lt;/p&gt;
&lt;h2 id=&#34;5-webroot-secureanywhere-antivirus-30-for-the-first-year&#34;&gt;5. Webroot SecureAnywhere Antivirus, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Providing baseline antivirus protection, Webroot SecureAnywhere Antivirus secures your data on one Windows or Mac device at $30 for the first year and $40 each year after. “Its claim is that it runs 60x faster than the scan time of other tested competitor antivirus products and takes 20 seconds to check your computer for malware by being fully cloud-based,” Bader said.&lt;/p&gt;
&lt;p&gt;What’s more, Webroot offers a VPN for WiFi Security for up to three devices along with antivirus software at $110 a year.&lt;/p&gt;
&lt;h2 id=&#34;6-cylance-smart-antivirus-45-per-block-of-data-protection&#34;&gt;6. Cylance Smart Antivirus, $45 per block of data protection&lt;/h2&gt;
&lt;p&gt;“Cylance technology is now incorporated into BlackBerry Cybersecurity and being incorporated into BlackBerry’s unified endpoint security solution,” Bader explains.&lt;/p&gt;
&lt;p&gt;This integrated threat prevention solution combines artificial intelligence (AI) to help “block malware infections with additional security controls that safeguard against script-based, fileless, memory, and external device-based attacks,” per its site. After taking a consultation, you’ll receive a quote, which is usually $45 per block of data protection.&lt;/p&gt;
&lt;h2 id=&#34;7-emsisoft-anti-malware-30-for-the-first-year&#34;&gt;7. Emsisoft Anti-Malware, $30 for the first year&lt;/h2&gt;
&lt;p&gt;Only available for Windows, Emsisoft Anti-Malware has a 30-day free trial and then costs $30 per year for each device. “The software also includes extensive protections for finding and removing malware from your system and malware removal assistance,” Bader notes.&lt;/p&gt;
&lt;h2 id=&#34;8-eset-nod32-antivirus&#34;&gt;8. ESET NOD32 Antivirus&lt;/h2&gt;
&lt;p&gt;For just $40 a year, ESET NOD32 Antivirus secures each Windows and Mac device while including malware, ransomware, and phishing protection. At $50 per year, ESET Internet Security “also protects Android devices and includes two additional protection features for privacy and banking and for network and smart devices,” Bader explains.&lt;/p&gt;
&lt;p&gt;The ultimate protection, however, is ESET Smart Security Premium: $60 per year per device with an included password manager, sensitive data encryption and protection against new threats. “It’s disappointing, though, that a customer must buy this higher cost package to be protected against never-before-seen threats,” Bader commented.&lt;/p&gt;
&lt;h2 id=&#34;9-f-secure-anti-virus&#34;&gt;9. F-Secure Anti-Virus&lt;/h2&gt;
&lt;p&gt;As one of the oldest antivirus companies — founded in 1988 — F-Secure “has a strong international presence in Europe, North America, and Asia Pacific regions,” per Bader. It’s available for Windows only and costs $36 per year to protect one computer or $40 for up to three computers.&lt;/p&gt;
&lt;p&gt;“F-Secure has additional security product add-ons including F-Secure SAFE for internet security ($70 for up to three devices per year), F-Secure FREEDOME VPN for secure and private browsing ($55 for up to three devices per year), and F-Secure ID PROTECTION for secure passwords and online identity ($60 for up to five devices per year),” Bader said. The premium package from F-Secure, TOTAL, provides full online protection at $90 a year for up to three devices.&lt;/p&gt;
&lt;h2 id=&#34;10-g-data-antivirus&#34;&gt;10. G Data Antivirus&lt;/h2&gt;
&lt;p&gt;G DATA Software is a German cybersecurity company that “claims to have made the first antivirus software in 1987,” Bader notes. The G DATA Antivirus protection for a single Windows PC costs $30 for the first year and $40 per year thereafter, while the Mac version is a bit more expensive at $40 for the first year and $55 each year after.&lt;/p&gt;
&lt;p&gt;“G DATA also offers an Internet Security package with a firewall and parental monitoring at $55 per year per device, and a Total Security bundle for $70 per year per device that includes a password manager, encrypted backups, access control, and an integrated tuner for performance and security,” Bader adds.&lt;/p&gt;
&lt;h2 id=&#34;11-malwarebytes-premium&#34;&gt;11. Malwarebytes Premium&lt;/h2&gt;
&lt;p&gt;If you frequent the Internet often, you’ve likely heard of Malwarebytes Premium: an antivirus software for Windows, Mac, Android, iOS, and Chrome, costing $40 per year for one device or $80 per year for up to five devices. It includes both antivirus protection and a browser guard, too.&lt;/p&gt;
&lt;p&gt;“For $100 a year for up to five devices, Malwarebytes Premium + Privacy includes a VPN for safe WiFi-only protection,” Bader highlights. “A free Windows version can be used to clean up an infected computer and limited trials to for the other antivirus protections.”&lt;/p&gt;
&lt;h2 id=&#34;12-sophos-home-premium&#34;&gt;12. Sophos Home Premium&lt;/h2&gt;
&lt;p&gt;Sophos Home Premium costs $45 for all of your Windows and Mac devices and is “basic antivirus software that scans and cleans malware from your system, protects your privacy online, and has parental web filtering,” per Bader. Conveniently, there are no complicated add-ons and upgraded packages.&lt;/p&gt;
&lt;p&gt;“Sophos Home Premium keeps the package simple to buy and use, with everything included,” he adds.&lt;/p&gt;
&lt;h2 id=&#34;13-trend-micro-antivirussecurity&#34;&gt;13. Trend Micro Antivirus+Security&lt;/h2&gt;
&lt;p&gt;“Trend Micro’s Internet Security software for Windows only includes online privacy controls and fixes and optimizes systems, and costs $80 per year for up to three devices,” Bader said.&lt;/p&gt;
&lt;p&gt;Its basic package, Antivirus+Security, protects a single Windows PC from viruses and ransomware and costs $20 for the first year and $40 for each year thereafter. Uniquely, Trend Micro’s Maximum Security bundle is the only product from Trend Micro for Windows, Mac, Android, iOS, and Chromebooks, that “includes a password manager and secures mobile devices along with the antivirus protection” and costs $90 per year for up to five devices,” Bader said.&lt;/p&gt;
&lt;h2 id=&#34;an-faq-on-antivirus-software&#34;&gt;An FAQ on Antivirus Software&lt;/h2&gt;
&lt;h3 id=&#34;what-is-a-computer-virus&#34;&gt;What is a computer virus?&lt;/h3&gt;
&lt;p&gt;Nobody likes them, but it’s important to know how these pesky tech inconveniences and breaches begin.&lt;/p&gt;
&lt;p&gt;“Viruses can propagate in many ways such as through email attachments, unwitting downloads from infected websites, and through shared USB memory sticks,” Bader explains. “Antivirus software exists for all popular computing platforms (Windows, Mac, Android, iOS) and scans your system, memory, and files (including downloads) for known viruses.”&lt;/p&gt;
&lt;p&gt;When the antivirus software detects a virus, it may take several actions including cleaning the virus from the system and placing infected files in quarantine.&lt;/p&gt;
&lt;h3 id=&#34;what-benefits-come-with-some-antivirus-software-besides-protection&#34;&gt;What benefits come with some antivirus software, besides protection?&lt;/h3&gt;
&lt;p&gt;Aside from giving you (and your files) peace of mind, Bader highlights other add-ons some programs include.&lt;/p&gt;
&lt;p&gt;“Antivirus software often comes bundled with other security features such as secure password keepers, protection against clicking on malicious links to websites and scans of the dark web to find if your information has been compromised and potentially sold to hackers,” he notes.&lt;/p&gt;
&lt;h3 id=&#34;what-does-malware-mean&#34;&gt;What does malware mean?&lt;/h3&gt;
&lt;p&gt;Malware is malicious software that you may receive through an email attachment or other file transfer that can take over your computer if you run it.  “Be cautious when clicking on links in the body of emails from unknown or faked senders, or opening any file sent to you that you weren’t expecting,” Bader tips off. “These are the main ways hackers send you malware.”&lt;/p&gt;
&lt;h3 id=&#34;what-does-spyware-mean&#34;&gt;What does spyware mean?&lt;/h3&gt;
&lt;p&gt;More niche, spyware is a type of malware that watches everything you do on the computer and steals information. “For instance, spyware can monitor your keystrokes for all of your passwords, including ones for your bank accounts, social media, and email accounts,” Bader explains. “Spyware gathers information that can be used against your or your business and sends it to third parties who may sell the private information on the dark web or craft even more personalized attacks against you and your friends.”&lt;/p&gt;
&lt;h3 id=&#34;what-does-adware-mean&#34;&gt;What does adware mean?&lt;/h3&gt;
&lt;p&gt;According to Bader, adware is different from spyware and is generally unwanted software, usually connected to your web browser, that watches your activity and puts annoying ads up on your screen. Some antivirus software packages can scan and remove adware from your computer.&lt;/p&gt;
&lt;h3 id=&#34;is-antivirus-software-necessary&#34;&gt;Is antivirus software necessary?&lt;/h3&gt;
&lt;p&gt;Like your car and home insurance, investing about $20 to $50 yearly is surely worth it. Not to mention, the service pairs well with a &lt;a href=&#34;https://nypost.com/article/best-password-managers-per-experts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;password protector&lt;/a&gt;, too.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nypost.com/article/best-antivirus-software/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nypost.com/article/best-antivirus-software/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ITSPmagazine Podcast: Large-Scale Data Analytics For Cybersecurity And Solving Real-World Grand Challenges</title>
      <link>http://localhost:1313/blog/20220331-itspmagazine/</link>
      <pubDate>Thu, 31 Mar 2022 12:53:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20220331-itspmagazine/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;large-scale-data-analytics-for-cybersecurity-and-solving-real-world-grand-challenges-redefining-cybersecurity-with-david-bader.mp3&#34;&gt;Listen&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;episode-summary&#34;&gt;EPISODE SUMMARY&lt;/h2&gt;
&lt;p&gt;We may see new &amp;ldquo;graph&amp;rdquo; processors in the future that can better handle the data-centric computations in data science. Will that be enough?&lt;/p&gt;
&lt;h2 id=&#34;episode-notes&#34;&gt;EPISODE NOTES&lt;/h2&gt;
&lt;p&gt;We may see new &amp;ldquo;graph&amp;rdquo; processors in the future that can better handle the data-centric computations in data science. Will that be enough?&lt;/p&gt;
&lt;h2 id=&#34;about-david&#34;&gt;About David&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor in the Department of Computer Science and founder of the Department of Data Science and inaugural Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;guest&#34;&gt;Guest&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Distinguished Professor and Director, Institute for Data Science, New Jersey Institute of Technology [&lt;a href=&#34;https://twitter.com/NJIT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@NJIT&lt;/a&gt;]&lt;br&gt;
On Twitter | &lt;a href=&#34;https://twitter.com/Prof_DavidBader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://twitter.com/Prof_DavidBader&lt;/a&gt;&lt;br&gt;
On LinkedIn | &lt;a href=&#34;https://www.linkedin.com/in/dbader13/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linkedin.com/in/dbader13/&lt;/a&gt;&lt;br&gt;
On Facebook | &lt;a href=&#34;https://www.facebook.com/ProfDavidBader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.facebook.com/ProfDavidBader&lt;/a&gt;&lt;br&gt;
Website: &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://davidbader.net/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;this-episodes-sponsors&#34;&gt;This Episode’s Sponsors&lt;/h2&gt;
&lt;p&gt;Imperva: &lt;a href=&#34;https://itspm.ag/imperva277117988&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://itspm.ag/imperva277117988&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;HITRUST: &lt;a href=&#34;https://itspm.ag/itsphitweb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://itspm.ag/itsphitweb&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/Bader-Research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Bader-Research&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Arkouda: &lt;a href=&#34;https://github.com/Bears-R-Us/arkouda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Bears-R-Us/arkouda&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NJIT Institute for Data Science: &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://datascience.njit.edu/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://redefining-cybersecurity.simplecast.com/episodes/large-scale-data-analytics-for-cybersecurity-and-solving-real-world-grand-challenges-redefining-cybersecurity-with-professor-david-bader-0NvJ7Ai4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://redefining-cybersecurity.simplecast.com/episodes/large-scale-data-analytics-for-cybersecurity-and-solving-real-world-grand-challenges-redefining-cybersecurity-with-professor-david-bader-0NvJ7Ai4&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The best password protectors of 2022, per a cybersecurity expert</title>
      <link>http://localhost:1313/blog/20220310-newyorkpost/</link>
      <pubDate>Thu, 10 Mar 2022 15:59:22 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220310-newyorkpost/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Victoria Giardina&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-new-york-post-composite&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New York Post Composite&#34; srcset=&#34;
               /blog/20220310-newyorkpost/Best-Password-Protectors_hu_d0752a1b879f74c6.webp 400w,
               /blog/20220310-newyorkpost/Best-Password-Protectors_hu_ccbc3afb233b60ce.webp 760w,
               /blog/20220310-newyorkpost/Best-Password-Protectors_hu_fa9e21d9ae64e569.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220310-newyorkpost/Best-Password-Protectors_hu_d0752a1b879f74c6.webp&#34;
               width=&#34;744&#34;
               height=&#34;495&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New York Post Composite
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We’ve all been there. The stress-inducing race to insert every password combination we’ve conjured in the recent months, only to have to click “forgot my password.”&lt;/p&gt;
&lt;p&gt;This common run-in happens with amateur tech users and savvy tech masters alike. But, by investing in password protector service, you won’t have to worry about remembering a single thing.&lt;/p&gt;
&lt;p&gt;Sure, you may ask, &lt;em&gt;Why can’t I just write it down in my notes app?&lt;/em&gt; It’s a fair question, but without a professional software platform to manage and organize not only passwords but your documents (like your resume) and sensitive codes, too, you may run into the risk of being hacked or worse — having your data wiped.&lt;/p&gt;
&lt;p&gt;“These services often have secure, generated passwords they’ll create for you and secure in your vault,” &lt;strong&gt;David Bader, PhD&lt;/strong&gt;, distinguished professor of data science at New Jersey Institute of Technology (NJIT) told the New York Post. “So, you won’t have to worry about staring at your keyboard wondering which letters you should capitalize or if you used enough exclamation points.”&lt;/p&gt;
&lt;p&gt;It’s also important to note that nowadays, the most secure passwords are “long, randomly generated and use a combination of lowercase and uppercase letters, numbers, and special characters that you have to click ‘shift’ and type to get,” Bader explains.&lt;/p&gt;
&lt;p&gt;Though passwords require a benchmark to pass, it’s important to &lt;strong&gt;use different passwords for different sites&lt;/strong&gt;. “It’s like having a different key for every lock,” Bader said. “It’s hard to remember all of your passwords and especially now that we need much stronger passwords — those that don’t reflect our names, children and cats and dogs — but seemingly random strings of numbers, letters and special characters; a password protector is a good service to use.”&lt;/p&gt;
&lt;p&gt;Ahead, find the seven best password protectors of 2022 to download and benefit from for personal use.&lt;/p&gt;
&lt;h2 id=&#34;1-keeper-password-manager-35-a-year&#34;&gt;1. Keeper Password Manager, $35 a year&lt;/h2&gt;
&lt;p&gt;This password keeper uses the key-strengthening PBKDF2 algorithm to help resist brute force attacks so that the user’s main password never leaves their device, along with a strong 256-bit AES encryption to protect the user’s passwords and data. “Keeper will also monitor the dark web looking for data breaches,” Bader adds.&lt;/p&gt;
&lt;p&gt;Keeper is portable across all of the common operating systems as well (Windows, macOS, Linux, Android, iOS, and browsers), making it simple to use. Not to mention, it’s just $75 for a family of five. “Overall, Keeper consistently gets high marks and is used by millions of people and thousands of businesses,” he highlights.&lt;/p&gt;
&lt;h2 id=&#34;2-1password-password-manager-36-a-year&#34;&gt;2. 1Password Password Manager, $36 a year&lt;/h2&gt;
&lt;p&gt;“1Password is used by over 100,000 businesses including IBM, Slack, PGA Tour, PagerDuty, GitLab, Under Armour, Intercom and Shopify,” Bader shares. “Passwords are kept in a secure vault using industry-standard 256-bit AES encryption and PBKDF2 key strengthening [the same as Keeper to resist hacks].”&lt;/p&gt;
&lt;p&gt;Plus, the user’s main password never leaves their device; PBKDF2 generates a new, strong 256-bit AES password to transmit each time to the vault. 1Password will also generate strong passwords for the user.&lt;/p&gt;
&lt;p&gt;With more than 15 million users, Bader approves of this password protector that’s apt for business enterprises just as much as it is for individual use. For a family of five, it’s only $60 for the year.&lt;/p&gt;
&lt;h2 id=&#34;3-passware-recovery-tool-49&#34;&gt;3. Passware Recovery Tool, $49&lt;/h2&gt;
&lt;p&gt;“Passware, which has been around since 1998, is the leading software to assist users who forgot their password and forensic teams with password recovery – that is, opening encrypted files, with a claimed 70% success rate,” Bader explains.&lt;/p&gt;
&lt;p&gt;Plus, the service is just a one-time payment of $49. Passware supports over 300 file types, and some of the main users include FBI, US Secret Service, Department of Justice, Department of Defense, Department of Homeland Security, NASA, Deloitte, Europol, Israel’s Lahav 433, and the UK Metropolitan Police.&lt;/p&gt;
&lt;h2 id=&#34;4-nordpass-password-manager-18-a-year&#34;&gt;4. NordPass Password Manager, $18 a year&lt;/h2&gt;
&lt;p&gt;NordPass is a relatively new service launched in 2019, that allows users to store their passwords and secure notes in a secure password vault. It runs across all the major operating systems (Windows, macOS, Linux, Android, iOS). Like 1Password, the user has a single password that opens their vault.&lt;/p&gt;
&lt;p&gt;“NordPass uses an encryption algorithm called XChaCha, a stream cipher, which is similar to Google’s TLS encryption algorithm,” Bader notes. “NordPass will scan the web to see if your password has been leaked in data breaches and also evaluate the strengths of your passwords.”&lt;/p&gt;
&lt;p&gt;Interestingly, NordPass is based in Panama, which does not have mandatory data retention laws and doesn’t participate in data sharing with any governments. So, if that’s something you’re extra keen on avoiding, this one may be for you — especially for just $60 for a family of six.&lt;/p&gt;
&lt;h2 id=&#34;5-dashlane-password-manager-60-a-year&#34;&gt;5. Dashlane Password Manager, $60 a year&lt;/h2&gt;
&lt;p&gt;“Dashlane software has been available for a decade, and runs either as a browser extension on Chrome, Edge, Firefox, and Safari, or on a mobile device such as iOS and Android,” Bader mentions. “Similar with 1Password and Keeper, Dashlane uses PBKDF2 on your device and industry-standard 256-bit AES encryption to protect your passwords in the secure vault.”&lt;/p&gt;
&lt;p&gt;In 2018, Dashlane reached 10 million users and 10,000 businesses around the world. Dashlane also has a free version that can store up to 50 passwords from one device.&lt;/p&gt;
&lt;p&gt;“For users who purchase a license, Dashlane has also partnered with HotSpot Shield to provide a VPN, which adds a layer of protection for your browsing and data when you’re connecting to the Internet on public WiFi,” Bader adds. Plus, it’s $90 a year for a family of six.&lt;/p&gt;
&lt;h2 id=&#34;6-lastpass-password-manager-36-a-year&#34;&gt;6. LastPass Password Manager, $36 a year&lt;/h2&gt;
&lt;p&gt;LastPass, available since 2008, is a password vault compatible with all the common browsers, operating systems (Windows, macOS and Linux), and mobile (iOS and Android). LastPass also uses the PBKDF2 algorithm and 256-bit AES encryption to protect your main password and everything stored in the secure vault.&lt;/p&gt;
&lt;p&gt;“Today, 30 million users and 85,000 businesses use LastPass,” Bader notes. “LastPass shares your information with Enzoic to perform dark web scans.” Plus, it’s $48 a year for a family of six, one of the best values we’ve seen.&lt;/p&gt;
&lt;h2 id=&#34;7-keepass-password-safe-free&#34;&gt;7. KeePass Password Safe, FREE&lt;/h2&gt;
&lt;p&gt;nlike most of the other password keepers, KeePass keeps all your information on your device rather than a secure vault in the cloud. However, there’s a distinct set of pros and cons, according to Bader:&lt;/p&gt;
&lt;h3 id=&#34;pros&#34;&gt;Pros:&lt;/h3&gt;
&lt;p&gt;This is the most secure way to keep your information safe, it is free and open source. Security professionals who have reviewed KeePass’s source code vouch for this security software. KeePass uses the best encryption algorithms including AES-256, ChaCha20, and TwoFish.&lt;/p&gt;
&lt;h3 id=&#34;cons&#34;&gt;Cons:&lt;/h3&gt;
&lt;p&gt;You will have to be a savvy user to manage your password database file. You can store the database on your current device, but then you won’t have access from other devices and also risk losing it if anything happens to your device.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nypost.com/article/best-password-managers-per-experts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nypost.com/article/best-password-managers-per-experts/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elon Musk fixade internet till Ukraina – på några timmar</title>
      <link>http://localhost:1313/blog/20220227-expressen/</link>
      <pubDate>Sun, 27 Feb 2022 18:44:49 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220227-expressen/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Adrianna Pavlica&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;NEW YORK. Ukrainas vice premiärminister bad den amerikanska entreprenören och miljardären Elon Musk om hjälp – några timmar senare var satellitnätverket Starlink tillgängligt för landets invånare.&lt;/p&gt;
&lt;p&gt;Tack vare Starlink kommer ukrainare att kunna använda internet, även om kablar med ”vanligt” internet slås ut.&lt;/p&gt;
&lt;p&gt;– Det här är ett exempel på kommunikation som ger människor möjlighet att förbli fria. Kommunikation är demokratins signum och Starlink är på många sätt avgörande för det ukrainska folket, säger &lt;strong&gt;David Bader&lt;/strong&gt;, professor vid New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;Det var på lördagen som Ukrainas vice premiärminister och minister för digital utveckling, Mykhailo Fedorov, riktade sig mot amerikanske entreprenören Elon Musk på Twitter, och uppmanade honom att göra satellitnätverket Starlink tillgängligt för Ukrainas invånare. Några timmar senare svarade Elon Musk att Starlink är aktiverat.&lt;/p&gt;
&lt;h2 id=&#34;vanligt-internet-sårbart&#34;&gt;Vanligt internet sårbart&lt;/h2&gt;
&lt;p&gt;Vad är då Starlink och vilken betydelse kan det ha i det pågående kriget? David Bader, professor och chef för avdelningen för datavetenskap vid New Jersey Institute of Technology, reder ut.&lt;/p&gt;
&lt;p&gt;– Normalt får man internet via fiberoptik, som är sårbar eftersom den går genom exempelvis kablar som kan slås ut, av misstag eller avsiktligt. Starlink däremot har inga kablar, och fungerar så länge satelliterna befinner sig i rymden, säger han.&lt;/p&gt;
&lt;h2 id=&#34;robust-system&#34;&gt;Robust system&lt;/h2&gt;
&lt;p&gt;För att man ska kunna koppla upp sig på Starlink, som på ett slags wifi, krävs dock viss hårdvara. Det är i nuläget oklart hur distributionen av denna hårdvara kommer att se ut.&lt;/p&gt;
&lt;p&gt;Elon Musk, som de senaste åren främst börjat att förknippats med SpaceX och sitt rymdfokus, har enligt David Bader omkring 2 000 Starlink-satelliter uppe. Och systemet, som nyligen användes för att få i gång internet i Tonga efter tsunamin, är robust.&lt;/p&gt;
&lt;p&gt;– Det är teoretiskt möjligt för Ryssland att skjuta ner en satellit, men det kräver en komplex åtgärd. Och det här satellitnätverket är just ett nätverk - för att slå ut det måste man i så fall skjuta ner tusentals satelliter, och det är i princip omöjligt, säger David Bader.&lt;/p&gt;
&lt;p&gt;Han betonar att kommunikationsmöjligheter påverkar säkerhet, energi, mat och andra aspekter som nu är hotade i Ukraina.  Och det har redan kommit rapporter om att internet slagits ut i delar av Ukraina.&lt;/p&gt;
&lt;h2 id=&#34;skulle-inte-kunna-berätta&#34;&gt;Skulle inte kunna berätta&lt;/h2&gt;
&lt;p&gt;Joseph Steinberg, amerikansk expert på cybersäkerhet och framväxande teknologier, ser två viktiga funktioner som Starlink nu fyller.&lt;/p&gt;
&lt;p&gt;– Att ha tillgång till internet är en nyckel. Om Ryssland hade möjlighet att bryta internetuppkopplingen i Ukraina skulle världen inte få veta vad som händer där. Varken regeringen, journalister eller invånare skulle kunna berätta. Att Starlink kommer upp gör det betydligt svårare för Ryssland att tysta den ukrainska regeringen till exempel, säger han.&lt;/p&gt;
&lt;p&gt;Det finns också en psykologisk aspekt som Joseph Steinberg gärna betonar.&lt;/p&gt;
&lt;p&gt;– Det här sänder ett meddelande till den ryska militären: att västvärldens länder och företag gör allt de kan för att stötta Ukraina. Det här kommer inte att vara som under annekteringen av Krim under Obama-administrationen, det här är en helt annan typ av reaktion från om världen. Det här kriget har triggat en emotionell aspekt som skiljer sig från 2014. Den här känslan av stöd påverkar också det ukrainska folket.&lt;/p&gt;
&lt;h2 id=&#34;möjlighet-att-förbi-fria&#34;&gt;”Möjlighet att förbi fria”&lt;/h2&gt;
&lt;p&gt;Han får medhåll från David Bader.&lt;/p&gt;
&lt;p&gt;– Det här är ett exempel på kommunikation som ger människor möjlighet att förbli fria. Kommunikation är demokratins signum och Starlink är på många sätt avgörande för det ukrainska folket, säger han.&lt;/p&gt;
&lt;p&gt;David Bader berättar att cyberattacker i dag är en del av krigföringen.&lt;/p&gt;
&lt;p&gt;– Så är det absolut. Vi har redan sett rapporter om cyberattacker i Ukraine. Det handlar till exempel om attacker som raderar datorers minne. De här attackerna har ökat de senaste dagarna, säger han.&lt;/p&gt;
&lt;p&gt;Enligt David Bader är alla länder sårbara när det kommer till cyberattacker.&lt;/p&gt;
&lt;p&gt;– Det handlar ofta om sofistikerade och komplexa attacker. Många länder är under ständig attack från både stater och personer, och det sker en konstant eskalering av läget.&lt;/p&gt;
&lt;p&gt;Fotnot: I en tidigare version av artikeln liknades Starlink vid ett vanligt wifi. Vi har nu förtydligat att det krävs viss hårdvara för att Starlink ska fungera.&lt;/p&gt;
&lt;p&gt;ViIl du få allt det senaste om Rysslands invasion i Ukraina? Ladda ner Expressens app för Iphone eller Android, och aktivera pushnotiser i utrikeskategorin.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.expressen.se/nyheter/elon-musk-fixade-internet-till-ukraina-pa-nagra-timmar/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.expressen.se/nyheter/elon-musk-fixade-internet-till-ukraina-pa-nagra-timmar/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT reaffirmed as an elite research university, retains R1 classification</title>
      <link>http://localhost:1313/blog/20220203-scienmag/</link>
      <pubDate>Thu, 03 Feb 2022 16:10:24 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220203-scienmag/</guid>
      <description>&lt;p&gt;New Jersey Institute of Technology (NJIT) has reaffirmed its status among the nation’s most elite and productive research institutions by once again achieving an R1 status — the highest designation — by the Carnegie Classification.&lt;/p&gt;
&lt;p&gt;New Jersey Institute of Technology (NJIT) has reaffirmed its status among the nation’s most elite and productive research institutions by once again achieving an R1 status — the highest designation — by the Carnegie Classification.&lt;/p&gt;
&lt;p&gt;First published in 1973, the Carnegie Classification of Institutions of Higher Education is the primary measure used by rating organizations and governmental agencies to describe colleges and universities. Institutions that grant doctoral degrees are divided into three tiers that represent their level of research activity in terms of research and development expenditures and doctorates awarded, with the coveted R1 indicating “very high research activity.”&lt;/p&gt;
&lt;p&gt;NJIT’s continuing status as an R1 institution demonstrates the university’s sustained growth of research and innovation. NJIT’s ability to secure major research grants will continue to foster transformative discoveries in areas ranging from healthcare and medicine to sustainable technologies and data analytics. NJIT is one of only 146 universities nationwide to earn the R1 ranking and one of just three R1 universities in New Jersey.&lt;/p&gt;
&lt;p&gt;“NJIT is very proud to have again earned this designation, affirming the strength and capacity of our research programs,” commented NJIT Provost and Senior Executive Vice President Fadi P. Deek. “NJIT’s commitment to high-impact applied research is a cornerstone of our strategic plan. Our efforts in this area have yielded major research grants for our faculty and success for our doctoral programs.”&lt;/p&gt;
&lt;p&gt;NJIT’s rise as a research university has been swift and profound. In 1979, the university’s research expenditures totaled $375,000; in 2021, it surpassed $165 million. Since 2010, total R&amp;amp;D expenditures have increased by 70%.&lt;/p&gt;
&lt;p&gt;NJIT’s strategic plan, Building on a Strong Foundation — NJIT 2025, names research as one of the university’s five critical priorities and positions it as a driver of other strategic priorities. NJIT’s key objectives in research include promoting collaborative research, fostering innovation and entrepreneurship, and promoting research partnerships with the goal of becoming nationally and internationally recognized for high-impact research.&lt;/p&gt;
&lt;p&gt;The university supports research in numerous ways, and takes seriously its role in nurturing talent on campus. Since 2010, NJIT has hired 156 new faculty members across STEM and other disciplines, and has increased its tenured and tenure-track faculty to over 320, 50% of whom were hired in the last ten years. These faculty have been instrumental in driving NJIT’s academic excellence by teaching in new modalities and implementing innovative teaching techniques. They have also developed exciting new courses and programs including Financial Technology, &lt;strong&gt;Data Science&lt;/strong&gt;, and Cyberpsychology.&lt;/p&gt;
&lt;p&gt;Since 2017, 14 young researchers have won CAREER awards from the National Science Foundation, described by the agency as among its most prestigious awards. The work of these faculty ranges from solving problems that rely on large-scale mathematical optimization, to the development of novel soft solid materials such as smart gels used as sealants and valve controls, to the creation of new methods to design lenses and mirrors to precisely control the intensity pattern and phase of light beams in applications such as optical data storage and astronomy.&lt;/p&gt;
&lt;p&gt;Supporting the prodigious growth in research, NJIT is now home to 140 labs, centers and research institutes with the strategic plan guiding the university to play a leading role in five emerging areas of multidisciplinary research – Bioscience and Bioengineering, Data Science and Management, the Environment and Sustainability, Material Science and Engineering, and Robotics and Machine Intelligence. NJIT also offers students the opportunity to conduct research even as undergraduates, and many spend significant time working closely with faculty in these hubs.&lt;/p&gt;
&lt;p&gt;The transformation of research facilities has been part of a larger, ongoing $500 million capital-building program transforming the entire campus. The gut-level renovation of the five-story Central King Building and construction of the 24,500 sq. ft. Life Sciences and Engineering Building are bringing students and faculty new teaching and research labs, rooms to conduct projects, and common areas where they can socialize and share ideas. A $3.7 million renovation of the Microfabrication Innovation Center now houses advanced equipment and a cleanroom environment for the fabrication of micro- and nanoelectronic and microfluidic devices and sensors. The Makerspace at NJIT, a 21,000-square-foot training-focused, rapid prototyping facility, provides the university’s education and research community with the latest design and fabrication equipment.&lt;/p&gt;
&lt;p&gt;NJIT President Joel S. Bloom noted, “Maintaining the R1 designation is a major achievement for NJIT and is the direct result of our faculty, researchers and staff, as well as the investments we have made to support their research.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://scienmag.com/njit-reaffirmed-as-an-elite-research-university-retains-r1-classification/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://scienmag.com/njit-reaffirmed-as-an-elite-research-university-retains-r1-classification/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumnus David Bader Named Association for Computing Machinery Fellow</title>
      <link>http://localhost:1313/blog/20220124-maryland/</link>
      <pubDate>Mon, 24 Jan 2022 20:15:09 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220124-maryland/</guid>
      <description>&lt;p&gt;The Association for Computing Machinery (ACM) named Department of Electrical and Computer Engineering (ECE) alumnus &lt;strong&gt;David Bader (Ph.D. ’96)&lt;/strong&gt; as one of 71 internationally selected members named 2021 ACM Fellows for their wide-ranging and fundamental contributions that run the spectrum of the computing field.&lt;/p&gt;
&lt;p&gt;ACM selected Bader for his contributions to high-performance computing systems, graph analytics and technical leadership in parallel computing.&lt;/p&gt;
&lt;p&gt;Bader is a Distinguished Professor and a founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;“Computing professionals have brought about leapfrog advances in how we live, work, and play,” said ACM President Gabriele Kotsis in the &lt;a href=&#34;https://www.acm.org/media-center/2022/january/fellows-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;organization’s press release&lt;/a&gt;. “New technologies are the result of skillfully combining the individual contributions of numerous men and women, often building upon diverse contributions that have emerged over decades. But technological progress would not be possible without the essential building blocks of individual contributors. The ACM Fellows program honors the creativity and hard work of ACM members whose specific accomplishments make broader advances possible.”&lt;/p&gt;
&lt;p&gt;In addition to being a fellow of the Institute of Electrical and Electronics Engineers (IEEE), the American Association for the Advancement of Science (AAAS) and the Society for Industrial and Applied Mathematics (SIAM), Bader has received numerous awards for his work. Most recently, these include the &lt;a href=&#34;https://ece.umd.edu/news/story/alumnus-david-bader-receives-2021-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021 Sidney Fernbach Award&lt;/a&gt;, recognition in the 2021 ROI-NJ inaugural list of technology influencers, an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;Bader also currently advises the White House on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).&lt;/p&gt;
&lt;p&gt;During his time at Maryland, Bader was advised by ECE Interim Chair and Professor Joseph JaJa, and founded and served as president of the Electrical and Computer Engineering Graduate Student Association (ECEGSA). In 2012, he was also selected as an inaugural recipient of ECE’s Distinguished Alumni Award.&lt;/p&gt;
&lt;p&gt;The ACM Fellows program recognizes the top 1% of ACM Members for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community. Fellows are nominated by their peers, with nominations reviewed by a distinguished selection committee.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/alumnus-david-bader-named-association-for-computing-machinery-fellow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/alumnus-david-bader-named-association-for-computing-machinery-fellow&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader ’90 ’91G elected as Fellow of Association for Computing Machinery</title>
      <link>http://localhost:1313/blog/20220121-lehigh/</link>
      <pubDate>Fri, 21 Jan 2022 15:56:38 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220121-lehigh/</guid>
      <description>

















&lt;figure  id=&#34;figure-njit-distinguished-professor-and-lehigh-ece-alum-david-bader-has-been-named-a-fellow-of-acm-the-acm-fellows-program-recognizes-the-top-1-of-acm-members-for-their-outstanding-accomplishments-in-computing-and-information-technology-andor-outstanding-service-to-acm-and-the-larger-computing-community&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;NJIT Distinguished Professor and Lehigh ECE alum David Bader has been named a Fellow of ACM. The ACM Fellows program recognizes the top 1% of ACM Members for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.&#34; srcset=&#34;
               /blog/20220121-lehigh/Bader-2018-web_1_hu_620650812c1c8625.webp 400w,
               /blog/20220121-lehigh/Bader-2018-web_1_hu_41d9619a9d099b40.webp 760w,
               /blog/20220121-lehigh/Bader-2018-web_1_hu_e0eabeebf2737814.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20220121-lehigh/Bader-2018-web_1_hu_620650812c1c8625.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      NJIT Distinguished Professor and Lehigh ECE alum David Bader has been named a Fellow of ACM. The ACM Fellows program recognizes the top 1% of ACM Members for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Electrical and computer engineering alum &lt;strong&gt;David Bader ’90 ’91G&lt;/strong&gt; has been named a Fellow of ACM, the Association for Computing Machinery.&lt;/p&gt;
&lt;p&gt;A Distinguished Professor at the New Jersey Institute of Technology, Bader is one of 71 top innovators recognized in the 2021 class of ACM Fellows. He is also founder of NJIT&amp;rsquo;s Department of Data Science and inaugural director of its Institute for Data Science.&lt;/p&gt;
&lt;p&gt;He earned his BS in electrical engineering from Lehigh in 1990 and his MS in computer engineering in 1991.&lt;/p&gt;
&lt;p&gt;Bader was recognized &amp;ldquo;for contributions to high-performance computing systems, graph analytics, and technical leadership in parallel computing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I am grateful for the computing community giving me the opportunity to serve our field throughout the years, including as Editor-in-Chief of the ACM Transactions on Parallel Computing,&amp;rdquo; says Bader, in a story published by NJIT.&lt;/p&gt;
&lt;p&gt;Read more in the full article, &amp;ldquo;&lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader Named ACM Fellow for Career in Data Science and Supercomputing&lt;/a&gt;,&amp;rdquo; on the NJIT website.&lt;/p&gt;
&lt;h3 id=&#34;related-links&#34;&gt;Related Links:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biography: David A. Bader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT: David Bader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;media-coverage&#34;&gt;Media Coverage:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; NJIT: &amp;ldquo;David Bader Named ACM Fellow for Career in Data Science and Supercomputing&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.acm.org/media-center/2022/january/fellows-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Association for Computing Machinery: &amp;ldquo;ACM Names 71 Fellows for Computing Advances that are Driving Innovation&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;departmentprogram&#34;&gt;Department/Program:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.lehigh.edu/node/267&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;College of Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.lehigh.edu/ece&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Electrical &amp;amp; Computer Engineering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://engineering.lehigh.edu/news/article/bader-90-91g-elected-fellow-association-computing-machinery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://engineering.lehigh.edu/news/article/bader-90-91g-elected-fellow-association-computing-machinery&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Named ACM Fellow for Career in Data Science and Supercomputing</title>
      <link>http://localhost:1313/blog/20220120-njit/</link>
      <pubDate>Thu, 20 Jan 2022 22:35:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220120-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The ACM this week announced NJIT &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; in its new class of Fellows.&lt;/p&gt;
&lt;p&gt;Bader &lt;a href=&#34;https://www.acm.org/media-center/2022/january/fellows-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;received the honor&lt;/a&gt; along with 70 peers in the computing industry, representing just 1 percent of Association for Computing Machinery membership.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The contributions of the 2021 Fellows run the gamut of the computing field — including cloud database systems, deep learning acceleration, high performance computing, robotics, and theoretical computer science,&amp;rdquo; ACM President Gabriele Kotsis stated.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I reflect on the citation &amp;lsquo;for contributions to high-performance computing systems, graph analytics, and technical leadership in parallel computing&amp;rsquo;,” Bader said. &amp;ldquo;I am grateful for the computing community giving me the opportunity to serve our field throughout the years, including as Editor-in-Chief of the ACM Transactions on Parallel Computing,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;Bader developed &lt;a href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=9546947&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a Linux supercomputer&lt;/a&gt; and connected it to the National Science Foundation&amp;rsquo;s National Technology Grid. That accomplishment was cited by the IEEE Computer Society last fall in awarding him the Sidney Fernbach award, &amp;ldquo;for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&amp;rdquo;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I am grateful for the computing community giving me the opportunity to serve our field throughout the years&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Bader is also known as a leader in scalable graph analytics. Today, virtually all supercomputers run Linux, while scalable graph analytics has become a mainstream application in fields such as cybersecurity and healthcare.&lt;/p&gt;
&lt;p&gt;He&amp;rsquo;s currently working on making supercomputer power &lt;a href=&#34;https://news.njit.edu/institute-data-science-aims-democratize-supercomputing-nsf-grant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;available to everyone&lt;/a&gt; and is developing new standards for &lt;a href=&#34;https://news.njit.edu/streaming-data-analytics-has-new-method-planned-njit-usc-harvard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;streaming data analytics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bader leads NJIT&amp;rsquo;s &lt;a href=&#34;https://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt; and teaches in Ying Wu College of Computing.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Dr. Bader is an internationally known expert and leader in the area of computer architecture and optimization. This is a very well-deserved honor and great recognition,&amp;rdquo; said NJIT&amp;rsquo;s Atam Dhawan, senior vice provost for research. &amp;ldquo;We are very proud of his continued accomplishments and many significant contributions to the scientific community and NJIT.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/david-bader-earns-acm-fellow-status-career-data-science-and-supercomuting&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Named ACM Fellow for Career in Data Science and Supercomputing</title>
      <link>http://localhost:1313/blog/20220120-spotnj/</link>
      <pubDate>Thu, 20 Jan 2022 20:20:38 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220120-spotnj/</guid>
      <description>&lt;p&gt;The ACM this week announced NJIT Distinguished Professor &lt;strong&gt;David Bader&lt;/strong&gt; in its new class of Fellows. Bader received the honor along with 70 peers in the computing industry, representing just 1 percent of Association for Computing Machinery membership.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spotonnewjersey.com/nj-colleges/902287/david-bader-named-acm-fellow-for-career.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spotonnewjersey.com/nj-colleges/902287/david-bader-named-acm-fellow-for-career.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ACM Names 71 Fellows for Computing Advances that are Driving Innovation</title>
      <link>http://localhost:1313/blog/20220119-acm/</link>
      <pubDate>Wed, 19 Jan 2022 22:30:16 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220119-acm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;New York, NY, January 19, 2022&lt;/strong&gt;—ACM, the Association for Computing Machinery, has named 71 members ACM Fellows for wide-ranging and fundamental contributions in areas including algorithms, computer science education, cryptography, data security and privacy, medical informatics, and mobile and networked systems ─ among many other areas. The accomplishments of the &lt;a href=&#34;https://awards.acm.org/award_winners?year=2020&amp;amp;award=158&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021 ACM Fellows&lt;/a&gt; underpin important innovations that shape the technologies we use every day.&lt;/p&gt;
&lt;p&gt;The ACM Fellows program recognizes the top 1% of ACM Members for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community. Fellows are nominated by their peers, with nominations reviewed by a distinguished selection committee.&lt;/p&gt;
&lt;p&gt;“Computing professionals have brought about leapfrog advances in how we live, work, and play,” said ACM President Gabriele Kotsis. “New technologies are the result of skillfully combining the individual contributions of numerous men and women, often building upon diverse contributions that have emerged over decades. But technological progress would not be possible without the essential building blocks of individual contributors. The ACM Fellows program honors the creativity and hard work of ACM members whose specific accomplishments make broader advances possible. In announcing a new class of Fellows each year, we celebrate the impact ACM Fellows make, as well as the many technical areas of computing in which they work.”&lt;/p&gt;
&lt;p&gt;In keeping with ACM’s global reach, the 2021 Fellows represent universities, corporations, and research centers in Belgium, China, France, Germany, India, Israel, Italy, and the United States.&lt;/p&gt;
&lt;p&gt;The contributions of the 2021 Fellows run the gamut of the computing field―including cloud database systems, deep learning acceleration, high performance computing, robotics, and theoretical computer science ─ to name a few.&lt;/p&gt;
&lt;p&gt;Additional information about the 2021 ACM Fellows, as well as previously named ACM Fellows, is available through the &lt;a href=&#34;https://awards.acm.org/fellows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACM Fellows website&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;2021-acm-fellows&#34;&gt;2021 ACM Fellows&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;br&gt;
&lt;em&gt;New Jersey Institute of Technology&lt;/em&gt;&lt;br&gt;
For contributions to high-performance computing systems, graph analytics, and technical leadership in parallel computing&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.acm.org/media-center/2022/january/fellows-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.acm.org/media-center/2022/january/fellows-2021&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A public-private conference takes up open source software security at the White House. MuddyWater attributed to Iran. Espionage and ransomware arrests.</title>
      <link>http://localhost:1313/blog/20220113-cyberwire/</link>
      <pubDate>Thu, 13 Jan 2022 18:47:09 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220113-cyberwire/</guid>
      <description>&lt;p&gt;A White House government-industry summit today addresses open-source software security. The US officially makes its second attribution of the week to a nation-state: it calls out Iran as the operator of the MuddyWater threat group. Israel arrests five on charges related to spying for Iran (they’re thought to have been recruited through catphishing). Citizen Lab finds Pegasus in Salvadoran phones. Ukraine arrests a ransomware gang. Thomas Etheridge from CrowdStrike on the importance of threat hunting for zero days. Our guest is &lt;strong&gt;Dr. David Bader&lt;/strong&gt; of New Jersey Institute of Technology discussing the challenges of securing massive-scale analytics. And ransomware hits US state and local governments.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;CYBW5976999746.mp3&#34;&gt;Listen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thecyberwire.com/podcasts/daily-podcast/1494/notes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thecyberwire.com/podcasts/daily-podcast/1494/notes&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IT Visionaries: Academics and Data Science Innovation with Dr. David Bader, Distinguished Professor and Director, Institute for Data Science, New Jersey Institute of Technology</title>
      <link>http://localhost:1313/blog/20220106-itvisionaries/</link>
      <pubDate>Thu, 06 Jan 2022 11:37:36 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220106-itvisionaries/</guid>
      <description>&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/jGk8re-Y9sA?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Podcast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://bit.ly/3EDlR7A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mission.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://podcasts.apple.com/us/podcast/academics-and-data-science-innovation-with-dr-david/id1431648914?i=1000547078776&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS82VVNBcTdZaQ/episode/MjliZWM4Y2MtN2U1Ni00MDU0LWIwYWQtMzYzNjgzZTlhNjQ1?sa=X&amp;amp;ved=0CAUQkfYCahcKEwjY1__12Jz1AhUAAAAAHQAAAAAQAQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Podcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/4Dw0KJX3AJvBstn66cWjNy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spotify&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The data science field is expanding because so many businesses and other institutions require skilled workers who can manage data as well as provide insights. Companies and students are clamoring for more academic programs. There is great need, but academic institutions are still transitioning to meet the demand. &lt;strong&gt;Dr. &lt;a href=&#34;https://www.linkedin.com/in/dbader13/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;&lt;/strong&gt;, Distinguished Professor and Director of the Institute for Data Science at the &lt;a href=&#34;https://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Jersey Institute of Technology&lt;/a&gt;, explains how his school is leading the charge to create opportunities for more students to study data science.&lt;/p&gt;
&lt;h2 id=&#34;main-takeaways&#34;&gt;Main Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Connecting Data and Graphs:&lt;/strong&gt; A current area of focus concerning data science for Bader and his institution is taking huge amounts of data and associating it with corresponding graphs. This process helps provide context to the data so insights can be gleaned.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Arriving at Insights With Less Data:&lt;/strong&gt; One difficulty with gaining insights is that typically it takes a lot of data points to make sense of a significant occurrence. Often, in the real world, insights need to be determined faster and with less data. Consider the archetype of the malevolent lone wolf who may have not had done the behavior previously. The answer is two-fold in order to stop a lone wolf type of situation. One, create more context to whatever data is obtainable. Second, consider data that may not be the exact same behavior but is still relevant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Academia Is Rising to the Challenge:&lt;/strong&gt; All manner of institutions now have massive amounts of data and need people to manage it and provide insights. Academia may be somewhat slow to change but there is a push from companies and from students for more data science programs. These sorts of programs are growing throughout higher education but there is much more room for expansion.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;For a more in-depth look at this episode, check out the article below.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;article-notes&#34;&gt;Article Notes&lt;/h2&gt;
&lt;p&gt;The data science field is expanding because so many businesses and other institutions require skilled workers who can manage data as well as provide insights. Companies and students are clamoring for more academic programs. There is great need, but academic institutions are still transitioning to meet the demand. &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, Distinguished Professor and Director of the Institute for Data Science at the New Jersey Institute of Technology, explained how his school is leading the charge to create opportunities for more students to study data science.&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;We talk with so many companies and also government, and we’re trying to create industry/academia partnerships to be able to solve many of those problems,” Bader said. “And the demand is just huge. We’re a pioneer in this area. We’re really leading the way and are trying to encourage other universities to move into this space. Universities sometimes have a reputation of moving slowly. But because of that, we decided that we really had a need. This region that we’re in is just growing and data science clearly is in the future of every company in every sector. And we took a leadership role. We’re also a part of some consortium. For instance, the Academic Data Science Alliance is a great way for universities and data science leaders to come together to talk about creating such programs and looking at what programs have already been created around the country.&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;On a recent episode of IT Visionaries, Bader chatted about how academic institutions are creating more programs to prepare future workers well-versed in data science. He also explained the wide-ranging data science research currently being conducted within academia and how its applications are relevant to many industries.&lt;/p&gt;
&lt;p&gt;Bader revealed the main research focus at the Institute for Data Science at the New Jersey Institute of Technology and how it can be applied across various areas.&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;What we’re working on is really getting insights and exploring data sets,” Bader said. “Imagine someone hands you a big trove of data, and you have no idea what’s contained in there, but yet you want to pull some intelligence out of it. So we work in that domain. Often, we’re looking at, for instance, data that can be viewed as graphs; where the entities may be represented as vertices and the relationships as edges between them kind of like a social network but on steroids. We’re looking at very massive data sets and the disciplines that we apply to are just ever growing. We look at health; personalized health and medicine. We look at cyber security applications. We look at urban sustainability. We even look at things like, how do we ensure trustworthy elections?&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;The data meets graphing system is about providing context and helping to make connections between different pieces of data. Then, Bader suggested that the discovery process is about following the trail to find appreciable relevant data that others might not see as worthwhile and discard.&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;Another way to think about it is that recommender systems are strong correlations versus what we look at is data at the very long end of the tail,” Bader said. “Often, this is the type of data that looks like noise. You may just throw it away. Versus, we look for signals within that noise to find…that one sign that may be [leading] to some egregious event or some very important matter that we have to attend to, depending on what discipline we’re looking at.&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Oftentimes, a large amount of data and a high number of particular occurrences are necessary in order to gain insight. In certain cases, it is not possible to wait for a huge data set of a certain repetitive behavior. As an example. Bader explained how data can be used to help prevent a lone wolf, intent on malevolence, from completing a horrific act.&lt;/p&gt;
&lt;p&gt;“&lt;em&gt;There are many cases where we’ve had lone wolves,” Bader said. “For instance, at Fort Hood in the United States, a number of years ago, we had Major Nidal Hasan who was a cleared army psychologist who one day, came in, and killed a number of people. And before that, there had never been any issue. There’s no disciplinary issues, et cetera. And we weren’t going to wait for statistical significance with lone wolves. For instance, you don’t want to wait until after an event like that happens. What you want to be able to do is give a security officer some heads-up that we think this person’s pattern of life has changed.&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;Regardless of the amount of data, it’s clear that efficiently creating context is paramount in order to gain actionable insights.&lt;/p&gt;
&lt;p&gt;To hear more about how the Institute for Data Science at the New Jersey Institute of Technology is tackling the big data challenges while educating the next generation of data experts, check out the full episode of IT Visionaries!&lt;/p&gt;
&lt;p&gt;To hear the entire discussion, tune into IT Visionaries &lt;a href=&#34;http://bit.ly/30iYuxo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;IT Visionaries is brought to you by the Salesforce Platform – the #1 cloud platform for digital transformation of every experience. Build connected experiences, empower every employee, and deliver continuous innovation – with the customer at the center of everything you do. Learn more at &lt;a href=&#34;https://www.salesforce.com/products/platform/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;salesforce.com/platform&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mission.org/?post_type=podcast&amp;amp;p=14205&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mission.org/?post_type=podcast&amp;p=14205&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Jersey Big Data Alliance 2021 Research Forum highlights a wealth of advanced research collaborations and funding opportunities in New Jersey</title>
      <link>http://localhost:1313/blog/20220106-njbda/</link>
      <pubDate>Thu, 06 Jan 2022 10:41:19 -0500</pubDate>
      <guid>http://localhost:1313/blog/20220106-njbda/</guid>
      <description>&lt;p&gt;At its December 10, 2021 Research Forum, the New Jersey Big Data Alliance presented a wide range of resources, collaborations, and funding opportunities available at the state and federal levels to the over 100 attendees who assembled virtually.&lt;/p&gt;
&lt;p&gt;The Research Forum was organized by the NJBDA Research Collaboration Committee, and facilitated by Forough Ghahramani (Edge), Committee Vice President. Speakers included NJBDA President Matt Hale (Seton Hall), &lt;strong&gt;David Bader&lt;/strong&gt; (NJIT), who announced the 2022 NJBDA Symposium; JD Jayaraman (New Jersey City University), who introduced the Journal of Big Data: Theory and Practice (JBDTP.org) and made a call for proposals (due March 15, 2022) and Barr von Olsen (Rutgers) provided information on The Eastern Regional Network (ERN) which focuses on multi-institutional collaborations and providing access to resources and instruments for advancing research.&lt;/p&gt;
&lt;p&gt;Melissa Handa (IEEE) discussed the IEEE DataPort; a platform of 1.6 million users to store, share, access and manage research data and access a competition platform. “IEEE DataPort is a valuable data platform designed to foster innovation and collaboration for data-centric researchers. All members of any NJBDA institution can use IEEE DataPort to enable their members to publish and store research datasets, make datasets accessible to others, search and access valuable research datasets, and manage data over time,” said Handa.&lt;/p&gt;
&lt;p&gt;Judith Sheft (NJEDA Commission on Science, Innovation and Technology) described the Research with NJ database. “The Research with New Jersey initiative (&lt;a href=&#34;https://www.researchwithnj.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.researchwithnj.com&lt;/a&gt;) supports Governor Murphy’s vision for a vibrant innovation economy by facilitating collaboration between the State’s vast technology and life sciences sectors and the thousands of researchers within our state’s world-class universities as they continue to advance new discoveries,” said Sheft. “This portal complements the work being done by NJBDA members and is a unique tool that showcases the breadth of innovation happening within our state.”&lt;/p&gt;
&lt;p&gt;NJBDA Founder Margaret Brennan-Tonetta (Rutgers) introduced the session focused on multi-institutional collaborations, highlighting the New Jersey applicants to the US-Economic Development Administration’s Build Back Better program. The panel was moderated by Pallavi Madakasira, Director, Clean Energy, New Jersey Economic Development Authority (NJEDA), and included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clean Energy Resource, Training and Innovation (CERTI) Cluster (Tabbetha Dobbins, Rowan)&lt;/li&gt;
&lt;li&gt;Biopharma Manufacturing (Vincent Smeraglia, Rutgers)&lt;/li&gt;
&lt;li&gt;Greater Newark Smart Port Regional Growth Cluster (Atam Dhawan, City of Newark/NJIT)&lt;/li&gt;
&lt;li&gt;Smart Aviation Growth Cluster (Carole M. Mattessich, Esq, Atlantic County Economic Alliance)&lt;/li&gt;
&lt;li&gt;Communications, Data, and Intelligent Technologies (CoDIT) Regional Growth Cluster (Victor Lawrence, Stevens)&lt;/li&gt;
&lt;li&gt;Building Inclusive Entrepreneurial Ecosystems in American Opportunity Zones (Anne-Marie Maman, Princeton)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The NJBDA presented a unique opportunity to bring together key stakeholders from academia, industry and government to have a robust conversation and exchange ideas that will further collaboration opportunities within the state,” said Madakasira. “The symposium took place against the backdrop of the Build Back Better challenge and enabled us to showcase the innovation happening throughout the state and especially the four clusters that the NJEDA and the State of New Jersey supported.”&lt;/p&gt;
&lt;p&gt;Attendees also heard a presentation from Rick McMullen (the Research Advisors Group) on National Science Foundation cyberinfrastructure funding opportunities.&lt;/p&gt;
&lt;p&gt;New Jersey Big Data Alliance President Matt Hale commented, “The NJBDA Alliance showcases the exciting ways NJ academics are doing research in the big data space. This includes the way we look at data, use it, collect it and present it.”&lt;/p&gt;
&lt;p&gt;“The 2021 NJBDA Research Forum successfully accomplished its objectives of bringing together a set of highly accomplished speakers to engage with the research community to highlight the amazing research collaborations that have been formed across New Jersey and beyond, including the Build Back Better Regional Challenge opportunity; and to raise awareness of funding opportunities, resources, and communities available to New Jersey researchers. In addition to researchers, participants included students, and professionals from government, industry and academia.” – Forough Ghahramani (Edge), Vice President, NJBDA Research Collaboration Committee.&lt;/p&gt;
&lt;p&gt;To learn more about the opportunities presented at the Research Forum, interested parties can contact Forough Ghahramani, EdD, Associate Vice President for Research, Innovation, and Sponsored Programs, Edge; and Vice President, Research Collaborations Committee, NJBDA at &lt;a href=&#34;mailto:forough.ghahramani@njedge.net&#34;&gt;forough.ghahramani@njedge.net&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;about-njbda&#34;&gt;About NJBDA&lt;/h2&gt;
&lt;p&gt;The New Jersey Big Data Alliance (NJBDA) was established in 2013 by Rutgers University, along with eight higher education partners, to catalyze collaboration among New Jersey academia, industry and government in building advanced computing and data analytics capabilities and expertise. Today, the NJBDA is a consortium of 18 higher education institutions and Edge, as well as industry and government members, that builds and leverages collaborations to increase competitiveness, generate a highly skilled workforce, drive innovation and catalyze data-driven economic growth for New Jersey.&lt;/p&gt;
&lt;p&gt;To learn more about the NJBDA, visit njbda.org.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://njbda.org/news/new-jersey-big-data-alliance-2021-research-forum-highlights-a-wealth-of-advanced-research-collaborations-and-funding-opportunities-in-new-jersey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njbda.org/news/new-jersey-big-data-alliance-2021-research-forum-highlights-a-wealth-of-advanced-research-collaborations-and-funding-opportunities-in-new-jersey/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Message from NJIT President Bloom</title>
      <link>http://localhost:1313/blog/20211220-njit/</link>
      <pubDate>Mon, 20 Dec 2021 11:13:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20211220-njit/</guid>
      <description>&lt;p&gt;Dear NJIT Community Members,&lt;/p&gt;
&lt;p&gt;As we prepare to depart campus and begin our winter break, I would like to express my most sincere thanks for the manner in which students, faculty, staff, and others worked together and sacrificed for one another to ensure a safe and successful Fall 2021 semester.&lt;/p&gt;
&lt;p&gt;The past few months have been filled with many successes for the NJIT community, some of which I will highlight below, but our most important achievement was bringing our campus community back to its full vibrance by safely and effectively returning to in-person learning, robust research activity, and a full slate of the activities that make for a complete living and learning experience. In doing so, we also welcomed NJIT’s largest and most diverse first-year class in university history.&lt;/p&gt;
&lt;p&gt;In addition to our safe and successful resumption of full activity on campus, NJIT advanced critical initiatives.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Most recently, a regional coalition led by the City of Newark and NJIT was announced as a finalist for up to $100 million in funding from the U.S. Economic Development Administration’s $1 billion Build Back Better Regional Challenge. Each of the 60 finalists, chosen from a pool of 529 applicants, will receive a grant of $500,000 to further develop their proposed projects. This is an extraordinary opportunity for both NJIT and the City of Newark, as well as our entire region, and our coalition is the only New Jersey-based proposal selected as a finalist.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Earlier this semester, NJIT announced an agreement with ENGIE North America to purchase renewable energy from a portfolio of hydropower facilities equal to nearly 100% of the university’s forecasted electricity consumption. This agreement achieves one of the strongest commitments for renewable power procurement in a retail energy purchase.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our university also garnered several recognitions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;U.S. News &amp;amp; World Report&lt;/em&gt; selected NJIT among its “Best Global Universities” in the eighth edition of this annual ranking. Only 271 U.S. universities made the 2022 list, which spans 91 countries across six continents. NJIT also rose 15 spots in the 2022 edition of &lt;em&gt;U.S. News &amp;amp; World Report&lt;/em&gt;’s Best National University Rankings and was ranked among the Top 50 Public National Universities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Martin Tuchman School of Management was named the top school in New Jersey and #34 in the nation for undergraduate entrepreneurship by The Princeton Review.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, NJIT’s students and faculty brought distinction to our university through their achievements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A group of undergraduate students in NJIT&amp;rsquo;s architecture program designed and built a prototype “tiny home” that may be part of the solution to Newark’s housing crisis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guiling (Grace) Wang, professor and associate dean for research in the Ying Wu College of Computing, recently became the first woman at NJIT named an IEEE Fellow, one of the electronic technology industry&amp;rsquo;s highest honors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The IEEE Computer Society named &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; as the recipient of the 2021 Sidney Fernbach Award, which recognizes outstanding contributions in the application of high-performance computers using innovative approaches.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Associate Professor of Mechanical and Industrial Engineering Shawn Chester won the coveted Thomas J.R. Hughes Young Investigator Award from the American Society of Mechanical Engineers, which recognizes special achievement in applied mechanics for researchers under the age of 41.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Senior Vice Provost Atam Dhawan was selected this year to receive one of IEEE’s preeminent honors, the Engineering in Medicine and Biology William J. Morlock Award, which is given every two years to an inventor of original electronics techniques and concepts used to solve biomedical problems. Atam was recognized for a pioneering invention that enables doctors to use light to look beneath the outer layer of the skin to detect diseases such as early-stage skin cancers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In reflecting upon the work of the past few months, I am extremely proud of our university and its people. We have navigated challenging circumstances, accomplished incredible things, and positioned our university for continued success and growth. You all have my sincere thanks and appreciation for all you do to make NJIT one of the premier polytechnic universities in the world, and I wish each of you a restful break as well as a joyous holiday season!&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;NJIT President Joel S. Bloom&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>International Team Developing New Algorithms for Streaming Graph Data</title>
      <link>http://localhost:1313/blog/20211216-njit/</link>
      <pubDate>Thu, 16 Dec 2021 11:33:53 -0500</pubDate>
      <guid>http://localhost:1313/blog/20211216-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Michael Giorgio&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-streaming-graph-algorithms&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Streaming Graph Algorithms&#34; srcset=&#34;
               /blog/20211216-njit/Graphs_hu_239a409483e44d45.webp 400w,
               /blog/20211216-njit/Graphs_hu_5498fcf8bb429c91.webp 760w,
               /blog/20211216-njit/Graphs_hu_f0aea9e1fe6cf28.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20211216-njit/Graphs_hu_239a409483e44d45.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Streaming Graph Algorithms
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Four professors across the globe are collaborating to solve real-world problems in cybersecurity, health, and various areas of social science through a project called Streaming Graph Algorithms under the new Institute for Future Technologies, a partnership between New Jersey Institute of Technology (NJIT) and Ben-Gurion of the Negev University (BGU).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of the Institute for Data Science at NJIT, and Michael Elkin,  professor in the department of computer science at BGU, are the project leads along with Ioannis Koutis, NJIT associate professor of computer science, and Ofer Neiman, BGU associate professor of computer science.&lt;/p&gt;
&lt;p&gt;Many real-world data sets can be abstracted as a graph with vertices representing objects and edges between vertices representing the connections between them. When the data sets are small, analyzing the associated graph is usually straightforward.  However, as the data sets grow in size, new algorithmic techniques are needed to manipulate the resulting huge graphs in order to analyze and extract valuable information from them. For example, a graph may represent a transportation network where vertices are intersections and each edge is a road.  Quickly answering queries about routing in the network requires new algorithms for sparsifying the graph such that the reduced graph preserves many of the features of the original one, such as distances between vertices.&lt;/p&gt;
&lt;p&gt;In other graph problems, such as analyzing network data in cybersecurity applications, the graph changes dynamically as new network traffic data arrives, making for an even more challenging problem of keeping track of the graph structure as the data “streams” in. Given this, rather than performing forensic analysis of security logs after a cybersecurity breach, the new streaming graph methods provide predictive analytics that prevent an attack before an intrusion occurs.&lt;/p&gt;
&lt;p&gt;Together, the researchers are exploring how to address the sparsity and lack of locality in data, the need for scalable algorithms, the treatment of dynamically streamed data sets, and the development of frameworks for solving such problems on high-performance computers.&lt;/p&gt;
&lt;p&gt;The researchers behind the Streaming Graph Algorithms project will take advantage of their initial results to seek additional funding from the National Science Foundation (NSF) in the U.S. and the U.S.–Israel Binational Science Foundation (BSF).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/international-team-developing-new-algorithms-streaming-graph-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/international-team-developing-new-algorithms-streaming-graph-data&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC21 General Chair Bronis R. de Supinski Recaps the First-Ever Hybrid SC Conference</title>
      <link>http://localhost:1313/blog/20211208-sc21/</link>
      <pubDate>Wed, 08 Dec 2021 06:29:51 -0500</pubDate>
      <guid>http://localhost:1313/blog/20211208-sc21/</guid>
      <description>&lt;p&gt;&lt;em&gt;by &lt;a href=&#34;https://sc21.supercomputing.org/author/bronis-r-de-supinski/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bronis R. de Supinski&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20211208-sc21/bronis_hu_a899cdce757ddd4f.webp 400w,
               /blog/20211208-sc21/bronis_hu_665a76f7a5f9cfe7.webp 760w,
               /blog/20211208-sc21/bronis_hu_b9130a0d43ea66c9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20211208-sc21/bronis_hu_a899cdce757ddd4f.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;sc21-marks-a-number-of-firsts&#34;&gt;SC21 Marks a Number of Firsts&lt;/h2&gt;
&lt;p&gt;It was the first time we hosted SC in St. Louis, where the famous Gateway Arch rises over the banks of the Mississippi River. It was the first time we delved into how the power of HPC is expanding to other disciplines, beyond traditional science and to areas such as arts and humanities. The SC21 theme, “Science &amp;amp; Beyond” represents that expansion and was highlighted in many ways, including in the SC21 Keynote by Dr. Vint Cerf, one of the “fathers of the internet,” who shared on-stage how advanced computing has a groundbreaking effect on how we can better appreciate and understand the study of languages and literatures, the arts, history and philosophy.&lt;/p&gt;
&lt;p&gt;SC21 was the first time we introduced an Inclusivity Talk, and we initiated the Best Reproducibility Advancement Award that will be presented annually.&lt;/p&gt;
&lt;p&gt;After two years of a global crisis, it was also the first time that SC delivered a hybrid experience for SC community members, which enabled us to reconvene in-person while offering attendees, participants, and volunteers worldwide a platform to attend remotely. SC21 welcomed more than 3,200 in-person attendees, 160 exhibitors and 300 volunteers. Overall, SC21 had more than 6,550 attendees, 200 exhibitors and 800 volunteers.&lt;/p&gt;
&lt;h2 id=&#34;banding-together-to-deliver-the-first-hybrid-sc-experience&#34;&gt;Banding Together to Deliver the First Hybrid SC Experience&lt;/h2&gt;
&lt;p&gt;When planning for SC21 in the midst of a pandemic, we were committed to continue making it an inspiring and engaging annual event for in-person and virtual attendees. The health and safety of our community was our top priority, which is why our committee, along with local staff at St. Louis’s America’s Center, worked together to implement guidelines for in-person attendance, such as proof of vaccination and mask use. We also took other precautions, which included providing readily available hand sanitizing stations and masks throughout the America’s Center, and social distancing in each room.&lt;/p&gt;
&lt;p&gt;As for the overall SC21 programming, our mission was to extend the experience to people who could not travel with a truly hybrid event. This required extensive work by our committee members, including several focused on virtual logistics, to develop an SC21 event using the HUBB platform, which has a proven track record of supporting both fully virtual and hybrid events. By using HUBB, we made it possible for remote participants to gain access to nearly 380 sessions and content. For live sessions, we also enabled remote attendees the option to engage and ask questions through HUBB Q&amp;amp;A features. I am proud of our team’s hard work in successfully creating this model.&lt;/p&gt;
&lt;p&gt;And to make all of that possible, we owe a big thanks to SCinet, our networking partners and SC21 Platinum Contributors, and the help of 165 volunteers from 87 organizations around the world, to provide reliable, high-performance networking to SC21 — a high-performance computing conference. SCinet is a complex network architecture that enabled us connection, onsite and virtually, for hundreds of talks, exhibits, and demonstrations. It is one of the world’s most powerful networks, and the team supporting SCinet for SC21 has set numerous post-pandemic networking records — an amazing feat given the struggles of building out the infrastructure in the middle of a pandemic.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-robust-sc-conference-program&#34;&gt;Creating a Robust SC Conference Program&lt;/h2&gt;
&lt;p&gt;The Technical Program is the hallmark to any SC conference. At this year’s event we continued to spark ideas and discussions that furthers our work and community with more than 380 program elements – from Papers and Workshops, to Tutorials and Birds of a Feather. These sessions reflected the innovation and high productivity of our community, despite challenges triggered by a pandemic.&lt;/p&gt;
&lt;h2 id=&#34;celebrating-achievements-made-in-high-performance-computing&#34;&gt;Celebrating Achievements Made in High-Performance Computing&lt;/h2&gt;
&lt;p&gt;At SC21, we highlighted notable achievements made in high performance computing through a number of awards, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The ACM-IEEE CS Ken Kennedy Award recognizes substantial contributions to programmability and productivity in computing, and substantial community service or mentoring contributions. It was awarded to David Abramson, a professor at the University of Queensland, for contributions to parallel and distributed computing tools, with application from quantum chemistry to engineering design. Abramson was also acknowledged for his mentorship and service to the field.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The IEEE-CS Sidney Fernbach Memorial Award honors innovative uses of HPC in problem solving, including creation of widely used and innovative software packages, application software, and tools. This year’s Sidney Fernbach Memorial Award honored &lt;strong&gt;David A. Bader&lt;/strong&gt;, a Distinguished Professor in the Department of Computer Science at New Jersey Institute of Technology for solving grand, social global challenges such as detecting and preventing disease in human populations, revealing community structure in large social networks, protecting our elections from cyber-threats, and improving the resilience of the electric power grid using high-performance data analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ACM Gordon Bell Prize, which recognizes outstanding achievement in HPC and how it is applied to applications in science, engineering, and large-scale data analytics, was awarded to a 14-member team, drawn from Chinese institutions, for their project on “Closing the “Quantum Supremacy” Gap: Achieving Real-Time Simulation of a Random Quantum Circuit Using a New Sunway Supercomputer.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ACM Gordon Bell Special Prize for High Performance Computing-Based COVID-19 Research was awarded in 2020 and again in 2021 to recognize outstanding research achievement towards the understanding of the COVID-19 pandemic through the use of HPC. The Gordon Bell Special Prize award for 2021 was awarded to a a six-member team for their project on the “Digital transformation of droplet/aerosol infection risk assessment realized on “Fugaku” for the fight against COVID-19.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The SC Best Reproducibility Advancement Award, which for the first time, was introduced this year to recognize outstanding efforts in improving transparency and reproducibility of methods for high performance computing, storage, networking and analysis. The Reproducibility Advancement Award honored Lawrence Livermore National Laboratory (LLNL) for developing a suite that simplifies evaluation of approximation techniques for scientific applications.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;building-on-our-mission-for-a-more-diverse-and-inclusive-environment&#34;&gt;Building on Our Mission for a More Diverse and Inclusive Environment&lt;/h2&gt;
&lt;p&gt;The HPC industry is becoming richer and more diverse every day. By fostering a more inclusive, diverse environment, which celebrates our community’s unique backgrounds and perspectives, we can inspire and drive innovation and new experiences. In that spirit, we made inclusivity part of every aspect of planning SC21.&lt;/p&gt;
&lt;p&gt;Nearly 50 members of the SC21 committee participated in a three-hour diversity and inclusion training, focused specifically on how to create inclusive conferences. We introduced a workplace and diversity track in the Exhibitor Forum, and, in an effort to create an inclusive space for people of all genders, we added an option for attendees and participants to list pronouns on their nametags and online profile, and also offered gender-neutral restrooms onsite in St. Louis.&lt;/p&gt;
&lt;p&gt;At SC21, we also introduced “affinity groups” for Women &amp;amp; Femmes, Africans/African Americans, the LGBTQIA+ community, People with Disabilities, Indigenous People, and Hispanic/Latinx people. Attendees came together in these spaces to create community, form mentoring relationships, and discuss HPC activities that are important to their career growth. We made affinity group spaces available in Discord to allow on-site and virtual attendees the opportunity to make these impactful connections.&lt;/p&gt;
&lt;p&gt;We also hosted a few presentations on the topic of inclusivity. SC21 introduced the first Inclusivity Talk — a special invited talk by Dr. Shaundra Daily of Duke University, who discussed the state of diversity in the field of computing and what we all can do to make our workplaces more welcoming to people from historically excluded groups. The Technical Program also featured a roundtable discussion on “The Importance of Diverse Perspectives in Advancing HPC” led by Dr. Valerie Taylor of Argonne National Laboratory.&lt;/p&gt;
&lt;h2 id=&#34;continuing-to-fuel-a-growing-community-and-conference&#34;&gt;Continuing to Fuel a Growing Community and Conference&lt;/h2&gt;
&lt;p&gt;Without doubt, the COVID-19 pandemic challenged the entirety of planning for SC21. From creating and synchronizing two conference models to support a hybrid experience, to ensuring all staff and in-person attendees followed new health and safety guidelines, our community volunteered countless hours of their time to create a unique, first-of-its-kind hybrid SC.&lt;/p&gt;
&lt;p&gt;The herculean efforts of our community in adapting and shifting to create new models and approaches while still keeping the spirit of SC alive have been inspiring. For that, I am deeply honored and humbled to be a part of this community that shared a vision in making SC21 possible in the midst of a global crisis.&lt;/p&gt;
&lt;p&gt;I want to express my gratitude to all volunteers, exhibitors, participants, and our society partners, the IEEE Computer Society and the Association for Computing Machinery (ACM), for supporting SC21 and fueling another inspiring conference.&lt;/p&gt;
&lt;p&gt;We look forward to connecting with you all at SC22 in Dallas, Texas.&lt;/p&gt;
&lt;p&gt;Bronis R. de Supinski, SC21 General Chair&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sc21.supercomputing.org/2021/12/08/sc21-general-chair-bronis-r-de-supinski-recaps-the-first-ever-hybrid-sc-conference/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sc21.supercomputing.org/2021/12/08/sc21-general-chair-bronis-r-de-supinski-recaps-the-first-ever-hybrid-sc-conference/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC21 Awards Recap: Awards Ceremony Winners</title>
      <link>http://localhost:1313/blog/20211207-hpcwire/</link>
      <pubDate>Tue, 07 Dec 2021 13:23:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20211207-hpcwire/</guid>
      <description>&lt;h2 id=&#34;ieee-cs-sidney-fernbach-memorial-award&#34;&gt;IEEE-CS Sidney Fernbach Memorial Award&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.computer.org/profiles/david-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David A. Bader, New Jersey Institute of Technology&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/sc21-awards-recap-awards-ceremony-winners/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/sc21-awards-recap-awards-ceremony-winners/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A. James Clark School of Engineering, Silver Terps Reunion, Celebrating the Classes of 1996 to 1981</title>
      <link>http://localhost:1313/blog/20211202-umcp/</link>
      <pubDate>Thu, 02 Dec 2021 13:30:59 -0500</pubDate>
      <guid>http://localhost:1313/blog/20211202-umcp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Ph.D. 1996 Electrical and Computer Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor and founder of the
Department of Data Science and inaugural Director of the Institute for
Data Science at New Jersey Institute of Technology. Prior to this, he
served as founding Professor and Chair of the School of Computational
Science and Engineering at Georgia Institute of Technology. Dr. Bader
is a Fellow of the IEEE, AAAS, and SIAM, and a recipient of the IEEE
Sidney Fernbach Award. He advises the White House, most recently on
the National Strategic Computing Initiative and Future Advanced
Computing Ecosystem.  Bader is a leading expert in solving global
grand challenges in science, engineering, computing, and data
science. His interests are at the intersection of high-performance
computing and real-world applications, including cybersecurity,
massive-scale analytics, and computational genomics. Dr. Bader is
Editor-in-Chief of the ACM Transactions on Parallel Computing and
previously served as Editor-in-Chief of the IEEE Transactions on
Parallel and Distributed Systems. In 2021, ROI-NJ recognized Bader on
its inaugural list of technology influencers, and in 2012, Bader was
the inaugural recipient of University of Maryland’s Electrical and
Computer Engineering Distinguished Alumni Award.  In 1998, Bader built
the first Linux supercomputer that led to a highperformance computing
(HPC) revolution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streaming Data Analytics Has New Method Planned by NJIT, USC, Harvard</title>
      <link>http://localhost:1313/blog/20211101-njit/</link>
      <pubDate>Mon, 01 Nov 2021 13:19:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20211101-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20211101-njit/StreamWare-logo_0_hu_d8539c7cf46fc236.webp 400w,
               /blog/20211101-njit/StreamWare-logo_0_hu_79e38f3c1bd6363f.webp 760w,
               /blog/20211101-njit/StreamWare-logo_0_hu_ea7ca9b0b68104ef.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20211101-njit/StreamWare-logo_0_hu_d8539c7cf46fc236.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;E-commerce sites, military planners and streaming media providers all want to analyze multiple sources of live data as quickly as possible, a holy grail of data science which is about to become possible through new software called StreamWare, from researchers at Harvard University, New Jersey Institute of Technology and the University of Southern California.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;What we&amp;rsquo;re trying to do is reimagine data science,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director of NJIT&amp;rsquo;s Institute for Data Science.&lt;/p&gt;
&lt;p&gt;Bader, along with Harvard professor David Brooks and USC Viterbi School of Engineering Prof. Viktor Prasanna, are the principal investigators hosting a &lt;a href=&#34;https://sc21.supercomputing.org/presentation/?id=bof153&amp;amp;sess=sess395&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;birds-of-a-feather session at the Supercomputing 2021 conference&lt;/a&gt; on Nov. 16 in St. Louis to drum up support for StreamWare, already in development by them and other colleagues. So far, they have cooperation pledges from AMD, Google, Intel, Microsoft and Xilinx. &amp;ldquo;We would also reach out to additional companies with whom we’ve collaborated previously, including for example, IBM, HPE, Qualcomm and ARM,&amp;rdquo; they stated.&lt;/p&gt;
&lt;p&gt;The group is supported by a $250,000 National Science Foundation planning grant, which they&amp;rsquo;ll use to study areas of improvement at the intersection of high-transaction online applications and the underlying server and network hardware — &amp;ldquo;A truly interdisciplinary team … to propose a vertically integrated system covering applications, algorithms, software and architecture,&amp;rdquo; USC&amp;rsquo;s Prasanna noted.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;What we&amp;rsquo;re trying to do is reimagine data science&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&amp;ldquo;The outcomes of the planning phase will include a proposal for the research activities to be carried out in the [next] grant, publications on the results of the survey activities and future research directions for enabling streaming data science, and curriculum for future graduate and undergraduate courses,&amp;rdquo; they stated in their grant proposal.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Rather than waiting to collect this information to a central location and process it, our thought is to create streaming analytics able to make decisions on the fly,&amp;rdquo; Bader explained. Currently, &amp;ldquo;There&amp;rsquo;s a lot of work [called] streaming but often that work is very simplistic in nature, for instance with a single data source and running a single analytic such as frequency counting.&amp;rdquo; Current efforts also have trouble scaling up, he added.&lt;/p&gt;
&lt;p&gt;The group will test new combinations of open-source, state-of-the-art algorithms and hardware accelerators, on data sets from astrophysics, smart grids and network science. Bader said he thinks prototypes could be ready to demonstrate in 3-6 months from now.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The proposed project will positively influence a wide range of applications in scientific and engineering domains. The developed data science kernel catalogue will enable the big data research community to prioritize research on improving the performance of these kernels,&amp;rdquo; the researchers stated in their grant proposal. &amp;ldquo;This project will also be directed towards promoting scientific education through the involvement of high school and university students, especially female and underrepresented minority students.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition, starting with Harvard, the project results will influence course curricula in data science and related fields. Ultimately, &amp;ldquo;Where we&amp;rsquo;re heading is predictive analytics,&amp;rdquo; Bader said, by using machine learning and artificial intelligence.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/streaming-data-analytics-has-new-method-planned-njit-usc-harvard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/streaming-data-analytics-has-new-method-planned-njit-usc-harvard&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Institute for Data Science to Hold Virtual Event with Gordon Bell, ‘ExaScale: More Than I Ever Imagined,’ on Oct. 6</title>
      <link>http://localhost:1313/blog/20211005-hpcwire/</link>
      <pubDate>Tue, 05 Oct 2021 10:25:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20211005-hpcwire/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20211005-hpcwire/NJIT-Data-Science-Event-Gordon-Bell-405x228_hu_bc98a5bf409e0383.webp 400w,
               /blog/20211005-hpcwire/NJIT-Data-Science-Event-Gordon-Bell-405x228_hu_967335a9e79291fd.webp 760w,
               /blog/20211005-hpcwire/NJIT-Data-Science-Event-Gordon-Bell-405x228_hu_8165e5f9f0163842.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20211005-hpcwire/NJIT-Data-Science-Event-Gordon-Bell-405x228_hu_bc98a5bf409e0383.webp&#34;
               width=&#34;405&#34;
               height=&#34;228&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey Institute of Technology’s Institute for Data Science is holding a virtual seminar with presenter Gordon Bell, Ph.D., Microsoft Emeritus Researcher, titled “Exascale: More than I ever imagined.”&lt;/p&gt;
&lt;p&gt;The seminar will be hosted by &lt;strong&gt;David A. Bader&lt;/strong&gt;, Distinguished Professor and Director of NJIT. The online event will take place on Wednesday, Oct. 6, 4:00 p.m. — 5:00 p.m. EDT.&lt;/p&gt;
&lt;h2 id=&#34;about-this-event&#34;&gt;About This Event&lt;/h2&gt;
&lt;p&gt;Computation has increased 17 orders of magnitude over the sixty years Bell has been visiting and celebrating gains in computational environments. The first “supercomputers” ran at a million ops per second and in 2021 the fastest computers operates at exa-ops or 10^18ops with gains of 10 million occurring in the last twenty-five year by exploiting parallelism. The relatively small community who have produced these gains through parallelism. Gains must be discovered at every level of the hardware and compilers to the applications and supporting algorithms. In the beginning, the challenge was to obtain enough computing resources to solve some small aspect of a scientific or engineering problem, but now the problem is really imagining the problems that can exploit these potentially infinite resource computing environments.&lt;/p&gt;
&lt;h2 id=&#34;about-the-presenter&#34;&gt;About the Presenter&lt;/h2&gt;
&lt;p&gt;Gordon Bell is a Microsoft Researcher Emeritus. He spent 23 years at Digital Equipment Corporation as Vice President of R&amp;amp;D, responsible for the first mini- and time-sharing computers and DEC’s VAX, with a 6 year sabbatical at Carnegie Mellon. In 1987, as NSF’s first, Ass’t Director for Computing (CISE), he led the National Research and Education Network panel that became the Internet. In 1987 he established the Gordon Bell Prize to recognize the extraordinary efforts to exploit modern highly parallel computers. Bell maintains three interests: computers: their evolution and use, technology-based startup companies, and lifelogging. He is a member or Fellow of the American Academy of Arts and Sciences, ACM, IEEE, the National Academy of Engineering, National Academy of Science, the Australia Academy of Technological Sciences and Engineering and received The 1991 National Medal of Technology. He is a founding trustee of the Computer History Museum, Mountain View, Calif., and lives in San Francisco. Visit his website &lt;a href=&#34;http://gordonbell.azurewebsites.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/njit-institute-for-data-science-to-hold-virtual-event-with-gordon-bell/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/njit-institute-for-data-science-to-hold-virtual-event-with-gordon-bell/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experts at NJIT&#39;s New AI Research Center to Study Theory and Applications</title>
      <link>http://localhost:1313/blog/20211001-njit/</link>
      <pubDate>Fri, 01 Oct 2021 16:33:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20211001-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-centers-goals-including-understanding-ai-theory-and-developing-useful-applications&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The center&amp;#39;s goals including understanding AI theory and developing useful applications&#34; srcset=&#34;
               /blog/20211001-njit/Artificial-Intelligence_hu_40e084d34f7389c2.webp 400w,
               /blog/20211001-njit/Artificial-Intelligence_hu_22f05494caf5c038.webp 760w,
               /blog/20211001-njit/Artificial-Intelligence_hu_1b248f06ccd8405c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20211001-njit/Artificial-Intelligence_hu_40e084d34f7389c2.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The center&amp;rsquo;s goals including understanding AI theory and developing useful applications
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Members of NJIT&amp;rsquo;s new Center for Artificial Intelligence Research will look to expand knowledge of the theory, data and applications of their field, which director Grace Wang said is quickly evolving from experimental to practical status.&lt;/p&gt;
&lt;p&gt;The center will rely heavily on external grants, such as from the National Science Foundation which devoted $868 million to artificial intelligence research this year. Degree offerings will include an M.S. in artificial intelligence, most likely starting in fall 2022, building on existing courses such as CS-677, Deep Learning, which has been offered since 2019 as well as new courses including natural language processing.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It is AI&amp;rsquo;s time,&amp;rdquo; now that hardware and networks are powerful enough to allow for software that computer scientists first imagined decades ago, explained Wang, a professor and associate dean for research in Ying Wu College of Computing. &amp;ldquo;It is an opportunity. You have to catch it or you are behind.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Wang helped select faculty from her own college and from the College of Science and Liberal Arts, Martin Tuchman School of Management and Newark College of Engineering for the new center. Beside computer science experts, center members include experts in business, civil engineering, electrical engineering, mathematics and physics.&lt;/p&gt;
&lt;p&gt;Artificial intelligence is quickly being applied beyond core computer science. There are applications in healthcare, geology, finance, physics and robotics, Wang said. Consumer applications that employ speech recognition and natural language processing such as Amazon Alexa, Google Assistant and Apple Siri rely on AI underneath, as do self-driving cars, she noted.&lt;/p&gt;
&lt;p&gt;Traditional scientific research can take weeks, months or even years to get results. Wang said her favorite thing about AI is that she can change an experiment&amp;rsquo;s parameters and get the results almost immediately.&lt;/p&gt;
&lt;p&gt;However, Wang and her doctoral student Ankan Dash both said the biggest challenge for artificial intelligence researchers is to fully understand why it works. Unlike traditional software where users enter their data, understand the algorithms and wait for the result, in artificial intelligence the users enter their data and their desired output, then train the models to learn how to produce it. Models can be very complicated and the results may not be obvious or have a meaningful explanation.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I feel after five years we could have real explainable AI. There&amp;rsquo;s not a theoretical proof that this really works. There is some theoretical gap,&amp;rdquo; Wang said. &amp;ldquo;I hope in five years this can be mapped.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We kind of know what&amp;rsquo;s going on,&amp;rdquo; Dash joked. &amp;ldquo;The biggest problem in AI is helping people understand how models actually work. Most of the problems related to AI, it&amp;rsquo;s a black box. &amp;hellip; It will be very nice if the research community focuses more on how the models actually work.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Dash joined NJIT last year to study under Wang, returned to India while classes were online during the pandemic, and said he was pleased to learn about the new center upon returning to campus this year. &amp;ldquo;I&amp;rsquo;m very happy,&amp;rdquo; said Dash, who explores how to apply artificial intelligence to aerospace, financial technology and remote sensing. &amp;ldquo;I think there will be much more interdisciplinary research between various departments.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The center will join NJIT&amp;rsquo;s Institute for Data Science, which already houses the Center for Big Data, Cybersecurity Research Center and Structural Analysis of Biomedical Ontologies Center.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The AI research center is a great addition to the capabilities housed already in the Institute for Data Science,&amp;rdquo;  director &lt;strong&gt;David Bader&lt;/strong&gt; said. &amp;ldquo;Machine learning and AI are revolutionizing how we learn from the massive datasets in areas ranging from genomics and medicine to understanding climate and space weather. This new center promises to make foundational contributions to the field of AI.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;NJIT has a unique connection to the history of artificial intelligence dating back 60 years. &lt;a href=&#34;https://amturing.acm.org/award_winners/pearl_2658896.cfm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Judea Pearl&lt;/a&gt;, who won the Turing Award in 2011 for his development of Bayesian networks — a cornerstone of modern AI — received an M.S. in electrical engineering at Newark College of Engineering in 1961.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/experts-njits-new-ai-research-center-study-theory-and-applications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/experts-njits-new-ai-research-center-study-theory-and-applications&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ACR member Dr. David Bader named recipient of the 2021 Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210930-coc/</link>
      <pubDate>Thu, 30 Sep 2021 10:29:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210930-coc/</guid>
      <description>

















&lt;figure  id=&#34;figure-david-bader-distinguished-professor-and-founder-department-of-data-science-new-jersey-institute-of-technology&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader, Distinguished Professor and Founder, Department of Data Science, New Jersey Institute of Technology&#34; srcset=&#34;
               /blog/20210930-coc/bader_hu_39e3fadc408a1954.webp 400w,
               /blog/20210930-coc/bader_hu_3573f7bfaa802e19.webp 760w,
               /blog/20210930-coc/bader_hu_19d5577756e49d09.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210930-coc/bader_hu_39e3fadc408a1954.webp&#34;
               width=&#34;225&#34;
               height=&#34;225&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader, Distinguished Professor and Founder, Department of Data Science, New Jersey Institute of Technology
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The award recognizes contributions in the application
of high-performance computers using innovative
approaches. &lt;strong&gt;Dr. Bader&lt;/strong&gt;, a Distinguished Professor
and Founder of the Department of Data Science at
the New Jersey Institute of Technology, was cited for
the development of Linux-based massively parallel
production computers and for pioneering
contributions to scalable discrete parallel algorithms
for real-world applications.&lt;/p&gt;
&lt;p&gt;Read more &lt;a href=&#34;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TEDxNJIT: Resurgence</title>
      <link>http://localhost:1313/blog/20210929-njitvector/</link>
      <pubDate>Wed, 29 Sep 2021 10:13:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210929-njitvector/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210929-njitvector/tedxnjit_hu_d76ec8fe16c86e96.webp 400w,
               /blog/20210929-njitvector/tedxnjit_hu_16815a8080999767.webp 760w,
               /blog/20210929-njitvector/tedxnjit_hu_9caa02a7e0d40d33.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210929-njitvector/tedxnjit_hu_d76ec8fe16c86e96.webp&#34;
               width=&#34;760&#34;
               height=&#34;645&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Karim Salem&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;On Sept. 22, NJIT had the opportunity to bring new and challenging ideas grouped with experience and advice from a wide set of professionals and students alike during its seventh TEDx event. Organized by Michael Ehrlich and team, TEDxNJIT was hosted virtually with the theme of “Resurgence,” meant to address the impact of the post-COVID world. The theme was highlighted by experts in various fields who shared some technological advancements and words of wisdom.&lt;/p&gt;
&lt;p&gt;Among the presenters was Julie Ancis, professor and Founding Director of Cyberpsychology at NJIT, who posed the question, “The Post-Pandemic Future: Are We Ready?”&lt;/p&gt;
&lt;p&gt;Ancis discussed how COVID-19 has impacted us in a variety of ways such as our work life, social interactions, wellbeing and mental state. She offered predictions for the future regarding the role of technology and some of the challenges we may face today. She began with the idea of the spread of misinformation, explaining that it may be one of the biggest challenges within this century.&lt;/p&gt;
&lt;p&gt;She observed that “beliefs in misinformation and fake news are associated with a range of behaviors and attitudes including health-related behaviors.”&lt;/p&gt;
&lt;p&gt;Ancis recommended some interventions to help combat the spread of the ”second epidemic” of misinformation, such as increasing critical thinking about the information people are looking at. Another method she recommends is being more aware of where information comes from. According to Ancis, many people are placed inside of an “echo chamber” where algorithms, based on likes and shares, limiting exposure to new and challenging ideas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor and the Director of the Institute for Data Science, followed up with his talk, “Solving Global Grand Challenges with High Performance Data Analytics.”&lt;/p&gt;
&lt;p&gt;The field of high-performance data has gone through several names over the years, but essentially, it is the collection of ever-growing data sets and the ability to calculate and process them in real-time. The applications of this field could be applied to many of today’s industries such as cybersecurity, from looking at the network traffic entering an organization or even a country to analyzing patterns of malicious attacks. Another field it could benefit is computational genomics, which could lead to personalized medicine instead of a single pharmaceutical for the entire population.&lt;/p&gt;
&lt;p&gt;Rachel Benyola, an Executive and Founder Coach at RKB Consulting, discussed ways of improving entrepreneurship with her talk, “Innovating the Entrepreneur: Resurgence of the Human Entrepreneur.”&lt;/p&gt;
&lt;p&gt;A young entrepreneur, Benyola noticed that even though there was a great benefit for bike rides to wear helmets, a majority would fail to do so. She took initiative to find out what exactly are the reasons for bike riders to neglect their safety and she reached two main conclusions; helmets are too bulky to carry around and they aren’t very stylish. After considering this feedback, she launched her company, AnneeLondon, which created collapsible bike and scooter helmets with style in mind.&lt;/p&gt;
&lt;p&gt;Her talk, however, was dedicated to a different theme, the unspoken truth about the struggles of being an entrepreneur. Benyola noted, “being a CEO and founder, I see that mental wellness is something that is so ignored in entrepreneurs. Entrepreneurs are seen as sort of these machines, these robots that just churn out innovation.”&lt;/p&gt;
&lt;p&gt;In her talk, Benyola pointed out that about 90% of startups fail and she believes that this number could be attributed to the lack of support and preparation offered to founders and innovators.&lt;/p&gt;
&lt;p&gt;With many other speakers and experts, TEDxNJIT has once again rekindled inspiration into the heart of its viewers and reminded us that sometimes the first step towards innovation, self-betterment and discovery is a conversation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://njitvector.com/2021/09/tedxnjit-resurgence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njitvector.com/2021/09/tedxnjit-resurgence/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader ’90 ’91G to receive Sidney Fernbach Award from IEEE CS</title>
      <link>http://localhost:1313/blog/20210928-lehigh/</link>
      <pubDate>Tue, 28 Sep 2021 11:43:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210928-lehigh/</guid>
      <description>

















&lt;figure  id=&#34;figure-ece-alum-david-bader-90-91g-is-distinguished-professor-in-the-department-of-computer-science-and-founder-of-the-department-of-data-science-in-the-ying-wu-college-of-computing-and-director-of-the-institute-for-data-science-at-njit&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ECE alum David Bader ’90 ’91G is Distinguished Professor in the Department of Computer Science and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at NJIT.&#34; srcset=&#34;
               /blog/20210928-lehigh/Bader-2018-web_0_hu_13490c238f2d4ce6.webp 400w,
               /blog/20210928-lehigh/Bader-2018-web_0_hu_2579726fe7a46a6.webp 760w,
               /blog/20210928-lehigh/Bader-2018-web_0_hu_1c5cef95c5eabd23.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210928-lehigh/Bader-2018-web_0_hu_13490c238f2d4ce6.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ECE alum David Bader ’90 ’91G is Distinguished Professor in the Department of Computer Science and founder of the Department of Data Science in the Ying Wu College of Computing and Director of the Institute for Data Science at NJIT.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Electrical and computer engineering alum &lt;strong&gt;&lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader ’90 ’91G&lt;/a&gt;&lt;/strong&gt; has been named as the 2021 recipient of the &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt;, given by the IEEE Computer Society (IEEE CS).&lt;/p&gt;
&lt;p&gt;Bader is a Distinguished Professor at the New Jersey Institute of Technology. He is also founder of NJIT&amp;rsquo;s Department of Data Science and inaugural director of its Institute for Data Science. He earned his BS in electrical engineering from Lehigh in 1990 and his MS in computer engineering in 1991.&lt;/p&gt;
&lt;p&gt;The award, established in 1992 in memory of high-performance computing pioneer Sidney Fernbach, recognizes outstanding contributions in the application of high-performance computers using innovative approaches. Bader was cited for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;
&lt;p&gt;“David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds,” said Satoshi Matsuoka, director of RIKEN Center for Computational Science. “As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;“Today, 100% of the Top 500 supercomputers in the world are Linux HPC systems, based on Bader’s technical contributions and leadership. This is one of the most significant technical foundations of HPC,” noted Steve Wallach, a guest scientist for Los Alamos National Laboratory and 2008 IEEE CS Seymour Cray Computer Engineering Award recipient.&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC.&lt;/p&gt;
&lt;p&gt;Bader is editor-in-chief of the ACM Transactions on Parallel Computing, and General Co-Chair of IPDPS 2021. He previously served as editor-in-chief of the IEEE Transactions on Parallel and Distributed Systems.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).&lt;/p&gt;
&lt;p&gt;The award will be presented to Bader at the &lt;a href=&#34;https://sc21.supercomputing.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SC21 Conference&lt;/a&gt; awards plenary session in St. Louis, Missouri, on November 16, 2021.&lt;/p&gt;
&lt;p&gt;For more details on Bader’s work and achievements, read the &lt;a href=&#34;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full press release&lt;/a&gt; from the IEEE Computer Society.&lt;/p&gt;
&lt;h2 id=&#34;related-links&#34;&gt;Related Links:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Biography: David A. Bader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Computer Society: Sidney Fernbach Award&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;media-coverage&#34;&gt;Media Coverage:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Computer Society: &amp;ldquo;David Bader Selected to Receive the 2021 IEEE Computer Society Sidney Fernbach Award&amp;rdquo;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;departmentprogram&#34;&gt;Department/Program:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.lehigh.edu/node/267&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;College of Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.lehigh.edu/ece&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Electrical &amp;amp; Computer Engineering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://engineering.lehigh.edu/news/article/bader-90-91g-receive-sidney-fernbach-award-ieee-cs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://engineering.lehigh.edu/news/article/bader-90-91g-receive-sidney-fernbach-award-ieee-cs&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumnus David Bader Receives 2021 Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210924-maryland-ece/</link>
      <pubDate>Fri, 24 Sep 2021 18:29:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210924-maryland-ece/</guid>
      <description>

















&lt;figure  id=&#34;figure-alumnus-david-bader-phd-96&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Alumnus David Bader (Ph.D., ’96)&#34; srcset=&#34;
               /blog/20210924-maryland-ece/article14331.large_hu_2caace1dc88e0e8e.webp 400w,
               /blog/20210924-maryland-ece/article14331.large_hu_e57ed5fd7ebd5652.webp 760w,
               /blog/20210924-maryland-ece/article14331.large_hu_efc10460fa6c85bb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210924-maryland-ece/article14331.large_hu_2caace1dc88e0e8e.webp&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Alumnus David Bader (Ph.D., ’96)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Electrical and Computer Engineering Alumnus &lt;strong&gt;&lt;a href=&#34;https://people.njit.edu/faculty/bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;&lt;/strong&gt; (Ph.D., ’96) is the recipient of the 2021 &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt; from the &lt;a href=&#34;https://www.computer.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Computer Society&lt;/a&gt; (IEEE CS). Bader is a Distinguished Professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach Award, established in 1992, recognizes outstanding contributions in the application of high-performance computers using innovative approaches. Bader was cited for “the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.”&lt;/p&gt;
&lt;p&gt;In the IEEE CS &lt;a href=&#34;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement&lt;/a&gt;, Satoshi Matsuoka, director of RIKEN Center for Computational Science said “David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds. As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;During Bader’s career, he has demonstrated that a Linux OS-based machine can be a production supercomputer with matching performance and utility, by integrating a high-performance scalable interconnection network, system services such as scalable booting methodology, system software including free and commercial compiler suites, high-utilization job schedulers, and diagnostic monitoring. He has developed scalable discrete algorithms for problems with irregular data structures, lack of locality, and unpredictable memory traces on Linux supercomputers. He launched and maintained the Graph500 ranking that influenced the HPC community to look beyond LINPACK as the sole performance ranking metric. He has attacked real-world problems such as conducting the first study of Twitter using streaming parallel graph algorithms to identify “important actors” during epidemics and disasters and applying streaming graph analysis to detect insider threats in corporate networks.&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the convergence of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC. Bader is editor-in-chief of the ACM Transactions on Parallel Computing, and General Co-Chair of IPDPS 2021, and previously served as editor-in-chief of the IEEE Transactions on Parallel and Distributed Systems.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE). Other notable awards he has received include recognition in the 2021 ROI-NJ inaugural list of technology influencers, 2014 Outstanding Senior Faculty Research Award from Georgia Tech, and 2012 inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award. During his time at UMD, Bader was advised by ECE Interim Chair and Professor Joseph JaJa, and founded and served as president of the Electrical and Computer Engineering Graduate Student Association (&lt;a href=&#34;http://gsa.ece.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECEGSA&lt;/a&gt;). Recently, Bader received an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach award consists of a certificate and a $2,000 honorarium. The award will be presented to Bader at the &lt;a href=&#34;https://sc21.supercomputing.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SC21 Conference&lt;/a&gt; awards plenary session in St. Louis, Missouri, on Tuesday morning, 16 November 2021.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/alumnus-david-bader-receives-2021-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/alumnus-david-bader-receives-2021-sidney-fernbach-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Selected for 2021 IEEE Computer Society Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210923-insidehpc/</link>
      <pubDate>Thu, 23 Sep 2021 07:22:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210923-insidehpc/</guid>
      <description>&lt;p&gt;LOS ALAMITOS, Calif., 22 September 2021 – The IEEE Computer Society (IEEE CS) has named &lt;strong&gt;David Bader&lt;/strong&gt; as the recipient of the 2021 &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt;.  Bader is a Distinguished Professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;Established in 1992 in memory of high-performance computing pioneer Sidney Fernbach, the Sidney Fernbach Award recognizes outstanding contributions in the application of high-performance computers using innovative approaches.  Bader was cited for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;
&lt;p&gt;“David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds,” said Satoshi Matsuoka, director of RIKEN Center for Computational Science. “As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;“Today, 100% of the Top 500 supercomputers in the world are Linux HPC systems, based on Bader’s technical contributions and leadership.  This is one of the most significant technical foundations of HPC,” noted Steve Wallach, a guest scientist for Los Alamos National Laboratory and 2008 IEEE CS Seymour Cray Computer Engineering Award recipient.&lt;/p&gt;
&lt;p&gt;Specifically, the Fernbach Award recognizes Bader’s contributions, in the following areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated that a Linux OS-based machine can be a production supercomputer with matching performance and utility, by integrating a high-performance scalable interconnection network, system services such as scalable booting methodology, system software including free and commercial compiler suites, high-utilization job schedulers, and diagnostic monitoring.&lt;/li&gt;
&lt;li&gt;Developed scalable discrete algorithms for problems with irregular data structures, lack of locality, and unpredictable memory traces on Linux supercomputers.&lt;/li&gt;
&lt;li&gt;Started and maintained the Graph500 ranking that influenced the HPC community to look beyond LINPACK as the sole performance ranking metric.&lt;/li&gt;
&lt;li&gt;Attacked real-world problems such as conducting the first study of Twitter using streaming parallel graph algorithms to identify “important actors” during epidemics and disasters and applying streaming graph analysis to detect insider threats in corporate networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viktor Prasanna, Charles Lee Powell Chair in Engineering and Professor of Electrical Engineering and Professor of Computer Science at the University of Southern California, summarized, “Professor Bader has developed innovative techniques to explore HPC for many challenging application areas where graph based techniques are being used. This has been an exciting research direction that requires advances in platform architecture, software systems, and parallelization. This award recognizes Professor Bader’s work in building such software as well as in developing scalable techniques for graph analytics.”&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics.  He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC.  Bader is editor-in-chief of the &lt;em&gt;ACM Transactions on Parallel Computing&lt;/em&gt;, and General Co-Chair of IPDPS 2021, and previously served as editor-in-chief of the &lt;em&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).  Other notable awards he has received include recognition in the 2021 ROI-NJ inaugural list of technology influencers, 2014 Outstanding Senior Faculty Research Award from Georgia Tech, and 2012 inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award. Recently, Bader received an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach award consists of a certificate and a $2,000 honorarium. The award will be presented to Bader at the SC21 Conference awards plenary session in St. Louis, Missouri, on Tuesday morning, 16 November 2021.&lt;/p&gt;
&lt;h2 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h2&gt;
&lt;p&gt;The IEEE Computer Society is the world’s home for computer science, engineering, and technology. A global leader in providing access to computer science research, analysis, and information, the IEEE Computer Society offers a comprehensive array of unmatched products, services, and opportunities for individuals at all stages of their professional careers. Known as the premier organization that empowers the people who drive technology, the IEEE Computer Society offers international conferences, peer-reviewed publications, a unique digital library, and training programs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2021/09/david-bader-selected-for-2021-ieee-computer-society-sidney-fernbach-award/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2021/09/david-bader-selected-for-2021-ieee-computer-society-sidney-fernbach-award/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT&#39;s David Bader Selected to Receive the 2021 IEEE Computer Society Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210923-spotnj/</link>
      <pubDate>Thu, 23 Sep 2021 07:18:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210923-spotnj/</guid>
      <description>&lt;p&gt;The IEEE Computer Society (IEEE CS) has named &lt;strong&gt;David Bader&lt;/strong&gt; as the recipient of the 2021 Sidney Fernbach Award. Bader is a distinguished professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at New Jersey Institute of Technology&amp;hellip;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njits-david-bader-selected-receive-2021-ieee-computer-society-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Read further at news source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spotonnewjersey.com/gateway/823764/njits-david-bader-selected-to-receive.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spotonnewjersey.com/gateway/823764/njits-david-bader-selected-to-receive.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Selected to Receive the 2021 IEEE Computer Society Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210922-sidneyfernbach/</link>
      <pubDate>Wed, 22 Sep 2021 21:44:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210922-sidneyfernbach/</guid>
      <description>&lt;p&gt;LOS ALAMITOS, Calif., 22 September 2021 – The IEEE Computer Society (IEEE CS) has named &lt;strong&gt;David Bader&lt;/strong&gt; as the recipient of the 2021 &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt;.  Bader is a Distinguished Professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;Established in 1992 in memory of high-performance computing pioneer Sidney Fernbach, the Sidney Fernbach Award recognizes outstanding contributions in the application of high-performance computers using innovative approaches.  Bader was cited for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;
&lt;p&gt;“David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds,” said Satoshi Matsuoka, director of RIKEN Center for Computational Science. “As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;“Today, 100% of the Top 500 supercomputers in the world are Linux HPC systems, based on Bader’s technical contributions and leadership.  This is one of the most significant technical foundations of HPC,” noted Steve Wallach, a guest scientist for Los Alamos National Laboratory and 2008 IEEE CS Seymour Cray Computer Engineering Award recipient.&lt;/p&gt;
&lt;p&gt;Specifically, the Fernbach Award recognizes Bader’s contributions, in the following areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated that a Linux OS-based machine can be a production supercomputer with matching performance and utility, by integrating a high-performance scalable interconnection network, system services such as scalable booting methodology, system software including free and commercial compiler suites, high-utilization job schedulers, and diagnostic monitoring.&lt;/li&gt;
&lt;li&gt;Developed scalable discrete algorithms for problems with irregular data structures, lack of locality, and unpredictable memory traces on Linux supercomputers.&lt;/li&gt;
&lt;li&gt;Started and maintained the Graph500 ranking that influenced the HPC community to look beyond LINPACK as the sole performance ranking metric.&lt;/li&gt;
&lt;li&gt;Attacked real-world problems such as conducting the first study of Twitter using streaming parallel graph algorithms to identify “important actors” during epidemics and disasters and applying streaming graph analysis to detect insider threats in corporate networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viktor Prasanna, Charles Lee Powell Chair in Engineering and Professor of Electrical Engineering and Professor of Computer Science at the University of Southern California, summarized, “Professor Bader has developed innovative techniques to explore HPC for many challenging application areas where graph based techniques are being used. This has been an exciting research direction that requires advances in platform architecture, software systems, and parallelization. This award recognizes Professor Bader’s work in building such software as well as in developing scalable techniques for graph analytics.”&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics.  He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC.  Bader is editor-in-chief of the &lt;em&gt;ACM Transactions on Parallel Computing&lt;/em&gt;, and General Co-Chair of IPDPS 2021, and previously served as editor-in-chief of the &lt;em&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).  Other notable awards he has received include recognition in the 2021 ROI-NJ inaugural list of technology influencers, 2014 Outstanding Senior Faculty Research Award from Georgia Tech, and 2012 inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award. Recently, Bader received an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach award consists of a certificate and a $2,000 honorarium. The award will be presented to Bader at the SC21 Conference awards plenary session in St. Louis, Missouri, on Tuesday morning, 16 November 2021.&lt;/p&gt;
&lt;h2 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h2&gt;
&lt;p&gt;The IEEE Computer Society is the world’s home for computer science, engineering, and technology. A global leader in providing access to computer science research, analysis, and information, the IEEE Computer Society offers a comprehensive array of unmatched products, services, and opportunities for individuals at all stages of their professional careers. Known as the premier organization that empowers the people who drive technology, the IEEE Computer Society offers international conferences, peer-reviewed publications, a unique digital library, and training programs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computer.org/press-room/2021-news/david-bader-to-receive-2021-ieee-cs-sidney-fernbach-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT&#39;s David Bader Selected to Receive the 2021 IEEE Computer Society Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210922-njit/</link>
      <pubDate>Wed, 22 Sep 2021 20:06:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210922-njit/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210922-njit/DavidBader2_hu_bdbeee4b4f1a5a78.webp 400w,
               /blog/20210922-njit/DavidBader2_hu_a154d3b10654dc55.webp 760w,
               /blog/20210922-njit/DavidBader2_hu_35dc1ffe45096305.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210922-njit/DavidBader2_hu_bdbeee4b4f1a5a78.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The IEEE Computer Society (IEEE CS) has named &lt;strong&gt;David Bader&lt;/strong&gt; as the recipient of the 2021 &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt;.  Bader is a Distinguished Professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at the New Jersey Institute of Technology.&lt;/p&gt;
&lt;p&gt;Established in 1992 in memory of high-performance computing pioneer Sidney Fernbach, the Sidney Fernbach Award recognizes outstanding contributions in the application of high-performance computers using innovative approaches.  Bader was cited for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;
&lt;p&gt;“David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds,” said Satoshi Matsuoka, director of RIKEN Center for Computational Science. “As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;“Today, 100% of the Top 500 supercomputers in the world are Linux HPC systems, based on Bader’s technical contributions and leadership.  This is one of the most significant technical foundations of HPC,” noted Steve Wallach, a guest scientist for Los Alamos National Laboratory and 2008 IEEE CS Seymour Cray Computer Engineering Award recipient.&lt;/p&gt;
&lt;p&gt;Specifically, the Fernbach Award recognizes Bader’s contributions, in the following areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated that a Linux OS-based machine can be a production supercomputer with matching performance and utility, by integrating a high-performance scalable interconnection network, system services such as scalable booting methodology, system software including free and commercial compiler suites, high-utilization job schedulers, and diagnostic monitoring.&lt;/li&gt;
&lt;li&gt;Developed scalable discrete algorithms for problems with irregular data structures, lack of locality, and unpredictable memory traces on Linux supercomputers.&lt;/li&gt;
&lt;li&gt;Started and maintained the Graph500 ranking that influenced the HPC community to look beyond LINPACK as the sole performance ranking metric.&lt;/li&gt;
&lt;li&gt;Attacked real-world problems such as conducting the first study of Twitter using streaming parallel graph algorithms to identify “important actors” during epidemics and disasters and applying streaming graph analysis to detect insider threats in corporate networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viktor Prasanna, Charles Lee Powell Chair in Engineering and Professor of Electrical Engineering and Professor of Computer Science at the University of Southern California, summarized, “Professor Bader has developed innovative techniques to explore HPC for many challenging application areas where graph based techniques are being used. This has been an exciting research direction that requires advances in platform architecture, software systems, and parallelization. This award recognizes Professor Bader’s work in building such software as well as in developing scalable techniques for graph analytics.”&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics.  He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC.  Bader is editor-in-chief of the &lt;em&gt;ACM Transactions on Parallel Computing&lt;/em&gt;, and General Co-Chair of IPDPS 2021, and previously served as editor-in-chief of the &lt;em&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).  Other notable awards he has received include recognition in the 2021 ROI-NJ inaugural list of technology influencers, 2014 Outstanding Senior Faculty Research Award from Georgia Tech, and 2012 inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award. Recently, Bader received an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach award consists of a certificate and a $2,000 honorarium. The award will be presented to Bader at the SC21 Conference awards plenary session in St. Louis, Missouri, on Tuesday morning, 16 November 2021.&lt;/p&gt;
&lt;h2 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h2&gt;
&lt;p&gt;The IEEE Computer Society is the world’s home for computer science, engineering, and technology. A global leader in providing access to computer science research, analysis, and information, the IEEE Computer Society offers a comprehensive array of unmatched products, services, and opportunities for individuals at all stages of their professional careers. Known as the premier organization that empowers the people who drive technology, the IEEE Computer Society offers international conferences, peer-reviewed publications, a unique digital library, and training programs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njits-david-bader-selected-receive-2021-ieee-computer-society-sidney-fernbach-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njits-david-bader-selected-receive-2021-ieee-computer-society-sidney-fernbach-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Selected to Receive the 2021 IEEE Computer Society Sidney Fernbach Award</title>
      <link>http://localhost:1313/blog/20210922-hpcwire/</link>
      <pubDate>Wed, 22 Sep 2021 06:30:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210922-hpcwire/</guid>
      <description>&lt;p&gt;LOS ALAMITOS, Calif., 22 September 2021 – The IEEE Computer Society (IEEE CS) has named &lt;strong&gt;David Bader&lt;/strong&gt; as the recipient of the 2021 &lt;a href=&#34;https://www.computer.org/volunteering/awards/fernbach&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sidney Fernbach Award&lt;/a&gt;.  Bader is a Distinguished Professor and founder of the Department of Data Science, and inaugural Director of the Institute for Data Science, at the New Jersey Institute of Technology.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210922-hpcwire/david_bader-300x294_hu_2bb020ba9a02f212.webp 400w,
               /blog/20210922-hpcwire/david_bader-300x294_hu_ac7e4cd4b084ac83.webp 760w,
               /blog/20210922-hpcwire/david_bader-300x294_hu_1b01e08ca120347b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210922-hpcwire/david_bader-300x294_hu_2bb020ba9a02f212.webp&#34;
               width=&#34;300&#34;
               height=&#34;294&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Established in 1992 in memory of high-performance computing pioneer Sidney Fernbach, the Sidney Fernbach Award recognizes outstanding contributions in the application of high-performance computers using innovative approaches.  Bader was cited for the development of Linux-based massively parallel production computers and for pioneering contributions to scalable discrete parallel algorithms for real-world applications.&lt;/p&gt;
&lt;p&gt;“David has expanded the realm of supercomputing from narrow sets of technical computing to be the leading edge of mainstream computing we see today in massive cluster-based supercomputers such as Fugaku, as well as hyperscaler clouds,” said Satoshi Matsuoka, director of RIKEN Center for Computational Science. “As supercomputing progresses onwards, we should further continue to observe other elements in which David has contributed to their genesis.”&lt;/p&gt;
&lt;p&gt;“Today, 100% of the Top 500 supercomputers in the world are Linux HPC systems, based on Bader’s technical contributions and leadership.  This is one of the most significant technical foundations of HPC,” noted Steve Wallach, a guest scientist for Los Alamos National Laboratory and 2008 IEEE CS Seymour Cray Computer Engineering Award recipient.&lt;/p&gt;
&lt;p&gt;Specifically, the Fernbach Award recognizes Bader’s contributions, in the following areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated that a Linux OS-based machine can be a production supercomputer with matching performance and utility, by integrating a high-performance scalable interconnection network, system services such as scalable booting methodology, system software including free and commercial compiler suites, high-utilization job schedulers, and diagnostic monitoring.&lt;/li&gt;
&lt;li&gt;Developed scalable discrete algorithms for problems with irregular data structures, lack of locality, and unpredictable memory traces on Linux supercomputers.&lt;/li&gt;
&lt;li&gt;Started and maintained the Graph500 ranking that influenced the HPC community to look beyond LINPACK as the sole performance ranking metric.&lt;/li&gt;
&lt;li&gt;Attacked real-world problems such as conducting the first study of Twitter using streaming parallel graph algorithms to identify “important actors” during epidemics and disasters and applying streaming graph analysis to detect insider threats in corporate networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Viktor Prasanna, Charles Lee Powell Chair in Engineering and Professor of Electrical Engineering and Professor of Computer Science at the University of Southern California, summarized, “Professor Bader has developed innovative techniques to explore HPC for many challenging application areas where graph based techniques are being used. This has been an exciting research direction that requires advances in platform architecture, software systems, and parallelization. This award recognizes Professor Bader’s work in building such software as well as in developing scalable techniques for graph analytics.”&lt;/p&gt;
&lt;p&gt;Bader’s interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics.  He has served as a lead scientist in several DARPA programs. He has co-authored over 300 scholarly papers and has best paper awards from ISC, IEEE HPEC, and ACM/IEEE SC.  Bader is editor-in-chief of the &lt;em&gt;ACM Transactions on Parallel Computing&lt;/em&gt;, and General Co-Chair of IPDPS 2021, and previously served as editor-in-chief of the &lt;em&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Bader is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI) and Future Advanced Computing Ecosystem (FACE).  Other notable awards he has received include recognition in the 2021 ROI-NJ inaugural list of technology influencers, 2014 Outstanding Senior Faculty Research Award from Georgia Tech, and 2012 inaugural recipient of University of Maryland’s Electrical and Computer Engineering Distinguished Alumni Award. Recently, Bader received an NVIDIA AI Lab (NVAIL) award, and a Facebook Research AI Hardware/Software Co-Design award.&lt;/p&gt;
&lt;p&gt;The Sidney Fernbach award consists of a certificate and a $2,000 honorarium. The award will be presented to Bader at the &lt;a href=&#34;https://sc21.supercomputing.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SC21 Conference&lt;/a&gt; awards plenary session in St. Louis, Missouri, on Tuesday morning, 16 November 2021.&lt;/p&gt;
&lt;h2 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h2&gt;
&lt;p&gt;The IEEE Computer Society is the world’s home for computer science, engineering, and technology. A global leader in providing access to computer science research, analysis, and information, the IEEE Computer Society offers a comprehensive array of unmatched products, services, and opportunities for individuals at all stages of their professional careers. Known as the premier organization that empowers the people who drive technology, the IEEE Computer Society offers international conferences, peer-reviewed publications, a unique digital library, and training programs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/david-bader-selected-to-receive-the-2021-ieee-computer-society-sidney-fernbach-award/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/david-bader-selected-to-receive-the-2021-ieee-computer-society-sidney-fernbach-award/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TED Talk At NJIT: Impact Of Tech In A Resurgent, Post-COVID World</title>
      <link>http://localhost:1313/blog/20210914-patch/</link>
      <pubDate>Tue, 14 Sep 2021 20:20:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210914-patch/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Eric Kiefer, Patch Staff&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;NEWARK, NJ — &lt;em&gt;The following news release comes courtesy of NJIT. Learn more about &lt;a href=&#34;https://patch.com/new-jersey/montclair/want-post-patch-its-easy-1-2-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;posting announcements or events&lt;/a&gt; to your local Patch site.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Speakers at TEDxNJIT 2021 will explain how technology impacts everything from knee-replacement surgery and the monitoring of traumatic brain injuries to how we&amp;rsquo;ll live in the wake of the global pandemic.&lt;/p&gt;
&lt;p&gt;Other speakers — including leaders from the Federal Aviation Administration and Los Alamos National Laboratory — will mull commercial space exploration, equitable economic growth, entrepreneurship, epidemic forecasting and the role of immigrants in developing vaccines for COVID-19 — all in under 18 minutes. The theme of &lt;a href=&#34;https://tedxnjit.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this year&amp;rsquo;s TEDxNJIT&lt;/a&gt; — the university&amp;rsquo;s 11th — is resurgence.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The organizing committee thought that resurgence was appropriate for the year when the world was emerging from the pandemic,&amp;rdquo; explained Raja Roy, chair of the committee and an assistant professor at NJIT&amp;rsquo;s &lt;a href=&#34;https://management.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Martin Tuchman School of Management&lt;/a&gt;. &amp;ldquo;It was also sufficiently broad and gave speakers the liberty to design their talk based on resurgence in their field.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The event will take place online Sept. 22 from 6 to 8 p.m. It&amp;rsquo;s free but you need to register in advance here: &lt;a href=&#34;https://www.eventbrite.com/e/tedxnjit-2021-tickets-159181525227&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eventbrite.com/e/tedxnjit-2021-tickets-159181525227&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;More than half of the speakers are connected to NJIT. Here&amp;rsquo;s a closer look at their credentials and talks.&lt;/p&gt;
&lt;p&gt;Julie Ancis&lt;br&gt;
Professor and Founding Director of Cyberpsychology&lt;br&gt;
NJIT&lt;br&gt;
The Post-Pandemic Future: Are We Ready?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Distinguished Professor and Director of the Institute for Data Science&lt;br&gt;
NJIT&lt;br&gt;
Solving Global Grand Challenges with High Performance Data Analytics&lt;/p&gt;
&lt;p&gt;James Barrood&lt;br&gt;
Founder and CEO&lt;br&gt;
Innovation+&lt;br&gt;
How Immigrants Are Helping to Save the World&lt;/p&gt;
&lt;p&gt;Rachel Benyola&lt;br&gt;
Executive and Founder Coach&lt;br&gt;
RKB Consulting&lt;br&gt;
Innovating the Entrepreneur: Resurgence of the Human Entrepreneur&lt;/p&gt;
&lt;p&gt;Robert Cohen &amp;lsquo;83 &amp;lsquo;84 &amp;lsquo;87&lt;br&gt;
President of Digital, Robotics and Enabling Technologies&lt;br&gt;
Stryker Corp.&lt;br&gt;
Chair, NJIT Board of Directors&lt;br&gt;
NJIT Alumnus&lt;br&gt;
New Era in Medical Device Design&lt;/p&gt;
&lt;p&gt;Ken Davidian&lt;br&gt;
Director of Research&lt;br&gt;
Federal Aviation Administration Office of Commercial Space Transportation&lt;br&gt;
Commercial Space Today: A Resurgence Yes, but of What?&lt;/p&gt;
&lt;p&gt;Sara Del Valle &amp;lsquo;00 M.S. &amp;lsquo;01&lt;br&gt;
Deputy Group Leader of Information Systems and Modeling Group, A-1&lt;br&gt;
Los Alamos National Laboratory&lt;br&gt;
NJIT Alumna&lt;br&gt;
Could We Someday Forecast Diseases Like We Forecast the Weather?&lt;/p&gt;
&lt;p&gt;Bernel Hall&lt;br&gt;
President and CEO&lt;br&gt;
Invest Newark&lt;br&gt;
Urban Legend: Debunking the Myth of Equitable Economic Growth to Finally Achieve the Society that We&amp;rsquo;ve Been Promised&lt;/p&gt;
&lt;p&gt;Yashwee Kothari &amp;lsquo;22&lt;br&gt;
Computer Science Major&lt;br&gt;
NJIT&lt;br&gt;
The Pain of the Silent&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.facebook.com/NewarkNJPatch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Don&amp;rsquo;t forget to visit the Patch Newark Facebook page.&lt;/a&gt; Send local news tips and correction requests to &lt;a href=&#34;mailto:eric.kiefer@patch.com&#34;&gt;eric.kiefer@patch.com&lt;/a&gt;. &lt;a href=&#34;https://patch.com/subscribe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sign up for Patch email newsletters&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://patch.com/new-jersey/newarknj/ted-talk-njit-impact-tech-resurgent-post-covid-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://patch.com/new-jersey/newarknj/ted-talk-njit-impact-tech-resurgent-post-covid-world&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TEDxNJIT 2021: The Impact of Tech in a Resurgent Post-Pandemic World</title>
      <link>http://localhost:1313/blog/20210908-njit/</link>
      <pubDate>Wed, 08 Sep 2021 15:46:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210908-njit/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210908-njit/tedx-2021_hu_8d57586b75ef4efb.webp 400w,
               /blog/20210908-njit/tedx-2021_hu_7cffeb388118fbad.webp 760w,
               /blog/20210908-njit/tedx-2021_hu_3239f894d5b19575.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210908-njit/tedx-2021_hu_8d57586b75ef4efb.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Speakers at TEDxNJIT 2021 will explain how technology impacts everything from knee-replacement surgery and the monitoring of traumatic brain injuries to how we’ll live in the wake of the global pandemic.&lt;/p&gt;
&lt;p&gt;Other speakers — including leaders from the Federal Aviation Administration and Los Alamos National Laboratory — will mull commercial space exploration, equitable economic growth, entrepreneurship, epidemic forecasting and the role of immigrants in developing vaccines for COVID-19 — all in under 18 minutes. The theme of &lt;a href=&#34;https://tedxnjit.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this year’s TEDxNJIT&lt;/a&gt; — the university’s 11th — is resurgence.&lt;/p&gt;
&lt;p&gt;“The organizing committee thought that resurgence was appropriate for the year when the world was emerging from the pandemic,” explained Raja Roy, chair of the committee and an assistant professor at NJIT’s &lt;a href=&#34;https://management.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Martin Tuchman School of Management&lt;/a&gt;. “It was also sufficiently broad and gave speakers the liberty to design their talk based on resurgence in their field.”&lt;/p&gt;
&lt;p&gt;The event will take place online Sept. 22 from 6 to 8 p.m. It’s free but you need to register in advance here: &lt;a href=&#34;https://www.eventbrite.com/e/tedxnjit-2021-tickets-159181525227&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eventbrite.com/e/tedxnjit-2021-tickets-159181525227&lt;/a&gt;. More than half of the speakers are connected to NJIT. Here’s a closer look at their credentials and talks.&lt;/p&gt;
&lt;p&gt;Julie Ancis&lt;br&gt;
Professor and Founding Director of Cyberpsychology&lt;br&gt;
NJIT&lt;br&gt;
&lt;em&gt;The Post-Pandemic Future: Are We Ready?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Distinguished Professor and Director of the Institute for Data Science&lt;br&gt;
NJIT&lt;br&gt;
&lt;em&gt;Solving Global Grand Challenges with High Performance Data Analytics&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;James Barrood&lt;br&gt;
Founder and CEO&lt;br&gt;
Innovation+&lt;br&gt;
&lt;em&gt;How Immigrants Are Helping to Save the World&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Rachel Benyola&lt;br&gt;
Executive and Founder Coach&lt;br&gt;
RKB Consulting&lt;br&gt;
&lt;em&gt;Innovating the Entrepreneur: Resurgence of the Human Entrepreneur&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Robert Cohen ’83 ’84 ’87&lt;br&gt;
President of Digital, Robotics and Enabling Technologies&lt;br&gt;
Stryker Corp.&lt;br&gt;
Chair, NJIT Board of Directors&lt;br&gt;
NJIT Alumnus&lt;br&gt;
&lt;em&gt;New Era in Medical Device Design&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ken Davidian&lt;br&gt;
Director of Research&lt;br&gt;
Federal Aviation Administration Office of Commercial Space Transportation&lt;br&gt;
&lt;em&gt;Commercial Space Today: A Resurgence Yes, but of What?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Sara Del Valle ’00 M.S. ’01&lt;br&gt;
Deputy Group Leader of Information Systems and Modeling Group, A-1&lt;br&gt;
Los Alamos National Laboratory&lt;br&gt;
NJIT Alumna&lt;br&gt;
&lt;em&gt;Could We Someday Forecast Diseases Like We Forecast the Weather?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Bernel Hall&lt;br&gt;
President and CEO&lt;br&gt;
Invest Newark&lt;br&gt;
&lt;em&gt;Urban Legend: Debunking the Myth of Equitable Economic Growth to Finally Achieve the Society that We’ve Been Promised&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yashwee Kothari ’22&lt;br&gt;
Computer Science Major&lt;br&gt;
NJIT&lt;br&gt;
&lt;em&gt;The Pain of the Silent&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/tedxnjit-2021-impact-tech-post-pandemic-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/tedxnjit-2021-impact-tech-post-pandemic-world&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Launches a Campus Chapter of the National Academy of Inventors</title>
      <link>http://localhost:1313/blog/20210804-njit/</link>
      <pubDate>Wed, 04 Aug 2021 10:20:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210804-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Tracey Regan&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-inaugural-members-of-the-newly-founded-njit-chapter-of-the-national-academy-of-inventors&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Inaugural members of the newly founded NJIT chapter of the National Academy of Inventors&#34; srcset=&#34;
               /blog/20210804-njit/NJIT-NAIGroup-02-Edit_hu_5501ef61ab058c41.webp 400w,
               /blog/20210804-njit/NJIT-NAIGroup-02-Edit_hu_5d4d9aded847842d.webp 760w,
               /blog/20210804-njit/NJIT-NAIGroup-02-Edit_hu_becf6ad7cb945ed3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210804-njit/NJIT-NAIGroup-02-Edit_hu_5501ef61ab058c41.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Inaugural members of the newly founded NJIT chapter of the National Academy of Inventors
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In conjunction with the 2021 Summer Research Symposium, NJIT launched a campus chapter of the National Academy of Inventors (NAI), an organization founded in 2010 to recognize and promote academic technology and innovation and to encourage its translation into devices and services that benefit society.&lt;/p&gt;
&lt;p&gt;NJIT inducted 32 faculty members at the ceremony, as well as nine honorary members, including President Joel Bloom and Provost Fadi Deek. Chapter members must hold patents issued from the U.S. Patent and Trademark Office (USPTO).&lt;/p&gt;
&lt;p&gt;Atam Dhawan, senior vice provost for research, called the chapter’s inauguration “a giant step forward in promoting research and innovation towards entrepreneurship and technology translation.”&lt;/p&gt;
&lt;p&gt;The chapter will promote translational research and its commercialization through campus R&amp;amp;D programs, grants, clubs and acceleration programs; offer invention-focused networking and educational activities, such as workshops and seminars on innovation and intellectual property development; and provide mentoring and advising services to faculty and student inventors on further development of IP assets.&lt;/p&gt;
&lt;p&gt;The university will recognize inventors and their achievements through events such as annual NAI-NJIT chapter awards and member recognition workshops, Innovation Day and the Summer Research Symposium.&lt;/p&gt;
&lt;p&gt;“NAI seeks to raise the importance of invention – without which we would not have modern communication networks, cures for diseases or new sources of carbon-free energy – and to help build networks and resources that will further this work,” said Dhawan, president of the NJIT-NAI chapter. “This is a strategic priority of NJIT as well, with its emphasis on applied research and technology translation.”&lt;/p&gt;
&lt;p&gt;In the keynote speech at the event, Elizabeth Daugherty, the outreach director for the USPTO’s eastern region and a board member of the NAI, spoke of the power of inventions to create the future.&lt;/p&gt;
&lt;p&gt;“Through these doors comes science and technology that we cannot contemplate today,” Daugherty said of the USPTO. “And it will come from inventors and innovators including you.”&lt;/p&gt;
&lt;p&gt;She added, however, “We need a higher percentage of our population to participate … we must also identify proper role models for young people so they will look to futures in engineering and tech, especially when it comes to women. This is where an NAI chapter can bring so much value to a university. Women constitute over half the population of the U.S., but their participation in STEM jobs and the IP system lacks far behind their male counterparts.”&lt;/p&gt;
&lt;p&gt;Shortly after its founding, the NAI established a Fellows program to highlight academic inventors who have demonstrated “a prolific spirit of innovation in creating or facilitating outstanding inventions that have made a tangible impact on quality of life, economic development and the welfare of society.” New fellows are inducted each year at a ceremony at USPTO headquarters in Alexandria, Virginia, where they share experiences and make new connections.&lt;/p&gt;
&lt;p&gt;Since the first year in 2014, nine NJIT inventors have been named fellows:&lt;/p&gt;
&lt;p&gt;2020: &lt;strong&gt;Rajesh Davé&lt;/strong&gt;, a chemical and materials engineer whose groundbreaking methods for re-engineering tiny particles have fueled advances in such diverse areas as weapons safety and drug delivery systems; &lt;strong&gt;MengChu Zhou&lt;/strong&gt;, an electrical and computer engineer and pioneer in automation science and engineering who optimizes systems, from manufacturing, to data centers, to transportation, to glean efficiencies and improve outcomes&lt;/p&gt;
&lt;p&gt;2019: &lt;strong&gt;Nirwan Ansari ’82&lt;/strong&gt;, an electrical and computer engineer whose research on telecoms networks formed part of the backbone for broadband access and later FIOS networks&lt;/p&gt;
&lt;p&gt;2018: &lt;strong&gt;Craig Gotsman&lt;/strong&gt;, a computer scientist who invented a number of cutting-edge software technologies for manipulating 3D geometric data, enabling their use in a variety of applications&lt;/p&gt;
&lt;p&gt;2017: &lt;strong&gt;Yun-Qing Shi&lt;/strong&gt;, an electric and computer engineer best known for devising methods to hide and retrieve data embedded in digitized images and speech&lt;/p&gt;
&lt;p&gt;2016: &lt;strong&gt;Kamalesh Sirkar&lt;/strong&gt;, a chemical engineer known for his innovations in industrial membrane technology used to separate and purify air, water and waste streams and to improve the quality of manufactured products such as pharmaceuticals, solvents and nanoparticles&lt;/p&gt;
&lt;p&gt;2015: &lt;strong&gt;Atam Dhawan&lt;/strong&gt;, an electrical and computer engineer whose patent on low-angle transillumination technology for the examination of skin lesions led to the formation of two companies with products that are now being used to treat spider vein diseases and to non-invasively diagnose skin cancers; &lt;strong&gt;Som Mitra&lt;/strong&gt;, a chemist and environmental scientist with patents in areas ranging from trace measurements to diverse nanotechnology applications ranging from flexible batteries, to solar cells, to seawater desalination&lt;/p&gt;
&lt;p&gt;2014: &lt;strong&gt;Gordon Thomas&lt;/strong&gt;, a physicist and diverse researcher whose inventions range from medical devices, to weapons sensors, to optical communications fiber&lt;/p&gt;
&lt;p&gt;Senior Members:&lt;/p&gt;
&lt;p&gt;2020: &lt;strong&gt;Eon Soo Lee&lt;/strong&gt;, a mechanical and industrial engineer who develops microfluidic biosensors to detect diseases&lt;/p&gt;
&lt;p&gt;2019: &lt;strong&gt;Sagnik Basuray&lt;/strong&gt;, a chemical and materials engineer who develops novel sensors, diagnostic devices and drug delivery systems&lt;/p&gt;
&lt;p&gt;2019: &lt;strong&gt;Bipin Rajendran&lt;/strong&gt;, an electrical and computer engineer who develops computing systems that aim to match the efficiency seen in nature by studying the organizational principles of the brain&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-launches-campus-chapter-national-academy-inventors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-launches-campus-chapter-national-academy-inventors&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader inducted as Honorary Member of the National Academy of Inventors</title>
      <link>http://localhost:1313/blog/20210730-nai/</link>
      <pubDate>Fri, 30 Jul 2021 18:43:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210730-nai/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210730-nai/NAI_hu_b57bdc8642e28f58.webp 400w,
               /blog/20210730-nai/NAI_hu_e152e423bdadfe00.webp 760w,
               /blog/20210730-nai/NAI_hu_d8151e3a8df79db9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210730-nai/NAI_hu_b57bdc8642e28f58.webp&#34;
               width=&#34;760&#34;
               height=&#34;601&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>The Circle of Omicron Delta Kappa</title>
      <link>http://localhost:1313/blog/20210723-odk/</link>
      <pubDate>Fri, 23 Jul 2021 17:00:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210723-odk/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; (University of Maryland, College Park, 1995) was recently selected as a Fellow of the Society for Industrial and Applied Mathematics.&lt;/p&gt;
&lt;p&gt;The Circle of Omicron Delta Kappa, Summer 2021, Volume 99, Number 2, Page 7.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who&#39;s Who in Tau Beta Pi</title>
      <link>http://localhost:1313/blog/20210628-taubetapi/</link>
      <pubDate>Mon, 28 Jun 2021 15:26:54 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210628-taubetapi/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210628-taubetapi/TBP-Bader_hu_65b6d3b9c6f67e53.webp 400w,
               /blog/20210628-taubetapi/TBP-Bader_hu_f667faae7273de96.webp 760w,
               /blog/20210628-taubetapi/TBP-Bader_hu_34f13aae4c5b83c0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210628-taubetapi/TBP-Bader_hu_65b6d3b9c6f67e53.webp&#34;
               width=&#34;681&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Ph.D.&lt;br&gt;
&lt;em&gt;Pennsylvania Alpha &amp;lsquo;90&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;has been elected Fellow of the Society for Industrial and Applied Mathematics. He is a distinguished professor in the department of computer science and inaugural Director of the Institute for Data Science at New Jersey Institute of Technology. He is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Creates New Department of Data Science</title>
      <link>http://localhost:1313/blog/20210610-njit/</link>
      <pubDate>Thu, 10 Jun 2021 20:28:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210610-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Dean Mudgett&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210610-njit/GITC_hu_9574b66716ac0f3c.webp 400w,
               /blog/20210610-njit/GITC_hu_64108243b93f90c3.webp 760w,
               /blog/20210610-njit/GITC_hu_e6c1b309b1516e94.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210610-njit/GITC_hu_9574b66716ac0f3c.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Building on existing academic and research strengths, NJIT&amp;rsquo;s Ying Wu College of Computing has created a new &lt;a href=&#34;http://ds.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Data Science&lt;/a&gt; where students will learn the deep underlying technologies driving this field and its broad spectrum of applications.&lt;/p&gt;
&lt;p&gt;The college has taken significant steps in recent years to position itself as a leader in data science education and research. It already offers an M.S. in Data Science, as well as graduate-level certificate programs in Big Data, Data Visualization and Data Mining. Starting in the Fall 2021 semester, the college will offer a B.S. in Data Science, opening the door to this growing career field to interested undergraduate students. The new department will house all of these degrees, as well as a future Ph.D. program.&lt;/p&gt;
&lt;p&gt;The department will complement NJIT’s &lt;a href=&#34;http://datascience.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt;, which focuses on multidisciplinary research and training of technology leaders to solve global challenges involving data and its analysis. The Institute looks to develop data-driven technologies through collaborations with NJIT’s existing research centers in Big Data, Medical Informatics and Cybersecurity and a future center for Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;Distinguished Professor and Director of the Institute of Data Science &lt;strong&gt;David Bader&lt;/strong&gt; sees the establishment of a department of data science and the addition of an undergraduate degree in data science as important next steps for NJIT’s leadership in the field.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Data science is critical to national competitiveness, which is why we launched the Institute for Data Science and created our master’s and graduate certificate programs. The National Science Foundation in a recent report stated, ‘The ability to manipulate data and understand data science is becoming increasingly critical to current and future discovery and innovation’,&amp;rdquo; Bader said. &amp;ldquo;Establishing a department of data science to house our degree and certificate programs makes perfect sense and will give NJIT students across all degree levels an opportunity to explore this exciting and dynamic field.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;NJIT is located in the heart of the region where data scientist jobs are most needed in the United States. Strategic investment in data science education presents significant financial benefit for the tri-state region. According to the U.S. Bureau of Labor Statistics, May 2020 Occupational Employment and Wages Report, the New York-Newark-Jersey City metropolitan region has the highest number of jobs for data scientists and mathematical scientists in the nation, with 4,730 employees and an annual mean wage of $129,250.&lt;/p&gt;
&lt;p&gt;“The field of data science is growing quickly, and the demand for talent is especially significant in the fast-growing New York/New Jersey metropolitan area,” said College of Computing Dean Craig Gotsman. “It is our duty and our privilege to generate the tech talent to meet this demand. Creating a new department solely focused on data science will add an exciting new dimension to the College of Computing. It will help us deliver the best possible academic experience to students interested in this fascinating field of study, and also help the college attract leading data scientists to our already talented and expert faculty.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/college-computing-creates-new-department-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/college-computing-creates-new-department-data-science&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Future of Supercomputers: Democratization Is Critical</title>
      <link>http://localhost:1313/blog/20210604-informationweek/</link>
      <pubDate>Fri, 04 Jun 2021 13:24:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210604-informationweek/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Distinguished Professor and Director of Institute for Data Science, New Jersey Institute of Technology&lt;/p&gt;


















&lt;figure  id=&#34;figure-credit-vladimircaribb-via-adobe-stock&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Credit: vladimircaribb via Adobe Stock&#34; srcset=&#34;
               /blog/20210604-informationweek/supercomputers-vladimircaribb-adobe-cp_hu_59dcb069d9a5827c.webp 400w,
               /blog/20210604-informationweek/supercomputers-vladimircaribb-adobe-cp_hu_15f22ef03920105d.webp 760w,
               /blog/20210604-informationweek/supercomputers-vladimircaribb-adobe-cp_hu_a55dd16aef2fe204.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210604-informationweek/supercomputers-vladimircaribb-adobe-cp_hu_59dcb069d9a5827c.webp&#34;
               width=&#34;489&#34;
               height=&#34;350&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Credit: vladimircaribb via Adobe Stock
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Supercomputing technology has indelibly changed how we approach complex issues in our world, from weather forecasting and climate modeling to protecting of the security of our nation from cyberattacks. All of the world’s most capable supercomputers now run on Linux, and with the 30th anniversary of the creation of Linux fast approaching this summer, it’s an important moment to consider how the US can strengthen its advanced cyberinfrastructure and invest in the next generation of supercomputers.&lt;/p&gt;
&lt;p&gt;Although supercomputers were once a rarity, these high-performance machines now have a ubiquitous presence in our lives, whether or not we’re aware of it. Everything, from the design of water bottles to accelerating vaccine research for COVID-19, is made possible by the phenomenal capabilities of supercomputers. The ability of these machines to model and solve complex problems has become an essential backbone of global invention and innovation, providing economic benefits as well as performing important scientific breakthroughs. Yet as future emergencies and problems become more unpredictable and more complex, the technology &amp;ndash; and especially American supercomputers &amp;ndash; must catch up to the global competition. To truly improve our national competitiveness, we must increase investment into strategic computing technologies and make significant efforts to democratize the use of supercomputers.&lt;/p&gt;
&lt;h2 id=&#34;revolutionary-leap-forward&#34;&gt;Revolutionary Leap Forward&lt;/h2&gt;
&lt;p&gt;Decades ago, the Linux supercomputing movement was a revolutionary leap forward from available computing technologies. I built the first Linux supercomputer, named Roadrunner, for about $400,000. Earlier attempts at clusters of Linux PCs, such as Beowulf, existed, but they lacked important system components that distinguishes supercomputers from a pile of computers. While Beowulf clusters could solve some problems that were neatly divided into independent tasks, the technology didn’t yet achieve fast communication among processors, which was needed to support the large set of scientific applications that run on supercomputers. In contrast, Roadrunner would later become a node on the National Technology Grid, allowing researchers to access supercomputers for large-scale problem-solving from their desktops. The investment into developing Roadrunner quickly proved to be the catalyst for the Linux supercomputing moment, inspiring a new wave of supercomputers created for broader commercial use.&lt;/p&gt;
&lt;p&gt;When Roadrunner went online, it was among the 100 fastest supercomputers in the world. Since then, the technology has only improved, and winning the global competition to build the top-ranked supercomputer has only intensified. Governments around the world have increased investment into developing state-of-the-art computing in order to compete with other countries. A symbolic representation of the global race, the Top500 list ranks the world’s fastest and most powerful supercomputers and reveals which countries recognize the importance of having a strong supercomputing infrastructure. While the technical capabilities of the ranked machines are certainly impressive on their own, make no mistake: they are indicators of the economic, military, and commercial capabilities of the countries represented. As the US Council on Competitiveness has said, “the country that wants to outcompete, must outcompute.”&lt;/p&gt;
&lt;p&gt;When it comes to performing complex scientific tasks, supercomputing technology proves to be invaluable. Issues at the nexus of nature and civilization, such as the COVID-19 pandemic, will always be of relevance to researchers and will always require cutting-edge tools. In a recent study, a team of researchers, including my colleagues at New Jersey Institute of Technology, successfully built models to track the movement of COVID-19 particles in supermarkets; their simulations provide valuable information on how the virus spreads. How were the simulations made? They were made possible thanks to the San Diego Supercomputer Center at University of California-San Diego. Investment drives innovation and even life-saving discoveries.&lt;/p&gt;
&lt;h2 id=&#34;democratization&#34;&gt;Democratization&lt;/h2&gt;
&lt;p&gt;The second step is democratization: the problem-solving capabilities of supercomputers will only improve as more people gain access to and learn to use the technologies. Women and other underrepresented groups in STEM fields currently have limited access to the power of supercomputing, and the high-performance computing field is currently losing out on important perspectives.&lt;/p&gt;
&lt;p&gt;A significant barrier to democratization is one of practicality: working with massive amounts of data, such as 10s of terabytes, usually requires knowledge of and access to high-performance computers. But thanks to an award from the National Science Foundation, my research team is developing new algorithms and software that allow for easier access to high-performance computing. The research project will focus on extending Arkouda, an open-source code library that is used by data scientists at the Department of Defense, and it will start to bridge the gap between ordinary people and high-performance computing technology. When we remove barriers of use and allow more people to interact with these technologies, we can utilize the full capabilities of supercomputers.&lt;/p&gt;
&lt;p&gt;Increasing investment and expanding the user base of supercomputers helps drive innovation and improvement forward in academia, government, and the private sector. If we can’t get advanced supercomputers in the hands of more people, the US will fall behind globally in solving some of tomorrow’s most pressing problems.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210604-informationweek/David_Bader_hu_aaa11f5cbee4bf00.webp 400w,
               /blog/20210604-informationweek/David_Bader_hu_1434967cf90f24f0.webp 760w,
               /blog/20210604-informationweek/David_Bader_hu_57025e1edabb1797.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210604-informationweek/David_Bader_hu_aaa11f5cbee4bf00.webp&#34;
               width=&#34;275&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor in the Department of Computer Science in the Ying Wu College of Computing and Director of the Institute for Data Science at New Jersey Institute of Technology. He is a Fellow of the IEEE, AAAS, and SIAM.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://beta.informationweek.com/security-and-risk-strategy/the-future-of-supercomputers-democratization-is-critical&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://beta.informationweek.com/security-and-risk-strategy/the-future-of-supercomputers-democratization-is-critical&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader awarded the IEEE Computer Society&#39;s Meritorious Service Award</title>
      <link>http://localhost:1313/blog/20210520-ipdps/</link>
      <pubDate>Thu, 20 May 2021 20:00:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210520-ipdps/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210520-ipdps/20210520-Bader-MeritoriousService_hu_235e1b88feb686bb.webp 400w,
               /blog/20210520-ipdps/20210520-Bader-MeritoriousService_hu_46e7164acb717aa4.webp 760w,
               /blog/20210520-ipdps/20210520-Bader-MeritoriousService_hu_df600eb0a1675012.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210520-ipdps/20210520-Bader-MeritoriousService_hu_235e1b88feb686bb.webp&#34;
               width=&#34;760&#34;
               height=&#34;561&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Citation: &amp;ldquo;For service as General Co-Chair for the 2021 International Parallel and Distributed Processing Symposium (IPDPS).&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Biden signs cybersecurity executive order after ransomware attack on fuel pipeline</title>
      <link>http://localhost:1313/blog/20210512-newsnation/</link>
      <pubDate>Wed, 12 May 2021 20:50:19 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210512-newsnation/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Tiffany Hudson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;WASHINGTON (NewsNation Now) — After a series of high-profile ransomware attacks, President Joe Biden signed an executive order Wednesday to strengthen the United States’ cyber defense practices.&lt;/p&gt;
&lt;p&gt;The executive order creates an increased private-public partnership to ensure the U.S. is protected against future attacks, modernizes the cybersecurity practices of the federal government and implements an “energy star” style rating system for software companies.&lt;/p&gt;
&lt;p&gt;“Recent cybersecurity incidents such as SolarWinds, Microsoft Exchange and the Colonial Pipeline incident are a sobering reminder that U.S. public and private sector entities increasingly face sophisticated malicious cyber activity from both nation-state actors and cybercriminals,” said a White House Official.&lt;/p&gt;
&lt;p&gt;Cybersecurity experts have predicted federal officials would be forced to take an increasingly more active role in managing private companies’ cyber defense systems after high-profile breaches and ransomware incidents impacted increasingly significant numbers of people.&lt;/p&gt;
&lt;p&gt;“I think we’re at the point where cybersecurity becomes a first-class citizen, for when we run our businesses and our governments,” said New Jersey Institute of Technology Distinguished Professor and Institute for Data Science director &lt;strong&gt;David Bader&lt;/strong&gt; in a recent interview discussing the Exchange Hack.&lt;/p&gt;
&lt;p&gt;Ransomware incidents have increasingly targeted higher profile targets and key infrastructure. The Colonial Pipeline incident impacted 45% of all the nation’s fuel, while another incident earlier this year could have poisoned an entire city in Florida.&lt;/p&gt;
&lt;p&gt;“The cost of status quo is simply unacceptable. Today the cost of insecure technology is borne at the end. By the victims and incident response and incident response and cleanup, small businesses, schools, hospitals and local governments bear the brunt of these costs,” said a senior administration official.&lt;/p&gt;
&lt;p&gt;The executive order requires IT providers to share information about breaches impacting the federal government and removing any barriers that would prevent someone from getting government assistance in the event of a hack.&lt;/p&gt;
&lt;p&gt;“I think we have to make a greater investment in education as it relates to being able to train and graduate more people proficient in cybersecurity,” said Biden during remarks Wednesday.&lt;/p&gt;
&lt;p&gt;A modernized labeling system for the software companies that rank security and require a baseline protection standard for all software sold to the government, the Biden administration hopes will strengthen the baseline national cybersecurity standards.&lt;/p&gt;
&lt;p&gt;The Biden administration will also create a cybersecurity safety review board that will be government-led with private sector partners and a standard playbook and logging system for when private corporations or the national sector are hacked.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newsnationnow.com/cybersecurity/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newsnationnow.com/cybersecurity/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://fox2now.com/news/national/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fox2now.com/news/national/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wkrn.com/news/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wkrn.com/news/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pennsylvanianewstoday.com/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/138139/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pennsylvanianewstoday.com/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/138139/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kxan.com/news/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.kxan.com/news/biden-signs-cybersecurity-executive-order-after-ransomware-attack-on-fuel-pipeline/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why have ransomware attacks increased and the FBI’s ‘unprecedented’ step to prevent them</title>
      <link>http://localhost:1313/blog/20210511-newsnation/</link>
      <pubDate>Tue, 11 May 2021 19:57:05 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210511-newsnation/</guid>
      <description>

















&lt;figure  id=&#34;figure-a-laptop-displays-a-message-after-being-infected-by-a-ransomware-as-part-of-a-worldwide-cyberattack-on-june-27-2017-in-geldrop--the-unprecedented-global-ransomware-cyberattack-has-hit-more-than-200000-victims-in-more-than-150-countries-europol-executive-director-rob-wainwright-said-may-14-2017-britains-state-run-national-health-service-was-affected-by-the-attack-photo-by-rob-engelaar--anp--afp--netherlands-out-photo-by-rob-engelaaranpafp-via-getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A laptop displays a message after being infected by a ransomware as part of a worldwide cyberattack on June 27, 2017 in Geldrop. – The unprecedented global ransomware cyberattack has hit more than 200,000 victims in more than 150 countries, Europol executive director Rob Wainwright said May 14, 2017. Britain’s state-run National Health Service was affected by the attack. (Photo by Rob Engelaar / ANP / AFP) / Netherlands OUT (Photo by ROB ENGELAAR/ANP/AFP via Getty Images)&#34; srcset=&#34;
               /blog/20210511-newsnation/GettyImages-802363994_hu_dec694f3d745454c.webp 400w,
               /blog/20210511-newsnation/GettyImages-802363994_hu_b0257d127c0c2222.webp 760w,
               /blog/20210511-newsnation/GettyImages-802363994_hu_b70ec5c336e0a8c8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210511-newsnation/GettyImages-802363994_hu_dec694f3d745454c.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A laptop displays a message after being infected by a ransomware as part of a worldwide cyberattack on June 27, 2017 in Geldrop. – The unprecedented global ransomware cyberattack has hit more than 200,000 victims in more than 150 countries, Europol executive director Rob Wainwright said May 14, 2017. Britain’s state-run National Health Service was affected by the attack. (Photo by Rob Engelaar / ANP / AFP) / Netherlands OUT (Photo by ROB ENGELAAR/ANP/AFP via Getty Images)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Tiffany Hudson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;CHICAGO (NewsNation Now) — On April 13, 2021, the federal government took the unprecedented step of entering hundreds of private computers to remove malicious code placed during the Microsoft Exchange hack.&lt;/p&gt;
&lt;p&gt;While the federal government limited its efforts to solely the Exchange vulnerability, some cybersecurity experts believe this is only the beginning of increased federal-private collaboration on cybersecurity.&lt;/p&gt;
&lt;p&gt;Georgia Tech School of Cybersecurity and Privacy Chair Richard DeMillo compared cloud-based systems like Exchange and Amazon’s Web Services (AWS) to an essential utility that requires federal involvement.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Cloud services are starting to look more and more like the old phone companies from that standpoint, that it’s a private service offered partially over public networks. And we’re putting more and more assets into those services.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GEORGIA TECH SCHOOL OF CYBERSECURITY AND PRIVACY CHAIR RICHARD DEMILLO&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;The Exchange flaw was first exposed in January, but its roots date back to a “zero day” vulnerability meaning the system has had that built-in flaw since creation.&lt;/p&gt;
&lt;p&gt;Between February and April, over 30,000 systems were made vulnerable to attack from a weakness in the Microsoft Exchange software that companies across the globe use for email, calendars and other functions.&lt;/p&gt;
&lt;p&gt;The Biden administration issued multiple statements expressing their deep concern over the issue and even created a multiagency task force dedicated to targeting the issue.&lt;/p&gt;
&lt;p&gt;“This is a significant vulnerability that could have far-reaching impacts. First and foremost, this is an active threat,” White House press secretary Jen Psaki told reporters during a daily press briefing.&lt;/p&gt;
&lt;p&gt;Microsoft removed the vulnerability and issued patches, but not every business fixed their internal systems. That’s when the federal government got involved.&lt;/p&gt;
&lt;p&gt;The Justice Department received a court order to infiltrate several hundred computers that did not remove the Exchange vulnerability themselves with the patches issued by Microsoft.&lt;/p&gt;
&lt;p&gt;“And very cleverly, the FBI went in with Microsoft, and use that own hacking server to disable itself. They were very clear that they didn’t search those machines for any other information, explained New Jersey Institute of Technology Distinguished Professor and Institute for Data Science director &lt;strong&gt;David Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“They also were very clear that they only removed this particular hacking tool, even if there are other zero day exploits on those machines that they knew about. They did not touch them.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NEW JERSEY INSTITUTE OF TECHNOLOGY DISTINGUISHED PROFESSOR AND INSTITUTE FOR DATA SCIENCE DIRECTOR DAVID BADER&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;Bader called it a “unique approach” to diffusing the vulnerability involving private-public partnership and the court system.&lt;/p&gt;
&lt;p&gt;“It’s the first time that we’ve seen the FBI take an action, really in the cyberspace, rather than through the judicial space,” stated Bader.&lt;/p&gt;
&lt;p&gt;Experts acknowledge the public has a right to be concerned about law enforcement accessing private computers systems, especially with it being new legal territory.&lt;/p&gt;
&lt;p&gt;“At some point, someone is going to say, well, so how do we know that you didn’t go beyond the order? How do you know that other information wasn’t compromised? And so to have a public oversight process to do that, I think just makes a lot of sense,” said DeMillo.&lt;/p&gt;
&lt;p&gt;That public-private pressure will increase as more companies and critical infrastructure become vulnerable to hacking and ransomware.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Of course, what’s happened over the last few years, is that this whole industry has built up that makes that a very efficient business and so not only do people build tools to allow you to take over someone’s system, they have marketplaces that sell those tools to third parties.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GEORGIA TECH SCHOOL OF CYBERSECURITY AND PRIVACY CHAIR RICHARD DEMILLO&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;The fuel pipeline that supplies 45% of all U.S. fuel is the latest target with operations expected to be impacted for at least a week.&lt;/p&gt;
&lt;p&gt;Hospital networks, police departments, security camera companies, water treatment facilities and even entire cities have fallen victim to high-profile ransomware attacks in recent months.&lt;/p&gt;
&lt;p&gt;“We rely on computers for everything. When I go to the bank, I want to make sure that my money is there. When I drive my car, I wanted to work without crashing off a bridge. And so these types of hacks that we’re just seeing emerging, are really game changing in terms of how we must respond to them,” said Bader.&lt;/p&gt;
&lt;p&gt;Bader said the frequency and scope of the latest ransomware attacks are a sign that companies need to take cybersecurity as seriously as they take other safety requirements for running a business.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Having a business means being operational. And as more and more businesses get shut down, because their computer systems are under attack, then they naturally will have to upgrade, or they may not be able to function as a business going forward. So, it’s unfortunate, but it’s a cost of doing business in this day.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NEW JERSEY INSTITUTE OF TECHNOLOGY DISTINGUISHED PROFESSOR AND INSTITUTE FOR DATA SCIENCE DIRECTOR DAVID BADER&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;He added, “So I think we’re at the point where cybersecurity becomes a first-class citizen, for when we run our businesses and our governments.”&lt;/p&gt;
&lt;p&gt;Both experts anticipate seeing the federal government take a more active role in enforcing cybersecurity best practices for private businesses, especially when foreign actors are involved in a hack.&lt;/p&gt;
&lt;p&gt;“Beyond a certain scale, you can’t rely on individuals to fight the nation-state attack, for example, you know, if we were to be invaded like England was in World War II with aircraft, citizen volunteers with binoculars only takes you so far,” said DeMillo. “So at some point, you need to you need to have people with guns and bombs that do things and that intrudes into private rights.”&lt;/p&gt;
&lt;p&gt;A 2019 estimate found that the overall cost of U.S. ransomware attacks is up to $9 billion a year in terms of recovery and lost productivity, according to cybersecurity firm Emisoft.&lt;/p&gt;
&lt;p&gt;DeMillo said there’s a lesson to take away from the Exchange hack and latest rash of ransomware attacks.&lt;/p&gt;
&lt;p&gt;“There seems to be a general feeling that you can just give people checklists, and they check off the things to do. And somehow that protects them, which is clearly not the case. The amount of money that’s being spent on the other side, is going up proportionally faster than the amount of money we’re spending on the defensive side,” said DeMillo.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“It’s an arms race that the adversaries are currently winning.”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GEORGIA TECH SCHOOL OF CYBERSECURITY AND PRIVACY CHAIR RICHARD DEMILLO&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newsnationnow.com/cybersecurity/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newsnationnow.com/cybersecurity/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kfor.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kfor.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wdtn.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wdtn.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://myfox8.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://myfox8.com/news/why-have-ransomware-attacks-increased-and-the-fbis-unprecedented-step-to-prevent-them/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux and Supercomputing: How my passion for building COTS systems led to an HPC revolution</title>
      <link>http://localhost:1313/blog/20210426-linuxsupercomputer/</link>
      <pubDate>Mon, 26 Apr 2021 21:11:08 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210426-linuxsupercomputer/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Annals_hu_6fdeafb210fddcea.webp 400w,
               /blog/20210426-linuxsupercomputer/Annals_hu_f169879abce43ec6.webp 760w,
               /blog/20210426-linuxsupercomputer/Annals_hu_37576dcc725e141e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Annals_hu_6fdeafb210fddcea.webp&#34;
               width=&#34;760&#34;
               height=&#34;257&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://davidbader.net/publication/2021-b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D.A. Bader, &amp;ldquo;Linux and Supercomputing: How my passion for building COTS systems led to an HPC revolution,&amp;rdquo; IEEE Annals of the History of Computing, 43(3):73-80, 2021. doi:10.1109/MAHC.2021.3101415&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Back in the early 1990s when I was a graduate student in electrical and computer engineering at the University of Maryland, the term “supercomputer” meant Single Instruction, Multiple Data (SIMD) vector processor machines (the Cray-1 was the most popular), or massively parallel multiprocessor systems, such as the Thinking Machine CM-5. These systems were bulky—a Cray-1 occupied 2.7m by 2m of floor area and contained 60 miles of wires&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;; expensive, selling for several million dollars; and required significant expertise to program and operate. Supercomputing was mainly a function of the U.S. Department of Defense and its Soviet counterpart, large government and academic labs, and large industrial users. Each system used its own proprietary software and none was compatible with any other.&lt;/p&gt;
&lt;p&gt;But something new was on the horizon—a revolution in supercomputing technology was beginning that would bring scalable, less expensive systems to a much wider audience. That revolution involved using a new, open-source, operating system called Linux and collections of commodity off-the shelf (COTS) servers to obtain the performance of a traditional supercomputer. I was deeply involved with that revolution from the start. In 1989, as an undergraduate student at Lehigh University, I built my first parallel computer, using several Commodore Amiga 1000 personal computers that the company had donated to Lehigh. They had been collecting dust in a closet when a friend and I networked them together. A year later, I designed parallel algorithms on a 128-processor nCUBE hypercube parallel computer donated by AT&amp;amp;T Bell Laboratories. Building these systems taught me that the development of powerful parallel machines required a simultaneous development of scalable, high performance algorithms and services. Otherwise, application developers would be forced to develop algorithms from scratch every time vendors introduced a newer, faster, hardware platform.&lt;/p&gt;
&lt;p&gt;By the late 1990s, the term “cluster computing” was common among computer science researchers and several of these systems had received significant publicity. One of the first cluster approaches to attract interest was Beowulf, which cost from a tenth to a third of the price of a traditional supercomputer. A typical setup consisted of server nodes, with each one controlling a set of client nodes connected by Ethernet and running the Linux operating system&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.  In the spring of 1998, Los Alamos National Laboratory introduced a more powerful version of Beowulf called Avalon, using 68 personal computers running on DEC Alpha microprocessors&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.  However, neither the Beowulf cluster nor Avalon were genuine supercomputers, for they could not deliver high performance across the broad set of applications that ran on contemporary supercomputers.&lt;/p&gt;
&lt;p&gt;The Beowulf project was not about developing a supercomputer per se, but rather aimed to &amp;ldquo;explore the potential of ‘Pile-of-PCs’” at the lowest possible cost and develop methods for applying these systems to NASA Earth and space science problems&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.   As Thomas Sterling, co-creator of the first Beowulf cluster, observed, &amp;ldquo;Basically, you can order most of Beowulf&amp;rsquo;s components from the back pages of Computer Shopper or get them for free over the &amp;lsquo;Net.&amp;rdquo;&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;  Beowulf clusters were limited to solving problems that could be neatly divided into independent tasks, because the communication among processors required to run massively parallel applications on supercomputers did not exist yet.&lt;/p&gt;
&lt;p&gt;Avalon was powerful enough to make it onto the “Top500 List” of supercomputers in 1998, but although the system was fast, it was not truly a supercomputer.  As a Beowulf cluster that could run several applications, Avalon’s nodes were connected via Ethernet and utilized message passing over TCP, which meant relatively low bandwidth, high latency, and serious performance issues when executing parallel programs. Avalon made the list based on its ability to run the LINPACK benchmark. But its limited connectivity meant it could only run applications with a minimal need for communication as well as some domain decomposition methods, where performance is based almost entirely on processor speed.&lt;/p&gt;
&lt;h2 id=&#34;from-experimental-clusters-to-the-first-bona-fide-linux-supercomputer&#34;&gt;From Experimental Clusters to the first bona fide Linux Supercomputer&lt;/h2&gt;
&lt;p&gt;Less attention has been focused on parallel computing systems using COTS components and open-source operating systems that were developed before Avalon and Beowulf. Building these systems was my passion. In January 1992, I joined the University of Maryland as an electrical and computer engineering doctoral student and visited the NASA Goddard Space Flight Center in search of fellowships in parallel computing. In August, I received the NASA Graduate Student Researcher Fellowship and built my first parallel computer using Ethernet-connected, Intel-based PCs and the FreeBSD operating system in 1993, prior to the Beowulf project. After receiving my Ph.D. in May 1996, over the next eighteen months I was a postdoc at the university and a National Science Foundation (NSF) research associate at its Institute for Advanced Computer Studies (UMIACS). In this role, I built an experimental computing cluster comprising 10 DEC AlphaServer nodes, each with four DEC Alpha RISC processors and a DEC PCI card connected to a DEC Gigaswitch ATM switch. It used either my own communication library or a freely available MPI implementation. This system was more advanced than Los Alamos National Laboratory’s (LANL) Avalon cluster, which used Fast Ethernet for interconnection rather than an ATM network with lower latency and higher throughput.&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-national-computational-science-alliance-and-roadrunner&#34;&gt;The National Computational Science Alliance and Roadrunner&lt;/h2&gt;
&lt;p&gt;From Maryland, in January 1998 I moved to the University of New Mexico and the Albuquerque High Performance Computing Center (AHPCC). There I had the opportunity to build and deploy, to my knowledge, the first bona fide Linux supercomputer while continuing to develop clusters of COTS processors into systems with the speed, performance, and services of a traditional supercomputer. I came to UNM with the idea of building the first x86 Linux supercomputer as a teaching tool for advanced computer design. My system design took a revolutionary new direction that differed significantly from Beowulf and the HPC research community’s cluster efforts. From my experience with real applications, I knew that Beowulf did not have the capabilities to run the broad set of scientific computing tasks on contemporary supercomputers, and more engineering was necessary to create a Linux-based system that would displace traditional supercomputers.&lt;/p&gt;
&lt;p&gt;While Beowulf optimized to minimize cost per megaFLOP and required only free software, my system design maximized performance per price per megaFLOP, and used both mass market commodity components and proprietary software and networks. Beowulf used only Ethernet for the system area network, and I engineered the first use of a proprietary scalable network, Myrinet, in a Linux system since communication was often an HPC bottleneck. Instead of a single network, Ethernet, my system design used three: a control network (Fast Ethernet with Gigabit Ethernet uplinks); a highly scalable data network (Myrinet switches); and a diagnostic network (chained RS-232 serial ports) to monitor the nodes for failures, provide staged boot up of systems, and enable remote power cycling capabilities for system maintenance. Donald Becker, co-founder of the Beowulf project, advocated for clusters that combined “independent machines…. With a cluster, you have the opportunity to incrementally scale, where an SMP is generally built to a [preconfigured] size.”&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;  I argued for and built clusters of SMP nodes.&lt;/p&gt;
&lt;p&gt;After becoming the sole principal investigator [PI] for the AHPCC’s SMP Cluster Computing Project, by spring 1998 I had built the first working Intel/Linux supercomputer using an Alta Technologies “AltaCluster,” consisting of eight dual, 333 MHz, Intel Pentium II nodes. This required my porting of software to Linux to provide necessary components; modifying the Linux kernel and shell to increase space for very large command lines; and porting the codes from members of the National Computational Science Alliance (NCSA) to Linux—none had run on Linux previously. My work also included a partnership with Myricom’s president and CEO Chuck Seitz to incorporate the first Myrinet interconnection network for Intel/Linux. I also ported a job scheduler, the Portable Batch System developed at NASA Ames Research Center, to the Linux system and installed RedHat’s “Extreme Linux” before its widespread distribution that May.&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-myricom-m2m-pci32c-network-interface-card-image-credit-cspi&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Myricom M2M-PCI32c network interface card. (Image credit: CSPi)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Fig1_hu_a6111c54554dfd7c.webp 400w,
               /blog/20210426-linuxsupercomputer/Fig1_hu_4792db58a86d663b.webp 760w,
               /blog/20210426-linuxsupercomputer/Fig1_hu_3e5fee31070c3e95.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Fig1_hu_a6111c54554dfd7c.webp&#34;
               width=&#34;315&#34;
               height=&#34;229&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Myricom M2M-PCI32c network interface card. (Image credit: CSPi)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Around this time, I also became a PI with the NCSA, an NSF-supported effort to integrate computational, visualization, and information resources into a national-scale &amp;ldquo;Grid.&amp;rdquo;&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;  NSF and NCSA, led by Larry Smarr, made a high risk, high payoff bet in my vision of the first Linux supercomputer widely available to national science communities by allocating US$400,000, based on demonstrations of my 1998 16-processor Linux machine prototype. I assembled a team and we built Roadrunner, which entered production mode in April 1999. Its hardware comprised fully configured workstations powered by 128 dual, 450 MHz, Intel Pentium II processors; a 512 KB cache; a 512 MB SDRAM with ECC; 6.4 GB IDE hard drive; and Myrinet interface cards. The Myrinet System Area Network (Myrinet/SAN) interconnection network was one of Roadrunner’s main improvements over previous Linux systems, such as Beowulf and Avalon. At full-duplex 1.28 GB/s bandwidth, it was twice as fast as Myrinet/LAN and about five times faster than Ethernet, with much lower latency: in the tens of microseconds. Roadrunner’s system software included the Red Hat Linux 5.2 operating system; sets of compilers from both the GNU Compiler Collection and the Portland Group; and the Portable Batch System (PBS) job scheduler originally designed for NASA’s supercomputers. These features enabled parallel programming, such as software-based distributed shared memory (DSM) and the Message Passing Interface (MPI), a standardized means of exchanging information between multiple computer nodes. For MPI, Roadrunner used MPICH, a high performance open-source MPI implementation from Argonne National Laboratory; Myricom GM network drivers; and MPICH GM, Myricom’s MPI implementation.&lt;/p&gt;
&lt;p&gt;Roadrunner was among the 100 fastest supercomputers in the world when it went online. It provided services that were lacking in the first Linux clusters but are now regarded as essential for supercomputing, such as node-based resource allocation, job monitoring and auditing, and resource reservations.&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;  At the time, Roadrunner was dubbed a supercluster, combining the low cost and accessibility of Linux clusters with the services, fast networking, and low latency of a supercomputer. It was however one of the Alliance’s first hardware deployments designed to bring supercomputing to the desktop. Roadrunner went on to become a node on the National Technology Grid.&lt;/p&gt;


















&lt;figure  id=&#34;figure-baders-chautauqua-talk-on-linux-supercomputers-slides-from-march-1999-image-credit-courtesy-of-the-author&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Bader&amp;#39;s Chautauqua talk on Linux Supercomputers (slides from March 1999). (Image credit: Courtesy of the author.)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Fig2_hu_7676b1a4d5d3e79b.webp 400w,
               /blog/20210426-linuxsupercomputer/Fig2_hu_9ac676939db8ce6f.webp 760w,
               /blog/20210426-linuxsupercomputer/Fig2_hu_899d595583fc0243.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Fig2_hu_7676b1a4d5d3e79b.webp&#34;
               width=&#34;760&#34;
               height=&#34;624&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Bader&amp;rsquo;s Chautauqua talk on Linux Supercomputers (slides from March 1999). (Image credit: Courtesy of the author.)
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-linux-prototype-on-lower-left-and-roadrunner-right-a-myricom-dual-8-port-san-myrinet-switch-sits-on-top-of-the-left-most-cabinet-of-the-prototype-and-four-octal-8-port-san-myrinet-switches-not-visible-connect-roadrunner-above-roadrunners-console-is-a-72-port-foundry-fast-ethernet-switch-with-gigabit-uplinks-to-the-vbns-and-internet-image-credit-courtesy-of-the-author&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Linux prototype on lower-left, and Roadrunner (right). A Myricom dual 8-port SAN Myrinet switch sits on top of the left-most cabinet of the prototype, and four octal 8-port SAN Myrinet switches (not visible) connect Roadrunner. Above Roadrunner’s console is a 72-port Foundry Fast Ethernet switch with Gigabit uplinks to the vBNS and Internet. (Image credit: Courtesy of the author.)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Fig3_hu_d733fd72d4e09980.webp 400w,
               /blog/20210426-linuxsupercomputer/Fig3_hu_f8ca40aabdb99c30.webp 760w,
               /blog/20210426-linuxsupercomputer/Fig3_hu_c0c89034e706a327.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Fig3_hu_d733fd72d4e09980.webp&#34;
               width=&#34;598&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Linux prototype on lower-left, and Roadrunner (right). A Myricom dual 8-port SAN Myrinet switch sits on top of the left-most cabinet of the prototype, and four octal 8-port SAN Myrinet switches (not visible) connect Roadrunner. Above Roadrunner’s console is a 72-port Foundry Fast Ethernet switch with Gigabit uplinks to the vBNS and Internet. (Image credit: Courtesy of the author.)
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-inside-a-roadrunner-cabinet-with-each-node-attached-to-three-networks-myrinet-ribbon-cable-fast-ethernet-cat5-and-diagnostic-rs232-serial-port-image-credit-courtesy-of-the-author&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Inside a Roadrunner cabinet with each node attached to three networks: Myrinet (ribbon cable), Fast Ethernet (CAT5), and Diagnostic (RS232 serial port). (Image credit: Courtesy of the author.)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/FIg4_hu_f1a76cc96d7faa5d.webp 400w,
               /blog/20210426-linuxsupercomputer/FIg4_hu_df53d7ae8d6f4062.webp 760w,
               /blog/20210426-linuxsupercomputer/FIg4_hu_9f33c52a51c21279.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/FIg4_hu_f1a76cc96d7faa5d.webp&#34;
               width=&#34;624&#34;
               height=&#34;468&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Inside a Roadrunner cabinet with each node attached to three networks: Myrinet (ribbon cable), Fast Ethernet (CAT5), and Diagnostic (RS232 serial port). (Image credit: Courtesy of the author.)
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-the-launch-of-roadrunner-makes-the-news-machine-one-of-100-speediest-in-world-with-david-bader-pictured-at-roadrunners-console-copyright-the-albuquerque-journal--reprinted-with-permission-permission-does-not-imply-endorsement&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The launch of Roadrunner makes the news. &amp;#34;Machine One of 100 Speediest in World&amp;#34; with David Bader pictured at Roadrunner&amp;#39;s console. (Copyright: The Albuquerque Journal.  Reprinted with permission. Permission does not imply endorsement.)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Fig5_hu_27facd2c2e772147.webp 400w,
               /blog/20210426-linuxsupercomputer/Fig5_hu_4319828592a8371d.webp 760w,
               /blog/20210426-linuxsupercomputer/Fig5_hu_e6e294ae358fb6da.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Fig5_hu_27facd2c2e772147.webp&#34;
               width=&#34;760&#34;
               height=&#34;450&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The launch of Roadrunner makes the news. &amp;ldquo;Machine One of 100 Speediest in World&amp;rdquo; with David Bader pictured at Roadrunner&amp;rsquo;s console. (Copyright: The Albuquerque Journal.  Reprinted with permission. Permission does not imply endorsement.)
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-ncsa-director-larry-smarr-left-unm-president-william-gordon-and-us-sen-pete-domenici-turn-on-the-roadrunner-supercomputer-in-april-1999-after-the-ceremony-sen-domenici-asked-if-this-new-capability-could-be-shared-with-his-friend-senator-ted-stevens-in-alaska-i-packaged-the-machine-for-shipping-to-its-new-home-the-arctic-region-supercomputing-center-affiliated-with-the-university-of-alaska-fairbanks-image-credit-reprinted-with-permission-of-ncsa&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;NCSA Director Larry Smarr (left), UNM President William Gordon, and U.S. Sen. Pete Domenici turn on the Roadrunner supercomputer in April 1999. After the ceremony, Sen. Domenici asked if this new capability could be shared with his friend Senator Ted Stevens in Alaska. I packaged the machine for shipping to its new home, the Arctic Region Supercomputing Center affiliated with the University of Alaska, Fairbanks. (Image credit: Reprinted with permission of NCSA.)&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Fig6_hu_e119109e9fb00709.webp 400w,
               /blog/20210426-linuxsupercomputer/Fig6_hu_d7dc55f7e51ce58a.webp 760w,
               /blog/20210426-linuxsupercomputer/Fig6_hu_1df2129053a304f0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Fig6_hu_e119109e9fb00709.webp&#34;
               width=&#34;760&#34;
               height=&#34;432&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      NCSA Director Larry Smarr (left), UNM President William Gordon, and U.S. Sen. Pete Domenici turn on the Roadrunner supercomputer in April 1999. After the ceremony, Sen. Domenici asked if this new capability could be shared with his friend Senator Ted Stevens in Alaska. I packaged the machine for shipping to its new home, the Arctic Region Supercomputing Center affiliated with the University of Alaska, Fairbanks. (Image credit: Reprinted with permission of NCSA.)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The Grid was envisioned as a way to give researchers access to supercomputers for large-scale problem solving from their desktops, no matter their location, through the nation’s fastest high-performance research networks. Alliance Director Larry Smarr likened the National Technology Grid to the power grid, where users could plug in and get the compute resources they needed, without having to worry about where those resources came from or their own location.&lt;/p&gt;
&lt;p&gt;Within the Alliance, computer scientists and software and hardware engineers worked closely with domain scientists to ensure that the systems being developed would meet the requirements of scientists needing supercomputers to solve complicated scientific problems. Scientific software that ran on Roadrunner included AZTEC, algorithms for solving sparse systems of linear equations; BEAVIS (Boundary Element Analysis of Viscous Suspensions), used for 3-D analysis of multiphase flows; Cactus, a numerical relativity toolkit for solving astrophysics problems; HEAT, a diffusion partial differential equation using conjugate gradient solver methods; HYDRO, a Lagrangian hydrodynamics code; and MILC, a set of codes developed by the MIMD Lattice Computation collaboration to study quantum chromodynamics.&lt;/p&gt;
&lt;p&gt;Roadrunner’s performance on the Cactus application benchmark showed near perfect scalability, unlike systems such as the NASA Beowulf cluster, the NCSA’s Microsoft Windows NT cluster computer, and Silicon Graphics Inc.’s family of high-end server computers, the Origin 2000. Several scientists who pioneered the use of the Roadrunner system shared their memories:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“It was a very exciting time; Linux clusters were emerging as a huge force to democratize supercomputing and software frameworks providing community toolkits to solve broad classes of science and engineering problems were also taking shape. The collaboration we had between the Cactus team at the Albert Einstein Institute in Germany and David Bader’s team with the Roadrunner supercluster was a pioneering effort that helped these movements gain traction around the world. The collaboration helped advance the goals of the Cactus team, led by Gabrielle Allen, whose efforts continue to this day as the underlying framework of the Einstein Toolkit. That toolkit now powers many efforts globally to address complex problems in multi-messenger astrophysics.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;— Edward Seidel, Ph.D.
President, University of Wyoming
Former Head of the Numerical Relativity and
E-Science Research Groups, Albert Einstein Institute&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“We tested our large weather prediction codes on Roadrunner and found it to be a powerful platform for code development and application, with the move to COTS hardware and software opening the doors to non-proprietary clusters for many researchers who until then only did their work on workstations and laptops. The Roadrunner network (Message Passing Interface) results were superior to those from previous clusters’ Ethernets in moving data from one processor to another during a weather forecast, thus enhancing the forecast turnaround time or forecast quality by allowing for more grid points to be used and a correspondingly more resolved weather feature prediction. We also used Roadrunner to produce detailed simulations of thunderstorms and turbulence generated at commercial airline flight levels.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;—Dan Weber
Retired Research Meteorologist&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;—Kelvin Droegemeier, Ph.D.
OU Regents Professor of Meteorology
Weathernews Chair Emeritus
Roger and Sherry Teigen Presidential Professor
Former Director, White House Office of Science and Technology Policy&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;“Roadrunner, to my knowledge, was the first Linux cluster-based supercomputer available to the research community. It was a forerunner of what has become a dominant approach in supercomputing.  In 1999, while just starting at MIT, I was able to obtain access to Roadrunner to test and scale a number of key parallel software technologies, which formed the basis of establishing our supercomputing center at MIT.  This early work pioneered on Roadrunner impacts thousands of researchers across MIT.”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;—Jeremy Kepner, Ph.D.
Head and Founder, MIT Lincoln Laboratory Supercomputing Center&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The development of the first Linux supercomputer had effects far beyond the needs of Alliance scientists. It permanently changed supercomputing and its impacts are still felt today.&lt;/p&gt;
&lt;h2 id=&#34;the-continuing-linux-supercomputing-revolution&#34;&gt;The Continuing Linux Supercomputing Revolution&lt;/h2&gt;
&lt;p&gt;As leader of the Alliance/UNM Roadrunner project, I presented my team’s work at professional events, such as the Alliance Chautauquas held at UNM, the University of Kentucky, and Boston University in 1999.&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;    After Roadrunner, I embarked on another Alliance project, working with IBM on development of LosLobos, IBM’s first Linux production system, which was assembled and operated at the University of New Mexico. LosLobos, which premiered on the Top500 list at number 24, consisted of 256 dual processor, Intel-based, IBM servers with Myrinet connections, creating a 512-processor machine capable of 375 gigaFLOPs.&lt;/p&gt;
&lt;p&gt;LosLobos entered production in summer 2000. The Linux supercomputing movement was well underway, thanks to the proliferation of commodity components, the development of high-speed COTS networks such as Myrinet, the rapid expansion of the open software movement, and the ability of researchers, myself included, to exploit all these developments. For the first time, supercomputers could be built at a relatively low cost. While LosLobos was used primarily by scientists to model and solve complicated problems in physics, biology, and other fields, IBM’s move toward the open-source framework was a sign of things to come. Within a year, it used the knowledge gained by working with my Alliance research group on LosLobos to create the first pre-assembled and pre-configured Linux server clusters for business.&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Today, all supercomputers on the Top500 list are Linux systems. Simply put, today’s machines are no longer purpose-built monoliths. Using an open-source operating system, running on commodity microprocessors, and networked with high-speed commodity interconnects, Linux cluster supercomputers can be easily customized for different uses, unlike vendor-specific Unix systems. They provide users speed, high-end services, and unprecedented flexibility, all at a lower cost than in traditional supercomputers. They can also be integrated into any datacenter, making feasible enterprise systems that are similar to those breaking scientific barriers.&lt;/p&gt;
&lt;p&gt;The ease of use of Linux supercomputers has had a profound impact on how scientists conduct their research and on the most pressing issues of our time, and I am proud of my role in this revolution in computing and discovery. Whether they are simulating astrophysical phenomena, the impacts of climate change, or biological functions at the cellular level, Linux supercomputers are today’s primary tool of knowledge discovery.&lt;/p&gt;
&lt;p&gt;Today, researchers are building a new generation of exascale computing systems – machines capable of calculating at least 1018 floating point operations per second (1 exaFLOPS). The Linux operating system is intrinsic to this effort because it provides the scale and flexibility to support high-performance computing at the exascale level. The framework that I developed in the 1990s remains the foundational infrastructure of today’s Linux supercomputers, including the fastest machines in the world.&lt;/p&gt;
&lt;p&gt;For me, this is both thrilling and gratifying. My interest in parallel computing dates to 1981, when I read an article on a parallel computing system for image processing and pattern recognition&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;.  I’ve spent my entire career making Linux-based COTS systems a viable and more affordable alternative to traditional supercomputers. I’ve incorporated popular compilers, job schedulers, and MPICH to COTS Linux deployments, and those innovations are still used today on Linux supercomputers, enabling Linux to become the OS of choice on high performance machines.&lt;/p&gt;
&lt;p&gt;Exascale supercomputers will provide unprecedented capability to integrate data analytics, AI, and simulation for advanced 3-D modeling. They will tackle problems related to neuroscience, nuclear fusion, the biology of cancer, and will give nations a competitive edge in energy R&amp;amp;D and national security. It is my hope that somewhere a young computer scientist is reading my published work and it is sparking the same inspiration in them as Siegel’s work inspired in me. My work has become one of the building blocks of 21st-century computing technologies, and I look forward to seeing how others build on my innovations with their own.&lt;/p&gt;
&lt;h3 id=&#34;about-the-author&#34;&gt;About the Author:&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a distinguished professor in the department of data science in the Ying Wu College of Computing and Director of the Institute for Data Science at the New Jersey Institute of Technology. Prior to this, he served as founding professor and chair of the School of Computational Science and Engineering, College of Computing, at the Georgia Institute of Technology. He is a Fellow of the IEEE, AAAS, and SIAM.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, collecting new collected works for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;additional-documents&#34;&gt;Additional Documents&lt;/h3&gt;
&lt;p&gt;&lt;a name=&#34;gal_19920221-NASA&#34;&gt;&lt;/a&gt;
NASA Graduate Student Researchers Program (GSRP) Letter from Dr. Gerald A. Soffen, Director of University Programs, NASA Goddard Space Flight Center (21 February 1992): 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/19920221-NASA_hu_7c8958969d437e27.webp 400w,
               /blog/20210426-linuxsupercomputer/19920221-NASA_hu_4423f40d6a75dcc1.webp 760w,
               /blog/20210426-linuxsupercomputer/19920221-NASA_hu_c0fd59d7a6c394fc.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/19920221-NASA_hu_7c8958969d437e27.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_19920330-NASA&#34;&gt;&lt;/a&gt;
NASA Award Letter from Dr. Gerald A. Soffen (30 March 1992): 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/19920330-NASA-1_hu_b7ff678371d6ac08.webp 400w,
               /blog/20210426-linuxsupercomputer/19920330-NASA-1_hu_3c8edb3af0190e78.webp 760w,
               /blog/20210426-linuxsupercomputer/19920330-NASA-1_hu_7a684e4ec983b38c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/19920330-NASA-1_hu_b7ff678371d6ac08.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/19920330-NASA-2_hu_ff1b7dd1c4b604f1.webp 400w,
               /blog/20210426-linuxsupercomputer/19920330-NASA-2_hu_1adb6a8fd581fd67.webp 760w,
               /blog/20210426-linuxsupercomputer/19920330-NASA-2_hu_a14a60203c3a28da.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/19920330-NASA-2_hu_ff1b7dd1c4b604f1.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_FreeBSD19970720&#34;&gt;&lt;/a&gt;
FreeBSD MPI port (20 July 1997): 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/FreeBSD19970720_hu_69c3b1b69d3f66b2.webp 400w,
               /blog/20210426-linuxsupercomputer/FreeBSD19970720_hu_2029efcc66b0d4e8.webp 760w,
               /blog/20210426-linuxsupercomputer/FreeBSD19970720_hu_41ff971c42970c32.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/FreeBSD19970720_hu_69c3b1b69d3f66b2.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_prop9804&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;files/LinuxSupercomputer/prop9804.pdf&#34;&gt;Proposal for Teaching High Performance Computing
using SMP Clusters (8 April 1998):&lt;/a&gt; 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/prop9804_hu_eca05ba73aada7b7.webp 400w,
               /blog/20210426-linuxsupercomputer/prop9804_hu_31b361d7295d11b5.webp 760w,
               /blog/20210426-linuxsupercomputer/prop9804_hu_cebe4e56466a146.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/prop9804_hu_eca05ba73aada7b7.webp&#34;
               width=&#34;300&#34;
               height=&#34;544&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_HPCERC9806&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;files/LinuxSupercomputer/HPCERC9806.pdf&#34;&gt;Proposal for Developing Algorithms and Applications for High Performance SMP Clusters (10 June 1998):&lt;/a&gt; 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/HPCERC9806_hu_941dd9ffb81e3381.webp 400w,
               /blog/20210426-linuxsupercomputer/HPCERC9806_hu_d02fc79e3184c4d6.webp 760w,
               /blog/20210426-linuxsupercomputer/HPCERC9806_hu_30870eccec1c5096.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/HPCERC9806_hu_941dd9ffb81e3381.webp&#34;
               width=&#34;300&#34;
               height=&#34;571&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_Dinner19990408&#34;&gt;&lt;/a&gt;
Invitation to RoadRunner Dedication, Reception, and Dinner (8 April 1999): 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/19990408-dinner1_hu_e8b75407964d43e0.webp 400w,
               /blog/20210426-linuxsupercomputer/19990408-dinner1_hu_45cf5139ce7514dd.webp 760w,
               /blog/20210426-linuxsupercomputer/19990408-dinner1_hu_6b2e7b86a97ebb3a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/19990408-dinner1_hu_e8b75407964d43e0.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/19990408-dinner2_hu_cd722f688234cdac.webp 400w,
               /blog/20210426-linuxsupercomputer/19990408-dinner2_hu_bc38edb21d290ca.webp 760w,
               /blog/20210426-linuxsupercomputer/19990408-dinner2_hu_c6fc12cedfdd7020.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/19990408-dinner2_hu_cd722f688234cdac.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_Access19990420&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;files/LinuxSupercomputer/Access19990420.pdf&#34;&gt;NCSA/Alliance Access, Ceremony Dedicates Linux Supercluster. Bader is pictured in the top-left photo. (20 April 1999):&lt;/a&gt; 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/Access19990420_hu_3f577f50a6535ed2.webp 400w,
               /blog/20210426-linuxsupercomputer/Access19990420_hu_32d635dc9417eb7c.webp 760w,
               /blog/20210426-linuxsupercomputer/Access19990420_hu_2b16fbf25d54e462.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/Access19990420_hu_3f577f50a6535ed2.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_clustertalk9905&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;files/LinuxSupercomputer/clustertalk9905.pdf&#34;&gt;Alliance / UNM Roadrunner Linux Cluster, David A. Bader. (10 May 1999):&lt;/a&gt; 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210426-linuxsupercomputer/clustertalk9905_hu_97c1149ce9c55b53.webp 400w,
               /blog/20210426-linuxsupercomputer/clustertalk9905_hu_28617df3d5798a8c.webp 760w,
               /blog/20210426-linuxsupercomputer/clustertalk9905_hu_898dad5b661ead38.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210426-linuxsupercomputer/clustertalk9905_hu_97c1149ce9c55b53.webp&#34;
               width=&#34;300&#34;
               height=&#34;588&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;em&gt;History of Cray Supercomputers&lt;/em&gt;. Hewlett Packard Enterprise, &lt;a href=&#34;https://www.hpe.com/us/en/compute/hpc/cray.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.hpe.com/us/en/compute/hpc/cray.html&lt;/a&gt; , accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Sterling, T., D. Savarese, D. Becker, J. Dorband, U. Ranawake and C. V. Packer. “BEOWULF: A Parallel Workstation for Scientific Computation.” &lt;em&gt;Proc. 24th Int. Conf. on Parallel Processing&lt;/em&gt; (1995), p. 11-14.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;“Linux and Supercomputers.” &lt;em&gt;Linux Journal&lt;/em&gt;, &lt;a href=&#34;https://www.linuxjournal.com/content/linux-and-supercomputers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.linuxjournal.com/content/linux-and-supercomputers&lt;/a&gt; , November 29, 2018, accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;D. Ridge, D. Becker, P. Merkey, and T. Sterling, &amp;ldquo;Beowulf: harnessing the power of parallelism in a pile-of-PCs,&amp;rdquo; &lt;em&gt;1997 IEEE Aerospace Conference&lt;/em&gt;, Snowmass, CO, USA, 1997, Vol. 2, pp. 79-91, &lt;a href=&#34;https://doi.org/10.1109/AERO.1997.577619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/AERO.1997.577619&lt;/a&gt; .&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Supercomputing gets a new Hero,” &lt;em&gt;Communications News&lt;/em&gt;, August 1, 1998, &lt;a href=&#34;https://www.thefreelibrary.com/Supercomputing&amp;#43;gets&amp;#43;a&amp;#43;new&amp;#43;hero-a021071072&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.thefreelibrary.com/Supercomputing+gets+a+new+hero-a021071072&lt;/a&gt; , accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;D. Bader and J. JáJá, “SIMPLE: A Methodology for Programming High Performance Algorithms on Clusters of Symmetric Multiprocessors (SMPs).” &lt;em&gt;Journal of Parallel and Distributed Computing&lt;/em&gt;, 58(1): 92-108, 1999. &lt;a href=&#34;https://doi.org/10.1006/jpdc.1999.1541&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1006/jpdc.1999.1541&lt;/a&gt; .&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Joab Jackson, “Donald Becker: The inside story of the Beowulf saga,” &lt;em&gt;GCN&lt;/em&gt;, 13 April 2005, &lt;a href=&#34;https://gcn.com/articles/2005/04/13/donald-becker--the-inside-story-of-the-beowulf-saga.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gcn.com/articles/2005/04/13/donald-becker--the-inside-story-of-the-beowulf-saga.aspx&lt;/a&gt; , accessed 21 July 2021.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;“Announcing Extreme Linux,” &lt;a href=&#34;https://www.redhat.com/en/about/press-releases/press-extremelinux&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.redhat.com/en/about/press-releases/press-extremelinux&lt;/a&gt; , May 13, 1998, accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;“The Grid links the Alliance together and provides access to its wide variety of resources to the national scientific research community. Using high performance networking, the Grid will link the highest performing systems to mid-range versions of these architectures, and then to the end users’ workstations, thereby creating a national Power-Grid.”: “National Computational Science Alliance,” 1997-2005, National Science Foundation Award #9619019, &lt;a href=&#34;https://www.nsf.gov/awardsearch/showAward?AWD_ID=9619019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/awardsearch/showAward?AWD_ID=9619019&lt;/a&gt; , accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;D. A. Bader, A. B. Maccabe, J. R. Mastaler, J. K. McIver, and P. A. Kovatch, &amp;ldquo;Design and Analysis of the Alliance/University of New Mexico Roadrunner Linux SMP SuperCluster,&amp;rdquo; &lt;em&gt;IEEE Computer Society International Workshop on Cluster Computing (IWCC)&lt;/em&gt;, Melbourne, Victoria, Australia, 1999, pp. 9-18, &lt;a href=&#34;https://doi.org/10.1109/IWCC.1999.810804&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/IWCC.1999.810804&lt;/a&gt; .&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;D.A. Bader, “CLUSTERS - The Most Rapidly Growing Architecture of High-End Computing,” &lt;a href=&#34;https://web.archive.org/web/20000203101440/http://chautauqua.ahpcc.unm.edu/agenda.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.archive.org/web/20000203101440/http://chautauqua.ahpcc.unm.edu/agenda.html&lt;/a&gt; , archived from the original on 3 February 2000, accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;“Chautauquas Revive an American Forum for a New Era,” Aug. 4, 1999. &lt;a href=&#34;https://davidbader.net/post/19990804-chautauqua/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://davidbader.net/post/19990804-chautauqua/&lt;/a&gt; , accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;“IBM unveils pre-packaged Linux clusters,” &lt;a href=&#34;https://www.computerweekly.com/news/2240043115/IBM-unveils-pre-packaged-Linux-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.computerweekly.com/news/2240043115/IBM-unveils-pre-packaged-Linux-clusters&lt;/a&gt; , November 14, 2001, accessed 8 July 2021.&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;H. J. Siegel, L. J. Siegel, F. C. Kemmerer, P. T. Mueller, H. E. Smalley and S. D. Smith, &amp;ldquo;PASM: A Partitionable SIMD/MIMD System for Image Processing and Pattern Recognition,&amp;rdquo; &lt;em&gt;IEEE Transactions on Computers&lt;/em&gt;, vol. C-30, no. 12 (Dec. 1981), pp. 934-947, &lt;a href=&#34;https://doi.org/10.1109/TC.1981.1675732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1109/TC.1981.1675732&lt;/a&gt; .&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ROI-NJ presents its first-ever ROI Influencers: Technology list</title>
      <link>http://localhost:1313/blog/20210421-roi-nj/</link>
      <pubDate>Wed, 21 Apr 2021 21:08:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210421-roi-nj/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210421-roi-nj/TT_Landing-page-696x409_hu_bf8b5e91dc779fd5.webp 400w,
               /blog/20210421-roi-nj/TT_Landing-page-696x409_hu_d1a39e787ba794bd.webp 760w,
               /blog/20210421-roi-nj/TT_Landing-page-696x409_hu_66812ee2d0799c79.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210421-roi-nj/TT_Landing-page-696x409_hu_bf8b5e91dc779fd5.webp&#34;
               width=&#34;696&#34;
               height=&#34;409&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Determining our first-ever ROI Influencers: Technology list was a process that followed our traditional format: We spoke to approximately a dozen New Jersey tech leaders (all of whom are on the list) to get their takes on various categories.&lt;/p&gt;
&lt;p&gt;Defining the list? That wasn’t as easy.&lt;/p&gt;
&lt;p&gt;It starts with the founders — those who have created the cool and innovative companies that dot the state. But it wouldn’t be complete without the investors that helped many of them grow, the academics who are mentoring and inspiring the next generation, the executives of tech companies that give the state such a rich ecosystem and the thought leaders that help bring everyone together.&lt;/p&gt;
&lt;p&gt;Our Top 10 list brings together the best in all of these categories.&lt;/p&gt;
&lt;p&gt;We’re overwhelmed with the quality of honorees on the list — and humbled to know that there are many others out there deserving of the recognition. We’re confident this list will only grow each year.&lt;/p&gt;
&lt;h2 id=&#34;academics&#34;&gt;Academics&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210421-roi-nj/David-Bader-230x300_hu_e94cc7b991caf724.webp 400w,
               /blog/20210421-roi-nj/David-Bader-230x300_hu_df160d6577fba8b5.webp 760w,
               /blog/20210421-roi-nj/David-Bader-230x300_hu_46d920316df11902.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210421-roi-nj/David-Bader-230x300_hu_e94cc7b991caf724.webp&#34;
               width=&#34;230&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Director, Institute for Data Science&lt;br&gt;
&lt;em&gt;New Jersey Institute of Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;An expert in massive-scale analytics and computational genomics, he also is an adviser to the White House and serves on the leadership team of the Northeast Big Data Innovation Hub.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.roi-nj.com/2021/04/19/roi-influencers/technology/2021-technology/roi-nj-presents-its-first-ever-roi-influencers-technology-list/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.roi-nj.com/2021/04/19/roi-influencers/technology/2021-technology/roi-nj-presents-its-first-ever-roi-influencers-technology-list/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.roi-nj.com/2021/04/19/roi-influencers/technology/2021-technology/roi-influencers-technology-2021-academics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.roi-nj.com/2021/04/19/roi-influencers/technology/2021-technology/roi-influencers-technology-2021-academics/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Institute for Data Science Aims to Democratize Supercomputing With NSF Grant</title>
      <link>http://localhost:1313/blog/20210317-njit/</link>
      <pubDate>Wed, 17 Mar 2021 13:25:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210317-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-new-algorithms-from-at-njit-can-make-supercomputer-power-available-to-almost-anyone&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New algorithms from at NJIT can make supercomputer power available to almost anyone&#34; srcset=&#34;
               /blog/20210317-njit/pexels-energepiccom-159888_hu_26c9b98a96010c22.webp 400w,
               /blog/20210317-njit/pexels-energepiccom-159888_hu_3012d64c3fbb875a.webp 760w,
               /blog/20210317-njit/pexels-energepiccom-159888_hu_56040c26aaa74d94.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210317-njit/pexels-energepiccom-159888_hu_26c9b98a96010c22.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New algorithms from at NJIT can make supercomputer power available to almost anyone
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Ordinary people could soon have greater ability to analyze massive amounts of information, based on new algorithms and software tools being designed at NJIT, intended to simplify access to a programming interface from data scientists at the Department of Defense.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s relatively straightforward to analyze data sets of up to several hundred gigabytes, as the required software is readily available to students and small businesses, but there&amp;rsquo;s a higher barrier to entry for working with tens of terabytes, which generally requires extensive training on high-performance computers, &lt;strong&gt;Institute for Data Science Director David Bader&lt;/strong&gt; explained.&lt;/p&gt;
&lt;p&gt;Bader anticipates that his team&amp;rsquo;s efforts, being designed with an award from the National Science Foundation, will greatly increase the user base for supercomputing especially among women, high school students and other underrepresented groups in STEM fields. Those groups tend to have the least access to that power today. If the user base increases, they&amp;rsquo;ll demand even more tools, which could cause the industry to rethink their design motivations and democratize high-end computing systems.&lt;/p&gt;
&lt;p&gt;To address this problem, Bader along with doctoral student Oliver Alvarado Rodriguez and research scientist Zhihui Du will spend the next year extending Arkouda, which is the defense-derived open-source code library written in Python, an everyday language taught as early as elementary school that&amp;rsquo;s also used for serious applications. They will build new algorithms and software that adds capabilities for common data structures such as graphs, lists, strings and trees. The software will be designed for simple usability, which hasn&amp;rsquo;t been a concern of most players in the high-performance computing field.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We have a large number of data scientists that want to manipulate data sets that are terabytes in size, and that&amp;rsquo;s been a challenging issue, but there hasn&amp;rsquo;t been much thought to the tooling. In the past they may use Apache tools like Hadoop and Spark … but what&amp;rsquo;s different now is we have a framework that will connect to Python, have a supercomputer in the background if you want it, and the data scientist doesn&amp;rsquo;t need to know about it,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;Still, there&amp;rsquo;s no such thing as a free lunch. &amp;ldquo;While our work with Arkouda will bridge the frustrating gap between practical data science and [high-performance computing] technology with this application, if you&amp;rsquo;re handling data sets that are tens of terabytes, it will require a high-performance computer on the other side,&amp;rdquo; Bader noted. &amp;ldquo;To get this magic to work, you would need a backend system that is capable of storing and processing your data set.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But, he continued, &amp;ldquo;The hard part is solved. A programmer need not learn to program a supercomputer … That&amp;rsquo;s our development work, so they can stay in Python.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader added that Arkouda — named after the Greek word for bear — was developed by William Reus and Mike Merrill, with the former giving a virtual presentation open to all in the NJIT community on Wednesday, March 24.&lt;/p&gt;
&lt;p&gt;Rodriguez, the doctoral student, received his undergraduate degree in computer science from William Paterson University but chose NJIT for his next step. &amp;ldquo;Dr. Bader reached out to me the week that everything closed down because of the pandemic. I did some research in undergrad in machine learning, so that started sparking my interest in that area,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Rodriguez wants to become a professor one day. For now, he&amp;rsquo;s enjoying the new research project. &amp;ldquo;Before I joined NJIT, I was more inclined toward doing cybersecurity research, but over the summer as I&amp;rsquo;ve done more work here, it&amp;rsquo;s really sparked my interest in high-performance computing,&amp;rdquo; he said. &amp;ldquo;Bridging that gap between laypeople and high-performance computing tools is a very important research focus.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/institute-data-science-aims-democratize-supercomputing-nsf-grant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/institute-data-science-aims-democratize-supercomputing-nsf-grant&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Simplified: Interview with Professor David Bader</title>
      <link>http://localhost:1313/blog/20210311-dataproducts/</link>
      <pubDate>Thu, 11 Mar 2021 13:09:10 -0500</pubDate>
      <guid>http://localhost:1313/blog/20210311-dataproducts/</guid>
      <description>&lt;p&gt;Today we’re delving into the world of high performance data analytics with &lt;strong&gt;David A. Bader&lt;/strong&gt;. Professor Bader is a Distinguished Professor in the Department of Computer Science at the New Jersey Institute of Technology (&lt;a href=&#34;https://www.njit.edu/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.njit.edu/)&lt;/a&gt;. Visit &lt;a href=&#34;https://davidbader.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://davidbader.net/&lt;/a&gt; to learn more about Professor Bader.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZBOfUCphhHs?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://dataproducts.io/2021/03/data-simplified-interview-with-professor-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dataproducts.io/2021/03/data-simplified-interview-with-professor-david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>“Bad” Voting Machine Rejected by New York</title>
      <link>http://localhost:1313/blog/20210202-smartelections/</link>
      <pubDate>Tue, 02 Feb 2021 14:39:41 -0500</pubDate>
      <guid>http://localhost:1313/blog/20210202-smartelections/</guid>
      <description>&lt;p&gt;&lt;strong&gt;New York City - February 2, 2021&lt;/strong&gt; &amp;ndash; The New York State Board of Elections unanimoulsy &lt;a href=&#34;https://www.youtube.com/watch?v=k-gA4X8-Q28&amp;amp;feature=youtu.be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rejected certification&lt;/a&gt; of a voting machine called the ExpressVote XL at a special late January meeting. The machine, made by ES&amp;amp;S, is referred to as a “hybrid” or “all-in-one” voting machine because it combines voting and tabulation in a single device. Rather than tabulating hand-marked paper ballots, the practice recommended by security experts, the ExpressVote XL generates a computer-printed summary card for each voter. The summary cards contain barcodes representing candidates’ names, and the machine tabulates votes from the barcodes. &lt;a href=&#34;https://securiosa.com/posts/how_the_expressvote_xl_could_alter_ballots.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Security experts&lt;/a&gt; warn that the system “could change a vote for one candidate to be a vote for another candidate,” if it were hacked. Colorado, a leader in election security, has &lt;a href=&#34;https://www.cnn.com/2019/09/16/politics/colorado-qr-codes-votes/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;banned barcodes in voting&lt;/a&gt;, due to the high risk.&lt;/p&gt;
&lt;p&gt;Professor Rich DeMillo, Chair of the School of Cybersecurity and Privacy at Georgia Tech, said, “It is encouraging that the commissioners refused to certify the ExpressVoteXL, an all-in-one ballot marking device that poses extraordinary risks for the voters of New York.”  DeMillo is one of over 50 experts, good government groups and disability advocates who &lt;a href=&#34;https://smartelections.us/expressvotexl-letter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;signed a letter opposing certification&lt;/a&gt; of the voting machine. Seven disability rights groups, led by Downstate New York Adapt, listed &lt;a href=&#34;https://smartelections.us/ess-problems&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;privacy concerns&lt;/a&gt; as a reason for their opposition. They noted the machine’s “skinny ballots” are a different size, and so might reveal voters’ choices.&lt;/p&gt;
&lt;p&gt;New York elected officials also weighed in against the voting system. Thirty-three Assembly members &lt;a href=&#34;https://smartelections.us/ess-problems#f197830f-2f56-4d36-b2ec-64c1c248d023&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sent a letter to the State Election Commissioners&lt;/a&gt; opposing certification, because, “The barcode printed on the summary card is not independently verifiable by the voter.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://codes.findlaw.com/ny/election-law/eln-sect-7-202.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New York law&lt;/a&gt; requires that voters have an opportunity both to vote privately, and to verify their votes. Arthur Schwartz, attorney for SMART Elections, says of the decision, ”I was thrilled after tangling with the Board last year over the Presidential Primary, to have the Board follow the law, not certify the ExpressVote XL, and protect rather than limit our right to vote.”&lt;/p&gt;
&lt;p&gt;Election security experts say the machine’s most serious defect is a design flaw that combines a printer and scanner in one system with a shared paper path. This allows the summary card with the votes to pass under the printhead after it is cast by the voter. If hacked, the voting machine, “&lt;a href=&#34;https://freedom-to-tinker.com/2021/01/11/ess-voting-machine-company-sends-threats/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;can add, delete, or change votes on individual ballots&lt;/a&gt;,” says Princeton Computer Science professor Andrew Appel. In an investigative series produced by SMART Elections, he called the hybrid design &lt;a href=&#34;https://smartelections.us/dominion-ice#8813358b-ec75-43e7-ae37-80d8418047ab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“a disaster&amp;quot;&lt;/a&gt;, and in a blog post he said &amp;ldquo;indeed it is a bad voting machine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;During the certification process, a number of other issues were revealed about the ExpressVote XL. &lt;a href=&#34;https://www.youtube.com/watch?v=S00imkkvtXk&amp;amp;feature=youtu.be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In public testimony&lt;/a&gt;, Kevin Skoglund, a cybersecurity and voting systems expert said that the system being submitted in New York “uses extremely outdated software. It runs on Windows 7, which became end-of-life a year ago.” (1:27:20)&lt;/p&gt;
&lt;p&gt;In an example of just how wrong an election can go, the ExpressVote XL miscounted tens of thousands of votes in a 2019 Northampton, Pennsylvania election. In a post-election statement, ES&amp;amp;S spokesperson Adam Carbiullido &lt;a href=&#34;https://www.facebook.com/CountyExecutiveLamontMcClure/videos/781532772320093/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;noted&lt;/a&gt;, “The ballot showed correctly on the screen, and printed correctly on the paper ballots, but the votes were not attributed to the proper candidates on the USBs (memory sticks).” Subsequently, the “Northampton County Election Commission Board announced a unanimous … &lt;a href=&#34;https://www.commoncause.org/new-york/wp-content/uploads/sites/20/2020/01/Common-Cause-New-York-The-ExpressVote-XL-Voting-Machine-Bad-for-New-Yorks-Elections.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vote of no confidence in the ExpressVote XL&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;Despite these issues, the ExpressVote XL “is the most expensive voting machine on the market at &lt;a href=&#34;https://docs.google.com/document/d/17fNnQDBjC44TELEZIAUFBil_TGveXNWHmoXS0BqvijQ/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$8,250 per machine&lt;/a&gt;,” according to Protect Our Vote Philly, a coalition of good government groups that fought the use of the ExpressVote XL in Philadelphia. Following the Philadelphia certification, an investigation revealed that ES&amp;amp;S did not disclose lobbying and lobbyist campaign contributions, including to the two city commissioners who selected the system. &lt;a href=&#34;https://fusion.inquirer.com/politics/election/philly-voting-machines-ess-contract-disclosure-20190814.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ES&amp;amp;S was fined 2.9 million dollars&lt;/a&gt;, but the city is still currently using the machine.&lt;/p&gt;
&lt;p&gt;Conflicts of interest could be raised in connection with the ExpressVote XL in New York as well.  In 2018, &lt;a href=&#34;https://www.ny1.com/nyc/all-boroughs/politics/2018/12/12/mike-ryan-board-of-elections-executive-director-resigns-from-election-systems-and-software-es-s-advisory-board&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NY1 reported&lt;/a&gt; that ES&amp;amp;S had paid for travel, hotels and dining for New York City Board of Elections Executive Director Michael Ryan. &lt;a href=&#34;https://smartelections.us/ess-problems#88365a43-43f0-41b8-9857-5512f77148b5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ryan subsequently signed a letter&lt;/a&gt; to the New York State Board of Elections asking to use the pricey ExpressVote XL.&lt;/p&gt;
&lt;p&gt;In addition to Pennsylvania, the ExpressVote XL is in use in Delaware and New Jersey and certified for use in California and Texas. &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, another security expert who signed the coalition letter said, “New York election officials made a good call, but the fact that a voting machine with this many security issues is being marketed and sold across the country is a clear indicator that we need to carefully examine our national certification process.” Following the decision, SMART Elections Executive Director Lulu Friesdat said, &amp;ldquo;If the legislature will now pass the “hybrid-ban bill”, we can make this protection permanent. The bill has been introduced in the New York &lt;a href=&#34;https://www.nyassembly.gov/leg/?bn=1115&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Assembly by Amy Paulin&lt;/a&gt;, and in the &lt;a href=&#34;https://www.nysenate.gov/legislation/bills/2021/s309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Senate by Zellnor Myrie&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SMART Elections is part of a broad nationwide coalition of partners working to bring better voting machines to New York and other states. Coalition partners sent over 500 letters to the New York State Board of Elections and over 700 letters to New York legislators, as well as making calls to New York Governor Andrew Cuomo. A &lt;a href=&#34;https://smartelections.us/expressvotexl-letter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full list of coalition partners&lt;/a&gt; can be found here.&lt;/p&gt;
&lt;p&gt;Media Notes: To schedule an interview with &lt;a href=&#34;https://www.netrootsnation.org/profile/shugahworksgmail-com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lulu Friesdat&lt;/a&gt;, or &lt;a href=&#34;https://www.arthurzschwartz.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arthur Schwartz&lt;/a&gt; or to request additional information on this issue, please contact Lulu Friesdat at &lt;a href=&#34;mailto:Lulu@SMARTelections.us&#34;&gt;Lulu@SMARTelections.us&lt;/a&gt; . &lt;a href=&#34;https://www.cs.princeton.edu/~appel/bio.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrew Appel&lt;/a&gt; and &lt;a href=&#34;https://www.scs.gatech.edu/people/richard-demillo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rich DeMillo&lt;/a&gt;, may be reached via their websites.&lt;/p&gt;
&lt;h2 id=&#34;about-smart-elections&#34;&gt;About SMART Elections&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://smartelections.us/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SMART Elections&lt;/a&gt; is a nonpartisan project dedicated to elevating the issue of election reform to an urgent national priority. We are collaborating to make U.S. elections more secure, accessible, accurate, and fair. To learn more, visit &lt;a href=&#34;https://smartelections.us&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://smartelections.us&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://go.reachmail.net/v/ms/a566b23f-5965-eb11-80c4-b083fed06a4f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://go.reachmail.net/v/ms/a566b23f-5965-eb11-80c4-b083fed06a4f&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://smartelections.us/expressvotexl-letter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://smartelections.us/expressvotexl-letter&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Virtual ICM Seminar with David Bader, ‘Solving Global Grand Challenges with High Performance Data Analytics,’ to Be Held Jan 29</title>
      <link>http://localhost:1313/blog/20210125-datanami/</link>
      <pubDate>Mon, 25 Jan 2021 19:54:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20210125-datanami/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210125-datanami/logo-SCFE21-300x105_hu_ed449599f728db5c.webp 400w,
               /blog/20210125-datanami/logo-SCFE21-300x105_hu_f1aabedafc31e48f.webp 760w,
               /blog/20210125-datanami/logo-SCFE21-300x105_hu_241a31f9b5b0434e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210125-datanami/logo-SCFE21-300x105_hu_ed449599f728db5c.webp&#34;
               width=&#34;300&#34;
               height=&#34;105&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science aims to solve grand global challenges such as: detecting and preventing disease in human populations; revealing community structure in large social networks; protecting our elections from cyber-threats, and improving the resilience of the electric power grid. Unlike traditional applications in computational science and engineering, solving these social problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for research on scalable algorithms and architectures, and development of frameworks for solving these real-world problems on high performance computers, and for improved models that capture the noise and bias inherent in the torrential data streams. In this talk, &lt;strong&gt;David Bader&lt;/strong&gt;, New Jersey Institute of Technology, will discuss the opportunities and challenges in massive data science for applications in social sciences, physical sciences, and engineering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor in the Department of Computer Science and Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. He is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI). Dr. Bader is a leading expert in solving global grand challenges in science, engineering, computing, and data science. His interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics, and he has co-authored over 250 articles in peer-reviewed journals and conferences.&lt;/p&gt;
&lt;p&gt;Dr. Bader has served as a lead scientist in several DARPA programs including High Productivity Computing Systems (HPCS) with IBM, Ubiquitous High Performance Computing (UHPC) with NVIDIA, Anomaly Detection at Multiple Scales (ADAMS), Power Efficiency Revolution For Embedded Computing Technologies (PERFECT), Hierarchical Identify Verify Exploit (HIVE), and Software-Defined Hardware (SDH). He has also served as Director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. Bader is a cofounder of the Graph500 List for benchmarking “Big Data” computing platforms. Bader is recognized as a “RockStar” of High Performance Computing by InsideHPC and as HPCwire’s People to Watch in 2012 and 2014. In April 2019, Bader was awarded an NVIDIA AI Lab (NVAIL) award, and in July 2019, Bader received a Facebook Research AI Hardware/Software Co-Design award. &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.njit.edu/~bader&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Register Now: &lt;a href=&#34;https://supercomputingfrontiers.eu/2020/tickets/ohmaerae9lo3quiepaih/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/2020/tickets/ohmaerae9lo3quiepaih/&lt;/a&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210125-datanami/U-Warsaw-ICM-logo-2021-300x82_hu_151e43f3a08e6a5e.webp 400w,
               /blog/20210125-datanami/U-Warsaw-ICM-logo-2021-300x82_hu_4da0abca9ecea62d.webp 760w,
               /blog/20210125-datanami/U-Warsaw-ICM-logo-2021-300x82_hu_c138816dedaaa382.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210125-datanami/U-Warsaw-ICM-logo-2021-300x82_hu_151e43f3a08e6a5e.webp&#34;
               width=&#34;300&#34;
               height=&#34;82&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Virtual ICM Seminars in Computer and Computational Science are a continuation of the Supercomputing Frontiers Europe conference, which took place virtually in March 2020 and will be back with the new HPC trends to explore in the summer 2021.&lt;/p&gt;
&lt;p&gt;Worldwide Open Science online meetings in HPC, Artificial Intelligence, Quantum Computing, BigData, IoT, computer and data networks are a place to meet and discuss with such personalities as Hiroaki Kitano (Systems Biology Institute, Tokyo / Sony Computer Science Laboratories), Stephen Wolfram (Founder &amp;amp; CEO, Wolfram Research), Alan Edelman (MIT), Aneta Afelt (ICM University od Warsaw, Espace-DEV, IRD Montpellier France), Simon Mutch (University of Melbourne) or Scott Aaronson (University of Texas at Austin).&lt;/p&gt;
&lt;p&gt;For the listing of all ICM seminars please check this link with recordings – &lt;a href=&#34;https://supercomputingfrontiers.eu/2020/past-seminars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/2020/past-seminars/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since March 2020, over 2250 people have participated in virtual meetings organized by ICM (SCFE + 13 seminars) from almost all the time zones. The &lt;a href=&#34;https://supercomputingfrontiers.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/&lt;/a&gt; website with access to the SCFE and seminars’ recordings, has been visited by more than 13k visitors from 122 countries.&lt;/p&gt;
&lt;h2 id=&#34;about-the-interdisciplinary-centre-for-mathematical-and-computational-modelling-icm-university-of-warsaw-uw&#34;&gt;About the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM), University of Warsaw (UW)&lt;/h2&gt;
&lt;p&gt;Established by a resolution of the Senate of the University of Warsaw dated 29 June 1993, the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM), University of Warsaw, is one of the top HPC centres in Poland. ICM is engaged in serving the needs of a large community of computational researchers in Poland through provision of HPC and grid resources, storage, networking and expertise. It has always been an active research centre with high quality research contributions in computer and computational science, numerical weather prediction, visualisation, materials engineering, digital repositories, social network analysis and other areas.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/this-just-in/virtual-icm-seminar-with-david-bader-solving-global-grand-challenges-with-high-performance-data-analytics-to-be-held-jan-29/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/this-just-in/virtual-icm-seminar-with-david-bader-solving-global-grand-challenges-with-high-performance-data-analytics-to-be-held-jan-29/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Virtual ICM Seminar with David Bader, ‘Solving Global Grand Challenges with High Performance Data Analytics,’ to Be Held Jan 29</title>
      <link>http://localhost:1313/blog/20210125-hpcwire/</link>
      <pubDate>Mon, 25 Jan 2021 19:54:07 -0500</pubDate>
      <guid>http://localhost:1313/blog/20210125-hpcwire/</guid>
      <description>&lt;p&gt;Jan. 25, 2021 — The Interdisciplinary Centre for Mathematical and Computational Modelling (ICM) at the University of Warsaw invites enthusiasts of Data Science, HPC and all people interested in challenging topics in Computer and Computational Science to the ICM Seminar in Computer and Computational Science that will be held on Friday, January 29, 4 pm CET. The event is free.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210125-hpcwire/logo-SCFE21-300x105_hu_ed449599f728db5c.webp 400w,
               /blog/20210125-hpcwire/logo-SCFE21-300x105_hu_f1aabedafc31e48f.webp 760w,
               /blog/20210125-hpcwire/logo-SCFE21-300x105_hu_241a31f9b5b0434e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210125-hpcwire/logo-SCFE21-300x105_hu_ed449599f728db5c.webp&#34;
               width=&#34;300&#34;
               height=&#34;105&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science aims to solve grand global challenges such as: detecting and preventing disease in human populations; revealing community structure in large social networks; protecting our elections from cyber-threats, and improving the resilience of the electric power grid. Unlike traditional applications in computational science and engineering, solving these social problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for research on scalable algorithms and architectures, and development of frameworks for solving these real-world problems on high performance computers, and for improved models that capture the noise and bias inherent in the torrential data streams. In this talk, &lt;strong&gt;David Bader&lt;/strong&gt;, New Jersey Institute of Technology, will discuss the opportunities and challenges in massive data science for applications in social sciences, physical sciences, and engineering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Distinguished Professor in the Department of Computer Science and Director of the Institute for Data Science at New Jersey Institute of Technology. Prior to this, he served as founding Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. He is a Fellow of the IEEE, AAAS, and SIAM, and advises the White House, most recently on the National Strategic Computing Initiative (NSCI). Dr. Bader is a leading expert in solving global grand challenges in science, engineering, computing, and data science. His interests are at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics, and he has co-authored over 250 articles in peer-reviewed journals and conferences.&lt;/p&gt;
&lt;p&gt;Dr. Bader has served as a lead scientist in several DARPA programs including High Productivity Computing Systems (HPCS) with IBM, Ubiquitous High Performance Computing (UHPC) with NVIDIA, Anomaly Detection at Multiple Scales (ADAMS), Power Efficiency Revolution For Embedded Computing Technologies (PERFECT), Hierarchical Identify Verify Exploit (HIVE), and Software-Defined Hardware (SDH). He has also served as Director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. Bader is a cofounder of the Graph500 List for benchmarking “Big Data” computing platforms. Bader is recognized as a “RockStar” of High Performance Computing by InsideHPC and as HPCwire’s People to Watch in 2012 and 2014. In April 2019, Bader was awarded an NVIDIA AI Lab (NVAIL) award, and in July 2019, Bader received a Facebook Research AI Hardware/Software Co-Design award. &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.njit.edu/~bader&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Register Now: &lt;a href=&#34;https://supercomputingfrontiers.eu/2020/tickets/ohmaerae9lo3quiepaih/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/2020/tickets/ohmaerae9lo3quiepaih/&lt;/a&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20210125-hpcwire/U-Warsaw-ICM-logo-2021-300x82_hu_151e43f3a08e6a5e.webp 400w,
               /blog/20210125-hpcwire/U-Warsaw-ICM-logo-2021-300x82_hu_4da0abca9ecea62d.webp 760w,
               /blog/20210125-hpcwire/U-Warsaw-ICM-logo-2021-300x82_hu_c138816dedaaa382.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20210125-hpcwire/U-Warsaw-ICM-logo-2021-300x82_hu_151e43f3a08e6a5e.webp&#34;
               width=&#34;300&#34;
               height=&#34;82&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Virtual ICM Seminars in Computer and Computational Science are a continuation of the Supercomputing Frontiers Europe conference, which took place virtually in March 2020 and will be back with the new HPC trends to explore in the summer 2021.&lt;/p&gt;
&lt;p&gt;Worldwide Open Science online meetings in HPC, Artificial Intelligence, Quantum Computing, BigData, IoT, computer and data networks are a place to meet and discuss with such personalities as Hiroaki Kitano (Systems Biology Institute, Tokyo / Sony Computer Science Laboratories), Stephen Wolfram (Founder &amp;amp; CEO, Wolfram Research), Alan Edelman (MIT), Aneta Afelt (ICM University od Warsaw, Espace-DEV, IRD Montpellier France), Simon Mutch (University of Melbourne) or Scott Aaronson (University of Texas at Austin).&lt;/p&gt;
&lt;p&gt;For the listing of all ICM seminars please check this link with recordings – &lt;a href=&#34;https://supercomputingfrontiers.eu/2020/past-seminars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/2020/past-seminars/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since March 2020, over 2250 people have participated in virtual meetings organized by ICM (SCFE + 13 seminars) from almost all the time zones. The &lt;a href=&#34;https://supercomputingfrontiers.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingfrontiers.eu/&lt;/a&gt; website with access to the SCFE and seminars’ recordings, has been visited by more than 13k visitors from 122 countries.&lt;/p&gt;
&lt;h2 id=&#34;about-the-interdisciplinary-centre-for-mathematical-and-computational-modelling-icm-university-of-warsaw-uw&#34;&gt;About the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM), University of Warsaw (UW)&lt;/h2&gt;
&lt;p&gt;Established by a resolution of the Senate of the University of Warsaw dated 29 June 1993, the Interdisciplinary Centre for Mathematical and Computational Modelling (ICM), University of Warsaw, is one of the top HPC centres in Poland. ICM is engaged in serving the needs of a large community of computational researchers in Poland through provision of HPC and grid resources, storage, networking and expertise. It has always been an active research centre with high quality research contributions in computer and computational science, numerical weather prediction, visualisation, materials engineering, digital repositories, social network analysis and other areas.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/virtual-icm-seminar-with-david-bader-solving-global-grand-challenges-with-high-performance-data-analytics-to-be-held-jan-29/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/virtual-icm-seminar-with-david-bader-solving-global-grand-challenges-with-high-performance-data-analytics-to-be-held-jan-29/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pierrelotichelsea.com/digital-icm-seminar-with-david-bader-solving-world-wide-grand-issues-with-superior-general-performance-details-analytics-to-be-held-jan-29.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pierrelotichelsea.com/digital-icm-seminar-with-david-bader-solving-world-wide-grand-issues-with-superior-general-performance-details-analytics-to-be-held-jan-29.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Rise of A.I. (TV Series) Official Trailer</title>
      <link>http://localhost:1313/blog/20210121-techtalkmedia/</link>
      <pubDate>Thu, 21 Jan 2021 16:09:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20210121-techtalkmedia/</guid>
      <description>
      &lt;div
          style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
        &lt;iframe
          src=&#34;https://player.vimeo.com/video/503264561?dnt=0&#34;
            style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allow=&#34;fullscreen&#34;&gt;
        &lt;/iframe&gt;
      &lt;/div&gt;

&lt;p&gt;Discover a world of artificial intelligence all around us, listening, seeing, hearing and soon it could overtake us! The Rise of A.I takes you deep inside the experts and creators behind artificial intelligence of our generation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.techtalkmedia.tv/projects/the-rise-of-ai-series.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.techtalkmedia.tv/projects/the-rise-of-ai-series.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replacing NJ’s old voting machines will come with big price tag. How big? Who knows</title>
      <link>http://localhost:1313/blog/20201221-njspotlight/</link>
      <pubDate>Mon, 21 Dec 2020 07:21:09 -0500</pubDate>
      <guid>http://localhost:1313/blog/20201221-njspotlight/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jeff Pillets, NJ Spotlight News&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20201221-njspotlight/Election-NJ-electors-voting-booth-ballot_hu_b2d9c63cac02c31a.webp 400w,
               /blog/20201221-njspotlight/Election-NJ-electors-voting-booth-ballot_hu_af8f771ec63fc25.webp 760w,
               /blog/20201221-njspotlight/Election-NJ-electors-voting-booth-ballot_hu_33033f8ef980883d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20201221-njspotlight/Election-NJ-electors-voting-booth-ballot_hu_b2d9c63cac02c31a.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;New Jersey officials estimate that replacing the state’s aged fleet of voting machines could cost between $60 million and $80 million.&lt;/p&gt;
&lt;p&gt;Add to that the price tag for new technology that would enable early in-person voting — a 2021 priority for state policymakers — and taxpayers could be looking at a $100 million bill in the next few years just to finance their own votes.&lt;/p&gt;
&lt;p&gt;That comes to about $22 for every one of the 4.5 million Jerseyans who cast ballots in this year’s general election.&lt;/p&gt;
&lt;p&gt;But other states that recently took on overhauls of their old voting equipment found that keeping the expense of democracy under control proved tricky, as the cost of employee training, along with maintenance and troubleshooting for the new technology soared. Hidden costs such as licensing fees also hit taxpayers hard.&lt;/p&gt;
&lt;p&gt;Citizen groups in Georgia, for example, said the actual cost of new voting machines installed last year grew to $82 million more than the $104 million budgeted for the statewide project.&lt;/p&gt;
&lt;p&gt;In New York, the cost of installing electronic poll books in early voting centers spiraled past initial estimates to a total of more than $175 million, according to a state elections board report that was leaked to the media. Louisiana taxpayers were also hit with a wave of unexpected costs when the price for their new voting system pushed past $100 million.&lt;/p&gt;
&lt;h2 id=&#34;market-dominated-by-a-few-companies&#34;&gt;Market dominated by a few companies&lt;/h2&gt;
&lt;p&gt;Academic experts who chart the use of technology in American elections say the high cost of machines is, at bottom, a simple function of supply and demand. They point out that the voting machine market in the U.S. is dominated by a tiny handful of firms that operate in a closed market.&lt;/p&gt;
&lt;p&gt;Those firms — Dominion Voting Systems, Hart InterCivic and the Omaha-based ES&amp;amp;S — control more than 90% of the market and are usually the only companies bidding on major voting machine contracts. In New Jersey, the trio are the only firms authorized by the state to sell voting machines, with most of the state’s 21 counties using machines made by ES&amp;amp;S, according to state data.&lt;/p&gt;
&lt;p&gt;“There’s just no competition,” said &lt;strong&gt;David A. Bader&lt;/strong&gt; of the New Jersey Institute of Technology, a computer scientist who consults on voting security issues. “The sole customer is the government.”&lt;/p&gt;
&lt;p&gt;The result, Bader said, is that voting machines end up being a highly specialized type of black box that requires expensive experts to maintain: Instead of being able to shop around for service, states and counties are usually stuck with the sole option of going back to the dealer.&lt;/p&gt;
&lt;p&gt;“Everything ends up being done by the voting machine company,” Bader said. “Training poll workers is done by the company. Firmware updates are done by the company. It can get expensive.”&lt;/p&gt;
&lt;h2 id=&#34;nj-counties-on-the-hook&#34;&gt;NJ counties on the hook?&lt;/h2&gt;
&lt;p&gt;Most New Jersey counties use machines that are at least 10 years old, with some even using equipment installed before 2005. Unlike some states that pay for elections out of the state budget, New Jersey makes counties pay for most elections.&lt;/p&gt;
&lt;p&gt;Frontline officials on local election boards say the huge expense of new voting equipment has scared them away from new machines, although three counties have recently used bond financing or federal grants to help them update old systems.&lt;/p&gt;
&lt;p&gt;Many county officials also say paying for electronic poll books is also impossible. “Absolutely no money for that,” said Hunterdon County Clerk Mary Melfi.&lt;/p&gt;
&lt;h2 id=&#34;wholl-pay-for-electronic-poll-books&#34;&gt;Who’ll pay for electronic poll books?&lt;/h2&gt;
&lt;p&gt;The electronic poll books, about the size of a laptop computer, would be installed in hundreds of new early voting centers across the state as early as next spring under legislation now moving through Trenton. Gov. Phil Murphy and the Democratic leaders who control the Legislature support the early voting law, although they have not specified a final cost or where the money would come from.&lt;/p&gt;
&lt;p&gt;Voting machine experts say there is only one thing worse than buying an expensive machine, and that is buying a bad machine. Even deep in the era of election hacking, they say, unsafe voting machines remain one of the biggest threats to American democracy.&lt;/p&gt;
&lt;p&gt;Greg Miller, a Silicon Valley engineer and lawyer who founded the nonprofit Open Source Election Technology institute in 2006, warned that New Jersey is headed into treacherous territory full of hack-prone, expensive voting kit.&lt;/p&gt;
&lt;p&gt;“The worst thing that can happen is that New Jersey replaces voting machines with equipment that doesn’t protect voters, and spends a fortune doing it,” Miller said. “Look at Georgia, they were hoodwinked.”&lt;/p&gt;
&lt;h2 id=&#34;what-happened-in-georgia&#34;&gt;What happened in Georgia&lt;/h2&gt;
&lt;p&gt;Georgia’s new $100-million-plus touch-screen voting machines, from the Canadian firm Dominion Voting Systems, started to fail as soon as they were rolled out for the June 9 primary election. Electronic poll books crashed, wiping out votes and forcing many people to return to the polls to cast new ballots. Many people stood in line for hours waiting to vote.&lt;/p&gt;
&lt;p&gt;By October, the system was still full of glitches. Test runs showed that touch-screen displays failed to show the names of all candidates in several races. Only days before the Nov. 3 election, technicians from Dominion, armed with thumb drives, rushed to install new software in all the state’s 34,000 voting machines.&lt;/p&gt;
&lt;p&gt;Security experts and voting rights groups worried that the last-minute jiggering left the machines open to hacking and new glitches in future elections.&lt;/p&gt;
&lt;p&gt;Miller said New Jersey could avoid similar problems by using voting machines that allow voters to mark all ballots on paper. Those machines, he said, also can come with the advantage of being a lot cheaper. Essex County is currently the only place in New Jersey with these so-called “voter verified paper ballot” machines.&lt;/p&gt;
&lt;p&gt;“Voting security is a flat-out moral imperative,” Miller said. “But the voting technology we use today has shown time and time again that it is systematically vulnerable. And it’s all been furthered by a closed oligopoly of vendors.”&lt;/p&gt;
&lt;h2 id=&#34;vendors-have-upper-hand&#34;&gt;Vendors have upper hand&lt;/h2&gt;
&lt;p&gt;Miller and other experts say the three major voting machine companies have the upper hand in dealing with local election officials, who simply do not have the technical knowledge to shop smartly for voting gear. So they end up signing exorbitant contracts to buy and maintain black boxes that are too often error-prone and open to potential hacking.&lt;/p&gt;
&lt;p&gt;In recent years, Miller’s group of some 70 engineering geeks and computing specialists has worked with groups like the National Security Agency to advise on voting security. Along with other innovators, they are seeking to create a new digital bible of open-source computer code that would be virtually hack-free and clear the way for a new generation of cheaper, safer voting hardware.&lt;/p&gt;
&lt;p&gt;“In three or four years, this whole world will be recreated,” Miller said.&lt;/p&gt;
&lt;p&gt;Until then, officials in New Jersey say they will keep doing their best with what they have. Election officers in several counties say that, despite all the new concerns about election hacking and potential fraud, they’ve done alright with the old machines and plan to stick with them.&lt;/p&gt;
&lt;p&gt;They point out that a recent statewide audit of ballots in this year’s mostly mail-in elections showed that, in 2020 at least, the system worked. The audit was the first of its kind in decades, made possible by the mail-in paper ballots. In past years, the old touch-screen machines used by most counties did not allow for such a precise check on the votes.&lt;/p&gt;
&lt;p&gt;“We can thank the paper ballot,” said Jamie Sheehan, a member of the Bergen County election board.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;jeff-pillets&#34;&gt;Jeff Pillets&lt;/h3&gt;
&lt;p&gt;Jeff Pillets writes for NJ Spotlight News as part of Votebeat, a nonprofit newsroom covering voting issues in eight states. He has spent more than 25 years as a reporter in New Jersey and has won numerous honors. In 2008, he was named a Pulitzer Prize finalist for stories in The Record exposing public corruption in a $1 billion redevelopment project in the Meadowlands.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Editor’s note: This coverage is made possible through &lt;a href=&#34;http://votebeat.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Votebeat&lt;/a&gt;, a nonpartisan reporting project covering local election integrity and voting access. The article is available for reprint under the terms of &lt;a href=&#34;https://votebeat.org/republishing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Votebeat’s republishing policy&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njspotlight.com/2020/12/replacing-nj-voting-machines-costly-complicated-closed-market-few-companies-hidden-costs-security-hacking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.njspotlight.com/2020/12/replacing-nj-voting-machines-costly-complicated-closed-market-few-companies-hidden-costs-security-hacking/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Edge: Members of the Advisory Council</title>
      <link>http://localhost:1313/blog/20201208-edgediscovery/</link>
      <pubDate>Tue, 08 Dec 2020 16:55:25 -0500</pubDate>
      <guid>http://localhost:1313/blog/20201208-edgediscovery/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader, Ph.D.&lt;/strong&gt;, Distinguished Professor in the Department of
Computer Science in the Ying Wu College of Computing and Director of
the Institute for Data Science at New Jersey Institute of Technology.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;NJEdge EdgeDiscovery&lt;br&gt;
Fall 2020/Winter 2021&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.flipsnack.com/njedge/edgediscovery-fallwinter2020-vol7.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.flipsnack.com/njedge/edgediscovery-fallwinter2020-vol7.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Plans New B.S. in Data Science Degree Program in Fall 2021</title>
      <link>http://localhost:1313/blog/20201110-njit/</link>
      <pubDate>Tue, 10 Nov 2020 10:59:49 -0500</pubDate>
      <guid>http://localhost:1313/blog/20201110-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Dean Mudgett&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20201110-njit/iStock-850494242_hu_f048c324491ad15c.webp 400w,
               /blog/20201110-njit/iStock-850494242_hu_b366068d3fea061.webp 760w,
               /blog/20201110-njit/iStock-850494242_hu_8c0fb32a5bce91e9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20201110-njit/iStock-850494242_hu_f048c324491ad15c.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Responding to the surge in demand for technical skills in data science, NJIT plans to launch in fall 2021 a new Bachelor of Science in Data Science undergraduate degree, co-managed by the Ying Wu College of Computing Department of Computer Science and College of Science and Liberal Arts Department of Mathematical Sciences.&lt;/p&gt;
&lt;p&gt;The program’s two tracks — computing and statistics &amp;ndash; will prepare students for careers in the burgeoning field of data science, which is the theory and practice of extracting information and structure from data, and then using it for adding value to the solution of a problem.&lt;/p&gt;
&lt;p&gt;Typically integrated into software solutions, data science has many applications in health and medicine, finance, marketing, economics, genomics, social networks, cybersecurity, journalism and in practically any field where data is collected. Data science also plays an important role within computer science and mathematics, such as in machine learning and statistical inference, probability, linear algebra, computer programming, software engineering, data mining, high-performance computing and cloud computing.&lt;/p&gt;
&lt;p&gt;NJIT already offers an M.S. degree program in data science. This typically attracts graduates of a computing undergraduate program looking to specialize and graduates of other STEM or business undergraduate programs looking to apply data science techniques to their field of choice.&lt;/p&gt;
&lt;p&gt;Realizing that data science skills are now no less important than general software skills for modern computing undergraduates, the B.S. degree program will follow a structure similar to the general-purpose undergraduate computer science program, while introducing the basics of data science early in the process and focusing on this through a sequence of specially-designed courses. This will generate well-rounded graduates who benefit from the best of both worlds.&lt;/p&gt;
&lt;p&gt;The new program will be one of the few undergraduate programs in data science to be offered in the greater New York area. Universities such as Drexel, Temple, Northeastern and Tufts recently established similar programs, given the exceptional demand for data science professionals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;, who led the creation of this new program, is also the director of NJIT&amp;rsquo;s Institute of Data Science. The Institute brings &amp;ldquo;existing research centers in big data, medical informatics and cybersecurity together with new research centers in data analytics and artificial intelligence, cutting across all NJIT colleges and schools, to conduct both basic and applied research.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader said that he sees the creation of an undergraduate program in Data Science as a quantum leap forward for NJIT. &amp;ldquo;Data science is critical to national competitiveness, which is why we launched the Institute for Data Science and created our master’s program. The National Science Foundation in a recent report stated, ‘The ability to manipulate data and understand data science is becoming increasingly critical to current and future discovery and innovation’,&amp;rdquo; he said. &amp;ldquo;The addition of a B.S program perfectly rounds out NJIT’s data science offerings by helping to address growing workforce needs and giving NJIT students across all degree levels an opportunity to explore this exciting and dynamic field.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In the tri-state region and nationally, there is a critical and growing need for a workforce skilled in data science in industry, labs, and government. Glassdoor ranked data scientist as the best job in America for the past five years and LinkedIn listed data scientists as one of fastest-growing jobs for the past six years.&lt;/p&gt;
&lt;p&gt;Developing New Jersey’s data science ecosystem has also been a focus of Gov. Phil Murphy and the New Jersey Economic Development Authority. By increasing student enrollment from across the state, the greater New York area and nationally, graduates armed with a B.S. in Data Science will help address that demand.&lt;/p&gt;
&lt;p&gt;The financial benefits of investing in data science education in the region are hard to overlook. A recent report from McKinsey predicts that data-driven technologies will bring an additional $300 billion of value to the U.S. health care sector alone. In a 2017 report by IBM, the tech giant predicted the need for analytics and data science positions to jump by about 364,000 positions by 2020.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;As we build this program, we have a wealth of insight into what the structure of a strong and competitive degree program should be, because of the expertise of our faculty in multiple facets of Data Science and our exposure to real data science challenges stemming from our research and interactions with industry,&amp;rdquo; said Professor Eliza Michalopoulou, Chair of the department of Mathematical Sciences.&lt;/p&gt;
&lt;p&gt;“The design of this new program has been informed by not only the work and experience of our own faculty, but also by the important work of the Association for Computing Machinery, the world&amp;rsquo;s largest educational and scientific computing society, who launched the ACM Data Science Task Force to develop an undergraduate curriculum in data science,&amp;quot; added Craig Gotsman, dean of NJIT&amp;rsquo;s Ying Wu College of Computing. &amp;ldquo;As we work to implement our strategic plan to significantly grow the College of Computing in all respects in the next five years, the B.S in Data Science will be a critically important addition to our degree offerings. The faculty and I look forward to welcoming the first students into the program next fall.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-offer-new-undergraduate-degree-data-science-starting-fall-2021&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-offer-new-undergraduate-degree-data-science-starting-fall-2021&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Technology VIPs, Including an Internet Pioneer, Visit NJIT for Inspiration</title>
      <link>http://localhost:1313/blog/20201030-ttivanguard/</link>
      <pubDate>Fri, 30 Oct 2020 13:50:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20201030-ttivanguard/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-leonard-kleinrock-who-devised-the-mathematical-model-for-packet-switching-in-1962&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Leonard Kleinrock, who devised the mathematical model for packet switching in 1962.&#34; srcset=&#34;
               /blog/20201030-ttivanguard/2008-Headshot-Kleinrock_hu_900bb650bda2c905.webp 400w,
               /blog/20201030-ttivanguard/2008-Headshot-Kleinrock_hu_8e879e8a6d541e4f.webp 760w,
               /blog/20201030-ttivanguard/2008-Headshot-Kleinrock_hu_f277ce511d46ef20.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20201030-ttivanguard/2008-Headshot-Kleinrock_hu_900bb650bda2c905.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Leonard Kleinrock, who devised the mathematical model for packet switching in 1962.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ttivanguard.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TTI/Vanguard&lt;/a&gt;, a prestigious organization of technology industry executives who meet a few times each year to study and debate emerging innovations, chose to virtually visit New Jersey Institute of Technology this week for their latest intellectual retreat.&lt;/p&gt;
&lt;p&gt;The group&amp;rsquo;s members, through exposure to wide swaths of cutting-edge technology research, advise their clients and employers about what directions to follow for commercialization and investment opportunities.&lt;/p&gt;
&lt;p&gt;The visit of 58 thought leaders from academia, corporations and government agencies — including Internet pioneer Leonard Kleinrock — was hosted by NJIT&amp;rsquo;s &lt;a href=&#34;https://datascience.njit.edu/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Data Science&lt;/a&gt;, which is led by &lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt;, a TTI/Vanguard member since the mid-2000s. Bader presented on &lt;a href=&#34;https://news.njit.edu/data-science-expert-bader-looks-fed-funding-info-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;large-scale data analytics&lt;/a&gt;, while other Institute faculty presented their own research on &lt;a href=&#34;https://news.njit.edu/institute-brain-and-neuroscience-research-meeting-challenges-understanding-and-healing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;brain injuries&lt;/a&gt;, &lt;a href=&#34;https://news.njit.edu/hunting-covid-19-smartphone-linked-biosensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COVID-19 biosensors&lt;/a&gt;, &lt;a href=&#34;https://news.njit.edu/computer-science-professors-startup-attracts-millions-investment-industry-leaders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computer encryption&lt;/a&gt;, &lt;a href=&#34;https://news.njit.edu/njit-cyberpsychology-seminar-series-begins-features-experts-intersection-psychology-and-tech&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cyberpsychology&lt;/a&gt;/&lt;a href=&#34;https://news.njit.edu/researching-effects-online-misinformation-and-realities-coronavirus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;disinformation&lt;/a&gt; and system optimization for &lt;a href=&#34;https://news.njit.edu/nsf-award-new-software-simplify-crowdsourcing-requests&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;human interactions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;TTI/Vanguard&amp;rsquo;s members include technologists and strategists at the most innovative companies and even governments around the world. Visiting NJIT for their virtual field trip recognizes our research prominence. As New Jersey&amp;rsquo;s leading tech school, it&amp;rsquo;s important that we engage with the broader community to solve some of the most pressing real-world grand challenges,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The data science applications are making societal impacts and economic growth in almost every area,&amp;rdquo; observed NJIT&amp;rsquo;s Atam Dhawan, senior vice provost for research.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It was a great pleasure hosting the TTI/Vanguard field trip featuring some of the collaborative research projects incorporating a spectrum of technologies and data analytics approaches to address high-impact applications,&amp;rdquo; Dhawan continued. &amp;ldquo;The strategic growth of the transdisciplinary, translational research enterprise has enabled NJIT to be classified as one of the top polytechnic research universities in the nation with Carnegie R1 research classification. I am sure that the relationship between NJIT and TTI/Vanguard would further facilitate the translation of innovative and breakthrough technologies to market with a positive impact in our lives.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;TTI/Vanguard — originally Technology Transfer Institute, founded in 1976 — is led by Kleinrock, a particularly esteemed technologist and &lt;a href=&#34;https://cacm.acm.org/magazines/2019/11/240360-an-interview-with-leonard-kleinrock/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one of the founding fathers of the internet&lt;/a&gt;. Kleinrock came up with the mathematical foundations of the internet&amp;rsquo;s core technology, specifically the idea that computers could make network connections by sending data broken into large numbers of equal-sized chunks, for his doctoral thesis in 1962. Today that&amp;rsquo;s taken for granted as packet switching. Then on Oct. 29, 1969, his team at UCLA sent the newborn network&amp;rsquo;s first message, which turned out to be &amp;ldquo;lo&amp;rdquo; because &amp;ldquo;gin&amp;rdquo; got cut off due to a programming bug in the receiving computer at Stanford Research Institute. He still teaches at UCLA, receiving many honors including the National Medal of Science, along with honorary degrees including one from NJIT in 2016 where &lt;a href=&#34;https://www.youtube.com/watch?v=pLid59lgHBE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;he gave a commencement speech&lt;/a&gt; on the theme of breaking boundaries.&lt;/p&gt;
&lt;p&gt;Kleinrock said his favorite part of this week&amp;rsquo;s NJIT presentations was the brain injury research from biomedical engineering Distinguished Professor Namas Chandra. &amp;ldquo;I felt the traumatic brain injury talk was really interesting. The topic is really important, the engineering looked really good,&amp;rdquo; he said. Another favorite topic was the biosensor work for COVID-19 research by chemistry and environmental science Distinguished Professor Wunmi Sadik. Kleinrock and other TTI/Vanguard officials said COVID-19 was, by necessity, a topic that appeared in their group&amp;rsquo;s other meetings in 2020. A speaker from Iceland, who works as a Microsoft software developer, told the group earlier this year how to preempt  other viruses which might jump from animals to humans and where else they may originate. &amp;ldquo;If I had my choice now as a young student, I&amp;rsquo;d probably go into some form of biomedical engineering,&amp;rdquo; Kleinrock said.&lt;/p&gt;
&lt;p&gt;Kleinrock was especially impressed with the work on full homomorphic encryption by professor Kurt Rohloff, a technology that has now reached efficiencies that permit its use in practical applications.&lt;/p&gt;
&lt;p&gt;Computer scientists of Kleinrock&amp;rsquo;s magnitude are often asked for their own future visions. He predicts a resurgence of analog circuitry, especially in the world of quantum computing, which &lt;a href=&#34;https://news.njit.edu/ying-wu-grad-students-get-quantum-computing-course-fall-20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;became part of graduate coursework&lt;/a&gt; in Ying Wu College of Computing this semester.&lt;/p&gt;
&lt;p&gt;For regular users, he&amp;rsquo;s fond of the makerspace movement and products such as microcontrollers, because they bring back a world where regular people can fix things and build their own things. Today he uses a Macbook Pro, iPad and iPhone, although his own computing background began with room-sized mainframes in the 1950s and eventually led to IBM and Tandy-Radio Shack personal computers in the 1980s.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A prediction I had a long time ago is the Internet should be invisible just the way electricity is invisible. The Internet is anything but … it&amp;rsquo;s wonderful but it&amp;rsquo;s clumsy,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Kleinrock noted that it&amp;rsquo;s the unpredictable developments that can be the most exciting. &amp;ldquo;That&amp;rsquo;s where I believe technology is heading overall. The infrastructure is being well-handled by sophisticated professionals. The things that we don&amp;rsquo;t predict well are the applications and the services. They come out of left field, they dominate and surprise us and take over. That&amp;rsquo;s both wonderful and terrible. We never predicted email or the web or search engines or YouTube or Napster or social networks, and they all suddenly took over.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Students today can learn from famed Bell Labs engineer Claude Shannon, the father of information theory, who possessed both mathematical brilliance but also physical intuition, Kleinrock said, adding, &amp;ldquo;I think that combination of experience and thought process is wonderful for creativity and innovation.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Students wishing to learn more about data science can attend the institute&amp;rsquo;s lecture series throughout the year.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/technology-vips-including-internet-pioneer-visit-njit-inspiration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/technology-vips-including-internet-pioneer-visit-njit-inspiration&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TTI/Vanguard Newsletter: Collaboration Everywhere!</title>
      <link>http://localhost:1313/blog/20201023-ttivanguard/</link>
      <pubDate>Fri, 23 Oct 2020 14:05:46 -0500</pubDate>
      <guid>http://localhost:1313/blog/20201023-ttivanguard/</guid>
      <description>&lt;p&gt;Dear Robin,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[next] registration is open for members. Do not miss big name speakers such as Tony Fadell, Sebastian Thrun, George Church and others. We’ll also explore cutting-edge topics such as socio-political-techno responses to the pandemic, autonomy and privacy, the chemistry of quantum computing, wastewater as a forensic tool and so much more! We kick off with a virtual field trip to the New Jersey Institute of Technology this Tuesday, October 27.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Members do have to register separately for the visit to NJIT and [next]&lt;/em&gt;
&lt;em&gt;NJIT virtual field trip &lt;a href=&#34;https://www.ttivanguard.com/TTI-VANGUARD/Virtual-Field-Trip-New-Jersey-Institute-Technology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;agenda&lt;/a&gt; and &lt;a href=&#34;&#34;&gt;registration&lt;/a&gt;&lt;/em&gt;
&lt;em&gt;[next] conference &lt;a href=&#34;https://www.ttivanguard.com/TTI-VANGUARD/next-Virtual-Conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;agenda&lt;/a&gt; and &lt;a href=&#34;&#34;&gt;registration&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Thank you so much for all of the compliments we have already received for these two upcoming events. Let&amp;rsquo;s give credit where credit is due. Longtime TTIV community member &lt;strong&gt;David Bader&lt;/strong&gt; and our own incomparable Nancy Kleinrock designed Tuesday&amp;rsquo;s visit to NJIT. Legendary journalists (amongst other things) John Markoff (former New York Times) and Gregg Zachary (former Wall Street Journal) built the [next] agenda. Hats off and thank you for a yeoman job by all.&lt;/p&gt;
&lt;p&gt;Anyone familiar with our organization knows that we adamantly oppose censorship, but free speech does not equal the right to distribute false information. So we applaud member firm’s Facebook’s decision to remove Holocaust denier content and Twitter’s announcement to follow suit.
&lt;a href=&#34;https://www.cnbc.com/2020/10/15/twitter-follows-facebook-on-removing-posts-that-deny-the-holocaust-.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cnbc.com/2020/10/15/twitter-follows-facebook-on-removing-posts-that-deny-the-holocaust-.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As you’ve surely heard, the U.S. Justice Department has brought an antitrust case against Google. Who better to comment on it than Steve Ballmer, who guided Microsoft through a similar action two decades ago. His recommendation to Sundar Pichai is to move toward the expected endgame as quickly as possible, rather than getting “stuck on the reasonableness of [Google’s] action” that led the tech giant to be broadly perceived as a monopoly (Jonathan Taplin, Boston, Apr 2017). In this wide-ranging interview, Ballmer also discusses his most recent endeavor, the nonpartisan nonprofit USAFacts, which “aims to make government data available so Americans can form their own positions on policy issues.” (Andrew Rasiej, Washington, D.C., May 2012)
&lt;a href=&#34;https://www.washingtonpost.com/politics/2020/10/22/technology-202-former-microsoft-ceo-steve-ballmer-wants-voters-have-facts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.washingtonpost.com/politics/2020/10/22/technology-202-former-microsoft-ceo-steve-ballmer-wants-voters-have-facts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Zoom hosted their annual conference, Zoomtopia. The event featured the possibility of increasing end-to-end encryption (along with all of the regulatory attention that will inevitably attract), immersive experiences, increased safety features, and the release OnZoom for hosting paid online events and welcoming third-party apps and integrations. (Raluca Ada Popa, San Francisco, Dec 2017)
&lt;a href=&#34;https://www.diginomica.com/zoomtopia-2020-zoom-becomes-platform-apps-and-paid-meetings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.diginomica.com/zoomtopia-2020-zoom-becomes-platform-apps-and-paid-meetings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whether it’s the battery in your phone or the one in your electric car, recharge time matters. Xiaomi is making strides with its 80-W wireless charger, claiming that it can top off its phone’s 4000-mAh battery in 20 minutes.
&lt;a href=&#34;https://www.theverge.com/2020/10/19/21522730/xiaomi-mi-80w-wireless-charging-technology-announced&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.theverge.com/2020/10/19/21522730/xiaomi-mi-80w-wireless-charging-technology-announced&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The TTI/Vanguard team loves collaboration, so it makes us extra happy to see two long-time members—Intel and MITRE—partner to accelerate ultrawideband technology research.
twitter.com/MITREcorp/status/1317148572509523968?s=20&lt;/p&gt;
&lt;p&gt;One of the U.S. military’s greatest logistical challenges (amongst many, many logistical challenges) is to get the right type of fuel to specific machines all over the world. Thanks to an $11.4M grant from the Army Research Laboratory, the University of Wisconsin is angling to alleviate some of that pressure with unmanned aircraft that can run on diesel, jet fuel, gasoline, or even ethanol—whichever energy source is closest. (Saul Griffith, San Francisco, Dec 2019)
&lt;a href=&#34;https://eml.iiconferences.com/e/81142/engine-research-at-uw-madison-/5p7sf6/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/engine-research-at-uw-madison-/5p7sf6/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the meantime, Airbus is betting on liquid hydrogen as its fuel of the future. (Joseph Romm, Montreal, Apr 2004; Amory Lovins, Toronto, Apr 2002)
&lt;a href=&#34;https://eml.iiconferences.com/e/81142/l-planes-by-2035-can-they-work/5p7sf8/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/l-planes-by-2035-can-they-work/5p7sf8/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Member organization NASA’s ORISIS-REX spacecraft has been studying the Bennu asteroid from a distance for two years, but this week it took the plunge and made contact—literally—by bumping into the surface with adequate force to stir up and gather a sample of rocks and dirt. We’ll be waiting for its ultimate delivery of the sample to Earth (in Utah on September 24, 2023, to be precise).
&lt;a href=&#34;https://eml.iiconferences.com/e/81142/1-science-nasa-osiris-rex-html/5p7sfb/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/1-science-nasa-osiris-rex-html/5p7sfb/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;MIT’s Kalyan Veeramachaneni (Austin, Feb 2014) has launched a collection of open-source data generation tools—which they collectively call the Synthetic Data Vault—to enable researchers to study properties of real datasets but without the privacy concerns. (Jeanette Wing, Washington, D.C., Sep 2019) &lt;a href=&#34;https://eml.iiconferences.com/e/81142/al-promise-synthetic-data-1016/5p7sfd/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/al-promise-synthetic-data-1016/5p7sfd/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ethan Zuckerman (Seattle, Mar 2020; Washington, D.C., Apr/May 2012; Washington, D.C., May 2007; Miami, Jul 2005; Atlanta, Nov 2000) has launched the new podcast Reimagining the Internet to promote the creation of “online spaces in the public interest, serving civic good instead of a corporate profit motive.”
&lt;a href=&#34;https://eml.iiconferences.com/e/81142/me-to-reimagining-the-internet/5p7sfg/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/me-to-reimagining-the-internet/5p7sfg/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A perfectly thrown football pass is a beautiful thing to behold. Except that it’s not perfect. Without a bit of wobble, the tip of the ball would not tilt down as the ball descends toward the receiver. Physicists from MIT, Lawrence Livermore National Lab, and the University of Nebraska teamed up to demystify the complicated dynamics of the pass.
&lt;a href=&#34;https://eml.iiconferences.com/e/81142/nce-football-pass-physics-html/5p7sfj/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eml.iiconferences.com/e/81142/nce-football-pass-physics-html/5p7sfj/621406331?h=FxiUXs_FgRa8Tmp-uV59cofR-5TSIBOGC6RVz0BQ6Gw&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Well, last week we concluded the newsletter with an item involving Qihoo 360; let’s do it again! This time they appear to have targeted smartwatch-wearing children—specifically, through the X4 watch its subsidiary 360 Kids Guard codesigned and built with Norweigian firm Xplora, which has the mission of “giving children a safe onboarding to digital life.” As Xplora explains, somewhere between early design and launch, a set of potentially privacy-invading real-time geotracking and ambient-sound transmission capabilities were excised from the watch, despite requests from parents to maintain contact with their kids, wherever they might be. A bit of inadvertently left-behind code made it possible for very determined hackers to track a young wearer’s location; Xplora has provided a patch. Still, this highlights the potential perils if security isn’t the number-one priority when designing an IoT device. (Jason Hong, Washington, D.C., Sep 2017; SRI field trip, Feb 2017; Steve Grobman, San Francisco, May 2016)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Magicians are very honest. They say, “I am going to fool you,” and they go right ahead and do it.&lt;/em&gt; —James Randi (1928–2020)&lt;/p&gt;
&lt;p&gt;Robin Lockett&lt;br&gt;
Director, Global Meetings&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ttivanguard.com/content/October-23-2020-NEWSLETTER-Collaboration-everywhere&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ttivanguard.com/content/October-23-2020-NEWSLETTER-Collaboration-everywhere&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Institute for Data Science Unveils 2020-2021 Talks, Many to Discuss COVID-19</title>
      <link>http://localhost:1313/blog/20201012-njit/</link>
      <pubDate>Mon, 19 Oct 2020 10:24:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20201012-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Evan Koblentz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-institute-for-data-science-will-host-many-more-talks-than-last-year&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Institute for Data Science will host many more talks than last year&#34; srcset=&#34;
               /blog/20201012-njit/NJIT-IDS_hu_3bfb90047aab2dc2.webp 400w,
               /blog/20201012-njit/NJIT-IDS_hu_d930c1e6a320fa08.webp 760w,
               /blog/20201012-njit/NJIT-IDS_hu_910668b134dba884.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20201012-njit/NJIT-IDS_hu_3bfb90047aab2dc2.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Institute for Data Science will host many more talks than last year
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;About two dozen experts on data science are giving seminars to the NJIT community this semester and next, with many of them excited to participate virtually from far away with the students and faculty here in New Jersey .&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of NJIT&amp;rsquo;s Institute for Data Science, said he is excited to host such prestigious guest speakers representing academia, government and industry. While the Institute hosted several guest speakers in the past year, such as the chief data scientist from The New York Times last spring, the broad scope of this year’s series covers more real-life applications of data science, he noted.&lt;/p&gt;
&lt;p&gt;The speakers so far were NJIT&amp;rsquo;s own Senjuti Basu Roy, who spoke on Optimization Opportunities in Human-in-the-Loop Systems; Yifan Hu (Yahoo Research Labs), whose topic was What’s in a Name? Deciphering Names Through Machine Learning; Adam McLaughlin, (D.E. Shaw Research), Accelerating GPU Betweenness Centrality; Michael Mahoney (University of California, Berkeley), Dynamical Systems and Machine Learning: Combining in a Principled Way Data-Driven Models and Domain-Driven Models; Srinivas Aluru (Georgia Institute of Technology), Genomes Galore: Big Data Challenges in Computational Genomics and Systems Biology; and Francine Berman (Rensselaer Polytechnic Institute), The Internet of Things: Utopia or Dystopia?&lt;/p&gt;
&lt;p&gt;Following is the rest of the schedule. Additional speakers are being planned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;October 14, 2020, Viktor Prasanna, University of Southern California, Accelerating Data Science at the Edge&lt;/li&gt;
&lt;li&gt;October 21, 2020, Jon Kleinberg, Cornell University, Fairness and Bias in Algorithmic Decision-Making&lt;/li&gt;
&lt;li&gt;October 28, 2020, Manish Parashar, National Science Foundation and Rutgers University, Transforming Science in the 21st Century: NSF’s Vision for a National Cyberinfrastructure Ecosystem&lt;/li&gt;
&lt;li&gt;November 4, 2020, Rick Stevens, Argonne National Laboratory, Artificial Intelligence for Science&lt;/li&gt;
&lt;li&gt;November 11, 2020, Leman Akoglu, Carnegie Mellon University, Graph-based Anomaly Detection: Problems, Algorithms, and Applications&lt;/li&gt;
&lt;li&gt;November 18, 2020, Narayan Srinivasa, Intel Labs, Towards building new AI using Neuromorphic Computing Systems​&lt;/li&gt;
&lt;li&gt;December 2, 2020, Helen Berman, Rutgers University, Building Community Resources for Structural Biology&lt;/li&gt;
&lt;li&gt;December 9, 2020, Chandra Bajaj, University of Texas at Austin, details coming soon&lt;/li&gt;
&lt;li&gt;January 27, 2021, Steven Skiena, SUNY Stony Brook, details coming soon&lt;/li&gt;
&lt;li&gt;February 3, 2021, Tanya Berger-Wolf, The Ohio State University, details coming soon&lt;/li&gt;
&lt;li&gt;February 10, 2021, Jeannette Wing, Columbia University, details coming soon&lt;/li&gt;
&lt;li&gt;February 17, 2021, Prashant Reddy, J.P. Morgan AI Research, details coming soon&lt;/li&gt;
&lt;li&gt;February 24, 2021, Deja Bond, Eelovee, details coming soon&lt;/li&gt;
&lt;li&gt;March 3, 2021, Vipin Kumar, University of Minnesota, details coming soon&lt;/li&gt;
&lt;li&gt;March 31, 2021, Danai Koutra, University of Michigan, details coming soon&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the speakers are discussing the coronavirus. &amp;ldquo;Understanding the spread of the virus, rapid detection, vaccine development and treatment for SARS-CoV-2 is a major contribution of the data science community,&amp;rdquo; Bader said. Georgia Tech&amp;rsquo;s Aluru covered how the virus spreads, Argonne’s Stevens will discuss machine learning for massive screening of COVID-19 molecular docking, while Rutgers/NSF&amp;rsquo;s Parashar will address the federal COVID-19 High Performance Computing Consortium. There are sure to be many more examples in the remaining talks because many aspects of modeling and simulation closely align with data science, Bader added.&lt;/p&gt;
&lt;p&gt;Bader said the lectures by Rensselaer&amp;rsquo;s Berman and Rutgers/NSF&amp;rsquo;s Parashar are especially exciting. All of the lectures will stream live on YouTube and most will also be archived there.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;NJIT is very fortunate to attract this broad set of speakers and interact with our students,&amp;rdquo; Bader continued. &amp;ldquo;These speakers represent the highest caliber of data science thought leaders in the nation.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datascience.njit.edu/events-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Click here for the full schedule and registration details.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/institute-data-science-unveils-2020-2021-talks-many-discuss-covid-19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/institute-data-science-unveils-2020-2021-talks-many-discuss-covid-19&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>1st Algorithmic Breakthrough in 40 years for solving the Minimum Spanning Tree (MST) Replacement Edges problem</title>
      <link>http://localhost:1313/blog/20200819-mst-replacement/</link>
      <pubDate>Wed, 19 Aug 2020 21:12:59 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200819-mst-replacement/</guid>
      <description>&lt;p&gt;One of the most studied algorithms in computer science is called
&amp;ldquo;&lt;a href=&#34;https://www.wikipedia.org/wiki/Minimum_spanning_tree&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minimum Spanning
Tree&lt;/a&gt;&amp;rdquo; or
MST. In this problem, one is given a graph comprised of vertices and
weighted edges, and asked to find a subset of edges that connects all
of the vertices, and the total sum of their weights is as small as
possible. Many real-world optimization problems are solved by finding
a minumum spanning tree, such as lowest cost for distribution on road
networks where intersections are vertices and weights could be length
of the road or time to drive that segment. In 1926, Czech scientist
Otakar Borůvka was the first to design an MST algorithm. Other famous
approaches to solving MST are often given by the name of the scientist
who designed MST algorithm in the late 1950&amp;rsquo;s such as Prim, Kruskal,
and Dijkstra.&lt;/p&gt;
&lt;p&gt;Several important variations of MST are also used in real
applications, including the replacement problem. Imagine a use case
when an edge in the MST degrades and either has a significanly
increased cost or is removed entirely. One must quickly find the
lowest cost &amp;ldquo;replacement edge&amp;rdquo; that reconnects the spanning
tree. Several algorithms are known for this MST replacement edge
problem. The first algorithm, due to Spira and Pan in 1975, took cubic
time in the number of vertices. They presented an $O(n^2)$ algorithm
to update the MST when new vertices are added, and could find all
replacement edges in $O(n^3)$ time, where $n$ is the number of
vertices in the graph. This was improved by Chin and Houck in 1978 to
a quadratic time algorithm, or $O(n^2)$, using a more efficient
approach to insert and delete vertices from the graph.  The best
approach to date is due to Tarjan in 1979, who gave an $O(m \alpha(n,
m))$ time algorithm using path compression, where $m$ is the number of
edges in the graph and $\alpha(n, m)$ is the inverse Ackermann&amp;rsquo;s
function. $\alpha()$ is a very slow growing function, usually a number
around 3 or 4 in practice, but still a gap has remained if a better
approach exists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For the first time in 40 years, progress has been made on this
important graph algorithm.&lt;/strong&gt; With Paul Burkhardt, we&amp;rsquo;ve designed a
simple algorithm that runs very fast in linear time and space, or
$O(n+m)$ where $n$ and $m$ are the number of vertices and edges,
respectively, in the graph. The paper entitled &lt;a href=&#34;https://arxiv.org/abs/1908.03473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Linear Time
Algorithm for Finding Minimum Spanning Tree Replacement
Edges&lt;/a&gt; is now available in Arxiv.
The main result of this paper is &lt;strong&gt;the first linear-time algorithm for
finding all replacement edges in the minimum spanning tree.&lt;/strong&gt; Our
linear time and space algorithm is an asymptotic improvement from all
prior algorithms, uses only simple arrays and Gabow-Tarjan disjoint
set union data structures, alleviates the need to use least common
ancestor (LCA) algorithms, and is easy to implement.&lt;/p&gt;
&lt;p&gt;Other important graph algorithms need to find replacement edges, a
step often considered their bottleneck in performance. For example,
&lt;strong&gt;the most vital edge&lt;/strong&gt; of a connected, weighted graph $G$ is the edge
whose removal causes the largest increase in the weight of the minimum
spanning tree.  When the graph contains bridges, the most vital edge
is undefined. Several algorithms were designed in the early 1990&amp;rsquo;s for
the most vital edge, including $O(m \log m)$ and $O(n^2)$ time from
Hsu et al. in 1991, and improvements to this approach by Iwano and
Katoh in 1993 with $O(m+n \log n)$ and $O(m \alpha(m,n))$ time
algorithms. When using our new MST replacement edge algorithm, we now
can find the most vital edge in linear time (or $O(n)$) by simply
finding the tree edge with maximum difference in weight from its
replacement edge.  Thus, our approach is the first linear algorithm
for finding the most vital edge of the minimum spanning tree.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Northeast Big Data Hub Seed Fund Open for Applications!</title>
      <link>http://localhost:1313/blog/20200804-nebdih/</link>
      <pubDate>Tue, 04 Aug 2020 22:09:32 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200804-nebdih/</guid>
      <description>&lt;h2 id=&#34;seed-fund-announcement&#34;&gt;Seed Fund Announcement&lt;/h2&gt;
&lt;p&gt;The Northeast Big Data Hub is delighted to announce our Seed Fund program this month. The Seed Fund is designed to promote collaboration and support the cross-pollination of tools, data, and ideas across disciplines and sectors including academia, industry, government, and communities. Funding provided through this program is intended to support the northeast region and align with the Major Goals and Focus Areas of the Northeast Big Data Hub.&lt;/p&gt;
&lt;p&gt;Two programs are open for applications as part of the 2020 Seed Fund program:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nebigdatahub.org/seedfund/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seed Fund Grants&lt;/a&gt;   |   &lt;a href=&#34;http://nebigdatahub.org/nsdc-seedfund/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Northeast Student Data Corps&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Applications will be considered in two rounds, with deadlines on August 31st and October 1st, 2020.&lt;/p&gt;
&lt;p&gt;Learn more via the above links. Join our webinar this Monday, August 10th from 1-2pm ET to hear from our seed fund steering committee chair &lt;strong&gt;David Bader&lt;/strong&gt; (NJIT) and find out how to apply.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eventbrite.com/e/northeast-big-data-innovation-hub-seed-fund-webinar-and-qa-tickets-115764501741?mc_cid=208e10f6fb&amp;amp;mc_eid=e425191e8b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Register for the Webinar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Any questions? Drop us a line at &lt;a href=&#34;mailto:contact@nebigdatahub.org&#34;&gt;contact@nebigdatahub.org&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Next Platform TV for July 7, 2020</title>
      <link>http://localhost:1313/blog/20200707-nextplatform/</link>
      <pubDate>Tue, 07 Jul 2020 10:32:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200707-nextplatform/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Nicole Hemsoth&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/SEuHZxlV7a8?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;We are featuring an in-depth interview with Dr. Satoshi Matsuoka, director of the RIKEN Center for Computational Science in Japan and professor of computer science at the Tokyo Institute of Technology. Matsuoka has been involved in the design of so many different supercomputers it is hard to keep track of them all, but the most important one is the Fugaku supercomputer at RIKEN has just won the Triple Crown in supercomputing, with the top performance on the HPL, HPCG, and HPL-AI benchmarks, or the Grand Slam if you count the Graph500 test as well. Like its K supercomputer predecessor, it is an all-CPU system with fat vector engines and a torus interconnect, and we expect it be around for a long time like K. But will Fugaku be the last of its kind? We ask that and many more questions.&lt;/p&gt;
&lt;p&gt;Keeping on the HPC front, we talk about high performance, high efficiency exascale computing with Dr. Giovanni Agosta, one of the creators of the RECIPE framework for energy optimization around thermal hotspots on today’s largest supercomputers.&lt;/p&gt;
&lt;p&gt;The program turns to &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, a pioneer in HPC and later, big data/analytics at scale where we discuss where AI/ML fit into both areas and what’s on the horizon for both systems and software, especially as data volumes and complexity continue to grow.&lt;/p&gt;
&lt;p&gt;Our focus shifts to quantum computing for scientific computing later in the show with Dr. Matthias Moller who describes offload and programming frameworks for hybrid HPC and quantum computing workloads on the horizon.&lt;/p&gt;
&lt;p&gt;We end the program Gary Smerdon, CEO at TidalScale, about the new generation of virtualization and composability that is now available in the datacenter, and representing something that was not invented by the hyperscalers (for once). But in the end, given the benefits that being able to glue machines together into shared memory systems as well as slice them up into virtual machine partitions – or run bare metal-ish with Kubernetes on top – they might either use TidalScale or invent something very much like it. Cloud builders and large enterprises will similarly be very keen on this new wave of composability, which goes far beyond virtualizing storage and networking and compute on a single machine.&lt;/p&gt;
&lt;h2 id=&#34;cheat-sheet-timestamps&#34;&gt;Cheat Sheet (Timestamps)&lt;/h2&gt;
&lt;p&gt;1:47 – Dr. Satoshi Matsuoka (Fugaku system)&lt;br&gt;
19:57 – &lt;strong&gt;Dr. David Bader&lt;/strong&gt; (HPC to Big Data to AI, future directions)&lt;br&gt;
31:07 – Dr. Matthias Moller (Quantum programming framework)&lt;br&gt;
37:38 – Dr. Giovanni Agosta (RECIPE framework for exascale computing)&lt;br&gt;
41:53 – Gary Smerdon (Virtualization and hyperconverged innovations)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nextplatform.com/2020/07/07/next-platform-tv-for-july-7-2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nextplatform.com/2020/07/07/next-platform-tv-for-july-7-2020/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Opportunities: New Seed Fund Program, Coming Soon</title>
      <link>http://localhost:1313/blog/20200625-nebdih/</link>
      <pubDate>Thu, 25 Jun 2020 15:46:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200625-nebdih/</guid>
      <description>&lt;p&gt;The Northeast Big Data Innovation Hub Seed Fund will be launching by this fall to support data science activities within our community. We are pleased to announce &lt;strong&gt;David Bader&lt;/strong&gt; as Chair of the Seed Fund Steering Committee. Bader is a Distinguished Professor in the Department of Computer Science and inaugural Director of the Institute for Data Science at New Jersey Institute of Technology, and a leading expert in solving global grand challenges in science, engineering, computing, and data science. Our &lt;a href=&#34;http://nebigdatahub.org/seedfund/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;full Seed Fund Steering Committee&lt;/a&gt; met for the first time earlier this month, and is hard at work developing and executing on this exciting new community program.&lt;/p&gt;
&lt;p&gt;Visit our &lt;a href=&#34;http://nebigdatahub.org/seedfund/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;webpage&lt;/a&gt; to keep up to date as we unveil more details about the Seed Fund and how to apply, and stay tuned for further updates in future issues!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Katie Naum, Operations Manager&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mailchi.mp/nebigdatahub/june252020?e=e425191e8b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mailchi.mp/nebigdatahub/june252020?e=e425191e8b&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dr. David Bader Driving Global Transformations through Data Science Solutions</title>
      <link>http://localhost:1313/blog/20200528-edgediscovery/</link>
      <pubDate>Thu, 28 May 2020 14:29:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200528-edgediscovery/</guid>
      <description>&lt;p&gt;New Jersey Institute of Technology
(NJIT) announced last summer that
the University would establish a new
Institute of Data Science, focusing on
cutting-edge interdisciplinary research
and development in all areas of digital
data. To head up this endeavor,
&lt;strong&gt;Dr. David Bader&lt;/strong&gt; joined NJIT as a
Distinguished Professor and Director
of the Institute for Data Science.
Previously serving as the Professor and
Chair of the School of Computational
Science and Engineering at Georgia
Institute of Technology, Bader has
become a leading expert in solving
global grand challenges in science,
engineering, computing, and data
science.&lt;/p&gt;
&lt;p&gt;Bader has built important
collaborations across government,
labs, industry, and academia with a
mission of turning ideas into highimpact
technologies that create worldclass
solutions. Partnerships have
included IBM, NVidia, Cray, and Intel,
with government agencies like the
National Science Foundation (NSF),
Department of Defense, Department
of Energy, and the Defense Advance
Research Project Agency (DARPA).
“Data is impacting decisions in
every field, from health care to
finance, arts, entertainment, and
cybersecurity,” says Bader. “I am drawn
to developing research programs in
the emerging area of data science.
At NJIT, we are at the hub of all of
these areas and the Institute for Data
Science brings together faculty from
computing, engineering, business and
management, and science to develop
data-driven technologies.”&lt;/p&gt;
&lt;h2 id=&#34;a-new-era-of-science&#34;&gt;A New Era of Science&lt;/h2&gt;
&lt;p&gt;As a graduate student, Bader attended
the University of Maryland and was
awarded the NASA Graduate Student
Researchers Fellowship. Bader’s
mentor from NASA was the late
Jerry Soften, who was NASA&amp;rsquo;s first
astrobiologist and the lead scientist on
the Viking mission to Mars. “Through
this mentorship, I learned a few key
lessons about science,” shares Bader.
“During this time, Soften was pursuing
a new field and pioneering as a
biologist as he looked for life outside
this planet. He had to develop the
science needed for something that
falls outside our typical disciplines. I
have taken that to heart, in that the
most interesting research these days
is at the interface between traditional
academic boundaries.”&lt;/p&gt;
&lt;p&gt;Bader says as we move towards
data science, we are looking at
data and the uses and impact of
this information across business
and management, health, financial
services, entertainment, and product
development. “We are entering
into a new era where we must
rethink disciplines and investigate
how machine learning and artificial
intelligence (AI) will further develop
and impact every aspect of our lives.”
With an expansive career of pursuing
highly competitive research, Bader has
managed over eighty projects with an
approximate $185 million worth of
contracts and grants. “I’ve worked with
industry leaders and often provide the
thought leadership for the design of
new algorithms and systems, such as
doing the hardware software co-design
to develop the types of data science
solutions that we will need now and
in the future,” says Bader. “One of the
attractions to research is my desire to
solve real problems and global grand
challenges where the work will have an
impact on people in a way that makes
their lives better or the world a safer
place.”&lt;/p&gt;
&lt;h2 id=&#34;pioneering-data-analytics&#34;&gt;Pioneering Data Analytics&lt;/h2&gt;
&lt;p&gt;Bader has had a lifelong interest in
computing and parallel processing
and has worked closely with many
of the parallel computers over the
last several decades. “Throughout
my entire professional career, I have
focused on data-intensive, highperformance
computing applications,”
shares Bader. “I explore how the
architectural designs of today’s
machines are savvier for solving
data science, machine learning, and
AI on massive scale data analytics.”
Bader’s research group has pioneered
high-performance data analytics
for streaming graphs, where they
review real-world observations and
the interactions between them. “We
explore interactions between people,
places, and things,” explains Bader.
“For example, the network traffic
in health care may include patient
records and hospital visits. We take
these irregular, unstructured datasets
with interactions and turn those
relationships into the abstraction of a
graph where the vertices represent the
people, places, and things. The edges
represent the interactions, which
may have, for instance, timestamps,
values, distances, or other attributes,
and through this graph space, we then
reason about the system.”
The research group may also look
at a social network and discover the
emergence of a new community
of interest. “We may examine
the network flow or traffic in an
organization and try to detect a cyber
threat,” says Bader. “Or we may be
working on areas such as tracking
the pandemic spread of disease
and making recommendations on
the best ways to prevent further
contamination.”&lt;/p&gt;
&lt;h2 id=&#34;the-institute-for-data-science&#34;&gt;The Institute for Data Science&lt;/h2&gt;
&lt;p&gt;NJIT is making significant investments
in technological research and
development and the Institute for
Data Science directly supports the
University’s strategic initiatives of
driving collaboration within the
thriving tech ecosystem in New Jersey
and surrounding areas. “The Institute
is a research focus at NJIT that brings
together three existing research
centers: one in big data, a second
in medical informatics, and a third
in cybersecurity research,” explains
Bader. “We are planning additional
centers, including AI and machine
learning and data sciences for the
financial services sector. The Institute
dissolves traditional boundaries and
unites faculty, students, and staff
across the entire organization to solve
global grand challenges. To solve these
problems, we need to bring a diverse
set of people together with a variety
of expertise and experience and NJIT
provides a place for this collaboration
in the heart of data science activity.”&lt;/p&gt;
&lt;p&gt;In the year ahead, the Institute will
host community workshops and
speakers, including evening classes in
data science for those wishing to gain
more experience. “Recently, we had
Chris Wiggins as a speaker, who is a
professor at Columbia University and
is the Chief Data Scientist of The New
York Times,” says Bader. “We’ve had
speakers from IBM, NVidia, and New
York Presbyterian Hospital to share
their knowledge and expertise, and
we will have a distinguished lecture
in March from Professor Vipin Kumar,
of the University of Minnesota, who is
one of the pioneers in the technique of
data mining and data sciences.”&lt;/p&gt;
&lt;p&gt;Bader says we are still at the early
stages of understanding data science,
in terms of the field’s usage and how to
integrate emerging areas like machine
learning and AI. “We are currently at a
very exciting stage in data science and
we must think about data holistically
and how this information integrates
into the world around us. We must
ensure we look at data science not
just as new technology, but explore
how the field can affect our day-to-day
lives.”&lt;/p&gt;
&lt;h2 id=&#34;connecting-the-community&#34;&gt;Connecting the Community&lt;/h2&gt;
&lt;p&gt;As a newly minted Research 1
institution, NJIT is at the cusp of
growing their world-class research
mission. Over eight hundred
computing students graduate from
NJIT each year, making the institution
the largest producer of computing
talent in the state. “Our new
designation as an R1 school and the
&lt;em&gt;U.S. News &amp;amp; World Report&lt;/em&gt; ranking us
among the top 100 schools, shows we
are well on our way to being a premier
institution,” says Bader. “Plus, the
establishment of the Institute for Data
Science is recognition of the strong
researchers, faculty, and students
that we have at NJIT and allows the
University to continue to attract,
recruit, and grow as an organization.”&lt;/p&gt;
&lt;p&gt;Bader says to further build this
momentum and broaden the impact
of the research community as a
whole, we need to keep developing
consortiums and communication
networks. “In a data-rich environment,
the networks provide the connectivity
of the researchers, the datasets, and
the equipment that is critical to making
progress and achieving success. I look
towards Edge and their establishment
of research networks in New Jersey
for providing a backbone of support,
high-speed data links between our
organizations, and connectivity
across the nation. This network is the
differentiating factor that allows us to
bring together the resources needed
to solve the most data-intensive
applications.”&lt;/p&gt;
&lt;p&gt;As the Director, Bader’s vision for the
Institute is to create a collaborative
hub that reaches all of the faculty
and students at NJIT who are using
data in their research, teaching, and
scholarship. “NJIT aims to produce
the skilled career professionals that
are needed in our community. I want
to help solve the problems we face
in Newark and in our surrounding
areas so we have higher-quality lives,
access to better medical care, safer
organizations that are free from
cyberattacks, and better entertainment
systems. For the Institute to be a
success, my goal is to continue to
positively impact the community
that surrounds us at NJIT and make
tangible contributions to the quality of
their lives.”&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;NJEdge EdgeDiscovery&lt;br&gt;
Spring/Summer 2020&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.flipsnack.com/njedge/edgediscovery-spring-summer-2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.flipsnack.com/njedge/edgediscovery-spring-summer-2020.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Leadership Updates: Steering Committee and Seed Fund Steering Committee</title>
      <link>http://localhost:1313/blog/20200513-nebdih/</link>
      <pubDate>Wed, 13 May 2020 07:26:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200513-nebdih/</guid>
      <description>&lt;p&gt;Dear Northeast Hub Community,&lt;/p&gt;
&lt;p&gt;As new program activities launch under our second phase of NSF funding, we are delighted to welcome new community members to our leadership team who will help guide the Hub forward, and to thank those whose service has helped us reach this point. Following a search made by our project team last month, we are pleased to welcome John Goodhue of the Massachusetts Green High Performance Computing Center, Carsten Eickhoff of Brown University, and Laura Dietz of the University of New Hampshire to our Steering Committee. The Steering Committee plays an active role in advancing the mission of the Hub and broadly represents the interests of Hub stakeholders, and our new members bring a wealth of experience to their roles. Learn more about their backgrounds at our leadership page, and please join us in welcoming them to the team.&lt;/p&gt;
&lt;p&gt;We also thank our current Steering Committee members whose terms are concluding. Vasant Honavar and Andrew McCallum have both provided outstanding commitment to the Northeast Big Data Hub, with each of them serving on the Steering Committee since our first phase. Both will continue as co-PIs on our NSF award #1916585, and Vasant will additionally transition in a new role representing the Northeast Big Data Hub on the National Coordinating Committee. Many thanks to both for their leadership and guidance.&lt;/p&gt;
&lt;p&gt;As part of our second phase, the Hub will administer an annual Seed Fund of $250,000 to support community activities in data science. We are pleased to announce that &lt;strong&gt;David Bader&lt;/strong&gt; of NJIT has agreed to serve as our Inaugural Seed Fund Steering Committee Chair, and is working with the Executive Director&amp;rsquo;s Office on a search for committee members. We look forward to sharing more details on the Seed Fund in the coming months.&lt;/p&gt;
&lt;p&gt;On behalf of the Hub, many thanks to all of our leadership and constituents, for everything you do. You make our community stronger.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Jeannette Wing
Principal Investigator&lt;/p&gt;
&lt;p&gt;Florence Hudson
Interim Executive Director&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://nebigdatahub.org/leadership/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nebigdatahub.org/leadership/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://us13.campaign-archive.com/?u=c1cdd0f77f3dbff8b6c8ec5ef&amp;amp;id=177dfd6903&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://us13.campaign-archive.com/?u=c1cdd0f77f3dbff8b6c8ec5ef&amp;id=177dfd6903&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meet the 2019-2020 New Faculty in Ying Wu College of Computing</title>
      <link>http://localhost:1313/blog/20200424-njit/</link>
      <pubDate>Fri, 24 Apr 2020 19:36:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200424-njit/</guid>
      <description>&lt;p&gt;&lt;em&gt;Written by: Prerna Dar&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-top-row-jacob-chakareski-matthew-toegel-ryan-tolboom-salam-daher-tomer-weiss-cody-buntain-bottom-row-david-bader-jertishta-qerimaj-pan-xu-ravi-varadarajan-przemyslaw-musialski-yao-shen&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Top row: Jacob Chakareski, Matthew Toegel, Ryan Tolboom, Salam Daher, Tomer Weiss, Cody Buntain. Bottom row: David Bader, Jertishta Qerimaj, Pan Xu, Ravi Varadarajan, Przemyslaw Musialski, Yao Shen.&#34; srcset=&#34;
               /blog/20200424-njit/Faculty%20collage_Evan%204.22.20_2_hu_e4b33069aea65572.webp 400w,
               /blog/20200424-njit/Faculty%20collage_Evan%204.22.20_2_hu_546b51691b3aa238.webp 760w,
               /blog/20200424-njit/Faculty%20collage_Evan%204.22.20_2_hu_b8474c27387dc492.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20200424-njit/Faculty%20collage_Evan%204.22.20_2_hu_e4b33069aea65572.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Top row: Jacob Chakareski, Matthew Toegel, Ryan Tolboom, Salam Daher, Tomer Weiss, Cody Buntain. Bottom row: David Bader, Jertishta Qerimaj, Pan Xu, Ravi Varadarajan, Przemyslaw Musialski, Yao Shen.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Whether promising young scholars, researchers with years of experience or seasoned instructors, each year, individuals from around the world join the faculty of Ying Wu College of Computing. In the 2019-2020 academic year, the college welcomed 12 new faculty members, continuing a strategic effort to broaden its impact in research and teaching.&lt;/p&gt;
&lt;p&gt;“I am delighted to welcome our newest cohort of faculty members to Ying Wu College of Computing,” Dean Craig Gotsman said.. “At 12 faculty, this is our largest annual cohort to date and demonstrates NJIT’s commitment to expanding the college, which has become a strong engine of growth for the university. I am happy to see such a talented group of faculty coming from close and far, and also from outside the USA. They will undoubtedly take us from strength to strength.”&lt;/p&gt;
&lt;h2 id=&#34;department-of-computer-science&#34;&gt;Department of Computer Science&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, distinguished professor and director, Institute for Data Science&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; holds a Ph.D. in electrical engineering from the University of Maryland. His research interests lie at the intersection of data science and high-performance computing, with applications in cybersecurity, massive-scale analytics and computational genomics. Prior to joining YWCC, Bader served as founding professor and chair of the School of Computational Science and Engineering at Georgia Institute of Technology. He served as the lead scientist in DARPA programs such as High Productivity Computing Systems in collaboration with IBM, and is an adviser to the White House on the National Strategic Computing Initiative.&lt;/p&gt;
&lt;p&gt;Fun Fact: Bader has a twin sister. He has a strong interest in genomics - the study of genes and their functions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Przemyslaw Musialski, associate professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Przemyslaw Musialski holds a Ph.D. in computer science from the Technical University of Vienna (TU Wien) in Austria. His research interests are geometric modeling, geometry processing, computer graphics and digital fabrication. He develops algorithmic solutions which help designers create products directly on computers, avoiding the need to manufacture preliminary prototypes. Prior to joining Ying Wu, Musialski headed the Computational Fabrication group at the Center for Geometry and Computational Design TU Vienna.&lt;/p&gt;
&lt;p&gt;Fun Fact: The name Przemyslaw means “Thinker” in ancient Polish. “A name like ‘Adam’ would have made my life easier,” he said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pan Xu, assistant professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Pan Xu holds a Ph.D. in computer science from the University of Maryland, and a Ph.D. in operations research from Iowa State University. His research interests span the intersection of algorithms, operations research and artificial intelligence (AI): stochastic optimization problems arising in e-commerce, internet advertising, crowdsourcing markets, data mining, databases and revenue management.&lt;/p&gt;
&lt;p&gt;Fun Fact: The Chinese character of Xu’s first name is highly complicated: it takes 19 strokes to write it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ravi Varadarajan, professor of practice&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ravi Varadarajan holds a Ph.D. in computer science from the University of Pennsylvania. His interests include data structures and algorithms, theory of computation, machine learning, parallel processing, and programming in Java and Python. Prior to joining YWCC, Varadarajan taught computer science at the University of Florida. He also worked for companies such as Informix and Lucent) leading and participating in large-scale software projects.&lt;/p&gt;
&lt;p&gt;Fun fact: Varadarajan is an avid tennis fan, loves listening to South Indian classical music and watching sci-fi shows like Star-Trek.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Yao Shen, university lecturer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Yao Shen holds a Ph.D. in computational chemistry from the University of Notre Dame. Her research focused on machine learning models to predict patient responses to drug treatment, where she integrated data from next-generation sequencing, chemical structures and patients&amp;rsquo; clinical data. Prior to joining YWCC, Shen was a research scientist at Columbia University.&lt;/p&gt;
&lt;p&gt;Fun Fact: Shen is an avid badminton player.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jertishta Qerimaj, university lecturer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Jertishta Qerimaj earned a M.S. in software engineering here at NJIT. Her work focuses on applying artificial intelligence to the financial industry. Qerimaj comes to YWCC with more than 15 years of industry experience as a software developer.&lt;/p&gt;
&lt;p&gt;Fun Fact: Qerimaj has never heard of or met anyone with her Albanian name.&lt;/p&gt;
&lt;h2 id=&#34;department-of-informatics&#34;&gt;Department of Informatics&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Jacob Chakareski, associate professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Jacob Chakareski completed his Ph.D. in electrical and computer engineering at Rice University and Stanford University. His research interests span networked virtual and augmented reality, unmanned aerial vehicles/internet of things (UAV/IOT) sensing and networking, real-time reinforcement learning, 5G wireless edge computing/caching, ubiquitous immersive communication and societal applications. Prior to joining NJIT, Chakareski was an assistant professor of electrical and computer engineering at the University of Alabama.&lt;/p&gt;
&lt;p&gt;Fun Fact: Chakareski can speak more than five languages.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Salam Daher, assistant professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Salam Daher holds a Ph.D. in modeling and simulation from the University of Central Florida (UCF). Her research focuses on using augmented reality (AR) to combine virtual content with the real world, such as 3D modeling of humans in synthetic environments. Prior to joining Ying Wu, Daher was a postdoctoral researcher at the Synthetic Reality Lab at UCF, focusing on the use of AR in healthcare simulation.&lt;/p&gt;
&lt;p&gt;Fun Fact: Daher’s artistic side is as strong as her scientific side. She draws, paints, plays music, writes poems, dances and does martial arts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cody Buntain, assistant professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cody Buntain holds a Ph.D. in computer science from the University of Maryland. His research focuses on social media and how people engage politically online, especially during disasters and times of social unrest. Working with political scientists, he tracks online foreign election interference in U.S. elections, develops techniques to detect this interference across online social platforms and devises information retrieval methods for rapid searches across these platforms. Prior to joining YWCC, Buntain was a postdoctoral research fellow at the Social Media and Political Participation Lab at New York University.&lt;/p&gt;
&lt;p&gt;Fun Fact: Buntain has owned two 1970s motorcycles, However, he’s never owned a motorcycle that actually worked.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Tomer Weiss, assistant professor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Tomer Weiss holds a Ph.D. in computer science from the University of California, Los Angeles. His research interests are visual computing, including the virtual simulation of physically embodied AI agents and algorithmic content creation for virtual, augmented and other immersive reality media. Weiss comes to YWCC from the Computer Graphics and Vision Laboratory at UCLA. Before becoming an academic, he held research scientist positions at Amazon, Autodesk Research and Wayfair.&lt;/p&gt;
&lt;p&gt;Fun Fact: Weiss did his graduate studies in Los Angeles where it was commonly said that any person you ask on the street either lives next to a celebrity or knows how to surf. Weiss’ Ph.D. advisor won an Oscar, and he live(d) next to Elon Musk and Jennifer Aniston.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Matthew Toegel, university lecturer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Matthew Toegel earned a M.S. in information systems from NJIT. His areas of interest include game development and web development utilizing technologies such as Unity 3D Editor and Google Services. Prior to joining YWCC, Toegel was a lead software engineer at Tata Consultancy Services TCS and a Unity developer at the Gluck Neuroscience Lab at Rutgers.&lt;/p&gt;
&lt;p&gt;Fun Fact: Back in college, Toegel launched a startup game studio. “I still continue to work on it. One of these days, I’ll have the bandwidth to help it grow,” he said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ryan Tolboom, university lecturer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Ryan Tolboom earned a M.S. in computer science from NJIT. His interests include semiconductor manufacturing, managing IT infrastructure and training teachers on effective use of classroom technology. Before coming to Newark, Tolboom worked as an educator and technology facilitator in the Monroe Township school system in New Jersey.&lt;/p&gt;
&lt;p&gt;Fun Fact: Tolboom loves pinball machines and used to own one. “They are always in some state of being broken, which makes each machine unique,” he said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/introducing-new-faculty-ying-wu-college-computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/introducing-new-faculty-ying-wu-college-computing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Snapping Foggy Narratives Into Focus</title>
      <link>http://localhost:1313/blog/20200415-njit-research/</link>
      <pubDate>Tue, 14 Apr 2020 20:28:05 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200415-njit-research/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of NJIT’s Institute for Data Science, works on
computing initiatives that will help people make sense of large,
diverse and evolving streams of data from news reports, distributed
sensors and lab test equipment, among other sources connected to
worldwide networks.&lt;/p&gt;
&lt;p&gt;When a patient arrives in an emergency room with
high fever, coughing and shivering, the speed
of diagnosis and treatment depends on the skills of the
medical staff, but also on information. If it’s a rare or newly
spreading infectious disease, reliance on today’s clinical
diagnostic methods may not immediately recognize it.
Ready access to a variety of data — disease genomics,
geospatial maps of its spread and electronic health records
— might even predict it.&lt;/p&gt;
&lt;p&gt;“When we track communities, for example during global
pandemics or for finding influencers in marketing, using
data streams rather than static snapshots enables us to
follow relationships that change over time,” says &lt;strong&gt;David
Bader&lt;/strong&gt;, director of NJIT’s newly established Institute for
Data Science. “We want to see how these relationships
change, such as in pandemic spread of disease, by
following global transportation, disease spread and social
interactions, in near-real time.”&lt;/p&gt;
&lt;p&gt;But that level of gathering, processing and assembly
requires new computing capabilities: the capacity to search
through massive volumes of data from diverse sources, from
unstructured texts and social media to passenger lists and
patient records, and the ability to perform complex queries
at unprecedented speeds. This is particularly important in
health care, he notes, where the speed of diagnosis can stop
the spread of deadly diseases in their tracks.&lt;/p&gt;
&lt;p&gt;“Big data is used to analyze problems related to massive
data sets. Today, they are loaded from storage into memory,
manipulated and analyzed using high-performance
computing (HPC) algorithms, and then returned in a useful
format,” he notes. “This end-to-end workflow provides an
excellent platform for forensic analysis; there is a critical
need, however, for systems that support decision-making
with a continuous workflow.”&lt;/p&gt;
&lt;p&gt;Bader has been working with NVIDIA to develop
methods to allow people to stream relationship entities
and analytics in continuously updated live feeds
through its RAPIDS.ai, an open accelerated data
science framework for accelerating end-to-end
data science and analytics pipelines entirely on
graphics processing units. These graph algorithms,
which contain far more memory access per unit of
computation than traditional scientific computing,
are used to make sense of large volumes of data
from news reports, distributed sensors and lab
test equipment, among other sources connected to
worldwide networks.&lt;/p&gt;
&lt;p&gt;In hardware, HPC systems use custom accelerators
that assist with loading and transforming data for
particular data science tasks. “For instance, we may
only need a few fields from electronic patient records
archived in storage. Rather than retrieve the entire
library before searching records for the information, we
could retrieve only the fields necessary for the query,
thus saving significant cost,” he notes, adding that with
accelerators, key tasks will move them closer to the hardware
and data storage systems.&lt;/p&gt;
&lt;p&gt;He notes that general-purpose computers are reaching
a performance plateau as they hit a ceiling on the number
of transistors that can be placed on a chip. While exploiting
parallelism using multicore processors provided a path
forward for some applications, the target is higher-performing,
specialized chips designed to perform specific
functions such as the Xilinx field-programmable gate
array for signal processing or Google’s tensor processing
unit. Cerebras, he adds, is setting records with the 1.2
trillion-transistors Wafer-Scale Engine for accelerating
deep learning.&lt;/p&gt;
&lt;p&gt;“The hardware-software co-design for analytics is
exciting as we enter a new era with the convergence of
data science and high-performance computing. As
data are created and collected, dynamic graph algorithms
make it possible to answer highly specialized and complex
relationship queries over the entire data set in near-real
time, reducing the latency between data collection and the
capability to take action,” Bader says.&lt;/p&gt;
&lt;p&gt;These systems are also designed to be energy-efficient
and easy to program, while reducing transaction times
by orders of magnitude. The goal is for analysts and data
scientists to be able to make queries in their subject domain
and receive rapid solutions that execute efficiently, rather
than requiring sophisticated programming expertise.&lt;/p&gt;
&lt;p&gt;“The variety of data we ingest continues to change and
we will see data sources we can’t envision today. We need to
create systems that will be able to ingest them,” he says.
Where will this all lead? Bader tells the story of one
company’s aspiration to turn ranting customers into raving
fans by tracking public blogs and social media, identifying
them in their customer records and even preemptively
responding with fixes and repairs before customers had
even contacted the company’s support team. A decade ago,
Dell reported a 98% resolution rate and a 34% conversion
rate turning online “ranters” to “ravers” using social
listening.&lt;/p&gt;
&lt;p&gt;“We are developing predictive analytics — the use of
data to anticipate the future,” Bader says. “Instead of
understanding what has happened, we wish to predict
what will happen.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://issuu.com/njit/docs/research_magazine_2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://issuu.com/njit/docs/research_magazine_2020&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shawn Cicoria, M.S. in Data Science, NJIT@JerseyCity</title>
      <link>http://localhost:1313/blog/20200328-jerseycity/</link>
      <pubDate>Sat, 28 Mar 2020 15:26:04 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200328-jerseycity/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/shawncicoria/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shawn Cicoria&lt;/a&gt;, Principle Software Engineer Manager, Microsoft, discusses the &lt;a href=&#34;https://jerseycity.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M.S. in Data Science program at NJIT@JerseyCity&lt;/a&gt;, highlighting &lt;strong&gt;Prof. David A. Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/xQt5GSgwk8k?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xQt5GSgwk8k&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=xQt5GSgwk8k&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Experts Presenting AI Answers to Real-World Problems at NYC Forum</title>
      <link>http://localhost:1313/blog/20200131-njit/</link>
      <pubDate>Fri, 31 Jan 2020 07:31:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20200131-njit/</guid>
      <description>&lt;p&gt;Experts in artificial intelligence from the Ying Wu College of Computing will highlight how their work solves real-world problems at a prestigious meeting in New York next week.&lt;/p&gt;
&lt;p&gt;The professors — Chaoran Cheng, Jing Li, Zhi Wei and Pan Xu — will share their session stages with researchers from IBM, Facebook, Yahoo and other prominent organizations for audiences at the 34th annual conference of the Association for the Advancement of Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Today, artificial intelligence is great at learning patterns, such as making movie recommendations and identifying friends in our photographs. These technologies also enable futuristic self-driving cars and robotic helpers. These examples are just the beginning,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, director of NJIT&amp;rsquo;s Institute for Data Science.&lt;/p&gt;
&lt;p&gt;Looking forward a few years, the technology&amp;rsquo;s predictive nature will become more reliable and better protect our privacy, Bader said.&lt;/p&gt;
&lt;p&gt;At the conference, Wei and Cheng are jointly presenting on using machine learning software to compare genetic mutations in newborns with documentation of mutations in medical literature. In some cases, mutations are so rare that they are not yet cataloged or annotated in current databases. Geneticists need to manually search relevant literature to understand the mutations and then develop personalized medicine. The proposed algorithm would help to recognize relevant mutations and substantially expedite the search.&lt;/p&gt;
&lt;p&gt;Wei is also using artificial intelligence to help researchers in China who are testing air quality monitoring devices controlled by mobile phones. The devices aren&amp;rsquo;t very accurate, but when their data is taken in aggregate it can be corrected by artificial intelligence and made to show important trends. That&amp;rsquo;s cheaper and more accessible than using a lesser amount of more expensive monitoring tools. The data is then presented to authorities who could suggest laws and policies to improve air quality.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The model will learn from the past and try to make some predictions &amp;hellip; just like a student who learns from practice,&amp;rdquo; Wei explained.&lt;/p&gt;
&lt;p&gt;Li&amp;rsquo;s topic is about how to improve allocation of server resources when a program has high data streaming requirements, such as those that need real-time speed or that are widely distributed. Her method uses a type of artificial intelligence called deep reinforcement learning, which is good for solving problems that involve extreme scalability challenges.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;To maximize system throughput, the resource allocation strategy that partitions the computation tasks of a stream-processing graph onto computing devices must simultaneously balance workload distribution and minimize communication,&amp;rdquo; she stated.
A graph in this context works like a map indicating to the software where and how to arrange its resources in the most efficient way. Li and her colleagues use a graph neural network as the mapping function and reinforcement learning to optimize the function. She said this is the first time where an artificial intelligence component has achieved success in this context.&lt;/p&gt;
&lt;p&gt;Xu&amp;rsquo;s application of artificial intelligence is designed to understand discrimination in ride-booking services such as Lyft and Uber. Most people think of discrimination in terms of drivers declining passengers based on race or ethnicity. However, Xu said the most common example is when passengers of any background are declined because their requested trips are too short or out-of-the-way to be worthwhile for drivers looking to maximize profit. That problem always existed in traditional taxicabs, but now it&amp;rsquo;s easier to study due to the availability of data.&lt;/p&gt;
&lt;p&gt;Lyft and Uber drivers are prevented by company policies from canceling trips due to the length, so instead they delay their arrival in hopes that passengers will do the canceling and hire someone else, Xu explained. Conventional discrimination, of course, is also still a problem — data analyzed by artificial intelligence shows that African-American riders have longer wait times, he said. Software helps determine all of this because the allocation of rides is one big real-time decision-making puzzle, Xu noted.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://informatics.njit.edu/news/njit-experts-presenting-ai-answers-real-world-problems-nyc-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://informatics.njit.edu/news/njit-experts-presenting-ai-answers-real-world-problems-nyc-forum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Evan Koblenz&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT’s Ying Wu College of Computing Launches New Location in Jersey City</title>
      <link>http://localhost:1313/blog/20191119-njit/</link>
      <pubDate>Tue, 19 Nov 2019 10:03:21 -0500</pubDate>
      <guid>http://localhost:1313/blog/20191119-njit/</guid>
      <description>&lt;p&gt;NJIT@JerseyCity is located at 101 Hudson
Street on the Jersey City waterfront and,
in addition to an ultra-modern learning
environment, also provides an expansive
view of the iconic Manhattan skyline.&lt;/p&gt;
&lt;p&gt;NJIT’s Ying Wu College of Computing
(YWCC) offers a master’s degree in Data
Science as well as graduate certificates in Big
Data and Data Mining at NJIT @JerseyCity.
YWCC plans to add a graduate certificate
in Data Visualization in spring 2020 and
further expand next fall to include Cyber
Security graduate programs. Non-credit data
science accelerator programs will begin later
this year.&lt;/p&gt;
&lt;p&gt;Jersey City was chosen for NJIT’s new
location because of the strong demand
for data scientists in the area. Most of the
graduate students at NJIT @JerseyCity
are working professionals attending the
programs in the evenings, hailing from
companies and organizations such as
Microsoft, IBM, Bank of America, JPMorgan
Chase, Prudential, BNY Mellon, TD
Ameritrade, NJ Transit, Con Edison and
City National Bank.&lt;/p&gt;
&lt;p&gt;Roberto Rivera, a business major working
as a market research professional for New
Jersey Transit, said he’s excited about the
new program at NJIT@JerseyCity. “I do a
lot of market analysis on the NJ Transit data
using traditional tools. I will be pursuing the
Certificate in Data Mining to enhance my
skillset and match the recent initiative of NJ
Transit, investing in technology that makes
our business smarter, faster and better. I
am fortunate to have the strong support of
my managers in this endeavor. They value
a combination of quality education with
practical skills.”&lt;/p&gt;
&lt;p&gt;“My employer collects an enormous
quantity of data from its customers on a
daily basis, and we are constantly looking
for ways to put it to good use and improve
our service,” added Evo Yaset, an electrical
engineer and student at NJIT@Jersey City.
Earning an M.S. in Data Science is a unique
opportunity for me to obtain the skills I
need to do precisely that. On top of that, the
location is very convenient, since I live in
Jersey City.”&lt;/p&gt;
&lt;h2 id=&#34;generating-tech-talent&#34;&gt;GENERATING TECH TALENT&lt;/h2&gt;
&lt;p&gt;For a number of years, data scientist has
ranked as one of the top jobs in the U.S. in
terms of salary and market. “The demand
is especially significant in the fast-growing
New York/New Jersey metropolitan area,
and this has been reflected in the incredible
interest we have witnessed in our Jersey City
programs. It is our duty and our privilege
to generate the tech talent to meet this
demand,” said YWCC Dean Craig Gotsman.&lt;/p&gt;
&lt;p&gt;In practice, not all technical professionals
working with data perform the same job and
there is a need for a variety of different skills.
NJIT@JerseyCity offers multiple programs to
meet the variety of skills needed.&lt;/p&gt;
&lt;p&gt;The certificate in Data Mining is
designed for data analysts, professionals
who work with large data sets with a focus
on data access, reporting, basic analysis
and visualization.&lt;/p&gt;
&lt;p&gt;The certificate in Big Data Essentials is for
the data engineers, professionals responsible
for managing the infrastructure for big data
storage, rapid access and pipeline processing
of large data sets and preparing the data
for analysis.&lt;/p&gt;
&lt;p&gt;The most comprehensive program is the
M.S. in Data Science. That program is for
those who want to pursue careers as data
scientists, who create and apply machine
learning models and predictive methods
to large data sets to extract actionable and
valuable information.&lt;/p&gt;
&lt;p&gt;One bonus for those pursuing one of
the certificate programs is that they can
“upgrade” their education down the road
and apply certificate courses taken toward an
M.S. in Data Science.&lt;/p&gt;
&lt;h2 id=&#34;cutting-edge-interdisciplinary-research&#34;&gt;CUTTING-EDGE INTERDISCIPLINARY RESEARCH&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Distinguished Professor David Bader&lt;/strong&gt; leads
the faculty at the Jersey City location. He
also is the director of NJIT’s new Institute
for Data Science. The Institute focuses on
cutting-edge interdisciplinary research and
development in all areas pertinent to
digital data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bader&lt;/strong&gt; teaches the Intro to Big Data course
at NJIT@JerseyCity and is joined by Pantelis
Monogioudis, who teaches Data Mining. In
addition to his work at NJIT, Monogioudis
is also head of Applied Machine Learning
Research at Nokia Bell Labs.&lt;/p&gt;
&lt;p&gt;Andrew Pole, of NJIT’s department of
Mathematical Sciences, teaches Applied
Statistics and Keith Williams of YWCC’s
Department of Informatics, teaches Web
Systems Development. Williams is a seasoned
software development professional, with many
years of instructional and entrepreneurial
hands-on experience in the field.&lt;/p&gt;
&lt;p&gt;Students at NJIT@JerseyCity will develop
a deep knowledge of modern computer
science and be active participants in projects
involving artificial intelligence, big data,
analytics, data mining, visualization and
machine learning.&lt;/p&gt;
&lt;p&gt;Beyond the academic degree programs,
NJIT @JerseyCity will offer non-credit Data
Science Accelerator programs later this
fall. In contrast to the typical boot camps
offered by others, these intense, deep-tech
five-week programs are designed for working
professionals who want to hone their data
science skills in a fast-changing marketplace.&lt;/p&gt;
&lt;p&gt;The expansion to Jersey City enables
NJIT’s Ying Wu College of Computing to
continue growing its reputation as the largest
producer of computing talent in the New
Jersey/New York metro region, enrolling
more than 3,000 students and graduating
more than 800 computing professionals each
year. NJIT has been recognized as a Top 100
National University, by U.S News &amp;amp; World
Report, is rated by Forbes as #1 nationally
for student upward economic mobility
and ranks in the Top 2% Nationwide in
Payscale.com’s College Salary Report.&lt;/p&gt;
&lt;p&gt;Tim Sullivan, CEO of New Jersey’s
Economic Development Authority (NJEDA),
said “Reclaiming New Jersey’s position as
the State of Innovation through targeted
investments in students and workers is
a pillar of Governor Murphy’s plan for a
stronger and fairer New Jersey economy.
NJIT’s expansion to Jersey City will create
new opportunities for New Jerseyans and
make our already highly educated workforce
even more attractive to employers and
entrepreneurs.”&lt;/p&gt;
&lt;p&gt;For more information on all
NJIT @JerseyCity programs, please visit
&lt;a href=&#34;http://jerseycity.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;jerseycity.njit.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Author: Brian Malina is the Director of Communications and Marketing at NJIT’s Ying Wu College of Computing.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>This Week in Neo4j</title>
      <link>http://localhost:1313/blog/20191118-neo4j/</link>
      <pubDate>Mon, 18 Nov 2019 09:06:04 -0500</pubDate>
      <guid>http://localhost:1313/blog/20191118-neo4j/</guid>
      <description>&lt;p&gt;Our featured community member this week is &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, Distinguished Professor at New Jersey Institute of Technology.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dr-david-bader--this-weeks-featured-community-member&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dr. David Bader – This Week’s Featured Community Member&#34; srcset=&#34;
               /blog/20191118-neo4j/this-week-in-neo4j-16-november-2019_hu_db19bfaec1f5db80.webp 400w,
               /blog/20191118-neo4j/this-week-in-neo4j-16-november-2019_hu_b8c965400f8490c7.webp 760w,
               /blog/20191118-neo4j/this-week-in-neo4j-16-november-2019_hu_dca3820536bfff3c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20191118-neo4j/this-week-in-neo4j-16-november-2019_hu_db19bfaec1f5db80.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dr. David Bader – This Week’s Featured Community Member
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Without doing too much ego-boosting, we can just say David is a graph-addict for a long time before it was a ‘thing’. Alongside his role as a professor, he’s a fellow of the IEEE, AAAS, and SIAM, advises the White House, and the National Strategic Computing Initiative (NSCI). David has also co-authored over 250 articles in peer-reviewed journals and conferences.&lt;/p&gt;
&lt;p&gt;While this is all incredibly impressive, what we would like to highlight is David’s strong desire to share graph thinking with the world. If you talk to David about graphs, the conversation will likely go for hours (as most of us graph-addicts relate to).&lt;/p&gt;
&lt;p&gt;He’s a natural thought-leader, not just in the sense of a hobby, but as a career path. As a Distinguished Professor at NJIT, he inspires growing minds to explore capabilities inside connected data (shameless plug for our educators out there: check out the &lt;a href=&#34;https://neo4j.com/graphacademy/edu-program&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neo4j Edu Program&lt;/a&gt;) . David even recently brought in &lt;a href=&#34;https://www.linkedin.com/mynetwork/invite-sent/michael-zelenetz-37b89438&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mike Zelenetz&lt;/a&gt; to present in his classroom presenting on ‘Graphing a Hospital’.&lt;/p&gt;
&lt;p&gt;David is regularly looking for interesting presentations in his classroom, so if you’re interested, you can contact him on the &lt;a href=&#34;https://community.neo4j.com/t/david-bader-worlds-leading-graph-expert-prof-davidbader-georgiatech/1897?_ga=2.148574583.1733116107.1574169088-2130131488.1574169088&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neo4j Community Site&lt;/a&gt;). You can also just ping him if you’re interested in nerding about graphs for hours…just be warned, he’s really knowledgeable and passionate – you’re head may explode with what you learn. :-)&lt;/p&gt;
&lt;p&gt;Oh, and if you ever head over to Georgia Institute of Technology, where David used to work, make sure you check out the graph-theory inspired, &lt;a href=&#34;https://arts.gatech.edu/content/seven-bridges-konigsberg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7 Bridges of Königsberg courtyard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;David, we are so grateful to have kind and giving graph-addicts like you in our community. Thank you for all that you do!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neo4j.com/blog/this-week-in-neo4j-apoc-pearls-exploring-full-text-search-exploring-structural-balance-graphistania-v2-0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://neo4j.com/blog/this-week-in-neo4j-apoc-pearls-exploring-full-text-search-exploring-structural-balance-graphistania-v2-0/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data science expert Bader looks to Fed funding for info analysis</title>
      <link>http://localhost:1313/blog/20191114-njit/</link>
      <pubDate>Thu, 14 Nov 2019 21:59:51 -0500</pubDate>
      <guid>http://localhost:1313/blog/20191114-njit/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20191114-njit/bader_hu_b033baf9498e0e8e.webp 400w,
               /blog/20191114-njit/bader_hu_56722cbe182c98f1.webp 760w,
               /blog/20191114-njit/bader_hu_dc4eb0d2bed1b883.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20191114-njit/bader_hu_b033baf9498e0e8e.webp&#34;
               width=&#34;760&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Evan Koblentz&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Data science has reached a point where techniques such as deep learning can beat humans at recognizing objects, although experts are still figuring out how to make explainable predictions from massive data, NJIT distinguished professor &lt;strong&gt;David Bader&lt;/strong&gt; said.&lt;/p&gt;
&lt;p&gt;Bader leads the university&amp;rsquo;s Institute for Data Science in collaboration with the Ying Wu College of Computing, Newark College of Engineering, Martin Tuchman School of Management, and College of Science and Liberal Arts. He also advises the National Strategic Computing Initiative, founded in 2015 under President Obama through the White House&amp;rsquo;s Office of Science and Technology Policy, which &lt;a href=&#34;https://www.nitrd.gov/pubs/National-Strategic-Computing-Initiative-Update-2019.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;released a report today&lt;/a&gt; reaffirming past goals and emphasizing the needs for federal funding of new hardware types; partnerships between academia, government, and industry; and creative approaches to data science and cybersecurity.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In 2021 the U.S. plans to achieve exascale and we have to start planning now as to how we&amp;rsquo;re going to address global grand challenges post-exascale,&amp;rdquo; Bader noted.&lt;/p&gt;
&lt;p&gt;Today&amp;rsquo;s largest data sets are measured in terabytes (1,000 gigabytes) or even petabytes (1,000 terabytes). Exascale refers to data sets that are 1,000 petabytes, which is 1 billion gigabytes.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The landscape of applications in computing and data science for real-world grand challenges has changed since the original report in 2015,&amp;rdquo; Bader said. &amp;ldquo;Data now incorporates every sector and aspect of life&amp;hellip;. Companies are now built around algorithms and the data that drives those. That&amp;rsquo;s a fundamental shift from where high-performance computing was a number of years ago more narrowly focused on scientific computation for uses in weather prediction, manufacturing, and energy.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;What&amp;rsquo;s next is to understand the new opportunities that come through support from the federal agencies, for instance National Science Foundation, National Institutes of Health, Department of Energy, among others, where we collaborate in partnership with industry and government within strategic priorities,&amp;rdquo; he continued.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important for people to understand what data science has accomplished, but also to understand where it needs to be better, Bader acknowledged. The technology is good at sorting large amounts of information for humans to analyze. Next, &amp;ldquo;Where we are still learning is how to better protect the nation from cyberattack, and how to better discover rare patterns in data sets that may have huge ramifications to organizations,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;Bader said a milestone will be when data science can suggest answers to questions we didn&amp;rsquo;t yet know to ask. He calls it a Carnac moment, in honor of Johnny Carson&amp;rsquo;s Carnac the Magnificent character who comically provided answers before questions were asked.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a seriously difficult challenge for the top experts and top computing systems available today. Bader posed several questions now being asked &amp;ndash; &amp;ldquo;How do we let data speak to us and be able to help us inform how to ask the right questions, and what is the power that we get out of that? What is the power of putting data sets together with each other and what is the possibility of innovation we can have across many sectors? How do we put data together to make our lives better, safer, and more interesting,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Forthcoming research opportunities for the NJIT Institute for Data Science and the data science community generally may lead to answers.&lt;/p&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h2 id=&#34;about-new-jersey-institute-of-technology&#34;&gt;About New Jersey Institute of Technology:&lt;/h2&gt;
&lt;p&gt;One of only 32 polytechnic universities in the United States, New Jersey Institute of Technology (NJIT) prepares students to become leaders in the technology-dependent economy of the 21st century. NJIT&amp;rsquo;s multidisciplinary curriculum and computing-intensive approach to education provide technological proficiency, business acumen and leadership skills. NJIT is rated an &amp;ldquo;R1&amp;rdquo; research university by the Carnegie Classification®, which indicates the highest level of research activity. NJIT conducts approximately $170 million in research activity each year and has a $2.8 billion annual economic impact on the State of New Jersey. NJIT is ranked #1 nationally by Forbes for the upward economic mobility of its lowest-income students and is ranked 53rd out of more than 4,000 colleges and universities for the mid-career earnings of graduates, according to PayScale.com. NJIT also is ranked by U.S. News &amp;amp; World Report as one of the top 100 national universities.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/pub_releases/2019-11/njio-dse111419.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eurekalert.org/pub_releases/2019-11/njio-dse111419.php&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/data-science-expert-bader-looks-fed-funding-info-analysis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/data-science-expert-bader-looks-fed-funding-info-analysis&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Chronicle of Higher Education / NJIT</title>
      <link>http://localhost:1313/blog/20191108-che/</link>
      <pubDate>Fri, 08 Nov 2019 10:48:01 -0500</pubDate>
      <guid>http://localhost:1313/blog/20191108-che/</guid>
      <description>&lt;h2 id=&#34;david-bader&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/h2&gt;
&lt;h2 id=&#34;distinguished-professor-and-director-of-njits-institute-for-data-science&#34;&gt;Distinguished Professor and Director of NJIT&amp;rsquo;s Institute for Data Science&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;What is NJIT&amp;rsquo;s new Institute for Data Science?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The growing abundance and variety of data we amass gives us unprecedented opportunities to improve lives in multifold arenas - manufacturing, health care, financial management, data protection, food safety and traffic navigation are just a few. The Institute for Data Science (IDS) will focus NJIT&amp;rsquo;s multidisciplinary research and workforce skills training on developing technology leaders who will solve global challenges involving data and high-performance computing (HPC). Within the Institute, collaboration among our existing research centers in big data, medical informatics and cybersecurity and our new centers in data analytics and artificial intelligence will generate data-driven technologies to achieve our goals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How will NJIT&amp;rsquo;s new Master&amp;rsquo;s in Data Science advance these efforts?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will train our master&amp;rsquo;s students to think about what questions to ask of data, how to formulate analytics to answer them, to develop high-performance machine learning, and to design new techniques to turn data into real-world intelligence. By engaging confidently with complex data science tasks, our graduates will make a difference in organizations large and small. In business, for example, they will help companies compete in the global economy by harnessing a range  of data in new ways: to make clear how policies affect every aspect of their enterprise, to develop transnational supply chains, and to discover efficiencies across systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What new capabilities will high-performance computing deliver?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are developing predictive analytics - the use of data to anticipate the future. Instead of understanding what has happened, we wish to predict what will happen. In cybersecurity, for instance, we would create cyber analytics to defend our critical infrastructure from attack, rather than perform forensic analyses of log files after a breach. In health informatics, we want to detect diseases in their early stages and develop personalized medicines to cure them. In manufacturing, we would identify defects before they cause catastrophic failures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How must we rethink fundamental aspects of computing to enable these capabilities?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Big data analysis is used to analyze problems related to massive datasets. Today, these datasets are loaded from storage into memory, manipulated and analyzed using HPC algorithms, and then returned in a useful format. This end-to-end workflow provides an excellent platform for forensic analysis; there is a critical need, however, for systems that support decision-making with a continuous workflow. Our HPC systems must focus on ingesting data streams; incorporating new microprocessors and custom data science accelerators that assist with loading and transforming data; and accelerating performance by moving key data science tasks and solutions from software to hardware. These workflows must be energy-efficient and easy to program, while reducing transaction times by orders of magnitude. Analysts and data scientists must be able to ask queries in their subject domain and receive rapid solutions that execute efficiently, rather than requiring sophisticated programming expertise.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Are researchers at the Institute working on these problems?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In collaboration with NVIDIA, a leading technology company that makes GPU accelerators such as the DGX Deep Learning server, we are contributing to RAPIDS.ai, an open GPU data science framework for accelerating end-to-end data science and analytics pipelines entirely on CPUs. The hardware-software co-design for analytics is exciting as we enter a new era with the convergence of data science and high-performance computing. These new analytics pipelines are more energy-efficient and run significantly faster, which is critical for making swift, data-driven decisions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputer analyzes web traffic across entire internet</title>
      <link>http://localhost:1313/blog/20191027-mit/</link>
      <pubDate>Sun, 27 Oct 2019 22:50:19 -0500</pubDate>
      <guid>http://localhost:1313/blog/20191027-mit/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Rob Matheson, MIT News Office&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-using-a-supercomputing-system-mit-researchers-developed-a-model-that-captures-what-global-web-traffic-could-look-like-on-a-given-day-including-previously-unseen-isolated-links-left-that-rarely-connect-but-seem-to-impact-core-web-traffic-right-image-courtesy-of-the-researchers-edited-by-mit-news&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Using a supercomputing system, MIT researchers developed a model that captures what global web traffic could look like on a given day, including previously unseen isolated links (left) that rarely connect but seem to impact core web traffic (right). *Image courtesy of the researchers, edited by MIT News*&#34; srcset=&#34;
               /blog/20191027-mit/MIT-Supercomputing_0_hu_6e68b5dab434747.webp 400w,
               /blog/20191027-mit/MIT-Supercomputing_0_hu_1944db8c8de23900.webp 760w,
               /blog/20191027-mit/MIT-Supercomputing_0_hu_47c7cd68baf530d1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20191027-mit/MIT-Supercomputing_0_hu_6e68b5dab434747.webp&#34;
               width=&#34;639&#34;
               height=&#34;426&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Using a supercomputing system, MIT researchers developed a model that captures what global web traffic could look like on a given day, including previously unseen isolated links (left) that rarely connect but seem to impact core web traffic (right). &lt;em&gt;Image courtesy of the researchers, edited by MIT News&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Using a supercomputing system, MIT researchers have developed a model that captures what web traffic looks like around the world on a given day, which can be used as a measurement tool for internet research and many other applications.&lt;/p&gt;
&lt;p&gt;Understanding web traffic patterns at such a large scale, the researchers say, is useful for informing internet policy, identifying and preventing outages, defending against cyberattacks, and designing more efficient computing infrastructure. A paper describing the approach was presented at the recent IEEE High Performance Extreme Computing Conference.&lt;/p&gt;
&lt;p&gt;For their work, the researchers gathered the largest publicly available internet traffic dataset, comprising 50 billion data packets exchanged in different locations across the globe over a period of several years.&lt;/p&gt;
&lt;p&gt;They ran the data through a novel “neural network” pipeline operating across 10,000 processors of the MIT SuperCloud, a system that combines computing resources from the MIT Lincoln Laboratory and across the Institute. That pipeline automatically trained a model that captures the relationship for all links in the dataset — from common pings to giants like Google and Facebook, to rare links that only briefly connect yet seem to have some impact on web traffic.&lt;/p&gt;
&lt;p&gt;The model can take any massive network dataset and generate some statistical measurements about how all connections in the network affect each other. That can be used to reveal insights about peer-to-peer filesharing, nefarious IP addresses and spamming behavior, the distribution of attacks in critical sectors, and traffic bottlenecks to better allocate computing resources and keep data flowing.&lt;/p&gt;
&lt;p&gt;In concept, the work is similar to measuring the cosmic microwave background of space, the near-uniform radio waves traveling around our universe that have been an important source of information to study phenomena in outer space. “We built an accurate model for measuring the background of the virtual universe of the Internet,” says Jeremy Kepner, a researcher at the MIT Lincoln Laboratory Supercomputing Center and an astronomer by training. “If you want to detect any variance or anomalies, you have to have a good model of the background.”&lt;/p&gt;
&lt;p&gt;Joining Kepner on the paper are: Kenjiro Cho of the Internet Initiative Japan; KC Claffy of the Center for Applied Internet Data Analysis at the University of California at San Diego; Vijay Gadepally and Peter Michaleas of Lincoln Laboratory’s Supercomputing Center; and Lauren Milechin, a researcher in MIT’s Department of Earth, Atmospheric and Planetary Sciences.&lt;/p&gt;
&lt;h2 id=&#34;breaking-up-data&#34;&gt;Breaking up data&lt;/h2&gt;
&lt;p&gt;In internet research, experts study anomalies in web traffic that may indicate, for instance, cyber threats. To do so, it helps to first understand what normal traffic looks like. But capturing that has remained challenging. Traditional “traffic-analysis” models can only analyze small samples of data packets exchanged between sources and destinations limited by location. That reduces the model’s accuracy.&lt;/p&gt;
&lt;p&gt;The researchers weren’t specifically looking to tackle this traffic-analysis issue. But they had been developing new techniques that could be used on the MIT SuperCloud to process massive network matrices. Internet traffic was the perfect test case.&lt;/p&gt;
&lt;p&gt;Networks are usually studied in the form of graphs, with actors represented by nodes, and links representing connections between the nodes. With internet traffic, the nodes vary in sizes and location. Large supernodes are popular hubs, such as Google or Facebook. Leaf nodes spread out from that supernode and have multiple connections to each other and the supernode. Located outside that “core” of supernodes and leaf nodes are isolated nodes and links, which connect to each other only rarely.&lt;/p&gt;
&lt;p&gt;Capturing the full extent of those graphs is infeasible for traditional models. “You can’t touch that data without access to a supercomputer,” Kepner says.&lt;/p&gt;
&lt;p&gt;In partnership with the Widely Integrated Distributed Environment (WIDE) project, founded by several Japanese universities, and the Center for Applied Internet Data Analysis (CAIDA), in California, the MIT researchers captured the world’s largest packet-capture dataset for internet traffic. The anonymized dataset contains nearly 50 billion unique source and destination data points between consumers and various apps and services during random days across various locations over Japan and the U.S., dating back to 2015.&lt;/p&gt;
&lt;p&gt;Before they could train any model on that data, they needed to do some extensive preprocessing. To do so, they utilized software they created previously, called Dynamic Distributed Dimensional Data Mode (D4M), which uses some averaging techniques to efficiently compute and sort “hypersparse data” that contains far more empty space than data points. The researchers broke the data into units of about 100,000 packets across 10,000 MIT SuperCloud processors. This generated more compact matrices of billions of rows and columns of interactions between sources and destinations.&lt;/p&gt;
&lt;h2 id=&#34;capturing-outliers&#34;&gt;Capturing outliers&lt;/h2&gt;
&lt;p&gt;But the vast majority of cells in this hypersparse dataset were still empty. To process the matrices, the team ran a neural network on the same 10,000 cores. Behind the scenes, a trial-and-error technique started fitting models to the entirety of the data, creating a probability distribution of potentially accurate models.&lt;/p&gt;
&lt;p&gt;Then, it used a modified error-correction technique to further refine the parameters of each model to capture as much data as possible. Traditionally, error-correcting techniques in machine learning will try to reduce the significance of any outlying data in order to make the model fit a normal probability distribution, which makes it more accurate overall. But the researchers used some math tricks to ensure the model still saw all outlying data — such as isolated links — as significant to the overall measurements.&lt;/p&gt;
&lt;p&gt;In the end, the neural network essentially generates a simple model, with only two parameters, that describes the internet traffic dataset, “from really popular nodes to isolated nodes, and the complete spectrum of everything in between,” Kepner says.&lt;/p&gt;
&lt;p&gt;Using supercomputing resources to efficiently process a “firehose stream of traffic” to identify meaningful patterns and web activity is “groundbreaking” work, says &lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor of computer science and director of the Institute for Data Science at the New Jersey Institute of Technology. “A grand challenge in cybersecurity is to understand the global-scale trends in Internet traffic for purposes, such as detecting nefarious sources, identifying significant flow aggregation, and vaccinating against computer viruses. [This research group has] successfully tackled this problem and presented deep analysis of global network traffic,” he says.&lt;/p&gt;
&lt;p&gt;The researchers are now reaching out to the scientific community to find their next application for the model. Experts, for instance, could examine the significance of the isolated links the researchers found in their experiments that are rare but seem to impact web traffic in the core nodes.&lt;/p&gt;
&lt;p&gt;Beyond the internet, the neural network pipeline can be used to analyze any hypersparse network, such as biological and social networks. “We’ve now given the scientific community a fantastic tool for people who want to build more robust networks or detect anomalies of networks,” Kepner says. “Those anomalies can be just normal behaviors of what users do, or it could be people doing things you don’t want.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://news.mit.edu/2019/supercomputer-analyzes-web-traffic-across-entire-internet-1028&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://news.mit.edu/2019/supercomputer-analyzes-web-traffic-across-entire-internet-1028&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Researchers Set to Receive Two Innovation Awards at HPEC’19</title>
      <link>http://localhost:1313/blog/20190925-hpec/</link>
      <pubDate>Wed, 25 Sep 2019 10:10:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190925-hpec/</guid>
      <description>&lt;p&gt;Defined by the practice of aggregating power in an effort to achieve greater performance, high-performance computing (HPC) is increasingly becoming more diverse. Now, this market, which is expected to reach $59.65 billion by 2025, is setting its sights on new applications including the use of graphics processing units (GPUs) for deep learning, cloud computing, and more.&lt;/p&gt;
&lt;p&gt;These applications will ultimately speed processing rates and cut computational costs for embedded computing systems used in transportation, healthcare, manufacturing, retail, and a host of other industries.&lt;/p&gt;
&lt;p&gt;But, there are specific requirements and unique challenges – known and unknown – to deploying HPC applications outside of a data center. To address these challenges and advance this growing research area, School of Computational Science and Engineering (CSE) HPC researchers are bringing their expertise to this week’s 2019 &lt;a href=&#34;http://www.ieee-hpec.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE High Performance Extreme Computing Conference&lt;/a&gt; (HPEC).&lt;/p&gt;
&lt;p&gt;HPEC is one of the leading conferences of its kind. It brings HPC and embedded systems researchers together to identify obstacles and develop effective solutions for delivering HPC capabilities to edge computing applications, augment big data with GPUs, and more.&lt;/p&gt;
&lt;p&gt;Researchers from CSE are set to present several papers at this year’s HPEC, which runs from Sept. 24 to 26 in Waltham, Massachusetts. Of these, two papers were submitted to the HPEC 2019 GraphChallenge and are set to receive Innovation Awards.&lt;/p&gt;
&lt;p&gt;“The GraphChallenge creates an annual benchmark that drives community development of new solutions for analyzing graphs and sparse data from social media, sensor feeds, and scientific data to discover relationships between events,” said CSE Ph.D. student Abdurrahman Yasar, an investigator on two of the four Innovation Award winning papers, and a 2018 GraphChallenge champion.&lt;/p&gt;
&lt;p&gt;One of the two CSE winning papers, Linear Algebra-Based Triangle Counting via Fine-Grained Tasking on Heterogeneous Environments, describes an update to the linear-algebraic formulation of the classic triangle-counting problem.&lt;/p&gt;
&lt;p&gt;“Triangle counting is a representative graph analysis algorithm with several applications and is also one of the three benchmarks used in the IEEE HPEC GraphChallenge,” said Yasar.&lt;/p&gt;
&lt;p&gt;“In this work we propose a novel multi-core multi-GPU triangle counting algorithm. Our new approach does not require architecture specific changes on code which is crucial for portability on heterogenous environments and the way we distribute the tasks between GPUs and CPUs is highly appreciated.”&lt;/p&gt;
&lt;p&gt;While triangle counting remains an important benchmark in the GraphChallenge, several other research areas are also prevalent in this year’s CSE HPEC proceedings.&lt;/p&gt;
&lt;p&gt;See the list below to view all Georgia Tech papers being presented at this year’s HPEC conference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Concurrent Katz Centrality for Streaming Graphs - Chunxing Yin, Jason Riedy&lt;/li&gt;
&lt;li&gt;Skip the Intersection: Quickly Counting Common Neighbors on Shared-Memory Systems – Xiaojing An, Kasimir Gabert, James Fox, Oded Green, &lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Improving Scheduling for Irregular Applications with Logarithmic Radix Binning – James Fox, Alok Tripathy, Oded Green&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;graph-challenge-champions&#34;&gt;Graph Challenge Champions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linear Algebra-Based Triangle Counting via Fine-Grained Tasking on Heterogeneous Environments - Abdurrahman Yasar, Sivasankaran Rajamanickam, Jonathan Berry, Michael Wolf, Jeff Young, Ümit V. Çatalyürek&lt;/li&gt;
&lt;li&gt;Scalable Triangle Counting on Distributed-Memory Systems – Seher Acer, Abdurrahman Yasar, Sivasankaran Rajamanickam, Michael Wolf, Ümit V. Çatalyürek&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/news/626750/researchers-set-receive-two-innovation-awards-hpec19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/626750/researchers-set-receive-two-innovation-awards-hpec19&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data Career Notes: September 2019 Edition</title>
      <link>http://localhost:1313/blog/20190919-datanami/</link>
      <pubDate>Thu, 19 Sep 2019 15:48:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190919-datanami/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Oliver Peckham&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this monthly feature, we’ll keep you up-to-date on the latest career developments for individuals in the high-performance computing community. Whether it’s a promotion, new company hire, or even an accolade, we’ve got the details. Check in each month for an updated list and you may even come across someone you know, or better yet, yourself!&lt;/p&gt;
&lt;p&gt;In this monthly feature, we’ll keep you up-to-date on the latest career developments for individuals in the high-performance computing community. Whether it’s a promotion, new company hire, or even an accolade, we’ve got the details. Check in each month for an updated list and you may even come across someone you know, or better yet, yourself!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/njit-professor-receives-facebook-research-award-for-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190919-datanami/David-Bader-150x150_hu_f13a274e75b12c1c.webp 400w,
               /blog/20190919-datanami/David-Bader-150x150_hu_7f9c5871f2930e36.webp 760w,
               /blog/20190919-datanami/David-Bader-150x150_hu_49adafe3befaabba.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190919-datanami/David-Bader-150x150_hu_f13a274e75b12c1c.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of the new Institute for Data Science at the New Jersey Institute of Technology (NJIT), has received an award from Facebook to support analytics research. Bader joined NJIT from Georgia Tech, where he was chair of the School of Computational Science and Engineering. His research aims to develop faster learning patterns to make it easier to extract actionable information from large datasets.&lt;/p&gt;
&lt;p&gt;“Every sector from finance to health, transportation, retail to security, and fashion and entertainment, is rich in data,” Bader said. “These are often large, noisy data sets, with missing elements. We want to create tools for better understanding of these data sets.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2019/09/19/big-data-career-notes-september-2019-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/2019/09/19/big-data-career-notes-september-2019-edition/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPC Career Notes: September 2019 Edition</title>
      <link>http://localhost:1313/blog/20190903-hpcwire/</link>
      <pubDate>Tue, 17 Sep 2019 10:22:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190903-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Oliver Peckham&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this monthly feature, we’ll keep you up-to-date on the latest career developments for individuals in the high-performance computing community. Whether it’s a promotion, new company hire, or even an accolade, we’ve got the details. Check in each month for an updated list and you may even come across someone you know, or better yet, yourself!&lt;/p&gt;
&lt;p&gt;In this monthly feature, we’ll keep you up-to-date on the latest career developments for individuals in the high-performance computing community. Whether it’s a promotion, new company hire, or even an accolade, we’ve got the details. Check in each month for an updated list and you may even come across someone you know, or better yet, yourself!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/njit-professor-receives-facebook-research-award-for-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190903-hpcwire/David-Bader-150x150_hu_f13a274e75b12c1c.webp 400w,
               /blog/20190903-hpcwire/David-Bader-150x150_hu_7f9c5871f2930e36.webp 760w,
               /blog/20190903-hpcwire/David-Bader-150x150_hu_49adafe3befaabba.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190903-hpcwire/David-Bader-150x150_hu_f13a274e75b12c1c.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, director of the new Institute for Data Science at the New Jersey Institute of Technology (NJIT), has received an award from Facebook to support analytics research. Bader joined NJIT from Georgia Tech, where he was chair of the School of Computational Science and Engineering. His research aims to develop faster learning patterns to make it easier to extract actionable information from large datasets.&lt;/p&gt;
&lt;p&gt;“Every sector from finance to health, transportation, retail to security, and fashion and entertainment, is rich in data,” Bader said. “These are often large, noisy data sets, with missing elements. We want to create tools for better understanding of these data sets.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2019/09/03/hpc-career-notes-september-2019-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2019/09/03/hpc-career-notes-september-2019-edition/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT Professor Receives Facebook Research Award for Data Science</title>
      <link>http://localhost:1313/blog/20190813-njit-facebook/</link>
      <pubDate>Tue, 13 Aug 2019 11:24:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190813-njit-facebook/</guid>
      <description>&lt;p&gt;The director of &lt;a href=&#34;http://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT&lt;/a&gt;’s new Institute for Data Science has received an award from Facebook to support real-world analytics research. The research aims to develop faster learning patterns to make it easier for companies to extract actionable information from extremely large data sets.&lt;/p&gt;
&lt;p&gt;Institute Director and Distinguished Professor David Bader joined NJIT last month from Georgia Tech, where he previously served as chair of the School of Computational Science and Engineering within the College of Computing.&lt;/p&gt;
&lt;p&gt;Data Science is a fast-growing area that impacts every industry. As the volume of data collected continues to grow, there is a constant challenge to find ways to analyze and decipher the data in a timely manner.&lt;/p&gt;
&lt;p&gt;“Many of the approaches in use today are not very scalable,” Bader said. “As we move from an experimental environment to the real world we have to be able to scale up.”&lt;/p&gt;
&lt;p&gt;Bader’s award will support his research in the area of artificial intelligence system hardware/software co-design. Specifically, it will support students performing research in designing and implementing scalable graph-learning algorithms.&lt;/p&gt;
&lt;p&gt;From time to time, Facebook makes unrestricted research awards to support organizations working in areas of high impact, according to Daron Green, director of Research Operations within Facebook Research, the division within the company that works with universities on long-term research projects.&lt;/p&gt;
&lt;p&gt;“Every sector from finance to health, transportation, retail to security, and fashion and entertainment, is rich in data,” Bader said. “These are often large, noisy data sets, with missing elements. We want to create tools for better understanding of these data sets.”&lt;/p&gt;
&lt;p&gt;Using graphs to model and analyze large data sets helps users visualize the data, enabling them to spot connections or patterns more easily than other traditional representations such as tables.&lt;/p&gt;
&lt;p&gt;As an example, Bader’s work could help the financial sector by improving the accuracy of fraud detection for credit card transactions by creating a more detailed classification system. This would provide added peace of mind to consumers by significantly reducing how often credit card companies reach out to cardholders to check on potential fraud.&lt;/p&gt;
&lt;p&gt;NJIT’s new Institute for Data Science focuses on cutting-edge interdisciplinary research and development in all areas related to data science. It will bring existing research centers in big data, medical informatics and cybersecurity together with new research centers in data analytics and artificial intelligence, cutting across all NJIT colleges and schools, and conduct both basic and applied research.&lt;/p&gt;
&lt;p&gt;In addition to leading the institute, Bader will also teach graduate level classes in data science at NJIT’s main campus in Newark and its new location in &lt;a href=&#34;https://computing.njit.edu/njit-expands-jersey-city&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jersey City&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By: Brian Malina&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://computing.njit.edu/news/njit-professor-receives-facebook-research-award-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://computing.njit.edu/news/njit-professor-receives-facebook-research-award-data-science&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT to Establish New Institute for Data Science</title>
      <link>http://localhost:1313/blog/20190811-therecord/</link>
      <pubDate>Sun, 11 Aug 2019 09:12:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190811-therecord/</guid>
      <description>&lt;p&gt;Continuing its mission to lead in computing technologies, &lt;a href=&#34;http://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT&lt;/a&gt; announced today that it will establish a new Institute for Data Science, focusing on cutting-edge interdisciplinary research and development in all areas pertinent to digital data. The institute will bring existing research centers in big data, medical informatics and cybersecurity together with new research centers in data analytics and artificial intelligence, cutting across all NJIT colleges and schools, and conduct both basic and applied research.&lt;/p&gt;
&lt;p&gt;The Institute for Data Science will be directed by Distinguished Professor &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;. Bader recently joined NJIT’s &lt;a href=&#34;https://computing.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ying Wu College of Computing&lt;/a&gt; from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.&lt;/p&gt;
&lt;p&gt;The institute will bring together scientists, engineers and users to develop data-driven technologies and apply them to solve fundamental and real-world problems. Beyond academic research, the institute will interact closely with the outside world to identify and solve important problems in the modern data-driven economy.&lt;/p&gt;


















&lt;figure  id=&#34;figure-distinguished-professor-and-director-of-the-institute-for-data-science-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distinguished Professor and Director of the Institute for Data Science David Bader&#34; srcset=&#34;
               /media/Bader-2019_hu_3ce7ae9672961b5.webp 400w,
               /media/Bader-2019_hu_ca7df59221b6b274.webp 760w,
               /media/Bader-2019_hu_f0810016cfd5eaba.webp 1200w&#34;
               src=&#34;http://localhost:1313/media/Bader-2019_hu_3ce7ae9672961b5.webp&#34;
               width=&#34;690&#34;
               height=&#34;711&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Distinguished Professor and Director of the Institute for Data Science David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Bader is a leading expert in data science, working at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics and computational genomics, and has co-authored over 230 articles in peer-reviewed journals and conferences. Bader is a fellow of the IEEE, the Society for Industrial and Applied Mathematics and the American Association for the Advancement of Science, and served on the White House&amp;rsquo;s National Strategic Computing Initiative (NSCI) panel. He was the editor-in-chief of IEEE Transactions on Parallel and Distributed Systems, is the current editor-in-chief of ACM Transactions on Parallel Computing and is a National Science Foundation CAREER Award recipient.&lt;/p&gt;
&lt;p&gt;Bader has also served as director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. He was recognized as a &amp;ldquo;RockStar&amp;rdquo; of High Performance Computing by InsideHPC and as one of HPCwire&amp;rsquo;s People to Watch in 2012 and 2014, respectively. He successfully launched his school&amp;rsquo;s Strategic Partnership Program at Georgia Tech, whose members include leading technology companies and government national laboratories.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njit.edu/provost/fadi/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fadi P. Deek&lt;/a&gt;, provost and senior executive vice president of NJIT, said that the new Institute for Data Science represents a strategic move for NJIT. “Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy. The new institute is a key step in that direction. I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njit.edu/research/vpr/about/dhawan-bio.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atam P. Dhawan&lt;/a&gt;, senior vice provost for research at NJIT, said: “The new institute and its distinguished director will be a giant leap forward for NJIT’s research enterprise. Beyond the increase in basic research that the new institute will generate, we will also collaborate closely in applied research with the thriving tech ecosystem in New Jersey and the New York metro area. These companies are hungry for data science expertise and solutions, and we will be in an excellent position to provide them.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Future Computing Community of Interest Meeting</title>
      <link>http://localhost:1313/blog/20190806-nitrd/</link>
      <pubDate>Tue, 06 Aug 2019 16:12:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190806-nitrd/</guid>
      <description>&lt;p&gt;On August 5-6, 2019, I was invited to attend the &lt;a href=&#34;https://www.nitrd.gov/nitrdgroups/index.php?title=FC-COI-2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Future Computing (FC) Community of Interest Meeting&lt;/a&gt; sponsored by the National Coordination Office (NCO) of &lt;a href=&#34;https://nitrd.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NITRD&lt;/a&gt;.  The Networking and Information Technology Research and Development (NITRD) Program is a formal Federal program that coordinates the activities of 23 member agencies to tackle multidisciplinary, multitechnology, and multisector cyberinfrastructure R&amp;amp;D needs of the Federal Government and the Nation.  The meeting was held in Washington, DC, at the NITRD NCO office.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190806-nitrd/NITRD-large_hu_8540fa83164332cb.webp 400w,
               /blog/20190806-nitrd/NITRD-large_hu_fb5e9880cbb89aed.webp 760w,
               /blog/20190806-nitrd/NITRD-large_hu_cf7207e1129ce1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190806-nitrd/NITRD-large_hu_8540fa83164332cb.webp&#34;
               width=&#34;751&#34;
               height=&#34;389&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The meeting set a goal to &amp;ldquo;explore the computing landscape for the coming decade and beyond, along with emerging and future application drivers, to inform agencies and to identify potential opportunities as well as gaps.  It will also examine new software concepts needed for the effective use of advances that come with the future computing systems to ensure that the federal government is poised to respond to unanticipated challenges and opportunities.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;And the rationale: &amp;ldquo;There is much uncertainty in the HEC community as clock rate increases attributed to Moore’s law and Dennard are expected to end in the near future. The increase in feature count is currently slowing and will likely end within a few years. In this period of uncertainty, it is essential for the community to understand and embrace technologies coming to fruition in both the near- and long-term that provide a path forward for HEC in the absence of Moore’s Law and Dennard scaling. The impact of data analytics (including the use of artificial intelligence/machine learning), involving the movement, manipulation and storage of significant amounts of data has had a disruptive effect on high-end computing as well. The public, academic and private sector all need to be prepared for these new computing modalities from a hardware, software and workforce perspective.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;During the day and a half agenda, activities alternated between talks and breakout sessions, with industry talks given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mark Papermaster (&lt;a href=&#34;https://www.amd.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AMD&lt;/a&gt;): Delivering the Future of High-Performance Computing&lt;/li&gt;
&lt;li&gt;Steve Scott (&lt;a href=&#34;https://www.cray.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cray&lt;/a&gt;):  Beyond Exascale: Playing the CMOS Endgame&lt;/li&gt;
&lt;li&gt;Mike Mayberry (&lt;a href=&#34;https://www.intel.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intel&lt;/a&gt;): Future Computing Models, Technology in the 2030-2040 Horizon&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I led a breakout session on &amp;ldquo;Enabling Data&amp;rdquo; where we discussed future end-to-end workflows, algorithm-driven businesses, data governance and trust, diversity, and convergence of AI and High Performance Computing.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190806-nitrd/20190806-Bader-USCapitol_hu_10f5a047521afdc1.webp 400w,
               /blog/20190806-nitrd/20190806-Bader-USCapitol_hu_bd8b1896deaae9b5.webp 760w,
               /blog/20190806-nitrd/20190806-Bader-USCapitol_hu_51248ec3b63b48f4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190806-nitrd/20190806-Bader-USCapitol_hu_10f5a047521afdc1.webp&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>HPC Career Notes: August 2019 Edition</title>
      <link>http://localhost:1313/blog/20190802-hpcwire/</link>
      <pubDate>Fri, 02 Aug 2019 07:24:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190802-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Oliver Peckham&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this monthly feature, we’ll keep you up-to-date on the latest career developments for individuals in the high-performance computing community. Whether it’s a promotion, new company hire, or even an accolade, we’ve got the details. Check in each month for an updated list and you may even come across someone you know, or better yet, yourself!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190802-hpcwire/David-Bader-150x150_hu_f13a274e75b12c1c.webp 400w,
               /blog/20190802-hpcwire/David-Bader-150x150_hu_7f9c5871f2930e36.webp 760w,
               /blog/20190802-hpcwire/David-Bader-150x150_hu_49adafe3befaabba.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190802-hpcwire/David-Bader-150x150_hu_f13a274e75b12c1c.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The New Jersey Institute of Technology (NJIT) &lt;a href=&#34;https://www.hpcwire.com/off-the-wire/njit-to-establish-new-institute-for-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;has announced&lt;/a&gt; that it is establishing a new Institute for Data Science directed by &lt;strong&gt;David Bader&lt;/strong&gt;. Bader — one of HPCwire‘s 2014 &lt;a href=&#34;https://www.hpcwire.com/people-watch-2014/david-bader-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;People to Watch&lt;/a&gt; — recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.&lt;/p&gt;
&lt;p&gt;“The new institute and its distinguished director will be a giant leap forward for NJIT’s research enterprise,” said Atam P. Dhawan, senior vice provost for research at NJIT. “Beyond the increase in basic research that the new institute will generate, we will also collaborate closely in applied research with the thriving tech ecosystem in the greater New York metro area. These companies are hungry for data science expertise and solutions, and we will be in an excellent position to provide them.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2019/08/02/hpc-career-notes-august-2019-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2019/08/02/hpc-career-notes-august-2019-edition/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data Career Notes: July 2019 Edition</title>
      <link>http://localhost:1313/blog/20190716-datanami/</link>
      <pubDate>Tue, 16 Jul 2019 17:42:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190716-datanami/</guid>
      <description>&lt;h2 id=&#34;david-bader&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190716-datanami/david-bader-150x150_hu_30b27a0e992386e9.webp 400w,
               /blog/20190716-datanami/david-bader-150x150_hu_19b22ed7b259aec.webp 760w,
               /blog/20190716-datanami/david-bader-150x150_hu_bfaf3bfab8681b1d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190716-datanami/david-bader-150x150_hu_30b27a0e992386e9.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The New Jersey Institute of Technology has announced that it will establish a new Institute for Data Science, directed by Distinguished Professor &lt;strong&gt;David Bader&lt;/strong&gt;. Bader recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing. Bader was recognized as one of HPCwire’s People to Watch in 2014.&lt;/p&gt;
&lt;p&gt;“Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy,” said Fadi P. Deek, provost and senior executive vice president of NJIT. “The new institute is a key step in that direction. I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2019/07/16/big-data-career-notes-july-2019-edition/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/2019/07/16/big-data-career-notes-july-2019-edition/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader to Lead New Institute for Data Science at NJIT</title>
      <link>http://localhost:1313/blog/20190710-insidehpc/</link>
      <pubDate>Wed, 10 Jul 2019 17:08:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190710-insidehpc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/David_Bader_%28computer_scientist%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor David Bader&lt;/a&gt;&lt;/strong&gt; will lead the new Institute for Data Science at the &lt;a href=&#34;http://news.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New Jersey Institute of Technology&lt;/a&gt;. Focused on cutting-edge interdisciplinary research and development in all areas pertinent to digital data, the institute will bring existing research centers in big data, medical informatics and cybersecurity together to conduct both basic and applied research.&lt;/p&gt;


















&lt;figure  id=&#34;figure-distinguished-professor-david-bader-bader-recently-joined-njits-ying-wu-college-of-computing-from-georgia-tech-where-he-was-chair-of-the-school-of-computational-science-and-engineering-within-the-college-of-computing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distinguished Professor David Bader. Bader recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.&#34; srcset=&#34;
               /blog/20190710-insidehpc/David-Bader_headshot-crop-291x300_hu_3ce7ae9672961b5.webp 400w,
               /blog/20190710-insidehpc/David-Bader_headshot-crop-291x300_hu_ca7df59221b6b274.webp 760w,
               /blog/20190710-insidehpc/David-Bader_headshot-crop-291x300_hu_f0810016cfd5eaba.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190710-insidehpc/David-Bader_headshot-crop-291x300_hu_3ce7ae9672961b5.webp&#34;
               width=&#34;690&#34;
               height=&#34;711&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Distinguished Professor David Bader. Bader recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;The institute will bring together scientists, engineers and users to develop data-driven technologies and apply them to solve fundamental and real-world problems. Beyond academic research, the institute will interact closely with the outside world to identify and solve important problems in the modern data-driven economy.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Bader is a leading expert in data science, working at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics and computational genomics, and has co-authored over 230 articles in peer-reviewed journals and conferences. Bader is a fellow of the IEEE, the Society for Industrial and Applied Mathematics and the American Association for the Advancement of Science, and served on the White House’s National Strategic Computing Initiative (NSCI) panel. He was the editor-in-chief of IEEE Transactions on Parallel and Distributed Systems, is the current editor-in-chief of ACM Transactions on Parallel Computing and is a National Science Foundation CAREER Award recipient.&lt;/p&gt;
&lt;p&gt;Bader has also served as director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. He was recognized as a “RockStar” of High Performance Computing by InsideHPC and as one of HPCwire’s People to Watch in 2012 and 2014, respectively. He successfully launched his school’s Strategic Partnership Program at Georgia Tech, whose members include leading technology companies and government national laboratories.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy,” said Fadi P. Deek, provost and senior executive vice president of NJIT. “The new institute is a key step in that direction. I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Atam P. Dhawan, senior vice provost for research at NJIT, said: “The new institute and its distinguished director will be a giant leap forward for NJIT’s research enterprise. Beyond the increase in basic research that the new institute will generate, we will also collaborate closely in applied research with the thriving tech ecosystem in the greater New York metro area. These companies are hungry for data science expertise and solutions, and we will be in an excellent position to provide them.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2019/07/david-bader-to-lead-new-institute-for-data-science-at-njit/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2019/07/david-bader-to-lead-new-institute-for-data-science-at-njit/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT to Establish New Institute for Data Science</title>
      <link>http://localhost:1313/blog/20190710-cra/</link>
      <pubDate>Wed, 10 Jul 2019 06:56:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190710-cra/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190710-cra/bader_hu_39924ddf44acbc25.webp 400w,
               /blog/20190710-cra/bader_hu_d1e186661a13a4ea.webp 760w,
               /blog/20190710-cra/bader_hu_912199f1ae788f27.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190710-cra/bader_hu_39924ddf44acbc25.webp&#34;
               width=&#34;200&#34;
               height=&#34;200&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Former CRA Board Member &lt;strong&gt;David Bader&lt;/strong&gt; will direct the new institute for data science at New Jersey Institute of Technology (NJIT). The institute will focus on cutting-edge interdisciplinary research and development in all areas pertinent to digital data. It will bring existing research centers in big data, medical informatics and cybersecurity together with new research centers in data analytics and artificial intelligence, cutting across all NJIT colleges and schools, and conduct both basic and applied research.&lt;/p&gt;
&lt;p&gt;Bader recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.&lt;/p&gt;
&lt;p&gt;From the &lt;a href=&#34;https://www.globenewswire.com/news-release/2019/07/09/1880451/0/en/NJIT-to-Establish-New-Institute-for-Data-Science.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;news release&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The institute will bring together scientists, engineers and users to develop data-driven technologies and apply them to solve fundamental and real-world problems. Beyond academic research, the institute will interact closely with the outside world to identify and solve important problems in the modern data-driven economy.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In October 2016, CRA released a &lt;a href=&#34;https://cra.org/data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statement&lt;/a&gt; on computing research and the emerging field of data science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cra.org/njit-to-establish-new-institute-for-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cra.org/njit-to-establish-new-institute-for-data-science/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT to establish new institute on data science</title>
      <link>http://localhost:1313/blog/20190709-roi-nj/</link>
      <pubDate>Tue, 09 Jul 2019 16:37:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190709-roi-nj/</guid>
      <description>

















&lt;figure  id=&#34;figure-new-jersey-institute-of-technology-in-newark---file-photo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;New Jersey Institute of Technology in Newark. *- File photo*&#34; srcset=&#34;
               /blog/20190709-roi-nj/NJIT-clocktower-crop-696x409_hu_9946a6eb9d6cdbe7.webp 400w,
               /blog/20190709-roi-nj/NJIT-clocktower-crop-696x409_hu_207bc90c5840410f.webp 760w,
               /blog/20190709-roi-nj/NJIT-clocktower-crop-696x409_hu_9d2f6a13b6bddcd5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190709-roi-nj/NJIT-clocktower-crop-696x409_hu_9946a6eb9d6cdbe7.webp&#34;
               width=&#34;696&#34;
               height=&#34;409&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      New Jersey Institute of Technology in Newark. &lt;em&gt;- File photo&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By Emily Bader&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;New Jersey Institute of Technology announced Tuesday it will establish a new institute focused on data science.&lt;/p&gt;
&lt;p&gt;The Institute for Data Science will conduct both basic and applied interdisciplinary research and development in all areas related to digital data to solve fundamental and real-world problems, NJIT said. It will also join together existing research centers in big data, medical informatics and cybersecurity with new research centers in data analytics and artificial intelligence.&lt;/p&gt;
&lt;p&gt;The institute will be directed by &lt;strong&gt;David Bader&lt;/strong&gt;, a professor and leading expert in data science. Bader recently joined NJIT’s Ying Wu College of Computing from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing. He was previously the director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor.&lt;/p&gt;
&lt;p&gt;Fadi P. Deek, provost and senior executive vice president of NJIT, said the institute is a strategic move for NJIT.&lt;/p&gt;
&lt;p&gt;“Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy. The new institute is a key step in that direction. I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us,” Deek said.&lt;/p&gt;
&lt;p&gt;“The new institute and its distinguished director will be a giant leap forward for NJIT’s research enterprise. Beyond the increase in basic research that the new institute will generate, we will also collaborate closely in applied research with the thriving tech ecosystem in the greater New York metro area. These companies are hungry for data science expertise and solutions, and we will be in an excellent position to provide them,” Atam P. Dhawan, senior vice provost for research at NJIT, said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.roi-nj.com/2019/07/09/education/njit-to-establish-new-institute-on-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.roi-nj.com/2019/07/09/education/njit-to-establish-new-institute-on-data-science/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT creates Institute of Data Science to propel AI economy</title>
      <link>http://localhost:1313/blog/20190709-njbiz/</link>
      <pubDate>Tue, 09 Jul 2019 15:23:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190709-njbiz/</guid>
      <description>&lt;p&gt;&lt;strong&gt;New Jersey Institute of Technology&lt;/strong&gt; is creating a center that will conduct basic and applied research focusing on interdisciplinary research and development for all areas pertaining to digital data.&lt;/p&gt;
&lt;p&gt;The Institute of Data Science, unveiled July 9, will be led by &lt;strong&gt;David Bader&lt;/strong&gt;, a distinguished professor at NJIT.&lt;/p&gt;
&lt;p&gt;At the institute, scientists, engineers and users will develop technologies applicable in the “real world,” NJIT said, working beyond academic research to solve “problems in the modern data-driven economy.” It will combine existing centers from across the university’s colleges and schools in big data, medical informatics and cybersecurity with new centers in data analytics and artificial intelligence.&lt;/p&gt;
&lt;p&gt;“Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy,” NJIT Provost and Senior Executive Vice President Fadi Deek said in a  statement. “The new institute is a key step in that direction.&lt;/p&gt;
&lt;p&gt;“I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us,” he added.&lt;/p&gt;
&lt;p&gt;A member of NJIT’s Ying Wu College of Computing, Bader recently arrived from Georgia Tech where he chaired the School of Computational Science and Engineering, a part of the College of Computing.&lt;/p&gt;
&lt;p&gt;At Georgia Tech, Bader successfully launched the Strategic Partnership Program, with members including leading technology companies and national government laboratories. He has co-authored more than 230 articles in peer-reviewed journals and is a fellow of the IEEE, the Society for Industrial and Applied Mathematics and the American Association for the Advancement of Science. He also served on the White House’s Strategic Computing Initiative panel.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://njbiz.com/njit-creates-institute-data-science-propel-ai-economy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://njbiz.com/njit-creates-institute-data-science-propel-ai-economy/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NJIT to Establish New Institute for Data Science</title>
      <link>http://localhost:1313/blog/20190708-njit-institute-for-data-science/</link>
      <pubDate>Mon, 08 Jul 2019 09:12:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190708-njit-institute-for-data-science/</guid>
      <description>&lt;p&gt;Continuing its mission to lead in computing technologies, &lt;a href=&#34;http://www.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NJIT&lt;/a&gt; announced today that it will establish a new Institute for Data Science, focusing on cutting-edge interdisciplinary research and development in all areas pertinent to digital data. The institute will bring existing research centers in big data, medical informatics and cybersecurity together with new research centers in data analytics and artificial intelligence, cutting across all NJIT colleges and schools, and conduct both basic and applied research.&lt;/p&gt;
&lt;p&gt;The Institute for Data Science will be directed by Distinguished Professor &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;David Bader&lt;/a&gt;. Bader recently joined NJIT’s &lt;a href=&#34;https://computing.njit.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ying Wu College of Computing&lt;/a&gt; from Georgia Tech, where he was chair of the School of Computational Science and Engineering within the College of Computing.&lt;/p&gt;
&lt;p&gt;The institute will bring together scientists, engineers and users to develop data-driven technologies and apply them to solve fundamental and real-world problems. Beyond academic research, the institute will interact closely with the outside world to identify and solve important problems in the modern data-driven economy.&lt;/p&gt;


















&lt;figure  id=&#34;figure-distinguished-professor-and-director-of-the-institute-for-data-science-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Distinguished Professor and Director of the Institute for Data Science David Bader&#34; srcset=&#34;
               /media/Bader-2019_hu_3ce7ae9672961b5.webp 400w,
               /media/Bader-2019_hu_ca7df59221b6b274.webp 760w,
               /media/Bader-2019_hu_f0810016cfd5eaba.webp 1200w&#34;
               src=&#34;http://localhost:1313/media/Bader-2019_hu_3ce7ae9672961b5.webp&#34;
               width=&#34;690&#34;
               height=&#34;711&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Distinguished Professor and Director of the Institute for Data Science David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Bader is a leading expert in data science, working at the intersection of high-performance computing and real-world applications, including cybersecurity, massive-scale analytics and computational genomics, and has co-authored over 230 articles in peer-reviewed journals and conferences. Bader is a fellow of the IEEE, the Society for Industrial and Applied Mathematics and the American Association for the Advancement of Science, and served on the White House&amp;rsquo;s National Strategic Computing Initiative (NSCI) panel. He was the editor-in-chief of IEEE Transactions on Parallel and Distributed Systems, is the current editor-in-chief of ACM Transactions on Parallel Computing and is a National Science Foundation CAREER Award recipient.&lt;/p&gt;
&lt;p&gt;Bader has also served as director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. He was recognized as a &amp;ldquo;RockStar&amp;rdquo; of High Performance Computing by InsideHPC and as one of HPCwire&amp;rsquo;s People to Watch in 2012 and 2014, respectively. He successfully launched his school&amp;rsquo;s Strategic Partnership Program at Georgia Tech, whose members include leading technology companies and government national laboratories.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njit.edu/provost/fadi/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fadi P. Deek&lt;/a&gt;, provost and senior executive vice president of NJIT, said that the new Institute for Data Science represents a strategic move for NJIT. “Complementing our new facility in Jersey City, which will focus on data science training, NJIT is making significant investments in technological R&amp;amp;D to drive the new AI economy. The new institute is a key step in that direction. I cannot think of anyone more qualified than Dr. Bader to lead the new institute and we are fortunate to have him join us.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.njit.edu/research/vpr/about/dhawan-bio.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Atam P. Dhawan&lt;/a&gt;, senior vice provost for research at NJIT, said: “The new institute and its distinguished director will be a giant leap forward for NJIT’s research enterprise. Beyond the increase in basic research that the new institute will generate, we will also collaborate closely in applied research with the thriving tech ecosystem in New Jersey and the New York metro area. These companies are hungry for data science expertise and solutions, and we will be in an excellent position to provide them.”&lt;/p&gt;
&lt;p&gt;For more information on the new institute, please contact &lt;a href=&#34;https://en.wikipedia.org/wiki/Craig_Gotsman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Craig Gotsman&lt;/a&gt;, dean of Ying Wu College of Computing, at &lt;a href=&#34;mailto:gotsman@njit.edu&#34;&gt;gotsman@njit.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.njit.edu/njit-establish-new-institute-data-science&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.njit.edu/njit-establish-new-institute-data-science&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;8 July 2019
&lt;em&gt;Written by: Brian Malina&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Showcase Parallel Computing Research at IPDPS 2019</title>
      <link>http://localhost:1313/blog/20190516-gatech-ipdps/</link>
      <pubDate>Thu, 16 May 2019 14:56:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190516-gatech-ipdps/</guid>
      <description>&lt;p&gt;Researchers from the School of Computational Science and Engineering (CSE) will present seven papers at the 33rd IEEE International Parallel and Distributed Processing Symposium (IPDPS 2019) in Rio De Janeiro, Brazil, May 20-24.&lt;/p&gt;
&lt;p&gt;“IPDPS is one of the premier parallel and distributed computing conferences in the world that provides broad coverage of all areas in high performance computing (HPC) and parallel computing,” said CSE Professor Ümit V. Çatalyürek, one of the leaders of Georgia Tech’s participation at this year’s symposium.&lt;/p&gt;
&lt;p&gt;“With seven papers out of 103 in the main conference alone, and many other invited talks, plenary panels and committee services, CSE researchers not only continue to push the research frontier in HPC and parallel processing, but also demonstrate their commitment to serve the parallel processing community at large,” he said.&lt;/p&gt;
&lt;p&gt;Several Georgia Tech faculty, including Çatalyürek, are serving on IPDPS committees and workshops.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Srinivas Aluru – HiCOMB Workshop Steering Committee&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;– IPDPS Steering Committee, HiCOMB Workshop Steering Committee&lt;/li&gt;
&lt;li&gt;Ümit V. Çatalyürek - Technical Committee on Parallel Processing (TCPP) Chair, IPDPS Steering Committee, Parallel and Distributed Scientific and Engineering Computing Panelist, Algorithms Primary Committee&lt;/li&gt;
&lt;li&gt;Ada Gavrilovska– Regular Program Committee, HPBDC Workshop Panel Moderator&lt;/li&gt;
&lt;li&gt;Vivek Sarkar– Regular Program Committee&lt;/li&gt;
&lt;li&gt;Richard Vuduc – iWAPT Program Committee, iWAPT Steering Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IPDPS also serves as the flagship activity of the TC on Parallel Processing (TCPP) which presents another addition to the Georgia Tech at IPDPS roster. CSE Professor and Institute for Data Engineering and Science Co-executive Director Srinivas Aluru was named as this year’s IEEE TCPP Outstanding Service Award winner in recognition of his professional service roles that have had a major impact on the parallel processing research community at large.&lt;/p&gt;
&lt;p&gt;Georgia Tech’s presence at IPDPS this year includes 10 researchers from CSE and three researchers from the School of Computer Science (SCS).&lt;/p&gt;
&lt;p&gt;Georgia Tech’s research:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A scalable clustering-based task scheduler for homogeneous processors using DAG partitioning&lt;/li&gt;
&lt;li&gt;Accelerating Sequence Alignment to Graphs&lt;/li&gt;
&lt;li&gt;Asynchronous Multigrid Methods&lt;/li&gt;
&lt;li&gt;Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems&lt;/li&gt;
&lt;li&gt;Load-Balanced Sparse MTTKRP on GPUs&lt;/li&gt;
&lt;li&gt;Overlapping Communications with Other Communications and its Application to Distributed Dense Matrix Computations&lt;/li&gt;
&lt;li&gt;ParILUT -  A Parallel Threshold ILU for GPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/news/621726/faculty-showcase-parallel-computing-research-ipdps-2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/621726/faculty-showcase-parallel-computing-research-ipdps-2019&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facebook Research: Announcing the winners of the AI System Hardware/Software Co-Design research awards</title>
      <link>http://localhost:1313/blog/20190510-facebook/</link>
      <pubDate>Fri, 10 May 2019 11:29:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190510-facebook/</guid>
      <description>&lt;p&gt;In January, Facebook invited university faculty to respond to a &lt;a href=&#34;https://research.fb.com/programs/research-awards/proposals/ai-system-hardware-software-co-design-request-for-proposals/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;call for research proposals on AI System Hardware/Software Co-Design&lt;/a&gt;. Co-design implies simultaneous design and optimization of several aspects of the system, including hardware and software, to achieve a set target for a given system metric, such as throughput, latency, power, size, or any combination thereof. Deep learning has been particularly amenable to such co-design processes across various parts of the software and hardware stack, leading to a variety of novel algorithms, numerical optimizations, and AI hardware.&lt;/p&gt;
&lt;p&gt;Facebook AI teams have also been using co-design to develop high-performance AI solutions for existing as well as future AI hardware. Through these research awards, we looked to support further exploration of co-design opportunities across a number of new dimensions.&lt;/p&gt;
&lt;p&gt;We received 88 submissions, many of which provided promising research direction. The selection committee was composed of 10 engineers representing a wide range of AI hardware/algorithm co-design research areas.&lt;/p&gt;
&lt;h3 id=&#34;research-award-winners&#34;&gt;Research award winners&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Efficient Neural Network Through Systematic Quantization&lt;/strong&gt;
Kurt Keutzer (UC Berkeley), Amir Gholami (UC Berkeley)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hardware-Centric AutoML: Design Automation for Efficient Deep Learning&lt;/strong&gt;
Song Han (Massachusetts Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Low Memory-Bandwidth DNN Accelerator for Training Sparse Models&lt;/strong&gt;
Mattan Erez (The University of Texas at Austin), Michael Orshansky (The University of Texas at Austin)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making Typical Values Matter in Out-of-the-Box Deep Learning Models&lt;/strong&gt;
Andreas Moshovos (University of Toronto)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ML-Driven HW-SW Co-Design of Efficient Tensor Core Architectures&lt;/strong&gt;
Tushar Krishna (Georgia Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Realistic Benefits of Near-Data Processing for Emerging ML Workloads&lt;/strong&gt;
Onur Mutlu (ETH Zurich)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scalable Graph Learning Algorithms&lt;/strong&gt;
David A. Bader (Georgia Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure-Exploiting Optimization Algorithms for Deep Learning&lt;/strong&gt;
Jorge Nocedal (Northwestern University)&lt;/p&gt;
&lt;h3 id=&#34;runners-up&#34;&gt;Runners-up&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A Holistic Approach to Scalable DNN Training&lt;/strong&gt;
Gennady Pekhimenko (University of Toronto)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accelerating Graph Recommender Systems with Co-designed Memory Extensions&lt;/strong&gt;
Scott Beamer (UC Santa Cruz)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Automatic Hardware-Software Co-Design for Deep Learning with TVM/AutoVTA&lt;/strong&gt;
Luis Ceze (University of Washington)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Differentiable Neural Architecture Search for Ads CTR Prediction&lt;/strong&gt;
Kurt Keutzer (UC Berkeley)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enabling Scalable Training Using Waferscale Systems&lt;/strong&gt;
Rakesh Kumar (University of Illinois at Urbana-Champaign)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fast Embeddings: Construction and Lookups&lt;/strong&gt;
Francesco Silvestri (University of Padua), Flavio Vella (Free University of Bozen-Bolzano)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hardware-Neural Architecture Search-Based Co-Design for Efficient NNs&lt;/strong&gt;
Diana Marculescu (Carnegie Mellon University)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Processing-in-Memory Architecture for Word Embedding&lt;/strong&gt;
Jung Ho Ahn (Seoul National University)&lt;/p&gt;
&lt;p&gt;Thank you to all the researchers who submitted proposals, and congratulations to the winners. To view our currently open research awards and to subscribe to our email list, visit our &lt;a href=&#34;https://research.fb.com/programs/research-awards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Research Awards page&lt;/a&gt;.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190510-facebook/shutterstock_1011467887_hu_d7e20a51abb2a400.webp 400w,
               /blog/20190510-facebook/shutterstock_1011467887_hu_9f933a41607bb7b5.webp 760w,
               /blog/20190510-facebook/shutterstock_1011467887_hu_ea51003ec279c4db.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190510-facebook/shutterstock_1011467887_hu_d7e20a51abb2a400.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>NVIDIA AI Laboratory (NVAIL)</title>
      <link>http://localhost:1313/blog/20190417-nvidia-nvail/</link>
      <pubDate>Wed, 17 Apr 2019 09:33:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190417-nvidia-nvail/</guid>
      <description>&lt;h1 id=&#34;georgia-tech-uc-davis-texas-am-join-nvail-program-with-focus-on-graph-analytics&#34;&gt;&lt;a href=&#34;https://news.developer.nvidia.com/graph-technology-leaders-combine-forces-to-advance-graph-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia Tech, UC Davis, Texas A&amp;amp;M Join NVAIL Program with Focus on Graph Analytics&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;By Sandra Skaff&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA is partnering with three leading universities — Georgia Tech, the University of California, Davis, and Texas A&amp;amp;M — as part of our NVIDIA AI Labs program, to build the future of graph analytics on GPUs.&lt;/p&gt;
&lt;p&gt;NVIDIA’s work with these three new NVAIL partners aims to ultimately create a one-stop shop for customers to take advantage of accelerated graph analytics algorithms.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190417-nvidia-nvail/NVAIL_hu_3b1e15b09a2724ca.webp 400w,
               /blog/20190417-nvidia-nvail/NVAIL_hu_bc32effdbc1366c4.webp 760w,
               /blog/20190417-nvidia-nvail/NVAIL_hu_66cfc654656d67d7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190417-nvidia-nvail/NVAIL_hu_3b1e15b09a2724ca.webp&#34;
               width=&#34;760&#34;
               height=&#34;404&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;This will enable users to iterate over different graph models to find the best one faster, to update models faster as the data changes or grows, and to extract better insights from their data and bring new offerings to customers.&lt;/p&gt;
&lt;p&gt;The importance of &lt;a href=&#34;https://developer.nvidia.com/discover/graph-analytics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graph analytics&lt;/a&gt; is increasing with the explosion of data, and especially data which can benefit from graph representations. Graph representations are ideal for representing data points along with the relationships between them using edges. Graph data is available in diverse applications, ranging from chemistry to computer science to sociology. As such, the opportunities for applying graph analytics algorithms are vast.&lt;/p&gt;
&lt;p&gt;Leading the way, Georgia Tech, UC Davis and Texas A&amp;amp;M are working towards solving graph analytics problems within the &lt;a href=&#34;https://github.com/rapidsai/cugraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuGRAPH&lt;/a&gt;framework inside &lt;a href=&#34;https://github.com/rapidsai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAPIDS&lt;/a&gt;, an open source set of software libraries that speeds data science and analytics pipelines.&lt;/p&gt;
&lt;h2 id=&#34;building-the-future-of-graph-analytics-with-rapids&#34;&gt;Building the Future of Graph Analytics with RAPIDS&lt;/h2&gt;
&lt;p&gt;RAPIDS was launched by NVIDIA last year with the goal of enabling end-to-end data analytics pipelines to run entirely on GPUs.  RAPIDS at its lowest level relies on the &lt;a href=&#34;https://www.nvidia.com/en-us/technologies/cuda-x/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUDA-X AI&lt;/a&gt; libraries for low-level optimization of GPU parallelism at the thread level, taking best advantage of the GPU’s high memory bandwidth, but then exposes these functionalities through simple and familiar python interfaces, making it highly suitable for running data science experiments.&lt;/p&gt;
&lt;p&gt;By using RAPIDS, data can be loaded onto GPUs using a Pandas-like interface, and then used for various connected machine learning and graph analytics algorithms without ever leaving the GPU. This level of interoperability is made possible through libraries like Apache Arrow.&lt;/p&gt;
&lt;p&gt;To accelerate the Apache Arrow development, NVIDIA started collaborating with Ursa Labs through the NVAIL (NVIDIA AI Labs) program. Apache Arrow provides an interoperable columnar representation of tabular data commonly called a DataFrame, which bridges between CPU and GPU memory, and Arrow is the data format of the RAPIDS &lt;a href=&#34;https://github.com/rapidsai/cudf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuDF&lt;/a&gt; library. The data in cuDF format is fed to machine learning and graph analytics algorithms, which are housed in the &lt;a href=&#34;https://github.com/rapidsai/cuml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuML&lt;/a&gt; and &lt;a href=&#34;https://github.com/rapidsai/cugraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuGRAPH&lt;/a&gt; libraries in RAPIDS.&lt;/p&gt;
&lt;p&gt;We are building cuGRAPH as a collection of GPU accelerated graph analytics algorithms that both consume and produce data in cuDF format. Graph algorithms are essential to making sense of large volumes of highly unstructured and sparse data. Reducing the latency between collecting that data and the capability to take action is critical for both expediting scientific progress as well as enabling new applications that rely on inferring from extremely large amounts of data.&lt;/p&gt;
&lt;p&gt;With these new NVAIL universities, we will ensure we have state-of-the-art graph analytics approaches for static, dynamic, and property graph structures.  We will support both the frontier- and linear-algebra-based approaches for graph analytics algorithms.&lt;/p&gt;
&lt;h2 id=&#34;texas-am&#34;&gt;Texas A&amp;amp;M&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://faculty.cse.tamu.edu/davis/welcome.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Tim Davis&lt;/a&gt; and his lab at Texas A&amp;amp;M are significant contributors to &lt;a href=&#34;http://faculty.cse.tamu.edu/davis/suitesparse.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SuiteSparse/GraphBLAS&lt;/a&gt;, which provides linear algebra based building blocks for implementing graph algorithms. In the case of traversal, for example, instead of traversing the nodes through the edges of a graph one at a time, we can implement the algorithm using the equivalent of matrix operations for graphs, called semirings. Working with Prof. Davis and his lab, we aim to combine the strengths of SuiteSparse/GraphBLAS, &lt;a href=&#34;https://developer.nvidia.com/nvgraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nvGraph&lt;/a&gt;, and &lt;a href=&#34;https://developer.nvidia.com/cusparse&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuSPARSE&lt;/a&gt;, into one library, which would be exposed to users through a &lt;a href=&#34;http://graphblas.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GraphBLAS&lt;/a&gt; API. This combination will ensure the acceleration of numerous graph algorithms and scaling to multiple NVIDIA GPUs.&lt;/p&gt;
&lt;h2 id=&#34;uc-davis&#34;&gt;UC Davis&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ece.ucdavis.edu/~jowens/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. John Owens&lt;/a&gt; and his lab at UC Davis are the creators of the open-source &lt;a href=&#34;https://github.com/gunrock&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gunrock&lt;/a&gt; framework, which provides frontier-based building blocks for implementing graph algorithms on GPUs. We plan on working with UC Davis to improve on the design, development, and implementation of cuGRAPH in several ways, specifically on scalability and programmability aspects. These improvements leverage Gunrock, which provides a frontier-based programming model and a path to scalable and multi-GPU methods. On the scalability aspect, we will build upon UC Davis’s existing approach for multi-GPU processing for integration into cuGRAPH, and explore multi-node capabilities. On the programmability aspect, we will bring the frontier model into cuGRAPH, which would make it as easy as possible for developers to write new algorithms and applications.&lt;/p&gt;
&lt;h2 id=&#34;georgia-tech&#34;&gt;Georgia Tech&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/people/david-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. David Bader&lt;/a&gt; and his lab at Georgia Tech are leaders in high performance computing algorithms, with a focus on both static and dynamic graph algorithms. With Prof. Bader and his lab, we will work on the design and implementation of scalable graph algorithms and graph primitives for integrating into cuGRAPH, leveraging their &lt;a href=&#34;https://github.com/hornet-gt/hornet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hornet&lt;/a&gt; framework. A few challenges we plan on tackling for both static and dynamic graph algorithms are improving tools for analyzing graph data, speeding up graph traversal using optimized data structures, and accelerating computations with better runtime support for dynamic work stealing and load balancing.&lt;/p&gt;
&lt;h2 id=&#34;interaction-with-other-gpu-graph-frameworks&#34;&gt;Interaction with other GPU graph frameworks&lt;/h2&gt;
&lt;p&gt;While the landscape of graph frameworks designed for GPUs is sparse, we believe that leveraging a combination of  nvGraph from NVIDIA, Gunrock from UC Davis, Hornet from Georgia Tech, and the SuiteSparse work out of Texas A&amp;amp;M will help us address next generation graph challenges.  While there is overlap between the various frameworks, each excels in different areas. Rather than forcing the data scientist to choose a framework based on their needs, RAPIDS &lt;a href=&#34;https://medium.com/rapids-ai/rapids-cugraph-1ab2d9a39ec6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuGRAPH&lt;/a&gt; will help integrate with all four frameworks. You can learn more about the future of RAPIDS in this &lt;a href=&#34;https://medium.com/rapids-ai/the-road-to-1-0-building-for-the-long-haul-657ae1afdfd6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can get started with RAPIDS cuGRAPH today: &lt;a href=&#34;https://github.com/rapidsai/notebooks/tree/branch-0.6/cugraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/rapidsai/notebooks/tree/branch-0.6/cugraph&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.developer.nvidia.com/graph-technology-leaders-combine-forces-to-advance-graph-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.developer.nvidia.com/graph-technology-leaders-combine-forces-to-advance-graph-analytics/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chronicle of Higher Education: Gazette</title>
      <link>http://localhost:1313/blog/20190411-chronicle/</link>
      <pubDate>Thu, 11 Apr 2019 06:43:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190411-chronicle/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://www.siam.org/Prizes-Recognition/Fellows-Program/All-SIAM-Fellows/Class-of-2019?_ga=2.97902303.1532888029.1553872807-1398634856.1529436618&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Society for Industrial and Applied Mathematics&lt;/a&gt; selected 28 fellows for 2019 in recognition of their research and service to the community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor and chair of computational science and engineering at the Georgia Institute of Technology, for contributions in high-performance algorithms and streaming analytics and for leadership in the field of computational science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.chronicle.com/article/Transitions-New-President-at/246103&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.chronicle.com/article/Transitions-New-President-at/246103&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Set to Return to Faculty, Research</title>
      <link>http://localhost:1313/blog/20190408-cse-bader/</link>
      <pubDate>Mon, 08 Apr 2019 18:05:57 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190408-cse-bader/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20190408-cse-bader/bader_hu_cc3d677c38a26b05.webp 400w,
               /blog/20190408-cse-bader/bader_hu_7e32ebf257f5435a.webp 760w,
               /blog/20190408-cse-bader/bader_hu_21aab6455f2413bd.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190408-cse-bader/bader_hu_cc3d677c38a26b05.webp&#34;
               width=&#34;740&#34;
               height=&#34;493&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;(&lt;em&gt;Georgia Tech, Atlanta, GA. 8 April 2019&lt;/em&gt;) After five years, Professor David Bader has decided not to seek another term as the chair of the School of Computational Science and Engineering (CSE) and is returning to faculty and his research.&lt;/p&gt;
&lt;p&gt;Bader, a founding faculty member of the school (then a division), became chair in summer 2014. Since then, enrollment in the school’s M.S. program has more than doubled to 190 students. The school also has 71 Ph.D. students and teaches hundreds of others in interdisciplinary degrees including the M.S. in Analytics and the M.S. and Ph.D. programs in bioinformatics and bioengineering.&lt;/p&gt;
&lt;p&gt;Research funding has also grown during Bader’s tenure as chair. CSE faculty had 83 active research projects with $67 million worth of funding ($34 million at Georgia Tech) as of August 2018. In that year, average research expenditures per CSE faculty member were $622,000 – the highest in the college. During his tenure as chair, the school’s research expenditures per year increased significantly from $4.3 million to $7.5 million.&lt;/p&gt;
&lt;p&gt;In 2015, Bader launched the school’s Strategic Partnership Program, which allows companies to work with CSE faculty and graduate students. The program has grown to include 11 partners, including Accenture, IBM, Northrop Grumman, NVIDIA, and Sandia National Laboratories.&lt;/p&gt;
&lt;p&gt;“Computational Science and Engineering represents the College of Computing’s strong commitment to interdisciplinary research and education,” said Zvi Galil, the John P. Imlay Jr. Deam of Computing. “We are proud of the school’s successes over the past five years.”&lt;/p&gt;
&lt;p&gt;Bader is a fellow of the IEEE, AAAS and SIAM and has advised the White House on the National Strategic Computing Initiative. He is the editor-in-chief of the ACM Transactions on Parallel Computing, and a previous editor-in-chief IEEE Transactions on Parallel and Distributed Systems. He is a leading expert in solving global grand challenges in science, engineering, computing, and data science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/news/620191/bader-set-return-faculty-research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/620191/bader-set-return-faculty-research&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SIAM Announces Class of 2019 Fellows</title>
      <link>http://localhost:1313/blog/20190329-siam-fellow/</link>
      <pubDate>Fri, 29 Mar 2019 18:58:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190329-siam-fellow/</guid>
      <description>&lt;h2 id=&#34;siam-recognizes-distinguished-work-through-fellows-program&#34;&gt;SIAM Recognizes Distinguished Work through Fellows Program&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.siam.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Society for Industrial and Applied Mathematics (SIAM)&lt;/a&gt; is pleased to announce the &lt;a href=&#34;https://sinews.siam.org/Details-Page/siam-announces-class-of-2019-fellows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019 Class of SIAM Fellows&lt;/a&gt;. These distinguished members were nominated for their exemplary research as well as outstanding service to the community. Through their contributions, SIAM Fellows help advance the fields of applied mathematics and computational science.&lt;/p&gt;
&lt;p&gt;SIAM congratulates these 28 esteemed members of the community, listed below in alphabetical order:&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Institute of Technology, is being recognized for contributions in high-performance algorithms and streaming analytics and for leadership in the field of computational science.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;In addition to raising the visibility of applied mathematics and computational science, the SIAM Fellows Program helps makes SIAM members more competitive for awards and honors as well as leadership positions in the broader society.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Helps Robot Find its Voice</title>
      <link>http://localhost:1313/blog/20190225-gatech/</link>
      <pubDate>Mon, 25 Feb 2019 10:40:57 -0500</pubDate>
      <guid>http://localhost:1313/blog/20190225-gatech/</guid>
      <description>

















&lt;figure  id=&#34;figure-shimi-the-robots-latest-venture-in-sound-explores-emotive-response-and-what-it-means-for-communication&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Shimi the Robot’s Latest Venture in Sound Explores Emotive Response and What It Means for Communication&#34; srcset=&#34;
               /blog/20190225-gatech/shimi42_hu_ab4eb4d2a8213f3b.webp 400w,
               /blog/20190225-gatech/shimi42_hu_4212a9230aef58aa.webp 760w,
               /blog/20190225-gatech/shimi42_hu_2452060938e646f6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190225-gatech/shimi42_hu_ab4eb4d2a8213f3b.webp&#34;
               width=&#34;740&#34;
               height=&#34;493&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Shimi the Robot’s Latest Venture in Sound Explores Emotive Response and What It Means for Communication
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;“It’s all about music,” said Gil Weinberg, the founding director of the College of Design’s Center for Music Technology (GTCMT), School of Music professor, and School of Interactive Computing adjunct professor.&lt;/p&gt;
&lt;p&gt;“Music is about rhythm, pitch, loudness, and tone. These are the same elements used to control vocal prosody, which helps convey emotion, humor, irony, and other subtle, yet crucially meaningful expressions. This project is about using music to allow our robot Shimi to show emotions not only through his voice, but through his body gestures as well.”&lt;/p&gt;
&lt;p&gt;Shimi is a personal robot that communicates with humans through music-driven vocal prosody and gestures rather than words.&lt;/p&gt;
&lt;p&gt;When Shimi debuted in 2012, it played songs from a user’s library, analyzed the music, and responded with corresponding gestures. Now, with the help of deep learning and with funding from the School of Computational Science and Engineering (CSE), Shimi can learn emotional cues in people’s voices and respond with emotive voice and movement.&lt;/p&gt;
&lt;p&gt;The project’s research team comprises Weinberg, Ph.D. student Richard Savery, and master’s student Ryan Rose.&lt;/p&gt;
&lt;p&gt;Using deep learning analysis of music and language datasets, the team trained Shimi to communicate emotions using non-linguistic channels. Shimi can also analyze a person’s tone and speech in order to respond in an emotionally appropriate way.&lt;/p&gt;
&lt;p&gt;“By modeling humans’ affective communication cues, such as body gestures and vocal prosody, we’ve created a language focused on emotion. With Shimi, we are not projecting words but still allowing for affective communication to occur,” Weinberg said.&lt;/p&gt;
&lt;p&gt;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/mDAmApNw5wo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;em&gt;Avoiding the “Uncanny Valley&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;“Society has seen countless efforts to re-create humanoid robots to interact with humans. Many of these robots fall prey to the same issue as their predecessors: The Uncanny Valley. In the Uncanny Valley, robots simply become too close to human, without being human, which tend to lead to a sense of eeriness and revulsion.&lt;/p&gt;
&lt;p&gt;“What then if a robot wasn’t trying to sound exactly like a human? What if we celebrated a robot for what it is, and for the things it can do that humans can’t?” Weinberg said.&lt;/p&gt;
&lt;p&gt;This is the logic behind steering clear of identifiable words, and instead equipping Shimi with the ability to respond to humans with non-verbal sounds while still being able to convey a general sense of mood.&lt;/p&gt;
&lt;p&gt;“If you are upset, Shimi could project that it is also upset, or maybe decide to encourage you using happy prosody,” Weinberg said.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-language-built-on-deep-learning-and-music&#34;&gt;Creating a Language Built on Deep Learning and Music&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-research-team-of-shimi-stands-in-front-of-the-newly-revamped-singing-robot-left-to-right-cse-chair-david-bader-phd-student-richard-savery-ms-student-ryan-rose-gtcmt-director-gil-weinberg-stand-behind-shimi-in-the-gtcmt-lab&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The research team of Shimi stands in front of the newly revamped singing robot. (Left to right) CSE Chair **David Bader, Ph.D.** Student Richard Savery, M.S. student Ryan Rose, GTCMT Director Gil Weinberg stand behind Shimi in the GTCMT lab.&#34; srcset=&#34;
               /blog/20190225-gatech/shimiteam1_hu_bf46afff90976b64.webp 400w,
               /blog/20190225-gatech/shimiteam1_hu_b5faa00e2a5b6e6f.webp 760w,
               /blog/20190225-gatech/shimiteam1_hu_58b5392378315efa.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20190225-gatech/shimiteam1_hu_bf46afff90976b64.webp&#34;
               width=&#34;760&#34;
               height=&#34;496&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The research team of Shimi stands in front of the newly revamped singing robot. (Left to right) CSE Chair &lt;strong&gt;David Bader, Ph.D.&lt;/strong&gt; Student Richard Savery, M.S. student Ryan Rose, GTCMT Director Gil Weinberg stand behind Shimi in the GTCMT lab.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To create Shimi’s voice, tone, and improvisational response for this project the team fed a Deep Learning network with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10,000 files from 15 improvisational musicians playing responses to different emotional queues&lt;/li&gt;
&lt;li&gt;300,000 samples of musical instruments playing different musical notes, to add musical expressivity to the spoken word&lt;/li&gt;
&lt;li&gt;One of the rarest languages in existence – a nearly extinct Australian aboriginal vernacular made up of 28 phonemes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By processing these datasets on NVIDIA’s Jetson Board, an embedded GPU optimized for machine learning, Weinberg and his team have been able to allow Shimi to use his new affective voice and sing as a self-contained robot that does not need network connectivity.&lt;/p&gt;
&lt;p&gt;Through the desire to combine music with deep learning, Georgia Tech researchers have coincidentally shown that communication simply needs empathy and a tune to take place.&lt;/p&gt;
&lt;p&gt;“What we are most excited about is the ability to synthesize various attributes of music, language, and movement through deep learning, and project music as the core element of a robotic communication to show that our robots can understand and convey human emotion,” Weinberg said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20200108084335/https://www.cc.gatech.edu/news/618422/deep-learning-helps-robot-find-its-voice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/618422/deep-learning-helps-robot-find-its-voice&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solving Real-World Problems: 5-Minute Interview with David Bader</title>
      <link>http://localhost:1313/blog/20190222-neo4j/</link>
      <pubDate>Fri, 22 Feb 2019 07:00:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20190222-neo4j/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ouwh5ed5SdU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://img.youtube.com/vi/ouwh5ed5SdU/0.jpg&#34; alt=&#34;Solving Real-World Problems: 5-Minute Interview with David Bader, Professor at Georgia Tech&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When &lt;strong&gt;David Bader&lt;/strong&gt; started working with graphs 25 years ago, it was a niche that required designing specific algorithms and even specific computers. Now the Neo4j graph database is used widely by analysts and researchers who work with Georgia Tech, rapidly asking questions and visualizing results.&lt;/p&gt;
&lt;p&gt;In this week’s five-minute interview (conducted at GraphConnect 2018 in NYC), we spoke with &lt;strong&gt;Dr. Bader&lt;/strong&gt; about using graph technology to solve real-world problems, including a knowledge graph to track emerging problems and threats.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ouwh5ed5SdU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube video&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HiPC Special Award Presented to David Bader, HiPC Perennials Club</title>
      <link>http://localhost:1313/blog/20181219-hipc/</link>
      <pubDate>Wed, 19 Dec 2018 18:18:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181219-hipc/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20181219-hipc/award_hu_f5346748c5f2c45b.webp 400w,
               /blog/20181219-hipc/award_hu_43a6e3c9b1f75b48.webp 760w,
               /blog/20181219-hipc/award_hu_191104ea23383f7d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20181219-hipc/award_hu_f5346748c5f2c45b.webp&#34;
               width=&#34;619&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;HiPC Special Award presented to &lt;strong&gt;David Bader&lt;/strong&gt;, Member, HiPC Perennials Club&lt;/p&gt;
&lt;p&gt;Viktor Prasanna&lt;br&gt;
HiPC Steering Chair&lt;/p&gt;
&lt;p&gt;at the 25th Anniversary of High Performance Computing, Data, and Analytics&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hipc.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://hipc.org/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech’s Leading High-Performance Computing Scientists Showcase Research Highlights at Supercomputing 2018</title>
      <link>http://localhost:1313/blog/20181112-gatech-sc18/</link>
      <pubDate>Mon, 12 Nov 2018 12:15:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181112-gatech-sc18/</guid>
      <description>&lt;p&gt;Georgia Tech high-performance computing (HPC) experts are gathered in Dallas this week to take part in the HPC community’s largest annual event — the &lt;a href=&#34;https://sc18.supercomputing.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Conference for High Performance Computing, Networking, Storage, and Analysis&lt;/a&gt; — commonly referred to as Supercomputing. This year’s conference, SC’18, opened Sunday at the Kay Bailey Hutchison Convention Center Dallas and runs through Nov. 16.&lt;/p&gt;
&lt;p&gt;“Georgia Tech researchers are presenting 23 separate events this week, including four technical paper presentations, several workshops, panels, and even a doctoral showcase with CSE [School of Computational Science and Engineering] Ph.D. student Patrick Flick,” said Center for High Performance Computing Director and CSE Professor Rich Vuduc.&lt;/p&gt;
&lt;p&gt;One of the papers being presented, &lt;em&gt;HiCOO: Hierarchical Storage of Sparse Tensors&lt;/em&gt;, by CSE recent graduate Jiajia Li and CSE Associate Professors Vuduc and Jimeng Sun was nominated as one of the five best student papers of 288 submissions in this year’s program.&lt;/p&gt;
&lt;p&gt;Attendees can also stop by the Georgia Tech booth (#1217) to talk with faculty and students, take a moment to watch the live feed of fish from the &lt;a href=&#34;https://www.georgiaaquarium.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia Aquarium&lt;/a&gt;, and view additional Georgia Tech HPC research with the research slide deck. Swag items are also available, including &lt;a href=&#34;https://www.cc.gatech.edu/content/superheroes-supercomputing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Superheroes of Supercomputing Cards&lt;/a&gt; and Georgia Tech-branded bags.&lt;/p&gt;
&lt;p&gt;The complete Georgia Tech SC’18 proceedings, projects, and research slides can be viewed &lt;a href=&#34;https://spark.adobe.com/page/nUNKgdyX5xsox/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See below for a list of Georgia Tech events at SC’18 (broken down by event type):&lt;/p&gt;
&lt;h3 id=&#34;workshops&#34;&gt;Workshops:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Hot Topics Discussion II: Thriving at Work&lt;br&gt;
Lorna Rivera, Lucy Nowell, Carissa Holohan&lt;/li&gt;
&lt;li&gt;A Preliminary Study of Compiler Transformations for Graph Applications on the Emu System&lt;br&gt;
AUTHOR/PRESENTERS: Prasanth Chatarasi, Vivek Sarkar&lt;/li&gt;
&lt;li&gt;A One Year Retrospective on a MOOC in Parallel, Concurrent, and Distributed Programming in Java&lt;br&gt;
Vivek Sarkar, Max Grossman, Zoran Budimlic, Shams Imam&lt;/li&gt;
&lt;li&gt;Shortest Path and Neighborhood Subgraph Extraction on a Spiking Memristive Neuromorphic Implementation&lt;br&gt;
Catherine Schuman, Kathleen Hamilton, Tiffany Mintz, Md Musabbir Adnan, Bon Woong Ku, Sung-Kyu Lim, Garrett S. Rose&lt;/li&gt;
&lt;li&gt;Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction&lt;br&gt;
Xin Xing, Bin Dong, Jonathan Ajo-Franklin, Kesheng Wu&lt;/li&gt;
&lt;li&gt;A Fast and Simple Approach to Merge and Merge Sorting Using Wide Vector Instructions&lt;br&gt;
Alex Watkins, Oded Green&lt;/li&gt;
&lt;li&gt;A Unified Runtime for PGAS and Event-Driven Programming&lt;br&gt;
Sri Raj Paul, Kun Chen, Akihiro Hayashi, Max Grossman, Vivek Sarkar&lt;/li&gt;
&lt;li&gt;A Study of OpenMP Device Offloading in LLVM: Correctness and Consistency&lt;br&gt;
Lechen Yu&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;birds-of-a-feather&#34;&gt;Birds of a Feather:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;17th Graph500 List&lt;br&gt;
Richard Murphy, &lt;strong&gt;David Bader&lt;/strong&gt;, Peter Kogge, Andrew Lumsdaine, Anton Korzh&lt;/li&gt;
&lt;li&gt;Strategies for Inclusive and Scalable HPC Outreach and Education&lt;br&gt;
Julie Mullen, Weronika Filinger, Tom Maiden, Nicholas Brown, Lorna Rivera, John Urbanic, Karina Nunez, Bryan Johnston, Karina Pesatova, Martin Quinson&lt;/li&gt;
&lt;li&gt;Advanced Architecture Testbeds: A Catalyst for Co-design Collaborations&lt;br&gt;
Kevin Barker, Jeffrey Young, Alice Koniges, Jeffrey Vetter, James Laros&lt;/li&gt;
&lt;li&gt;HPC Graph Toolkits and GraphBLAS Forum&lt;br&gt;
José Moreira (IBM), Antonino Tumeo (PNNL), Aydin Buluç (LBNL), Mahantesh Halappanavar (PNNL), John Feo (PNNL), &lt;strong&gt;David Bader&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;An Update on the Next-Generation BLAS Proposal&lt;br&gt;
Piotr Luszczek, Jack Dongarra, Jason Riedy, Greg Henry, James Demmel, Mark Gates, Xiaoye S. Li, Ping Tak P. Tang&lt;/li&gt;
&lt;li&gt;Workloads and Benchmarks for System Acquisition&lt;br&gt;
Neil Bright, Laura Brown, Henry Neeman, Alex Younts&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;papers&#34;&gt;Papers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimizing High Performance Distributed Memory Parallel Hash Tables for DNA k-mer Counting&lt;br&gt;
Tony C. Pan, Sanchit Misra, Srinivas Aluru&lt;/li&gt;
&lt;li&gt;HiCOO: Hierarchical Storage of Sparse Tensors&lt;br&gt;
Jiajia Li, Jimeng Sun, Rich Vuduc&lt;/li&gt;
&lt;li&gt;Accelerating Quantum Chemistry with Vectorized and Batched Integrals&lt;br&gt;
Hua Huang, Edmond Chow&lt;/li&gt;
&lt;li&gt;Detecting MPI Usage Anomalies via Partial Program Symbolic Execution&lt;br&gt;
Fangke Ye, Jisheng Zhao, Vivek Sarkar&lt;/li&gt;
&lt;li&gt;Poster: Modeling Single-Source Shortest Path Algorithm Dynamics to Control Performance and Power Tradeoffs&lt;br&gt;
Sara Karamati, Jeffrey Young, Rich Vuduc&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;panels&#34;&gt;Panels&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Runtime for Exascale and Beyond: Convergence or Divergence?&lt;br&gt;
Marc Snir, Vivek Sarkar, Pavan Balaji, Laxmikant Kale, Sean Treichler, Hartmut Kaiser, Raymond Namyst&lt;/li&gt;
&lt;li&gt;Convergence between HPC and Big Data: The Day After Tomorrow&lt;br&gt;
Bilel Hadri, Sadaf Alam, Katie Antypas, &lt;strong&gt;David Bader&lt;/strong&gt;, Dan Reed, Rio Yokota&lt;/li&gt;
&lt;li&gt;Students@SC: Making the Best of Your HPC Education&lt;br&gt;
Rebecca Hartman, Elisa Heymann, Jesmin Jahan Tithi, Rich Vuduc, Kenneth Weiss, Xinghui Zhao&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;doctoral-showcase&#34;&gt;Doctoral Showcase&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Parallel and Scalable Combinatorial String and Graph Algorithms on Distributed Memory Systems&lt;br&gt;
Patrick Flick, Srinivas Aluru&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;core-research-areas&#34;&gt;Core Research Areas:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Data Engineering and Science&lt;/li&gt;
&lt;li&gt;Electronics and Nanotechnology&lt;/li&gt;
&lt;li&gt;National Security&lt;/li&gt;
&lt;li&gt;Public Service, Leadership, and Policy&lt;/li&gt;
&lt;li&gt;Systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://cse.gatech.edu/news/614172/georgia-techs-leading-high-performance-computing-scientists-showcase-research-highlights&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cse.gatech.edu/news/614172/georgia-techs-leading-high-performance-computing-scientists-showcase-research-highlights&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSE Chair David Bader Named Editor-in-Chief of ACM Transactions on Parallel Computing</title>
      <link>http://localhost:1313/blog/20181102-hpcwire/</link>
      <pubDate>Fri, 02 Nov 2018 16:54:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181102-hpcwire/</guid>
      <description>&lt;p&gt;School of Computational Science and Engineering Chair and Professor &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt; has been named Editor-in-Chief (EiC) of &lt;a href=&#34;https://topc.acm.org/index.cfm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACM Transactions on Parallel Computing&lt;/a&gt; (ACM ToPC).&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20181102-hpcwire/david-bader_cropped_400x-300x294_hu_11f44990bf1b9e02.webp 400w,
               /blog/20181102-hpcwire/david-bader_cropped_400x-300x294_hu_1c762eb9f1d741f2.webp 760w,
               /blog/20181102-hpcwire/david-bader_cropped_400x-300x294_hu_94cfcc4958c88bd2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20181102-hpcwire/david-bader_cropped_400x-300x294_hu_11f44990bf1b9e02.webp&#34;
               width=&#34;300&#34;
               height=&#34;294&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;ACM Transactions on Parallel Computing is a forum for novel and innovative work on all aspects of parallel computing, and addresses all classes of parallel-processing platforms, from concurrent and multithreaded to clusters and supercomputers.&lt;/p&gt;
&lt;p&gt;“I am excited for this opportunity to operate as the Editor-in-Chief of such a prestigious publication for a three-year term. I am a founding board member of ToPC, and follow the founding EiC, &lt;a href=&#34;https://www.cs.cmu.edu/~gibbons/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phil Gibbons&lt;/a&gt;, in this role,” said Bader.&lt;/p&gt;
&lt;p&gt;Bader’s term of appointment formally begins November 1.&lt;/p&gt;
&lt;p&gt;Bader is also a Fellow of the &lt;a href=&#34;https://www.ieee.org/membership/fellows/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE&lt;/a&gt; and &lt;a href=&#34;https://www.aaas.org/fellows/listing?field_last_name_value=&amp;amp;name_combine=&amp;amp;field_institutional_affiliation_value=&amp;amp;field_address_city=&amp;amp;field_address_administrative_area=All&amp;amp;field_address_country_code=All&amp;amp;field_year_elected=&amp;amp;field_primary_aaas_sectio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AAAS&lt;/a&gt; and advises the White House, most recently on the &lt;a href=&#34;https://www.nitrd.gov/nsci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Strategic Computing Initiative&lt;/a&gt;. He is a leading expert in solving global grand challenges in science, engineering, computing, and data science. He has co-authored over 230 articles in peer-reviewed journals and conferences and has presented significant contributions to the field of parallel computing; aligning his history and passions well to this new role.&lt;/p&gt;
&lt;p&gt;“The nominating committee selected Professor Bader as the second EiC of ACM ToPC among several exceptionally well-qualified candidates,” said Nominating Committee Chair and Professor at the University of Maryland Institute for Advanced Computer Studies Uzi Vishkin.&lt;/p&gt;
&lt;p&gt;“We are expecting him to solidify the standing of ACM ToPC as the flagship ACM journal on parallel computing and for taking the journal to new heights.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/cse-chair-david-bader-named-editor-in-chief-of-acm-transactions-on-parallel-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/cse-chair-david-bader-named-editor-in-chief-of-acm-transactions-on-parallel-computing/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ACM Transactions on Parallel Computing Names David Bader as Editor-in-Chief</title>
      <link>http://localhost:1313/blog/20181101-acm-topc/</link>
      <pubDate>Thu, 01 Nov 2018 17:49:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181101-acm-topc/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20181101-acm-topc/bader_hu_7204875942e4136f.webp 400w,
               /blog/20181101-acm-topc/bader_hu_bb1f910f1b4a4fba.webp 760w,
               /blog/20181101-acm-topc/bader_hu_e481ee3778839f8f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20181101-acm-topc/bader_hu_7204875942e4136f.webp&#34;
               width=&#34;299&#34;
               height=&#34;168&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://topc.acm.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACM Transactions on Parallel Computing&lt;/a&gt; (TOPC) welcomes David Bader as new Editor-in-Chief, for the term November 1, 2018 to October 31, 2021. David is a Professor and Chair in the School of Computational Science and Engineering and College of Computing at Georgia Institute of Technology.&lt;/p&gt;
&lt;h2 id=&#34;about-topc&#34;&gt;About TOPC&lt;/h2&gt;
&lt;p&gt;ACM Transactions on Parallel Computing (TOPC) is a forum for novel and innovative work on all aspects of parallel computing, including foundational and theoretical aspects, systems, languages, architectures, tools, and applications. It will address all classes of parallel-processing platforms including concurrent, multithreaded, multicore, accelerated, multiprocessor, clusters, and supercomputers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSE Chair David Bader Named Editor-in-Chief of ACM Transactions on Parallel Computing</title>
      <link>http://localhost:1313/blog/20181031-eic-topc/</link>
      <pubDate>Wed, 31 Oct 2018 08:26:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181031-eic-topc/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20181031-eic-topc/bader_hu_108c98f6356ebf11.webp 400w,
               /blog/20181031-eic-topc/bader_hu_da86f59e3ab65c4c.webp 760w,
               /blog/20181031-eic-topc/bader_hu_d0d46bcdabfb6fec.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20181031-eic-topc/bader_hu_108c98f6356ebf11.webp&#34;
               width=&#34;220&#34;
               height=&#34;124&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;School of Computational Science and Engineering Chair and Professor &lt;strong&gt;David Bader&lt;/strong&gt; was named Editor-in-Chief (EiC) of &lt;a href=&#34;https://topc.acm.org/index.cfm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACM Transactions on Parallel Computing (ACM ToPC)&lt;/a&gt; this morning.&lt;/p&gt;
&lt;p&gt;ACM Transactions on Parallel Computingis a forum for novel and innovative work on all aspects of parallel computing, and addresses all classes of parallel-processing platforms, from concurrent and multithreaded to clusters and supercomputers.&lt;/p&gt;
&lt;p&gt;“I am excited for this opportunity to operate as the Editor-in-Chief of such a prestigious publication for a three-year term. I am a founding board member of TOPC, and follow the founding EiC, &lt;a href=&#34;http://www.cs.cmu.edu/~gibbons/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Phil Gibbons&lt;/strong&gt;&lt;/a&gt;, in this role,” said Bader.&lt;/p&gt;
&lt;p&gt;Bader’s term of appointment will formally begin November 1.&lt;/p&gt;
&lt;p&gt;Bader is also a Fellow of the &lt;a href=&#34;https://www.ieee.org/membership/fellows/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE&lt;/a&gt; and &lt;a href=&#34;https://www.aaas.org/fellows/listing?field_last_name_value=&amp;amp;name_combine=&amp;amp;field_institutional_affiliation_value=&amp;amp;field_address_city=&amp;amp;field_address_administrative_area=All&amp;amp;field_address_country_code=All&amp;amp;field_year_elected=&amp;amp;field_primary_aaas_sectio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AAAS&lt;/a&gt; and advises the White House, most recently on the &lt;a href=&#34;https://www.nitrd.gov/nsci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Strategic Computing Initiative&lt;/a&gt;.  He is a leading expert in solving global grand challenges in science, engineering, computing, and data science. He has co-authored over 230 articles in peer-reviewed journals and conferences and has presented significant contributions to the field of parallel computing; aligning his history and passions well to this new role.&lt;/p&gt;
&lt;p&gt;“The nominating committee selected Professor Bader as the second EiC of ACM ToPC among several exceptionally well-qualified candidates,” said Nominating Committee Chair and Professor at the University of Maryland Institute for Advanced Computer Studies &lt;strong&gt;Uzi Vishkin&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;“We are expecting him to solidify the standing of ACM ToPC as the flagship ACM journal on parallel computing and for taking the journal to new heights.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/news/613561/cse-chair-david-bader-named-editor-chief-acm-transactions-parallel-computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cse.gatech.edu/news/613561/cse-chair-david-bader-named-editor-chief-acm-transactions-parallel-computing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kristen Perez&lt;/em&gt;&lt;br&gt;
&lt;em&gt;Communications Officer I&lt;/em&gt;&lt;br&gt;
&lt;em&gt;&lt;a href=&#34;mailto:kristen.perez@cc.gatech.edu&#34;&gt;kristen.perez@cc.gatech.edu&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NVIDIA Introduces RAPIDS Open-Source GPU-Acceleration Platform for Large-Scale Data Analytics and Machine Learning</title>
      <link>http://localhost:1313/blog/20181010-rapids/</link>
      <pubDate>Wed, 10 Oct 2018 06:39:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20181010-rapids/</guid>
      <description>&lt;p&gt;NVIDIA today announced a GPU-acceleration platform for data science and machine learning, with broad adoption from industry leaders, that enables even the largest companies to analyze massive amounts of data and make accurate business predictions at unprecedented speed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.rapids.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAPIDS(TM) open-source software&lt;/a&gt; gives data scientists a giant performance boost as they address highly complex business challenges, such as predicting credit card fraud, forecasting retail inventory and understanding customer buying behavior. Reflecting the growing consensus about the GPU&amp;rsquo;s importance in data analytics, an array of companies is supporting RAPIDS &amp;ndash; from pioneers in the open-source community, such as Databricks and Anaconda, to tech leaders like Hewlett Packard Enterprise, IBM and Oracle.&lt;/p&gt;
&lt;p&gt;Analysts estimate the server market for data science and machine learning at $20 billion annually, which &amp;ndash; together with scientific analysis and deep learning &amp;ndash; pushes up the value of the high performance computing market to approximately $36 billion.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Data analytics and machine learning are the largest segments of the high performance computing market that have not been accelerated &amp;ndash; until now,&amp;rdquo; said Jensen Huang, founder and CEO of NVIDIA, who revealed RAPIDS in his keynote address at the &lt;a href=&#34;https://www.nvidia.com/en-eu/gtc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPU Technology Conference&lt;/a&gt;. &amp;ldquo;The world&amp;rsquo;s largest industries run algorithms written by machine learning on a sea of servers to sense complex patterns in their market and environment, and make fast, accurate predictions that directly impact their bottom line.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Building on CUDA and its global ecosystem, and working closely with the open-source community, we have created the RAPIDS GPU-acceleration platform. It integrates seamlessly into the world&amp;rsquo;s most popular data science libraries and workflows to speed up machine learning. We are turbocharging machine learning like we have done with deep learning,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;RAPIDS offers a suite of open-source libraries for GPU-accelerated analytics, machine learning and, soon, data visualization. It has been developed over the past two years by NVIDIA engineers in close collaboration with key open-source contributors.&lt;/p&gt;
&lt;p&gt;For the first time, it gives scientists the tools they need to run the entire data science pipeline on GPUs. Initial RAPIDS benchmarking, using the XGBoost machine learning algorithm for training on an &lt;a href=&#34;https://www.nvidia.com/en-us/data-center/dgx-2/#source=pr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA DGX-2(TM) system&lt;/a&gt;, shows 50x speedups compared with CPU-only systems. This allows data scientists to reduce typical training times from days to hours, or from hours to minutes, depending on the size of their dataset.&lt;/p&gt;
&lt;p&gt;Close Collaboration with Open-Source Community
RAPIDS builds on popular open-source projects &amp;ndash; including Apache Arrow, pandas and scikit-learn &amp;ndash; by adding GPU acceleration to the most popular Python data science toolchain. To bring additional machine learning libraries and capabilities to RAPIDS, NVIDIA is collaborating with such open-source ecosystem contributors as Anaconda, BlazingDB, Databricks, Quansight and scikit-learn, as well as Wes McKinney, head of Ursa Labs and creator of Apache Arrow and pandas, the fastest-growing Python data science library.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;RAPIDS, a GPU-accelerated data science platform, is a next-generation computational ecosystem powered by Apache Arrow,&amp;rdquo; McKinney said. &amp;ldquo;NVIDIA&amp;rsquo;s collaboration with Ursa Labs will accelerate the pace of innovation in the core Arrow libraries and help bring about major performance boosts in analytics and feature engineering workloads.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To facilitate broad adoption, NVIDIA is integrating RAPIDS into Apache Spark, the leading open-source framework for analytics and data science.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At Databricks, we are excited about RAPIDS&amp;rsquo; potential to accelerate Apache Spark workloads,&amp;rdquo; said Matei Zaharia, co-founder and chief technologist of Databricks, and founder of Apache Spark. &amp;ldquo;We have multiple ongoing projects to integrate Spark better with native accelerators, including Apache Arrow support and GPU scheduling with Project Hydrogen. We believe that RAPIDS is an exciting new opportunity to scale our customers&amp;rsquo; data science and AI workloads.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Broad Ecosystem Support and Adoption&lt;br&gt;
Tech-leading enterprises across a broad range of industries are early adopters of NVIDIA&amp;rsquo;s GPU-acceleration platform and RAPIDS.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;NVIDIA&amp;rsquo;s GPU-acceleration platform with RAPIDS software has immensely improved how we use data &amp;ndash; enabling the most complex models to run at scale and deliver even more accurate forecasting,&amp;rdquo; said Jeremy King, executive vice president and chief technology officer at Walmart. &amp;ldquo;RAPIDS has its roots in deep collaboration between NVIDIA&amp;rsquo;s and Walmart&amp;rsquo;s engineers, and we plan to build on this relationship.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Additionally, some of the world&amp;rsquo;s leading technology companies are supporting RAPIDS through new systems, data science platforms and software solutions:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;HPE is committed to advancing the way customers live and work. Artificial intelligence, analytics and machine learning technology can play a critical role in uncovering insights that can help customers achieve breakthrough results and improve the world we live in. HPE is unique in the market in that we provide complete AI and data analytics solutions from strategic advisory to purpose-built GPU accelerator technology, operational support and a strong partner ecosystem to tailor the right solution for each customer. We are excited to partner with NVIDIA on RAPIDS to accelerate the application of data science and machine learning to help our customers drive faster and more insightful outcomes.&amp;rdquo;&lt;br&gt;
&amp;ndash; Antonio Neri, CEO, Hewlett Packard Enterprise&lt;/p&gt;
&lt;p&gt;&amp;ldquo;IBM has built the world&amp;rsquo;s leading platform for enterprise AI, regardless of deployment model. We look forward to extending our successful partnership with NVIDIA, leveraging RAPIDS to provide new machine learning tools for our clients.&amp;rdquo;&lt;br&gt;
&amp;ndash; Arvind Krishna, senior vice president of Hybrid Cloud and director of IBM Research&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The compute world today requires powerful processing to handle complex workloads like data science and analytics &amp;ndash; it&amp;rsquo;s a job for NVIDIA GPUs. RAPIDS is accelerating the speed at which this processing and machine learning training can be done. We are excited to support this new suite of open-source software natively on Oracle Cloud Infrastructure and look forward to working with NVIDIA to support RAPIDS across our platform, including the Oracle Data Science Cloud, to further accelerate our customers&amp;rsquo; end to-end data science workflows. RAPIDS software runs seamlessly on the Oracle Cloud, allowing customers to support their HPC, AI and data science needs, all while taking advantage of the portfolio of GPU instances available on Oracle Cloud Infrastructure.&amp;rdquo;&lt;br&gt;
&amp;ndash; Clay Magouyrk, senior vice president of Software Development, Oracle Cloud Infrastructure&lt;/p&gt;
&lt;p&gt;Support from other leading innovators &amp;ndash; including Cisco, Dell EMC, Lenovo, NERSC, NetApp, Pure Storage, SAP and SAS, as well as a wide range of data science pioneers &amp;ndash; is appended to this press release.&lt;/p&gt;
&lt;p&gt;Availability&lt;/p&gt;
&lt;p&gt;Access to the RAPIDS open-source suite of libraries is immediately available at &lt;a href=&#34;http://www.rapids.ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.rapids.ai&lt;/a&gt;, where the code is being released under the Apache license. Containerized versions of RAPIDS will be available this week on the &lt;a href=&#34;https://www.nvidia.com/en-us/gpu-cloud/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA GPU Cloud container registry&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;About NVIDIA&lt;br&gt;
NVIDIA&amp;rsquo;s NVDA, +1.28% invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined modern computer graphics and revolutionized parallel computing. More recently, GPU deep learning ignited modern AI &amp;ndash; the next era of computing &amp;ndash; with the GPU acting as the brain of computers, robots and self-driving cars that can perceive and understand the world. More information at &lt;a href=&#34;http://nvidianews.nvidia.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nvidianews.nvidia.com/&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additional-supporting-quotations&#34;&gt;Additional Supporting Quotations&lt;/h2&gt;
&lt;p&gt;Anaconda - Scott Collison, CEO&lt;br&gt;
&amp;ldquo;NVIDIA has made the training and deployment of complex AI models scalable and economically viable. Today&amp;rsquo;s RAPIDS announcement by NVIDIA extends the same benefits to earlier data transformation stages of the data science lifecycle. Anaconda is proud to have helped NVIDIA develop these new capabilities, which will be available to the community of 7 million users of the Anaconda Distribution through our public package repository. We&amp;rsquo;ll also be including them in Anaconda Enterprise, which, combined with NVIDIA DGX, delivers a high-performance, proven solution for business. Anaconda Enterprise on NVIDIA DGX will enable IT organizations of all sizes to accelerate data science and AI workflows.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;BlazingDB - Rodrigo Aramburu, CEO&lt;br&gt;
&amp;ldquo;We are thrilled to be early contributors to the RAPIDS open-source software from NVIDIA, and have built BlazingSQL, a free to use version of our distributed GPU SQL engine, on RAPIDS. Our partnership with NVIDIA has provided immense value to us as a startup as we collaborated with the RAPIDS team, joined as key contributors to cuDF, and will continue to support the RAPIDS software as we build our vision of integrating Data Lakes with AI, all using SQL.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Cisco - Kaustabh Das, vice president of Product Management, Data Center Group&lt;br&gt;
&amp;ldquo;Cisco and NVIDIA are collaborating on AI/ML software stacks on NVIDIA GPU-optimized Cisco UCS platforms to simplify and accelerate AI/ML workload deployment. We are excited to learn that, with RAPIDS, NVIDIA is expanding their GPU applicability with accelerated software stacks to address traditional machine learning and big data analytics. We look forward to the possibilities for our GPU-accelerated server portfolio, including the recently launched Cisco UCS C480 ML M5 Rack Server, a best in class, purpose-built server with eight NVIDIA V100 GPUs and NVIDIA NVLink interconnect.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Dell EMC - Ravi Pendekanti, senior vice president of Product Management and Marketing, Servers &amp;amp; Infrastructure Systems&lt;br&gt;
&amp;ldquo;Dell EMC is committed to providing our customers with world-class IT infrastructures that enable them to gain real, competitive business advantage. We work with our ecosystem partners to ensure our customers have the latest data science tools available to help them transform data insights into business outcomes. Our goal is to combine the new GPU-accelerated open-source data science software from NVIDIA with our portfolio of NVLink-enabled Dell EMC PowerEdge servers to significantly accelerate the fields of machine learning and big data analytics.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;FASTDATA.io - Alen Capalik, founder and CEO&lt;br&gt;
&amp;ldquo;The RAPIDS open-source project launched by NVIDIA is going to revolutionize the data science pipeline. At FASTDATA.io, we&amp;rsquo;re excited that our Plasma Engine &amp;ndash; the first software to fully leverage NVIDIA GPUs for real-time processing of infinite data in motion &amp;ndash; will play a part in that revolution.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Georgia Tech - &lt;strong&gt;David Bader&lt;/strong&gt;, professor&lt;br&gt;
&amp;ldquo;Georgia Tech is excited to contribute to RAPIDS, an open-source playground for NVIDIA GPU-accelerated analytics. In this age of massive data, our contribution to the RAPIDS graph libraries will help data scientists gain meaningful knowledge from ever-growing datasets.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Graphistry - Leo Meyerovich, co-founder and CEO&lt;br&gt;
&amp;ldquo;Graphistry, one of the first GPU cloud startups, has been quietly bringing new levels of visibility to sensitive F500 and federal teams that must comb through records in finance, cybersecurity, operations, and sales. As an early contributor to RAPIDS and a force behind Apache Arrow, Graphistry has taken a big bet on RAPIDS. The firm is already known for having redefined the visual compute fabric to be a real-time blending of browser and cloud GPUs, and is working with the RAPIDS team to add next-level tabular analytics to its existing graph GPU visual analytics core.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;H2O.ai - Sri Ambati, founder and CEO&lt;br&gt;
&amp;ldquo;Machine learning is transforming businesses and NVIDIA GPUs are speeding them up. With the support of the open source communities and customers, H2O.ai made machine learning on GPUs mainstream and won recognition as a leader in data science and machine learning platforms by Gartner. NVIDIA&amp;rsquo;s support of the GPU machine learning community with RAPIDS, its open-source data science libraries, is a timely effort to grow the GPU data science ecosystem and an endorsement of our common mission to bring AI to the data center. Thanks to our partnership, H2O Driverless AI powered by NVIDIA GPUs has been on an exponential adoption curve &amp;ndash; making AI faster, cheaper and easier.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;INRIA (scikit-learn) - Gael Varoquaux, director of Scikit-Learn Operations&lt;br&gt;
&amp;ldquo;NVIDIA is demonstrating real progress in accelerating data science with new productivity tools such as RAPIDS. Combining very fast computation in a high-language is a game changer for data-analytics teams. We are excited that NVIDIA has chosen to make RAPIDS compatible with scikit-learn. We believe that it can benefit our community and look forward to collaborating with NVIDIA.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Kinetica - Nima Negahban, co-founder and CTO&lt;br&gt;
&amp;ldquo;The RAPIDS suite of open-source libraries is a significant improvement in enabling data scientists to leverage the power of the GPU across their model development toolchain. RAPIDS can dramatically simplify and optimize training and improve model accuracy, without any significant logical redesign effort on the part of the data scientist. We&amp;rsquo;re excited to partner with NVIDIA in this journey to democratize AI &amp;ndash; with NVIDIA driving model development and training and Kinetica driving operationalization and deployment of those models, enabling enterprises to gain maximum insight from their data.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Lenovo - Kirk Skaugen, president of Data Center Group&lt;br&gt;
&amp;ldquo;Enterprise customers and academia continue to be challenged in working with and analyzing massive amounts of data as they develop and test new strategies. The new RAPIDS open-source software promises to accelerate workflows by running them end-to-end on NVIDIA GPUs. We believe this innovation and collaboration will make a significant impact for customers.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;MapR - John Schroeder, CEO&lt;br&gt;
&amp;ldquo;RAPIDS is a breakthrough announcement for data science and, more importantly, the ability to directly impact an organization with data science. MapR is supporting this effort by focusing on complementary data management and deployment activities to accompany the end-to-end RAPIDS data science training and model workflow.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;NERSC - Rollin Thomas, Python data analytics lead&lt;br&gt;
&amp;ldquo;NERSC supports more than 7,000 researchers at universities, national labs and in industry. They increasingly want productive, high-performance ways of interacting with their data from complex science simulations or experimental and observational facilities like particle accelerators and telescopes. We look forward to working with NVIDIA to put new high-performance Python data analytics tools like RAPIDS in the hands of our users to accelerate their pace of discovery across many scientific disciplines.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;NetApp - Octavian Tanase, senior vice president of ONTAP&lt;br&gt;
&amp;ldquo;Organizations must take advantage of new artificial intelligence capabilities to drive competitive advantage and accelerate digital transformation. The combination of RAPIDS powered by NVIDIA GPUs and NetApp&amp;rsquo;s AFF A800 cloud-connected all-flash storage will help customers confidently tap into growing data resources with virtually unlimited scalability and performance needed to feed, train and operate data-hungry AI applications.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;NumFOCUS - Andy Terrel, president of the board of directors&lt;br&gt;
&amp;ldquo;NVIDIA&amp;rsquo;s support of NumFOCUS represents an investment to the community. As two leaders in data science, we feel our work together will bring better tools to science and business alike.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;OmniSci - Todd Mostak, CEO and co-founder&lt;br&gt;
&amp;ldquo;Data scientists use OmniSci on NVIDIA GPUs to accelerate data exploration and feature engineering when creating machine learning models. Now our users can interactively query and visualize data at scale in OmniSci, and then pipe the results into RAPIDS&amp;rsquo; open-source libraries, enabling powerful end-to-end data science workflows. Together, NVIDIA and OmniSci make it much faster to build and iterate on models, resulting in increased accuracy and quicker time to deployment.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Pure Storage - Matt Burr, general manager of FlashBlade&lt;br&gt;
&amp;ldquo;Our customers look to data for insights that separate them from the competition and deliver ever-increasing value for their end users. RAPIDS amplifies the impact of NVIDIA GPU acceleration and Pure Storage FlashBlade for data science and machine learning workflows to help more data scientists speed their training pipelines while maintaining optimal low-latency performance for faster time to results.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Quansight - Travis Oliphant, NumPy and SciPy creator, co-founder and director of Anaconda, founder and CEO of Quansight&lt;br&gt;
&amp;ldquo;NVIDIA has long been a leader in accelerated tools for advanced analytics and has consistently offered freely available high-speed libraries for use by developers in the data-science community. I am thrilled to see their expanded open-source framework for data-science and their commitment to an end-to-end software and hardware solution. These innovations will enable a dramatic speed-up of the entire data-science workflow and unleash innovation across the broader open-source ecosystem.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;SAP - Juergen Mueller, chief innovation officer&lt;br&gt;
&amp;ldquo;SAP has worked with NVIDIA closely over the past several years to take advantage of GPU acceleration for many SAP Leonardo Machine Learning-enabled solutions. We are furthering that collaboration now to explore the possibilities offered by RAPIDS, which promises to hypercharge data science pipelines on GPUs. This is an important step to accelerate data science and machine learning for data scientists as we bring intelligence to enterprises with SAP Leonardo and SAP HANA.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;SAS - Saratendu Sethi, head of Artificial Intelligence and Machine Learning&lt;br&gt;
&amp;ldquo;We are working closely with NVIDIA to contribute to the new GPU-accelerated data science library. We look forward to future SAS Viya offerings to take advantage of RAPIDS so that our customers can gain valuable insight from their data even faster.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;SQream - Ami Gal, CEO&lt;br&gt;
&amp;ldquo;The work NVIDIA has done on RAPIDS presents an exciting opportunity for dramatically speeding up the data science pipeline. By combining SQream DB&amp;rsquo;s capability of piping in very large amounts of data into the RAPIDS data science platform, we expect that data scientists will be able to run models faster and on more data than ever before.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;University of California, Davis - John Owens, professor and Gunrock project lead
&amp;ldquo;We are delighted to be part of the RAPIDS community and look forward to working with NVIDIA and its partners in building the highest-performance, most comprehensive ecosystem for data analytics.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;For further information, contact:&lt;br&gt;
Kristin Bryson&lt;br&gt;
PR Director for Data Center AI, HPC and Accelerated Computing&lt;br&gt;
NVIDIA Corporation&lt;br&gt;
+1-203-241-9190&lt;br&gt;
&lt;a href=&#34;mailto:kbryson@nvidia.com&#34;&gt;kbryson@nvidia.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Certain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, and availability of the RAPIDS GPU-acceleration platform; the sizes of the server market for data science and machine learning and of the high performance computing market; the benefits and impact of NVIDIA&amp;rsquo;s collaboration with Ursa Labs; and Walmart&amp;rsquo;s relationship plans with NVIDIA are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners&amp;rsquo; products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company&amp;rsquo;s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.&lt;/p&gt;
&lt;p&gt;(c) 2018 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX and RAPIDS are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 GlobeNewswire, Inc. All rights reserved&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.marketwatch.com/press-release/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning-2018-10-10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.marketwatch.com/press-release/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning-2018-10-10&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader on Real World Challenges for Big Data Analytics</title>
      <link>http://localhost:1313/blog/20180808-pasc18/</link>
      <pubDate>Wed, 08 Aug 2018 06:42:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180808-pasc18/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=T6fW2GlZDbo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://img.youtube.com/vi/T6fW2GlZDbo/0.jpg&#34; alt=&#34;Bader PASC18 interview&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this video from &lt;a href=&#34;https://pasc18.pasc-conference.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASC18&lt;/a&gt;, &lt;strong&gt;David Bader&lt;/strong&gt; from Georgia Tech summarizes his &lt;a href=&#34;https://pasc18.pasc-conference.org/program/keynote-presentations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keynote talk&lt;/a&gt; on Big Data Analytics.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Emerging real-world graph problems include: detecting and preventing disease in human populations; revealing community structure in large social networks; and improving the resilience of the electric power grid. Unlike traditional applications in computational science and engineering, solving these social problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for research on scalable algorithms, and development of frameworks for solving these real-world problems on high performance computers, and for improved models that capture the noise and bias inherent in the torrential data streams. In this talk, Bader will discuss the opportunities and challenges in massive data-intensive computing for applications in social sciences, physical sciences, and engineering.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20180808-pasc18/Bader-PASC2018_hu_7d497060c9e7e03e.webp 400w,
               /blog/20180808-pasc18/Bader-PASC2018_hu_a88c7c1b00a8e4d0.webp 760w,
               /blog/20180808-pasc18/Bader-PASC2018_hu_2ea89c7364a3e01d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180808-pasc18/Bader-PASC2018_hu_7d497060c9e7e03e.webp&#34;
               width=&#34;300&#34;
               height=&#34;222&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;David Bader is Professor and Chair of the School of Computational Science and Engineering at Georgia Institute of Technology, and is regarded as one of the world’s leading experts in data sciences. His interests are at the intersection of high performance computing (HPC) and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. Bader has co-authored over 200 articles in peer-reviewed journals and conferences, and is an associate editor for high-impact publications including IEEE Transactions on Computers, ACM Transactions on Parallel Computing, and ACM Journal of Experimental Algorithmics. He is a Fellow of the IEEE and AAAS, and has served on a number of advisory committees in scientific computing and cyber-infrastructure, including the White House’s National Strategic Computing Initiative. Bader has served as a lead scientist in several DARPA programs and is a co-founder of the Graph500 list, a rating of “Big Data” computing platforms. He was recognized as a “&lt;a href=&#34;https://insidehpc.com/2011/11/announcing-our-newest-rock-star-of-hpc-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rock Star of HPC&lt;/a&gt;” by insideHPC and as HPCwire’s “People to Watch” in 2012 and 2014.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=B8zz-_GWSCY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://img.youtube.com/vi/B8zz-_GWSCY/0.jpg&#34; alt=&#34;Bader PASC18 interview&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this keynote video from &lt;a href=&#34;https://pasc18.pasc-conference.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASC18&lt;/a&gt;, David Bader from Georgia Tech presents: &lt;em&gt;Massive-Scale Analytics Applied to Real-World Problems&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2018/08/david-bader-real-world-challenges-big-data-analytics/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2018/08/david-bader-real-world-challenges-big-data-analytics/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Massive-Scale Analytics Applied to Real-World Problems</title>
      <link>http://localhost:1313/blog/20180719-pasc18/</link>
      <pubDate>Thu, 19 Jul 2018 06:59:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180719-pasc18/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=B8zz-_GWSCY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://img.youtube.com/vi/B8zz-_GWSCY/0.jpg&#34; alt=&#34;Bader PASC18 interview&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this keynote video from &lt;a href=&#34;https://pasc18.pasc-conference.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PASC18&lt;/a&gt;, David Bader from Georgia Tech presents: &lt;em&gt;Massive-Scale Analytics Applied to Real-World Problems&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Emerging real-world graph problems include: detecting and preventing disease in human populations; revealing community structure in large social networks; and improving the resilience of the electric power grid. Unlike traditional applications in computational science and engineering, solving these social problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for research on scalable algorithms and development of frameworks for solving these real-world problems on high performance computers, and for improved models that capture the noise and bias inherent in the torrential data streams. In this talk, Bader will discuss the opportunities and challenges in massive data-intensive computing for applications in social sciences, physical sciences, and engineering.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20180719-pasc18/Bader-PASC2018_hu_5fd6784e10bbf767.webp 400w,
               /blog/20180719-pasc18/Bader-PASC2018_hu_dbc45241930ac5b.webp 760w,
               /blog/20180719-pasc18/Bader-PASC2018_hu_55d89ae069f87d5f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180719-pasc18/Bader-PASC2018_hu_5fd6784e10bbf767.webp&#34;
               width=&#34;275&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; is Professor and Chair of the School of Computational Science and Engineering at Georgia Institute of Technology, and is regarded as one of the world’s leading experts in data sciences. His interests are at the intersection of high performance computing (HPC) and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. Bader has co-authored over 200 articles in peer-reviewed journals and conferences, and is an associate editor for high-impact publications including IEEE Transactions on Computers, ACM Transactions on Parallel Computing, and ACM Journal of Experimental Algorithmics. He is a Fellow of the IEEE and AAAS, and has served on a number of advisory committees in scientific computing and cyber-infrastructure, including the White House’s National Strategic Computing Initiative. Bader has served as a lead scientist in several DARPA programs and is a co-founder of the Graph500 list, a rating of “Big Data” computing platforms. He was recognized as a “Rock Star of HPC” by insideHPC and as HPCwire’s “People to Watch” in 2012 and 2014.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2018/07/massive-scale-analytics-applied-real-world-problems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2018/07/massive-scale-analytics-applied-real-world-problems/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The PlayStation Supercomputer</title>
      <link>http://localhost:1313/blog/20180608-dcd/</link>
      <pubDate>Fri, 08 Jun 2018 06:50:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180608-dcd/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Sebastian Moss&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In 1999, Sony was ascendant. Its video gaming business was its most profitable division, the PlayStation 2 was the world’s most successful console, and hopes were high that the successor would prove even more popular.&lt;/p&gt;
&lt;p&gt;To achieve this, the designers believed they would have to make the PlayStation 3 the most powerful console possible, with its own custom microprocessor.&lt;/p&gt;
&lt;p&gt;Sony turned to IBM and realized that “there was a potential synergy between the consumer oriented technology that Sony worked on, and the more business and data center oriented technology that IBM worked on,” Peter Hofstee, distinguished researcher at IBM, told &lt;em&gt;DCD&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A year of discussions culminated in a grand vision to develop a radically new chip architecture that the two companies hoped would become integral to consumer electronics, edge devices and data centers.&lt;/p&gt;


















&lt;figure  id=&#34;figure-cell-broadband-engine-architecture-on-a-ps3-board--wikimedia-commonsgreenpro&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cell Broadband Engine Architecture on a PS3 board – Wikimedia Commons/Greenpro&#34; srcset=&#34;
               /blog/20180608-dcd/CellBroadbandEngineArchitecturePS3Board.original_hu_e92f62c00ac4b8bb.webp 400w,
               /blog/20180608-dcd/CellBroadbandEngineArchitecturePS3Board.original_hu_a0017687f80da39d.webp 760w,
               /blog/20180608-dcd/CellBroadbandEngineArchitecturePS3Board.original_hu_c7b6127558673c19.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180608-dcd/CellBroadbandEngineArchitecturePS3Board.original_hu_e92f62c00ac4b8bb.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cell Broadband Engine Architecture on a PS3 board – Wikimedia Commons/Greenpro
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;behind-the-cell&#34;&gt;Behind the Cell&lt;/h2&gt;
&lt;p&gt;This innovation would be known as the Cell Broadband Engine Architecture, and would help commercialize the concept of heterogeneous computing, which relies on more than one kind of processor or core.&lt;/p&gt;
&lt;p&gt;Sony and IBM also recruited Toshiba, which previously worked on the PS2’s Emotion Engine CPU, and together formed the STI Alliance, with a budget of approximately $400 million.&lt;/p&gt;
&lt;p&gt;With the Cell chip set to launch as part of the PS3, Sony’s decisions were handled by Ken Kutaragi, the ‘father of the PlayStation’ who was at the time widely expected to become the next Sony president.&lt;/p&gt;
&lt;p&gt;“This was research on a development schedule,” Hofstee said. “We did over 600 patents leading into this thing. There were some very significant technical challenges along the way. We would get the architecture team and the key leaders from Sony and Toshiba every morning for weeks, sometimes months on end and we would hash through these things in a very collegial way.”&lt;/p&gt;
&lt;p&gt;The principal idea of the chip was to combine a general-purpose Power Architecture core with streamlined coprocessing elements. “One proposal, which maybe does not seem as exciting today, but was pretty aggressive at the time, was a four-core Power proposal. But Kutaragi decided that that was actually not aggressive enough,” Hofstee said.&lt;/p&gt;
&lt;p&gt;“We ended up with a chip that had nine cores in total - one Power core, and eight Synergistic Processor Elements,” he added. Hofstee was the chief architect of the SPEs, which consisted of a Synergistic Processing Unit (SPU) and a Memory Flow Controller (which had direct memory access, a memory management unit and a bus interface).&lt;/p&gt;
&lt;p&gt;Initial plans called for just 64KB of memory for every SPE, but the software team realized very early on that this amount would not suffice. Memory was increased to 128KB, and still, that wasn’t enough. “I said fine by me, but it is a zero sum game, so you can have eight SPEs with 128KB, or six SPEs with 256KB.&lt;/p&gt;
&lt;p&gt;“That was when we briefly went from eight to six, then Kutaragi saw that and he said ‘what is this, I didn’t approve that? It has to be eight.’” Later in the meeting, when asked why, he replied: “Eight is a beautiful number.”&lt;/p&gt;
&lt;p&gt;“I will never forget that,” Hofstee told &lt;em&gt;DCD&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The PlayStation 3 launched, after some delays, in late 2006 with a 90nm Cell chip inside it. A year later, IBM released the BladeCenter QS21, an energy-efficient server with 1.05 giga–floating point operations per second (GFlops) per watt, and a peak performance of approximately 460 GFlops. Elsewhere, the Cell chip made its way into Mercury Computer Systems servers, Toshiba TVs, Hitachi Medical scanners and Leadtek PCIe cards.&lt;/p&gt;
&lt;p&gt;“There were applications for technology in financial services, such as high performance computing for Wall Street, as well as applications for security, oil and gas sectors, gaming and entertainment, and bioinformatics,” &lt;strong&gt;Professor David Bader&lt;/strong&gt; told &lt;em&gt;DCD&lt;/em&gt;. “The design was very forward-looking, and the performance improvement was tremendous.”&lt;/p&gt;
&lt;p&gt;Bader directed the first Sony-Toshiba-IBM Center of Competence for the Cell Processor, at the Georgia Institute of Technology, which offered training and insights into how to use accelerators and multi-core technologies in data centers.&lt;/p&gt;
&lt;p&gt;“This was a chip that was purpose-built for high-end computing as well as gaming systems, that spans the whole gamut of low-end to high-end. It also created a groundswell in the applications space, with new understanding that applications across many sectors, from healthcare to finance to security, could all take advantage of accelerators.”&lt;/p&gt;
&lt;h2 id=&#34;the-machine-that-could&#34;&gt;The machine that could&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-roadrunner-supercomputer---los-alamos-national-laboratory&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Roadrunner supercomputer -– Los Alamos National Laboratory&#34; srcset=&#34;
               /blog/20180608-dcd/Roadrunnersupercomputer.original_hu_9ee997ef0c345cb6.webp 400w,
               /blog/20180608-dcd/Roadrunnersupercomputer.original_hu_86af9fa190151920.webp 760w,
               /blog/20180608-dcd/Roadrunnersupercomputer.original_hu_aba8cbff723410d1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180608-dcd/Roadrunnersupercomputer.original_hu_9ee997ef0c345cb6.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Roadrunner supercomputer -– Los Alamos National Laboratory
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Indeed, IBM did target the high-end computing market: in 2008 it built the world’s most powerful supercomputer to model the decay of the US nuclear arsenal for the Los Alamos National Laboratory. Roadrunner, a $100m system with 12,960 Cell chips (now 60nm with added memory) and 6,480 AMD Opteron dual-core processors, became the world’s first machine to sustain 1.0 petaflops using the Linpack benchmark, roughly double the performance of the next best system.&lt;/p&gt;
&lt;p&gt;“In retrospect, if you look at the Top500 chart [a list of the world’s fastest supercomputers], we made quite a step up,” Hofstee said. “You expect a certain growth rate on the list, but you can actually see that the Cell chip jumped off that quite a bit - we held the record a little bit longer than it’s typically held.”&lt;/p&gt;
&lt;p&gt;The system was also incredibly energy-efficient, another benefit of the Cell. Hofstee explained: “We went with this philosophy: instead of turning something off when it’s not needed, turning it on only when it is needed.”&lt;/p&gt;
&lt;p&gt;The team would run programs and watch which pieces of the chip turned on. “If we would see a floating point unit turn on when the program wasn’t supposed to have a floating point workload, we would know that something’s not quite right. This is an area where people have made more progress since then, but I really think we were the first to adopt this philosophy.”&lt;/p&gt;
&lt;p&gt;Bader concurred: “Up to this point in time, when you needed memory, it would be like going to the store and buying a whole case of food and bringing it back, even if you needed just one can. With the Cell, for the first time, you could take a shopping list, go to the store, and fill up your cart just with the individual cans that you may need, and then come home. That was game-changing.”&lt;/p&gt;
&lt;p&gt;But, despite the efficiency gains and its high performance, sales of Cell-powered servers were limited. “It was early technology that lacked the set of programmers and programming tools that were readily available for the processor,” Bader said.&lt;/p&gt;
&lt;p&gt;“The programming effort to use Cell required skilled and specially-trained programmers. The real challenge is to produce a radically new architecture that’s more efficient and capable while at the same time having the software co-design with programmers who are ready to take advantage of it.”&lt;/p&gt;
&lt;p&gt;Hofstee added: “It was fairly difficult for commercial users to adopt Cell if you knew that you had to write your software in such a way that it would only run on Cell. People have to really take a big leap of faith if you’re asking them to do that.&lt;/p&gt;
&lt;p&gt;“We were just a tad too early. There were a number of near misses. For example, OpenCL, the open standard for parallel programming of heterogeneous systems, came just too late.”&lt;/p&gt;
&lt;p&gt;Hofstee also believes that one could have taken a compiler from supercomputer manufacturer Cray “and retargeted it to a Cell chip, basically by thinking of the local store as a large vector register file.” At the time, he didn’t think there were any open Cray compilers around, but later discovered the Department of Energy had open-sourced one.&lt;/p&gt;
&lt;p&gt;“I wish I had known,” he said. “That would have made it more palatable for a community that just couldn’t make a decision to write their future on the architecture.”&lt;/p&gt;
&lt;p&gt;Meanwhile, as server sales struggled, Sony’s core consumer product, the PS3, was also in trouble. Initially sold at a significant loss, the console was costing the company billions, its sales lagged behind Microsoft’s Xbox 360, and developers found the unique architecture difficult to make games for. By 2007, Kutaragi had relinquished active management of the business he had built.&lt;/p&gt;
&lt;p&gt;“There were plans to have a second generation Cell which was supposed to have 32 SPEs and four Power processing elements, and scale it up,” Professor Gaurav Khanna told DCD.&lt;/p&gt;
&lt;p&gt;“What IBM mentioned is that Sony pulled the funding and there were practical reasons for that - Sony had lost a lot of money, they made a lot of investment, the PS3 didn’t quite take traction as quickly as they’d liked. Without Sony funding the next generation, it just died. That’s the hearsay I have heard.”&lt;/p&gt;
&lt;h2 id=&#34;unlocking-the-secrets-of-the-universe&#34;&gt;Unlocking the secrets of the universe&lt;/h2&gt;


















&lt;figure  id=&#34;figure-ps3-dualshock-3-controller---tookapic&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;PS3 DualShock 3 controller -– Tookapic&#34; srcset=&#34;
               /blog/20180608-dcd/PS3Controller.original_hu_56f2631479b1677f.webp 400w,
               /blog/20180608-dcd/PS3Controller.original_hu_b51a0555ca2c889c.webp 760w,
               /blog/20180608-dcd/PS3Controller.original_hu_a60629917ab85756.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180608-dcd/PS3Controller.original_hu_56f2631479b1677f.webp&#34;
               width=&#34;760&#34;
               height=&#34;454&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      PS3 DualShock 3 controller -– Tookapic
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Khanna, a black hole astrophysicist and associate director of the Center for Scientific Computing at the University of Massachusetts Dartmouth, was one of those attracted by the Cell chip’s promise.&lt;/p&gt;
&lt;p&gt;His subject of study, while fascinating, is an area “that generally is underfunded simply because there aren’t necessarily very direct practical applications for the research we do, it’s basic research, fundamental physics.”&lt;/p&gt;
&lt;p&gt;It does, however, require significant computing power to simulate black holes. Short on funding, Khanna had an idea - to use a cluster of PS3s. Not only did they offer a Cell chip at a low price, “Sony made it even easier by offering the possibility of putting Linux on it, and the entire scientific community uses Linux as its bread and butter. It just had to happen, it was such an obvious thing to try to do.”&lt;/p&gt;
&lt;p&gt;After contacting Sony because he “couldn’t really go to the National Science Foundation and say ‘give me a couple of thousand dollars for gaming,’” Khanna was given eight consoles by the Japanese corporation and created the Gravity Grid cluster.&lt;/p&gt;
&lt;p&gt;“We were able to take those eight machines and show some nice parallel performance, the code soon produced really good results - we even published a paper or two in the first year of this system being built. Then our Dean got interested and he gave us money to double it to sixteen.”&lt;/p&gt;
&lt;p&gt;Khanna’s PS3 cluster was the first to lead to published scientific results, although other researchers soon began to build their own. “There are 20 or so papers we’ve published using the cluster at this point,” he said.&lt;/p&gt;
&lt;p&gt;And those papers were made possible with an unusually affordable set up. “We did some nice estimates on how much it would have cost with a conventional system, it was easily tenfold. Essentially, one $400 PlayStation was equivalent in performance to 16-32 cores of your several-thousand-dollar Xeon-class x86 server.”&lt;/p&gt;
&lt;p&gt;As for energy efficiency, a “PlayStation on full load would consume about 100 watts of power; and at that time a Xeon-class dual socket type server would easily take about a kilowatt. It was quite remarkable.”&lt;/p&gt;
&lt;p&gt;Khanna’s work was not just being watched by other astrophysicists - it also caught the eye of the Air Force Research Laboratory (AFRL), which was eager to explore low power consumption yet high performance chips, especially for use in autonomous drones. “They were quite impressed with the performance of a PlayStation, especially its power efficiency,” he said. AFRL had a lot more resources, and set up a 300 PS3 cluster, which it soon expanded to 1,760 consoles.&lt;/p&gt;
&lt;h2 id=&#34;from-gaming-to-warfare&#34;&gt;From gaming, to warfare&lt;/h2&gt;


















&lt;figure  id=&#34;figure-the-condor-cluster---us-air-force&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Condor Cluster -– US Air Force&#34; srcset=&#34;
               /blog/20180608-dcd/Condor.original_hu_6fba2196bd5a27b0.webp 400w,
               /blog/20180608-dcd/Condor.original_hu_d19620a9eddb8806.webp 760w,
               /blog/20180608-dcd/Condor.original_hu_1c230a44549ec4f0.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180608-dcd/Condor.original_hu_6fba2196bd5a27b0.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Condor Cluster -– US Air Force
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The ‘Condor Cluster’ became the 33rd most powerful supercomputer in the world and ‘the fastest interactive computer in the US Department of Defense,’ capable of 500 trillion floating point operations per second.&lt;/p&gt;
&lt;p&gt;AFRL’s director of high power computing, Mark Barnell, said at the time that it cost $2 million, about 5-10 percent of an equivalent enterprise HPC system, and it consumed just 10 percent of the power that one would expect.&lt;/p&gt;
&lt;p&gt;Khanna was called in to explain what he had learned from his cluster, and began to help AFRL with their system - in return for access to the hardware.&lt;/p&gt;
&lt;p&gt;By late 2012, the Air Force decided to repurpose the floor space, and, with Khanna the Condor’s most active user, he was offered all of the 1,760 systems. After calculating how many the campus could support, he took 400.&lt;/p&gt;
&lt;p&gt;“One challenge we had is that they had substantial power and cooling requirements. The bill we had to renovate a lab to house 400 units was more than $500,000,” he said.&lt;/p&gt;
&lt;p&gt;“We decided that we had to innovate again, to come up with a crazy idea again.” Khanna looked to markets with large volumes, and found reefer containers - refrigerated units for shipping perishable produce over long distances.&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-reefer-gravity-grid-cluster---professor-khanna&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Reefer Gravity Grid Cluster -– Professor Khanna&#34; srcset=&#34;
               /blog/20180608-dcd/ReeferGravityGrid.original_hu_55fc64dacb43f3e1.webp 400w,
               /blog/20180608-dcd/ReeferGravityGrid.original_hu_2b2c07bf1baebf67.webp 760w,
               /blog/20180608-dcd/ReeferGravityGrid.original_hu_7154dd408850581c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180608-dcd/ReeferGravityGrid.original_hu_55fc64dacb43f3e1.webp&#34;
               width=&#34;507&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Reefer Gravity Grid Cluster -– Professor Khanna
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;“For $30,000 we had this relatively huge freezer unit that basically gives us cooling capacity of 80,000 British Thermal Units per hour, which is exactly what we needed,” he said.&lt;/p&gt;
&lt;p&gt;“At a tenth of the cost, we had a mini data center. That’s what we’ve had now for a couple of years, and the PlayStations have been happily running. We’ve had no trouble, it’s been able to maintain the temperature in a very stable way.”&lt;/p&gt;
&lt;p&gt;Khanna’s decade-old system is one of the few remaining implementations of the Cell chip, along with Sony’s PS3 game streaming service, PS Now, which is reported to use racks featuring eight custom console units each (Sony declined to comment). But the impact of the Cell can still be felt elsewhere.&lt;/p&gt;
&lt;p&gt;“The heterogeneous computing aspect of that has very much survived,” Hofstee said. “If you look at the supercomputers that IBM is starting to deliver now, the pre-exascale Summit and Sierra supercomputers, it’s Power plus an Nvidia GPU, but I would say it’s very much in the same spirit of Power plus the SPEs that we had on Cell.”&lt;/p&gt;
&lt;p&gt;He continued: “I’m very happy with the impact we’ve had on the overall arc of computing architecture.”&lt;/p&gt;
&lt;p&gt;As for Bader, he found that the ideas and knowledge produced in the Cell Center made an impact on the broader computing community. Meanwhile, the lessons about software tools and support “really helped provide a commercial success to emerging accelerators such as GPUs, and multi-core technologies.”&lt;/p&gt;
&lt;p&gt;He believes that the Cell was “a catalyst, an inflection point between pure CPU computing and moving to accelerators, and understanding the ecosystem required for accelerators, as well as bringing parallel computing to the mainstream.&lt;/p&gt;
&lt;p&gt;“Up to that point, every commercial application was single-threaded and the Cell entered the market at a time where developers made the shift to understanding multi-core.”&lt;/p&gt;
&lt;p&gt;Khanna added: “Now everyone’s getting back to it, a smartphone System-on-a-Chip has CPU and GPU on the same die. In many ways one could say that the legacy of the Cell is perhaps the most common processor out there.&lt;/p&gt;
&lt;p&gt;“I feel bad, really pained that IBM and Sony had this game almost a decade before everybody else got on top of it and sort of dropped it, it’s really painful to see. Maybe the Cell was just too ahead of its time, and now is the time.”&lt;/p&gt;
&lt;p&gt;The physicist, meanwhile, was left inspired by the process of turning a consumer product into a data center.&lt;/p&gt;
&lt;p&gt;The most recent prototype he has built consists of 32 Nvidia Shield tablets. ”Essentially it is a 16 teraflop system that only uses 300 watts of power, which is almost fivefold more efficient than the most power-efficient supercomputer on Green500, the list of the most efficient HPC systems in the world.“&lt;/p&gt;
&lt;p&gt;He told DCD: “We need to misuse more consumer electronics to do science. It’s cheaper for us, it’s more efficient. Why don’t we just piggyback on the innovation that’s happening in consumer electronics?”&lt;/p&gt;
&lt;h2 id=&#34;the-distributed-supercomputer&#34;&gt;The distributed supercomputer&lt;/h2&gt;
&lt;p&gt;Before the Cell chip found its way into the Roadrunner supercomputer and broke the one petaflop mark on the Linpack benchmark, it had already helped make history.&lt;/p&gt;
&lt;p&gt;Folding@home, a Stanford University distributed computing project, became the first platform to reach a petaflop in performance on a single precision float calculation - with 74 percent of the system’s power coming from PS3s.&lt;/p&gt;
&lt;p&gt;Over its five and a half year lifetime on the games console, more than 15 million PS3s contributed over 100 million hours of computing to Folding@home, used to simulate molecular dynamics, including for protein folding and computational drug design.&lt;/p&gt;
&lt;p&gt;“The PS3 system was a game changer for Folding@home, as it opened the door for new methods and new processors, eventually also leading to the use of GPUs,” research lead Vijay Pande said. “We have had numerous successes in recent years, [with simulations leading] to a new strategy to fight Alzheimer’s Disease.”&lt;/p&gt;
&lt;p&gt;While it no longer runs on the PS3, with its early help Folding@home has grown to become one of the world’s fastest computing systems, hitting around 135 petaflops in January 2018.&lt;/p&gt;
&lt;h2 id=&#34;shooting-bullets-at-black-holes&#34;&gt;Shooting bullets at black holes&lt;/h2&gt;
&lt;p&gt;We asked Professor Khanna to tell us about one of the papers he was able to publish with the help of the Cell chip, here’s what he had to say:&lt;/p&gt;
&lt;p&gt;“Black holes have different properties. They tend to spin, and when you get close to a black hole, if you think of a spinning object in water, it drags space time with it around and around. This is called space drag or frame drag. It turns out that the theory of black holes suggests that black holes can only spin to a maximum speed, if they spin faster than that they couldn’t exist.&lt;/p&gt;
&lt;p&gt;“A thought experiment with black hole physicists for many years has been ‘what if you take a black hole that’s already spinning close to the maximum and you throw in a particle that is going to try to spin it even more?’ Imagine if you fire a bullet into it at an angle that pushes it further in its spin: Couldn’t you overspin the black hole? There have been many suggestions of the type in the literature. But that’s something that many people, including Stephen Hawking, have been very skeptical about, because if you overspin a black hole the event horizon disappears and sort of pops, and you’re left with the bare naked singularity. That’s something that many black hole physicists think you would just not see in nature. Singularities should be inaccessible to us.&lt;/p&gt;
&lt;p&gt;“What we did is we decided to do a computational simulation using the black holes to actually do this to see what happens if we shoot a bullet into a black hole that’s almost at full spin to see if we could overspin it. What we found was something fascinating. What we found is that in order for this bullet to be of the kind that it would overspin the black hole, it has to have a very high speed and a good deal of mass. The bullet would actually become fairly energetic, and this energetic bullet would actually impact the black hole as it approaches it in a way that the event horizon - or the radius that the black hole would be capturing things within - would shrink.&lt;/p&gt;
&lt;p&gt;“Essentially, if you’ve got this type of bullet, it would simply not be captured by the black hole, it would just pass right by and the black hole would essentially ignore it. Because its size would shrink in essence.&lt;/p&gt;
&lt;p&gt;“We popularized this by saying that what would happen is, if a black hole saw a bullet coming towards it with enough mass and mass energy, the black hole would think ’this would pop my horizon and destroy me,’ so it would simply duck and ignore it.&lt;/p&gt;
&lt;p&gt;“So black holes can actually dodge bullets in that way. It’s fascinating that physics finds ways to prevent this from happening, and this came out of a detailed computation that we did using the PlayStations.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datacenterdynamics.com/analysis/the-playstation-supercomputer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datacenterdynamics.com/analysis/the-playstation-supercomputer/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keynote Speaker at Lehigh University 125th Anniversary of EE</title>
      <link>http://localhost:1313/blog/20180428-lehigh-125-ee/</link>
      <pubDate>Sat, 28 Apr 2018 06:32:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180428-lehigh-125-ee/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20180428-lehigh-125-ee/Bader-Lehigh_hu_5fe47822d4ba868.webp 400w,
               /blog/20180428-lehigh-125-ee/Bader-Lehigh_hu_d2181bb10dc939b0.webp 760w,
               /blog/20180428-lehigh-125-ee/Bader-Lehigh_hu_73d9a07a58a955c6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180428-lehigh-125-ee/Bader-Lehigh_hu_5fe47822d4ba868.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In April 2018, the ECE department at Lehigh University celebrated a huge milestone- the 125th Anniversary of the department!  Alumni, students, guest and faculty members all showed their support in our one day celebration.  Many thanks to our key note speakers who took time out of their day to talk to our department about the “Future Directions of ECE”, “Electrical Engineering: Future in Industry &amp;amp; Areas”, and “How can we Improve ECE at Lehigh”.  Our Keynote Speakers included: &lt;strong&gt;David Bader&lt;/strong&gt; (Professor &amp;amp; Chair of the School of Computational Science and Engineering, College of Computing at Georgia Institute of Technology), Michael Reffle (Infinera Corporation) and Philip Wong (Professor at Standford University).  Our Panel Speakers included: Aref Chowdhury (CTO of Optics at Nokia), Jeff Karper (Manager, Power Systems Engineering at Lockheed Martin-Rotary &amp;amp; Mission Systems), Sean Anderson (Principal Engineering with MACOM Technology Solutions), Thomas Charisoulis and Eileen Mazzochette.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wordpress.lehigh.edu/ece/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://wordpress.lehigh.edu/ece/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>&#34;I Lost 50 Pounds Making One Simple Change&#34;</title>
      <link>http://localhost:1313/blog/20180323-readersdigest/</link>
      <pubDate>Fri, 23 Mar 2018 19:14:25 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180323-readersdigest/</guid>
      <description>&lt;p&gt;&lt;em&gt;By &lt;a href=&#34;https://www.rd.com/author/sw-57628/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jen Babakhan&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;More than one-third of Americans are obese: We&amp;rsquo;re facing a national crisis, and solutions are in short supply. Here&amp;rsquo;s how one man turned a personal tracker into weight-loss success, one step at a time.&lt;/p&gt;


















&lt;figure  id=&#34;figure-image-courtesy-of-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*Image courtesy of David Bader*&#34; srcset=&#34;
               /blog/20180323-readersdigest/RD-Bader_hu_fda3693196fa5543.webp 400w,
               /blog/20180323-readersdigest/RD-Bader_hu_d1d5f2cf4fa164f4.webp 760w,
               /blog/20180323-readersdigest/RD-Bader_hu_1ba823462ab82b8e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180323-readersdigest/RD-Bader_hu_fda3693196fa5543.webp&#34;
               width=&#34;760&#34;
               height=&#34;506&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Image courtesy of David Bader&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Even though &lt;strong&gt;David Bader&lt;/strong&gt; had followed a vegan diet since his early twenties, his life-long struggle with weight remained. There are many &lt;a href=&#34;https://www.rd.com/health/healthy-eating/going-vegan/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;great reasons for going vegan&lt;/a&gt;, but Bader, a 48-year-old Georgia Tech professor, couldn’t keep the pounds off eating that way. “I’ve had a weight problem my entire life—in college I weighed 243 pounds at my heaviest. I lost weight when I became vegan, but quickly gained it back as I raised my daughter while working full time.” Bader says that everyday tasks became a challenge due to all the extra weight he was carrying around. “Even a walk would leave me out of breath. My life was focused on my daughter and my professional career. I put everything ahead of my own life and health, and had given up on ever losing weight.” Diet alone wasn’t getting the job done, and Bader was growing frustrated. Then he made the move that would change everything.&lt;/p&gt;
&lt;p&gt;Perplexed by the pounds that refused to budge, Bader bought a Fitbit tracker on a whim. Using the device, he was able to track steps taken, distance traveled, and his heart rate. He explains: “I purchased a Fitbit Charge HR that records my steps, heart rate, and activities, and soon I began tracking my daily steps. Just about a year ago, I started walking as a way to simplify my life, and focus on what was important—raising my daughter as she finished high school,” he says. “At first, I was walking 10,000 steps every day, yet my weight remained the same. I began by walking between two buildings at work, and getting used to wearing the Fitbit tracker. During this time, I learned how to use features of my smartphone, such as talk-to-text, and voice commands. That allowed me to multitask while walking, such as answering emails while walking on safe paths where I wouldn’t have to deal with traffic.” Motivated by his newfound ability to work while exercising, Bader continued to raise his personal exercise goals. “Quickly, I was able to increase the distances I was walking each day, until after two or three weeks I was walking a minimum of 27,000 steps per day—the equivalent of 12 miles. I was determined to walk at least that number of steps daily, and usually walked about 13 and a half miles a day. I started losing about two pounds a week.”&lt;/p&gt;
&lt;p&gt;Inspired to include walking in all areas of his life, Bader began involving others in his exercise routine. He says, “Many of my meetings with students and staff turned from sitting around a conference table to taking walks around campus. On days with bad weather, I would walk around the hallways of my building, or around the organic foods section of my local supermarket!” He had tapped into one of the proven ways to shed pounds: &lt;a href=&#34;https://www.rd.com/health/fitness/walk-your-way-to-weight-loss/1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Walking your way to weight loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A computer science professor, Bader applied his love for technical analysis to his new healthy habit. “I find that I walk about 7,000 steps an hour, so I try to walk about four hours a day. That’s approximately two hours in the morning, one in the afternoon, and one at night.” Maintaining this schedule requires some planning for Bader. “Since I tend to have a number of phone calls at work, I time these calls and teleconference meetings for when I can walk. I’ve also made a number of changes including avoiding elevators when I can take the stairs, parking at the furthest parking space, and walking for errands as often as I can.” Not every day is as conducive to physical activity as he would like. He says, “The most challenging days are when I’m flying for travel, or at a conference where everyone sits all day. I’ve found strategies for getting in walking even then, such as pacing in the back of the lecture hall. I just cannot sit anymore.”&lt;/p&gt;
&lt;p&gt;Since purchasing the Fitbit, Bader has lost 53 pounds and life is better than ever. He says, “I weighed 223 pounds before I began walking. I now weigh 170. My waist, which was 46 inches, is down to 34! My resting heart rate has decreased from 74 beats per minute to 60. This is the lightest I’ve ever been in my life, and I don’t get tired.” Bader has no plans to stop walking—the benefits are much too great. “I could walk forever, and I love to dance all night long! My stamina is out of this world. I feel like I’m getting younger, not older,” he says. Bader’s advice to others hoping to follow in his 27,000 daily steps? “Don’t give up! Anyone can get healthy. Get a Fitbit and walk! It doesn’t matter how much—just get up and take some steps!” Try starting with these &lt;a href=&#34;https://www.rd.com/health/fitness/walking-for-weight-loss/1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;eight easy walking strategies for weight loss&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jen Babakhan is a freelance writer who loves to help others through the written word. She writes about faith and motherhood at her site &lt;a href=&#34;https://www.jenbabakhan.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.jenbabakhan.com&lt;/a&gt;, and loves to inspire others through Instagram, where she can be found @jenbabakhan. You can also find her on Facebook @JenBabakhan, Writer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rd.com/health/fitness/i-lost-50-pounds-making-one-simple-change/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.rd.com/health/fitness/i-lost-50-pounds-making-one-simple-change/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader from Georgia Tech Joins PASC18 Speaker Lineup</title>
      <link>http://localhost:1313/blog/20180309-pasc18/</link>
      <pubDate>Fri, 09 Mar 2018 22:43:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180309-pasc18/</guid>
      <description>&lt;p&gt;Today PASC18 announced that this year’s Public Lecture will be held by &lt;strong&gt;David Bader&lt;/strong&gt; from Georgia Tech. Dr. Bader will speak on Massive-Scale Analytics Applied to Real-World Problems.&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader-from-georgia-tech&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*David Bader from Georgia Tech*&#34; srcset=&#34;
               /blog/20180309-pasc18/DavidBader_hu_79627a49bf7e5c19.webp 400w,
               /blog/20180309-pasc18/DavidBader_hu_3c1676f72d48a81d.webp 760w,
               /blog/20180309-pasc18/DavidBader_hu_351a1f5204d11d4a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180309-pasc18/DavidBader_hu_79627a49bf7e5c19.webp&#34;
               width=&#34;296&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;David Bader from Georgia Tech&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Emerging real-world graph problems include: detecting and preventing disease in human populations; revealing community structure in large social networks; and improving the resilience of the electric power grid. Unlike traditional applications in computational science and engineering, solving these social problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for research on scalable algorithms and development of frameworks for solving these real-world problems on high performance computers, and for improved models that capture the noise and bias inherent in the torrential data streams. This talk will discuss the opportunities and challenges in massive data-intensive computing for applications in social sciences, physical sciences, and engineering.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;David A. Bader is Professor and Chair of the School of Computational Science and Engineering at Georgia Institute of Technology, and is regarded as one of the world’s leading experts in data sciences. His interests are at the intersection of high performance computing (HPC) and real-world applications, including cybersecurity, massive-scale analytics, and computational genomics. Bader has co-authored over 200 articles in peer-reviewed journals and conferences, and is an associate editor for high-impact publications including IEEE Transactions on Computers, ACM Transactions on Parallel Computing, and ACM Journal of Experimental Algorithmics. He is a Fellow of the IEEE and AAAS, and has served on a number of advisory committees in scientific computing and cyber-infrastructure, including the White House’s National Strategic Computing Initiative. Bader has served as a lead scientist in several DARPA programs and is a co-founder of the Graph500 list, a rating of “Big Data” computing platforms. He was recognized as a “Rock Star of HPC” by InsideHPC and as HPCwire’s “People to Watch” in 2012 and 2014.&lt;/p&gt;
&lt;p&gt;PASC18 takes place July 2-4 in Basel, Switzerland.&lt;/p&gt;
&lt;p&gt;In related news, PASC18 also announced that it will host the ComPat Training Workshop on Multiscale Modelling &amp;amp; Simulation on HPC after the main conference closes on July 5, 2018.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2018/03/david-bader-georgia-tech-joins-pasc18-speaker-lineup/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2018/03/david-bader-georgia-tech-joins-pasc18-speaker-lineup/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSE Explores the Boundaries of HPC, Data Science, and India</title>
      <link>http://localhost:1313/blog/20180222-gatech/</link>
      <pubDate>Thu, 22 Feb 2018 20:51:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180222-gatech/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20180222-gatech/image_hu_9522b7bf161560c7.webp 400w,
               /blog/20180222-gatech/image_hu_ebae906f9bcdbac2.webp 760w,
               /blog/20180222-gatech/image_hu_8b03c5b1a0dc82e7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180222-gatech/image_hu_9522b7bf161560c7.webp&#34;
               width=&#34;740&#34;
               height=&#34;490&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;As 2017 drew to a close, several faculty and alumni from the School of Computational Science and Engineering (CSE) traveled across the globe to attended two conferences in India – The 24th IEEE International Conference on High Performance Computing (HiPC2017), held in Jaipur, and the IEEE International Conference on Machine Learning and Data Science (ICMLDS2017) in Greater Noida.&lt;/p&gt;
&lt;p&gt;CSE Professor and Associate Chair Ümit Çatalyürek served as the program chair of HiPC2017, which, coincidentally, was where he presented his first peer-reviewed paper in 1995. Now, 22 years later, Çatalyürek oversaw all aspects of the conference, from the technical program – with a committee of more than 100 individuals – that featured three keynote talks and 41 papers, selected from over 180 submissions. CSE Chair &lt;strong&gt;David Bader&lt;/strong&gt; and CSE Professor and IDEaS co-Director Srinivas Aluru are members of the HiPC Steering Committee and attended the conference.&lt;/p&gt;
&lt;p&gt;“The quality of the papers was very good this year and I was very happy. It can be challenging to attract such good submissions when you have HPC conferences in locations that are not historically known for being pillars for that field of research,” said Çatalyürek.&lt;/p&gt;
&lt;p&gt;Among these papers, was CSE Professor Srinivas Aluru’s, Parallel Exact Dynamic Bayesian Network Structure Learning with Application to Gene Networks, which earned a best paper finalist award.&lt;/p&gt;
&lt;p&gt;CSE recent graduate, Devaret Makkar, presented the paper Exact and Parallel Triangle Counting in Dynamic Graphs, which discusses a new dynamic graph algorithm for counting triangles, on behalf of co-authors CSE Research Scientist Oded Green and CSE Chair &lt;strong&gt;David Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Several days before HiPC2017, &lt;strong&gt;Bader&lt;/strong&gt; attended ICMLDS2017, the first of an international conference series hosted by Bennett University, where he participated as as a keynote speaker.&lt;/p&gt;
&lt;p&gt;Bennett University was established – just 18 months ago prior to the conference – as a startup academic venture in the suburbs of Delhi. It was founded by The Times Group, a conglomerate that owns several media brands throughout India.&lt;/p&gt;
&lt;p&gt;One of these brands, The Economic Times, interviewed &lt;strong&gt;Bader&lt;/strong&gt; about his keynote at ICMLDS2017, publishing it in the paper the next morning. The Times of India, the sister platform to The Economic Times, also interviewed &lt;strong&gt;Bader&lt;/strong&gt; about 5G abilities and published the Q&amp;amp;A online.&lt;/p&gt;
&lt;p&gt;The Times Group established Bennett University as an independent institute with the goals of revitalizing the research industry, addressing research needs across the region, and ultimately encouraging economic development of the country. The university, still in its infancy, hosted the conference as a means to introduce those in academia, industry, and international audiences, to the new concept.&lt;/p&gt;
&lt;p&gt;“The Times of India realized they needed a competitive advantage to encourage economic development of the country. In order to accomplish this with the creation of Bennett University, they gave the new school latitude to higher faculty and started collaborative research labs that are often joint with industry. In computing, they’ve started labs centered around companies such as Dell, NVIDIA, and others,” said &lt;strong&gt;Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;“One area they are investing in is using HPC for data science, and they have formed unique departments at the interface of traditional disciplines similar to our school [CSE] at Georgia Tech.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/news/602745/cse-explores-boundaries-hpc-data-science-and-india&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/602745/cse-explores-boundaries-hpc-data-science-and-india&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader serves on 2018 IEEE Fellow Committee</title>
      <link>http://localhost:1313/blog/20180101-ieee-fellow/</link>
      <pubDate>Mon, 01 Jan 2018 15:44:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20180101-ieee-fellow/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20180101-ieee-fellow/pic_hu_414b15840a38b2df.webp 400w,
               /blog/20180101-ieee-fellow/pic_hu_f935a312446bf64a.webp 760w,
               /blog/20180101-ieee-fellow/pic_hu_fdcafd08d939349d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20180101-ieee-fellow/pic_hu_414b15840a38b2df.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>5G will enable a new era of opportunity, says David Bader</title>
      <link>http://localhost:1313/blog/20171223-5g/</link>
      <pubDate>Sat, 23 Dec 2017 06:45:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20171223-5g/</guid>
      <description>&lt;p&gt;Recently, &lt;strong&gt;David Bader&lt;/strong&gt; visited India to give a keynote talk at IEEE International Conference on Machine Learning and Data Science at Bennett University, Greater Noida. David A. Bader is Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology. He is a fellow of the IEEE and AAAS and served on the White House’s National Strategic Computing Initiative (NSCI) panel. He was in conversation with Prof. Deepak Garg, Chair, of Computer Science Engineering at Bennett University.&lt;/p&gt;
&lt;p&gt;Some excerpts from the Interview, which will help start-ups to take up some ideas for their new ventures&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question: Big data and data analytics have made a huge impact on businesses in 2017, with trends like artificial intelligence and cloud services being used for their advantage. What are some of these trends that will continue and remain relevant in coming years, and what can be the new trends that take enterprises by storm?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: The enterprise is collecting data at massive rates – and we see the power of information for search – for example, Google – and in advertising such as understand shopper trends from purchases and making better recommendations. The cloud services are now available to collect huge volumes of data, and tools provided to make sense of this information. With Big Data and analytics, more accurate artificial intelligence is possible through Deep Learning technologies. But we are just at the early stages of a data economy. As we collect larger and more types of data and build IT systems that can handle tremendous streams of data, we will move towards predictive analytics. For instance, take cybersecurity. Today, system logs may be collected and after a breach occurs, we apply forensics to understand what flaws were exploited, what information was taken, and who took it. In the future, we will be able to detect attacks in real-time and stop them before breaches occur. In healthcare, as we move towards electronic medical records for patients, data analytics promises to give more accurate diagnoses and move us closer to predictive medicine. With the Internet of Things, and a world of sensors in every facet of life, every aspect of life may be improved through data analytics – from living in more energy-efficient ways, preventative repairs of everything from washing machines to automobiles at the earliest detection and before catastrophic failures, and better understanding of people and populations. It’s an exciting time in data science as we have so much potential to provide strategic predictions and real-time analytics that can improve many facets of our lives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There are so many articles online that talk about dangers of AI for the society. What do you perceive as a tangible threat and if you will like to give some direction on this issue?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: One of the biggest threats is the over-reliance of AI on the society. For instance, much bias is currently held in AI models for financial and legal decisions. Even though these are often touted as “unbiased” AI techniques, often these proprietary systems contain inherent flaws that make profound mistakes. For instance, there is AI software now that assists judges in the courtroom with the sentencing of offenders based upon predicted recidivism. However, no algorithm is “unbiased” – it inherently captures the bias of the programmer and methods. When using AI and algorithms, we must know better the accuracy and biases with the approach. As another example, financial decisions are often based on past behaviours of communities and associations; which may reinforce poverty within populations and not allow for class mobility. The computer science community must find ways within data science applications to quantify the accuracy and biases in approaches that everyone can easily understand. Until then, it is important that we do not over-rely on these automated “data” approaches.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can one leverage high-performance computing and big data in the Health Care and Pharma companies&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: Today, health care is mostly reactive – which proves to be more difficult to treat patients and leads to more expensive treatments. With high-performance computing and big data, we have the perfect confluence to move to predictive medicine – and we know detecting and treating disease earlier, even preventing disease, is far more effective and less costly. Up to now, healthcare information, such as patient data and medicine efficacy, has been stored on paper records – often handwritten – by doctors, and unusable for large-scale studies. As more medical practitioners move to electronic patient records we are optimistic that causal patterns may be found that improve predictive medicine. Even as more and more people can affordably have their DNA sequenced, we may learn how to group people to decide faster which medicines are more likely effective with less adverse reactions. High-performance computing is also helping with medical image processing and planning radiation therapy for cancer diagnoses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bennett University has a special Research Group called MISHA (Machine Intelligence for Smart places and Healthcare Applications). Our curriculum offers Machine Learning and Data Science courses. Based on your expertise, how could one leverage technology to equip students to make them Industry ready?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: Nearly every sector that has data can benefit from Machine Learning and Data Science. Bennett University is quite forward looking to offer this curriculum and having research group with a futuristic vision. I was amazed to see the supercomputing infrastructure of the world standard in a young University like Bennett. Many of these skills will help the budding students to solve some of the big problems of humanity. These techniques are already proving useful for better detection and classifications of cancer, early detection of heart failure onset, and learning to prescribe effective and safe treatment combinations for multimorbidity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Self-driving cars is the buzz in the automotive world. Could you please elaborate on the advancements in AI in this sector?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: Self-driving cars are a major research area, as highlighted by companies such as Google and Uber that are preparing fleets of cars. And I predict that in a few years, we will see more of these self-driving cars in cities, making deliveries, and operating on transit corridors. The challenge of self-driving cars is making safe decisions in real-time when faced with the unexpected. Deep Learning techniques may improve the accuracy for pattern detection and making decisions when faced with the unknown. Other ethical challenges vex the field, such as making quick decisions during an unavoidable crash and choosing the value between potential obstacles or people.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How will 5G change the market dynamics in Telecom over the next few years?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: 5G will enable a new era of opportunity as we are able to collect and transmit data sensed around the world, opening innovation to our creative minds for new ways to improve our human condition. Take our homes, for example. We are just seeing the entry of automated learning technologies, from thermostats to “surround” Wi-Fi, and energy-efficient dryers. Soon, all our devices will be on 5G, allowing them all to communicate with each other and share information. Not only does this give us a super-fast mobile network, but it enables new applications and our products working cooperatively. These devices may better understand our individual patterns of life, and predict our likes and dislikes, and automate more aspects of our lives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Could AI also be a game changer in the legal system – We’ve heard of companies in the US talk about “Robot Lawyers”. How can AI help in hastening litigation processes – what are the possible outcomes with AI applications?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Answer: While some think that AI could provide “robot lawyers” and automate the legal system, I’m somewhat of a sceptic to trust my legal protections solely to AI! Certainly, data science can help scour the case histories for related information and legal claims, and reduce fraud within the legal system. However, AI relies on models – and it will be some time before we know how to create truly “unbiased” decision systems, and if even at all possible! So, until then, I hope we are cautious with placing our fate in the hands of robots.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://timesofindia.indiatimes.com/blogs/breaking-shackles/5g-will-enable-a-new-era-of-opportunity-says-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://timesofindia.indiatimes.com/blogs/breaking-shackles/5g-will-enable-a-new-era-of-opportunity-says-david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Employees Keep It Moving on Campus</title>
      <link>http://localhost:1313/blog/20171211-gatech/</link>
      <pubDate>Sun, 17 Dec 2017 10:27:13 -0500</pubDate>
      <guid>http://localhost:1313/blog/20171211-gatech/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Victor Rogers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Debbie Dorsey wants to live to be 110 years old. Her great-grandfather lived to the age of 95. Her grandmother lived to be 99. And, her 85-year-old mother still lives on her own.&lt;/p&gt;
&lt;p&gt;“I hope the odds are in my favor!” Dorsey said jokingly. But, the 58-year-old is serious about her health.&lt;/p&gt;
&lt;p&gt;Dorsey, director of administration in the Division of Student Life, works out five or six days a week, with jogging and running three to four miles as one of a variety of exercises. When she runs outside it’s usually during her lunch hour, and the Pi Mile course through campus is her typical route.&lt;/p&gt;
&lt;p&gt;“The Pi Mile course is a good mix of hills and flats, so it’s a good workout,” she said. It also allows her to keep up with what is going on around campus.&lt;/p&gt;
&lt;p&gt;“We have a beautiful campus, and I like telling the landscaping crews what a great job I think they do,” she said. “I get ideas for my own yard too: what’s native, growing conditions, sun or shade, and such.”&lt;/p&gt;


















&lt;figure  id=&#34;figure-debbie-dorsey-stretches-outside-the-campus-recreation-center-photo-by-christopher-moore&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Debbie Dorsey stretches outside the Campus Recreation Center. Photo by Christopher Moore&#34; srcset=&#34;
               /blog/20171211-gatech/dorsey_debra_k_dsc_5967_hu_9314a4844ef4b9c2.webp 400w,
               /blog/20171211-gatech/dorsey_debra_k_dsc_5967_hu_dcf5f15125c676e6.webp 760w,
               /blog/20171211-gatech/dorsey_debra_k_dsc_5967_hu_32564ef133fab0d8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171211-gatech/dorsey_debra_k_dsc_5967_hu_9314a4844ef4b9c2.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Debbie Dorsey stretches outside the Campus Recreation Center. Photo by Christopher Moore
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Around the time of the Atlanta Olympics in 1996, Dorsey started jogging regularly to build up her endurance for playing field hockey.&lt;/p&gt;
&lt;p&gt;“The majority of the players were younger than me, so I had to do something to try and hold my own,” Dorsey said. “I’m not sure I have always been successful, but I’m having fun trying, and it gets me outside.”&lt;/p&gt;
&lt;p&gt;Getting outside is what &lt;strong&gt;David A. Bader&lt;/strong&gt; enjoys most about walking on campus.&lt;/p&gt;
&lt;p&gt;“Our campus has incredible people and beauty in every corner,” said Bader, professor and chair of the School of Computational Science and Engineering in the College of Computing. “I enjoy the public art, smiling faces, and conversations around the community. It’s also a great way to relieve daily stress, gain better health, and appreciate the world on a daily basis.”
Even though he became a vegan as a senior in college and eliminated animal products such as meat, fish, dairy, and eggs from his diet, Bader said he has had a weight problem his entire life.&lt;/p&gt;
&lt;p&gt;In August 2016, he started walking as a way to simplify his life and refocus on what was important: raising his daughter as she finished high school. He purchased a fitness tracker that records his steps, heartrate, and activities, and, soon, he was tracking his daily steps.&lt;/p&gt;
&lt;p&gt;At first, he walked 10,000 steps per day, as he got used to wearing the tracker. He also learned how to use his smartphone features such as talk-to-text and voice commands, so that he could answer emails and take calls while walking on a “safe path that intersects least with automobiles, roads, and traffic of all sorts,” he said.&lt;/p&gt;
&lt;p&gt;“When the weather is good, I often find a route around the Clough Commons, the Campanile, Tech Tower, and green spaces on campus,” Bader said. “Here, I often meet students, staff, and faculty, and see people I may not ordinarily bump into.”&lt;/p&gt;
&lt;p&gt;He said his staff use Slack, a team collaboration and messaging tool, to keep in touch, so he can be back in the office in a few minutes. He also turned many of his meetings with students and staff into “talking walks” on campus instead of huddles in a conference room. Within a few weeks, Bader was walking 27,000 steps each day, and the pounds — more than 50 — came off.&lt;/p&gt;
&lt;p&gt;Bader’s weight dropped from 223 pounds, when he started walking a year ago, to 170 pounds today. His waist was 46 inches and is now 34. He said he now has so much energy and endurance that he feels he could walk forever without getting tired.&lt;/p&gt;


















&lt;figure  id=&#34;figure-left-david-bader-in-july-2016-right-david-bader-in-july-2017&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Left: David Bader in July 2016. Right: David Bader in July 2017.&#34; srcset=&#34;
               /blog/20171211-gatech/davidbader_thenandnow_hu_fed4fa66c98d62d6.webp 400w,
               /blog/20171211-gatech/davidbader_thenandnow_hu_31d50fb1babdf1ee.webp 760w,
               /blog/20171211-gatech/davidbader_thenandnow_hu_8c5e7b435b717fcb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171211-gatech/davidbader_thenandnow_hu_fed4fa66c98d62d6.webp&#34;
               width=&#34;760&#34;
               height=&#34;314&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Left: David Bader in July 2016. Right: David Bader in July 2017.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Bader’s colleagues noticed his walking last year and encouraged him to join HealthTrails, the University System of Georgia’s (USG) six-weeklong team competition that tracks steps walked, water consumption, and kindness. In spring 2017, he tied for the top spot at Georgia Tech with Randy Ory, crime analyst with the Georgia Tech Police Department, and he also tied for No. 1 at the USG level.&lt;/p&gt;
&lt;p&gt;Now, he walks more than 12 miles a day, typically two hours in the morning, one hour in the afternoon, and an hour in the evening, seven days a week. And, he has managed his time to successfully “multitask,” walking while working.&lt;/p&gt;
&lt;h2 id=&#34;strength-in-numbers&#34;&gt;Strength in Numbers&lt;/h2&gt;
&lt;p&gt;Julie Hawkins, a donor relations associate and campaign and stewardship event coordinator in the Office of Development, also has mastered the logistics of blending walking with work. She walks three to four days a week during her lunch break, and her route is three miles around campus, beginning at the Wardlaw Center.&lt;/p&gt;
&lt;p&gt;“I try to keep my pace at between a 15- to 17-minute mile so I can get my heartrate up and finish in a timely manner, so I can return to my desk and eat lunch. I usually try to leave by 11:30 a.m. and return around 12:30 p.m.,” she said.&lt;/p&gt;
&lt;p&gt;Sometimes Hawkins walks alone, but she prefers walking with a coworker or two.&lt;/p&gt;
&lt;p&gt;“We talk both about work and our personal lives, and it certainly makes the walk go faster! If I am on my own, I typically listen to audiobooks or podcasts. Right now, I am listening to Big Little Lies as I walk,” she said.&lt;/p&gt;
&lt;p&gt;Hawkins has been at Tech for almost 12 years, and her walking route developed over time. Her basic route takes her to the field level of Wardlaw and down the sideline of Grant Field.&lt;/p&gt;
&lt;p&gt;“Sometimes when I am trying to find new venues for events or am preparing for a groundbreaking/dedication of a building, I venture off my route to look at parking areas, and potential tent setup locations. It can be fun to mix work into my walks from time to time,” she said.&lt;/p&gt;


















&lt;figure  id=&#34;figure-julie-hawkins-walks-down-the-sideline-of-grant-field-back-to-the-wardlaw-center-photo-by-christopher-moore&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Julie Hawkins walks down the sideline of Grant Field back to the Wardlaw Center. Photo by Christopher Moore&#34; srcset=&#34;
               /blog/20171211-gatech/julie_hawkins_hu_23ccdb6459901a37.webp 400w,
               /blog/20171211-gatech/julie_hawkins_hu_700b87eda0e433e0.webp 760w,
               /blog/20171211-gatech/julie_hawkins_hu_81c586e3d4daa2ec.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171211-gatech/julie_hawkins_hu_23ccdb6459901a37.webp&#34;
               width=&#34;760&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Julie Hawkins walks down the sideline of Grant Field back to the Wardlaw Center. Photo by Christopher Moore
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Like Bader, Hawkins participated in the spring HealthTrails, and she enjoyed the extra push to help lead her group, which placed fairly high in the challenge.&lt;/p&gt;
&lt;p&gt;“There are days when it is tempting to just sit and not move much, and having an extra incentive is needed,” she said.&lt;/p&gt;
&lt;p&gt;As Hawkins walks down the sideline of the football field, she just might cross paths with Head Football Coach Paul Johnson, Co-Offensive Line Coach Ron West, Assistant Coach Mike Sewak, Director of Player Personnel Andy Lutz, and Defensive Coordinator Ted Roof as they get their steps in.&lt;/p&gt;
&lt;p&gt;“We usually go five days a week, probably at least five miles a day,” Johnson said. “We’ll get three-and-a-half miles at lunch and one-and-a-half during practice.”&lt;/p&gt;
&lt;p&gt;“Pretty much every day, Coach West and Coach Sewak and I go walking,” Johnson said. “And then, Coach Lutz and Coach Roof walk at a different time.”&lt;/p&gt;
&lt;p&gt;He said the group started walking to keep fit when running was no longer an option.&lt;/p&gt;
&lt;p&gt;“I’m not able to run anymore because of my knees,” Johnson said, “and neither can a couple of the coaches. So, we just started walking.”&lt;/p&gt;


















&lt;figure  id=&#34;figure-coaches-mike-sewak-paul-johnson-and-ron-west-pass-the-ken-byers-tennis-complex-during-a-recent-walk-photo-by-christopher-moore&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Coaches Mike Sewak, Paul Johnson, and Ron West pass the Ken Byers Tennis Complex during a recent walk. Photo by Christopher Moore&#34; srcset=&#34;
               /blog/20171211-gatech/walking_coach_dsc_0880_hu_4d7af76dd06ce1ab.webp 400w,
               /blog/20171211-gatech/walking_coach_dsc_0880_hu_a2128eb788efbbc.webp 760w,
               /blog/20171211-gatech/walking_coach_dsc_0880_hu_176b2b63fda8c833.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171211-gatech/walking_coach_dsc_0880_hu_4d7af76dd06ce1ab.webp&#34;
               width=&#34;760&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Coaches Mike Sewak, Paul Johnson, and Ron West pass the Ken Byers Tennis Complex during a recent walk. Photo by Christopher Moore
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The group has a couple of different routes they walk.&lt;/p&gt;
&lt;p&gt;“There are some hills. One day we’ll go up 10th Street, through campus, and up the hill (by McAuley Aquatic Center). Another day, we’ll take a loop and go by Centennial Olympic Park Drive,” he said.&lt;/p&gt;
&lt;p&gt;“We talk (while walking), but we go at a pretty good pace. When it’s just Coach Sewak and myself, we’re going pretty good. And, Coach West has gotten to where he keeps up with us now.”&lt;/p&gt;
&lt;p&gt;Johnson also logs plenty of steps during a game.&lt;/p&gt;
&lt;p&gt;“I actually kept a phone in my pocket one time during a game just to see [how many steps I took], and I got about 19,000 steps.”&lt;/p&gt;
&lt;h2 id=&#34;motivation-to-get-moving&#34;&gt;Motivation to Get Moving&lt;/h2&gt;
&lt;p&gt;Not only is Dorsey striving to live past 100, another goal of hers is to never have to take any medicine.&lt;/p&gt;
&lt;p&gt;“So far, so good!” she said. “I’m a pretty independent person so I want to be able to take care of myself well into my 90s. You know — carry my own groceries, 50-pound bags of dog food for my St. Bernard, and move my plants indoors for the winter.”&lt;/p&gt;
&lt;p&gt;Dorsey’s advice for someone who is just beginning to jog includes wearing the right shoes. “It will make your experience — whether walking, jogging, or running — more enjoyable and safer.”
She also suggests looking into some of the G.I.T. FIT classes at the Campus Recreation Center (CRC) for core training and overall endurance. She participates in various noon classes at the CRC, such as interval training, treadfit, and boot camp, and she recommends them all — regardless of age.&lt;/p&gt;
&lt;p&gt;And Bader says, “Don’t give up. Anyone can get healthy. Get a FitBit and walk.” He also suggests making good choices regarding diet, such as drinking water instead of soda, and giving up anything with high fructose corn syrup or chemicals such as aspartame.&lt;/p&gt;
&lt;p&gt;“Try to choose fruits and vegetables, and give up processed and fast foods,” he said. “These will help your health head in the right direction.”&lt;/p&gt;
&lt;p&gt;Hawkins advises new walkers to “enjoy it. Just absorb all the action around as you walk. You’ll see funny things everywhere and also get to see what is going on around our beautiful campus. A green space in the center of a city is not really common, and we are lucky to have great sidewalks and crosswalks, which you just don’t find everywhere. Find a friend who can push you to move faster and go more often.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.gatech.edu/features/employees-keep-it-moving-campus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.gatech.edu/features/employees-keep-it-moving-campus&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>International Conference on Machine learning and Data Science</title>
      <link>http://localhost:1313/blog/20171214-icmlds/</link>
      <pubDate>Thu, 14 Dec 2017 20:42:19 -0400</pubDate>
      <guid>http://localhost:1313/blog/20171214-icmlds/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20171214-icmlds/icmls-1_hu_21da056b1107ed76.webp 400w,
               /blog/20171214-icmlds/icmls-1_hu_e3b64591a8abeff.webp 760w,
               /blog/20171214-icmlds/icmls-1_hu_38af10c1183af1b1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171214-icmlds/icmls-1_hu_21da056b1107ed76.webp&#34;
               width=&#34;749&#34;
               height=&#34;350&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Computer Science and Engineering Department (CSE) of Bennett University organised its first International Conference on Machine learning and Data Science, at the University campus, in Greater Noida. Touted as the biggest conference in this space, the conference will further boost India’s crucial leap in embracing the AI revolution. The conference witnessed an overwhelming response from organizations, researchers and academicians. There were thought provoking deliberations on the new wave of technologies and their impact in the world of big data, machine learning and AI. The two-day conference was organised by CSE in association with IEEE Computer Society.&lt;/p&gt;
&lt;p&gt;The conference was inaugurated by Roger Fujii, IEEE Computer Society President (2016). Speaking on the vision of IEEE for India Fujii said “India is a vital part in our future of membership and technology growth in the field of computing. We are pleased to be in India to establish a relationship with all the major Universities including Bennett University, so that we can develop and nurture the relationship such that India becomes a very important player in the computer society”.&lt;/p&gt;
&lt;p&gt;The event saw a footfall of Over 50 Organizations – IBM, NVIDIA, TCS, Wipro, Accenture, DELL and Infosys, to name a few. Around 400 participants representing premier institutions from all over India like IITs, NIT, IIMs, JNU, DTU, DU, KIIT UNIVERSITY, and MNIT Jaipur were present.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; – Professor and Chair, School of Computational Science and Engineering, Georgia Institute of Technology and Professor Inderjit S Dhillon – University of Texas delivered key note addresses that highlighted cutting edge research in the area of data science and AI. Dr. Yaj Medury – Vice Chancellor, Bennett University opened the conference welcoming the delegates and Dr Suneet Tuli – Dean, School of Engineering and Applied Sciences delivered the vote of thanks.&lt;/p&gt;
&lt;p&gt;The conference also coincides with the recent MoU signed between Bennett University and technology giant NVIDIA, which places the varsity as the first educational institute in the country to get DGX-1 V100 AI super computer. Following this, the CSE department of the university has designed a specialised curriculum to include deep learning – a type of machine-learning method that will facilitate to provide solutions to some of the unsolved societal problems in the realm of Healthcare &amp;amp; Pharma, smart cities, automotive industry and also in the search for Extra-terrestrial Intelligence.&lt;/p&gt;
&lt;p&gt;Speaking at the event, Dr. Deepak Garg, Head – CSE, Bennett University said “This conference is a key milestone in our quest to be a leader in Computer Engineering research and education. We have got the requisite gravitas by attracting quality faculty and embracing Innovative learning pedagogy for the Internet generation. This conference provided us with phenomenal outreach for our prospective collaborators, Industry connect. Participants were really impressed with our unparalleled computing infrastructure, interdisciplinary projects and student achievements”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Professor Bader&lt;/strong&gt;’s Keynote on Massive – Scale Streaming Analytics delved on providing a solution to real-world challenges ranging from healthcare, informatics to cyber security. He elaborated on how predictive methods is capable of making accurate business decisions. Addressing a gap in the education sector and talent for a trained workforce in machine learning, Bader emphasised that “Nearly every sector that has data can benefit from Machine Learning and Data Science. It will be wise for Institutions globally to take advantage of the myriad possibilities that Machine Learning has to offer and design a curriculum around new approaches that will help in augmenting the workforce required for implementing technologies”.&lt;/p&gt;
&lt;p&gt;Aditya Chaudhuri, M.D – Communication, Media and Technology at Accenture hosted an exclusive lunch for key delegates, to further the discussion on future technologies. Avant-garde workshops, live demos from IBM on their machine learning applications on Watson and AR/VR (Augmented reality/Virtual reality) demo on holodeck station from NVIDIA were also some of the key highlights of the conference.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bennett.edu.in/international-conference-machine-learning-data-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bennett.edu.in/international-conference-machine-learning-data-science/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bennett University, IEEE hold global meet on machine learning and data science</title>
      <link>http://localhost:1313/blog/20171214-bennettuniv/</link>
      <pubDate>Thu, 14 Dec 2017 06:27:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20171214-bennettuniv/</guid>
      <description>

















&lt;figure  id=&#34;figure-the-conference-coincided-with-the-recent-mou-signed-between-bennett-university-and-nvidia-making-it-the-first-educational-institute-in-the-country-to-get-the-dgx-1v100-ai-supercomputer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The conference coincided with the recent MoU signed between Bennett University and Nvidia, making it the first educational institute in the country to get the DGX-1V100 AI supercomputer.&#34; srcset=&#34;
               /blog/20171214-bennettuniv/BennettUniv_hu_c29870df3ae37b83.webp 400w,
               /blog/20171214-bennettuniv/BennettUniv_hu_65a4e9d8e09fe5e2.webp 760w,
               /blog/20171214-bennettuniv/BennettUniv_hu_28976a515688c4c3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171214-bennettuniv/BennettUniv_hu_c29870df3ae37b83.webp&#34;
               width=&#34;310&#34;
               height=&#34;233&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The conference coincided with the recent MoU signed between Bennett University and Nvidia, making it the first educational institute in the country to get the DGX-1V100 AI supercomputer.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;NEW DELHI: &lt;a href=&#34;https://economictimes.indiatimes.com/topic/Bennett-University&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bennett University&lt;/a&gt;&amp;rsquo;s computer science and engineering (CSE) department
held its first international conference on machine learning and data science at its Greater
Noida campus that saw researchers and academicians deliberating on the new wave of
technologies and their impact on the world of big data, machine learning and artificial
intelligence (AI).&lt;/p&gt;
&lt;p&gt;The two-day conference was organised by the department in association with the Institute
of Electrical and Electronics Engineers (IEEE) Computer Society. Bennett University has
been set up by the Times Group, which publishes ET.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are pleased to be in India to establish a relationship with all the major universities
including Bennett University, so that we can develop and nurture the relationship such that
India becomes a very important player in the computer society,&amp;rdquo; said Roger Fujii, 2016
president of the society.&lt;/p&gt;
&lt;p&gt;The event saw attendance from 50 organisations including IBM, Nvidia, Tata Consultancy Services, Wipro, Accenture, Dell and Infosys,
among others. Also attending were about 400 participants representing premier institutions such as the Indian Institutes of Technology,
National Institutes of Technology, Indian Institutes of Management, Jawaharlal Nehru University, Delhi Technological University,
University of Delhi, Kalinga Institute of Industrial Technology and Malaviya National Institute of Technology, Jaipur.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A Bader&lt;/strong&gt;, professor and chair, School of Computational Science and Engineering, Georgia Institute of Technology, and professor
Inderjit S Dhillon of the University of Texas delivered keynote addresses that highlighted cutting-edge research in the areas of data
science and AI. Bennett University vice-chancellor Yaj Medury opened the conference.&lt;/p&gt;
&lt;p&gt;The conference coincided with the recent MoU signed between Bennett University and Nvidia, making it the first educational institute in&lt;/p&gt;
&lt;p&gt;the country to get the DGX-1V100 AI supercomputer.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This conference is a key milestone in our quest to be a leader in computer engineering research and education,&amp;rdquo; said CSE head
Deepak Garg. &amp;ldquo;It provided us with phenomenal outreach for our prospective collaborators as well as industry connect.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://economictimes.indiatimes.com/industry/services/education/bennett-university-ieee-hold-global-meet-on-machine-learning-and-data-science/articleshow/62074454.cms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://economictimes.indiatimes.com/industry/services/education/bennett-university-ieee-hold-global-meet-on-machine-learning-and-data-science/articleshow/62074454.cms&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>White Hats, Black Hats, and Grey Matter: Tackling Cybersecurity</title>
      <link>http://localhost:1313/blog/20171201-gatech/</link>
      <pubDate>Fri, 01 Dec 2017 21:09:10 -0500</pubDate>
      <guid>http://localhost:1313/blog/20171201-gatech/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Gordana Goudie, Tara La Bouff, Jacqueline Nemeth, and Mike Terrazas&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A secure internet and its applications are now essential to almost every aspect of our daily lives. Yet connected technology has opened the door for criminals and foreign governments to launch cyberattacks with increasing scale and impact.&lt;/p&gt;
&lt;p&gt;Today, America’s national defense, economic prosperity, and individual freedoms depend upon cybersecurity.&lt;/p&gt;
&lt;p&gt;As the storm of demand for cybersecurity solutions and talent grows, Georgia Institute of Technology researchers, faculty members, and students are tackling cybersecurity from multiple angles.&lt;/p&gt;
&lt;p&gt;Black, White, and Nuances of Grey
In the realm of cybersecurity, white hats are good-guy defenders and black hats are the adversaries. But it takes both to really put grey matter to work in solving one of the most vexing challenges of our time.&lt;/p&gt;
&lt;p&gt;Few universities can approach cybersecurity with the same breadth and depth as Georgia Tech. Few have the cooperation of top-tier academic researchers, plus 500 cybersecurity engineers inside a multimillion-dollar research division that is the Georgia Tech Research Institute (GTRI), and a deep history of supporting classified military, government, and law enforcement operations.&lt;/p&gt;
&lt;p&gt;Under this unique combination of resources and skill, Georgia Tech is creating the next wave of cybersecurity solutions. Tech&amp;rsquo;s grey hat hackers study how malicious black hats operate and adapt in order to help the white hats prepare for the next attack.&lt;/p&gt;
&lt;p&gt;Seven units and 12 labs across Georgia Tech and GTRI are engaged in cybersecurity. With coordination by the Institute for Information Security &amp;amp; Privacy (IISP), faculty and students across a range of disciplines can connect to tackle new facets of the cybersecurity problem.&lt;/p&gt;
&lt;p&gt;Look what’s underway…&lt;/p&gt;
&lt;h2 id=&#34;computing&#34;&gt;COMPUTING&lt;/h2&gt;
&lt;p&gt;Researchers at the College of Computing advance cybersecurity by uncovering devastating vulnerabilities between operating systems, hardware, and software; mobile apps; and the machine-learning algorithms that power automated decisions.&lt;/p&gt;
&lt;h3 id=&#34;visionary-leaders&#34;&gt;Visionary Leaders&lt;/h3&gt;
&lt;p&gt;Cybersecurity students from the Georgia Tech College of Computing
The college spearheaded Georgia Tech’s entry into cybersecurity academic research in the 1990s with the launch of the Georgia Tech Information Security Center. An annual Cyber Security Summit soon followed.&lt;/p&gt;
&lt;p&gt;Since then, Georgia Tech has consistently been one of the top universities represented by research published at the world’s most important cybersecurity conferences, and the College of Computing has led the way.&lt;/p&gt;
&lt;p&gt;In fact, many of the Institute for Information Security &amp;amp; Privacy’s most influential researchers, such as Mustaque Ahamad, Taesoo Kim, Annie Antón, and Sasha Boldyreva, are based in the College’s three schools.&lt;/p&gt;
&lt;p&gt;With funding from the National Science Foundation, Department of Defense and industry leaders such as Intel and Google, more than two dozen professors are working on the frontiers of cybersecurity. For example, &lt;strong&gt;David Bader&lt;/strong&gt;, Polo Chau, and Le Song are developing new big data analysis algorithms for security analytics as well as new security protection for machine learning.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/features/white-hats-black-hats-and-grey-matter-tackling-cybersecurity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/features/white-hats-black-hats-and-grey-matter-tackling-cybersecurity&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>15th Graph500 List Reveals Top Machines for Running Data Applications</title>
      <link>http://localhost:1313/blog/20171115-hpcwire/</link>
      <pubDate>Wed, 15 Nov 2017 14:58:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20171115-hpcwire/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;http://graph500.org/?page_id=12&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;15th Graph500 list&lt;/a&gt; – which ranks supercomputers based on how quickly they can build knowledge from massive-scale data sets – was released Nov. 15 at &lt;a href=&#34;http://sc17.supercomputing.org/presentation/?id=bof184&amp;amp;sess=sess341&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Supercomputing 2017&lt;/a&gt; (SC17), with Japan’s K-Computer defending its position in the number-one spot several years in a row.&lt;/p&gt;
&lt;p&gt;The Graph500 is recognized as a leading indicator of high-performance computing (HPC) development and investment globally and often reveals trends regarding new technologies used in the machines. It provides a benchmark standard to test a supercomputer’s abilities to construct, search, and conduct edge-detection for undirected graphs.&lt;/p&gt;
&lt;p&gt;Georgia Tech School of Computational Science and Engineering Chair &lt;strong&gt;David Bader&lt;/strong&gt;, Peter Kogge of the University of Notre Dame, Andrew Lumsdaine of Pacific Northwest National Laboratory, and Rich Murphy of Micron Technology Inc., head the Graph500 executive committee. This committee – along with an International Multidisciplinary Steering Committee that comprises 30 international HPC experts from academia, national laboratories, and industry – rank Graph500 machines based on the benchmark standards.&lt;/p&gt;
&lt;p&gt;“The supercomputers measured are used to analyze big data for cybersecurity, medical informatics, social networks, data enrichment, and symbolic networks. The list is released twice a year to coincide with the two largest high-performance computing HPC conferences – Supercomputing and International Supercomputing Conference (ISC) – and it is the dominant international benchmark for analytics platforms,” said Bader.&lt;/p&gt;
&lt;p&gt;The Graph500 executive committee presented the top three:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RIKEN Advanced Institute for Computational Science (AICS)’s K Computer&lt;/li&gt;
&lt;li&gt;National Supercomputing Center in Wuxi’s Sunway TaihuLight&lt;/li&gt;
&lt;li&gt;Lawrence Livermore National Laboratory’s DOE/NNSA/LLNL Sequoia&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the announcement of the top three and highlighting the top 10 entries, the session also provided a discussion of Kogge’s &lt;em&gt;Summary of Graph500 Analytics, Kernel 2 Reference Implementation (SSSP) and Results&lt;/em&gt;, as well as the stream benchmark discussion and proposals.&lt;/p&gt;
&lt;p&gt;“The entries are truly international with machines from the U.S., Japan, Russia, China, Canada representing the countries with the most entries. Many other countries are represented as well amongst this year’s participants,” said Bader during the announcement ceremony.&lt;/p&gt;
&lt;p&gt;“Data intensive supercomputer applications are increasingly important workloads, especially for big data problems, but are ill-suited for most of today’s computing platforms. The Graph500 has demonstrated the challenges of even simple analytics and its participants have been extraordinary,” said Bader.&lt;/p&gt;
&lt;p&gt;Since the first Graph500 list with nine machines at SC10 in Nov. 2010, Graph500 has grown drastically to 235 entries in this year’s 15th Graph500 list at SC17.&lt;/p&gt;
&lt;p&gt;The 16th Graph500 list will be released at ISC, next June in Frankfurt, Germany.The 15th Graph500 list – which ranks supercomputers based on how quickly they can build knowledge from massive-scale data sets – was released Nov. 15 at Supercomputing 2017 (SC17), with Japan’s K-Computer defending its position in the number-one spot several years in a row.&lt;/p&gt;
&lt;p&gt;The Graph500 is recognized as a leading indicator of high-performance computing (HPC) development and investment globally and often reveals trends regarding new technologies used in the machines. It provides a benchmark standard to test a supercomputer’s abilities to construct, search, and conduct edge-detection for undirected graphs.&lt;/p&gt;
&lt;p&gt;Georgia Tech School of Computational Science and Engineering Chair David Bader, Peter Kogge of the University of Notre Dame, Andrew Lumsdaine of Pacific Northwest National Laboratory, and Rich Murphy of Micron Technology Inc., head the Graph500 executive committee. This committee – along with an International Multidisciplinary Steering Committee that comprises 30 international HPC experts from academia, national laboratories, and industry – rank Graph500 machines based on the benchmark standards.&lt;/p&gt;
&lt;p&gt;“The supercomputers measured are used to analyze big data for cybersecurity, medical informatics, social networks, data enrichment, and symbolic networks. The list is released twice a year to coincide with the two largest high-performance computing HPC conferences – Supercomputing and International Supercomputing Conference (ISC) – and it is the dominant international benchmark for analytics platforms,” said Bader.&lt;/p&gt;
&lt;p&gt;The Graph500 executive committee presented the top three:&lt;/p&gt;
&lt;p&gt;RIKEN Advanced Institute for Computational Science (AICS)’s K Computer
National Supercomputing Center in Wuxi’s Sunway TaihuLight
Lawrence Livermore National Laboratory’s DOE/NNSA/LLNL Sequoia
In addition to the announcement of the top three and highlighting the top 10 entries, the session also provided a discussion of Kogge’s Summary of Graph500 Analytics, Kernel 2 Reference Implementation (SSSP) and Results, as well as the stream benchmark discussion and proposals.&lt;/p&gt;
&lt;p&gt;“The entries are truly international with machines from the U.S., Japan, Russia, China, Canada representing the countries with the most entries. Many other countries are represented as well amongst this year’s participants,” said Bader during the announcement ceremony.&lt;/p&gt;
&lt;p&gt;“Data intensive supercomputer applications are increasingly important workloads, especially for big data problems, but are ill-suited for most of today’s computing platforms. The Graph500 has demonstrated the challenges of even simple analytics and its participants have been extraordinary,” said Bader.&lt;/p&gt;
&lt;p&gt;Since the first Graph500 list with nine machines at SC10 in Nov. 2010, Graph500 has grown drastically to 235 entries in this year’s 15th Graph500 list at SC17.&lt;/p&gt;
&lt;p&gt;The 16th Graph500 list will be released at &lt;a href=&#34;http://isc-hpc.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISC&lt;/a&gt;, next June in Frankfurt, Germany.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/15th-graph500-list-reveals-top-machines-running-data-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/15th-graph500-list-reveals-top-machines-running-data-applications/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Thanks Bader for Serving on the NSF Advisory Committee for Cyberinfrastructure (ACCI)</title>
      <link>http://localhost:1313/blog/20171101-nsf-acci/</link>
      <pubDate>Wed, 01 Nov 2017 19:52:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20171101-nsf-acci/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20171101-nsf-acci/certificate_hu_a51de17d534ab292.webp 400w,
               /blog/20171101-nsf-acci/certificate_hu_4b17d35caafd9cf0.webp 760w,
               /blog/20171101-nsf-acci/certificate_hu_f674d7f224e37bf3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171101-nsf-acci/certificate_hu_a51de17d534ab292.webp&#34;
               width=&#34;760&#34;
               height=&#34;581&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20171101-nsf-acci/letter_hu_34f17e08a0b27f3d.webp 400w,
               /blog/20171101-nsf-acci/letter_hu_6bf5495aaaff2d97.webp 760w,
               /blog/20171101-nsf-acci/letter_hu_1aa3f3248c7524af.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20171101-nsf-acci/letter_hu_34f17e08a0b27f3d.webp&#34;
               width=&#34;583&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dear &lt;strong&gt;Dr. Bader&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;I would like to thank you for your service as a member of the NSF Advisory Committee for Cyberinfrastructure (ACCI) from 2014 thru 2017. As you are aware, NSF has assumed a leadership role in the development and support of a comprehesive cyberinfrastructure essential to 21st century advances in science and engineering research and education. This can only occur with contributions such as yours. Your contribution as a member of the ACCI has provided valuable perspective and advice on the Foundation&amp;rsquo;s plans and strategic directions for cyberinfrastructure as well as important contact with the science and engineering community and other stakeholders impacted by our cyberinfrastructure portfolio.&lt;/p&gt;
&lt;p&gt;The NSF relies heavily on close interactions with the science and engineering communities. We serve and seek advice through a number of mechanisms, including advisory committees, taskforces, working groups, and workshops. I hope you will consider helping us again in the future when the occasion arises. Your continuing interest and input will certainly be appreciated.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;James Kurose&lt;br&gt;
Assistant Director, CISE&lt;/p&gt;
&lt;p&gt;Irene Qualters&lt;br&gt;
Office Director, OAC&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Awarded IARPA Contract to Evaluate Emu Technology System</title>
      <link>http://localhost:1313/blog/20170920-iarpa/</link>
      <pubDate>Wed, 20 Sep 2017 22:42:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20170920-iarpa/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20170920-iarpa/inside-chick_2_hu_6407c0f9e44295c.webp 400w,
               /blog/20170920-iarpa/inside-chick_2_hu_785f035a31300dfc.webp 760w,
               /blog/20170920-iarpa/inside-chick_2_hu_ea00b379926883a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20170920-iarpa/inside-chick_2_hu_6407c0f9e44295c.webp&#34;
               width=&#34;220&#34;
               height=&#34;138&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Georgia Tech’s School of Computational Science and Engineering finalized a $662,525 contract for a one-year Intelligence Advanced Research Projects Activity (IARPA) grant. CSE Senior Research Scientist Jason Riedy leads the project titled, Evaluating Memory-Centric Architectures for High Performance Data Analysis, while working alongside CSE Chair &lt;strong&gt;David Bader&lt;/strong&gt; and School of Computer Science Professor Tom Conte.&lt;/p&gt;
&lt;p&gt;The IARPA funding will be used to assess the capabilities of one of the first commercially available, memory-centric architectures. It is believed the new platform – known as Emu Chick –  holds promise for many applications for intelligence communities (IC).&lt;/p&gt;
&lt;p&gt;“Deriving knowledge from massive irregular data requires random-access bandwidth scalability beyond what current computing platforms can provide. Important data analysis computations like streaming graph analysis and sparse multiliniear modeling are not well-served by current architectures,” said Riedy.&lt;/p&gt;
&lt;p&gt;Emu Chick is an 8-node Emu computer housed in a tower case. The architecture aims to provide sufficient memory and storage bandwidth for many common data analysis applications by moving computation to data. The Georgia Tech team will evaluate this platform on its performance, usability, and scalability for high-performance data analysis.&lt;/p&gt;
&lt;p&gt;In addition to procuring this new architecture, Georgia Tech has tools and implementations for evaluating these data analysis tasks that can be ported immediately to Emu’s Cilk dialect, these include widely used benchmarks like the Graph500, streaming graph frameworks like STINGER, and the ParTI tensor package. The team intends to compare their results on the Emu Chick with the results on other architectures including Intel CPUs and NVIDIA GPUs.&lt;/p&gt;
&lt;p&gt;The Emu Chick will be the first member of the Rogues Gallery under Conte’s Center for Research into Novel Computing Hierarchies (CRNCH). The research of the center is focused on getting over one of the biggest hurdles facing computing today: the impending end of Moore’s Law. Novel architecture will be needed to ensure computers stay efficient and competitive in this changing landscape. The Rogues Gallery contributes to this goal by collecting the most unique hardware today, from newer machines like the Emu Chick to more common field programmable gate arrays (FPGA), so that fellow academics and industry professionals can learn and collaborate on new machines. The Rogues Gallery should be open by late fall, with CRNCH researchers also pursuing hardware related to embedded systems, neuromorphic and quantum computing&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/news/596243/georgia-tech-awarded-iarpa-contract-evaluate-emu-technology-system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cse.gatech.edu/news/596243/georgia-tech-awarded-iarpa-contract-evaluate-emu-technology-system&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Partners with USC for $6.8 Million DARPA Project</title>
      <link>http://localhost:1313/blog/20170710-darpa-hive/</link>
      <pubDate>Mon, 10 Jul 2017 18:50:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20170710-darpa-hive/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20170710-darpa-hive/image_hu_431951254357e9bb.webp 400w,
               /blog/20170710-darpa-hive/image_hu_9529cd576b3a5602.webp 760w,
               /blog/20170710-darpa-hive/image_hu_414695aabca1ab16.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20170710-darpa-hive/image_hu_431951254357e9bb.webp&#34;
               width=&#34;619&#34;
               height=&#34;316&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;(&lt;em&gt;Georgia Tech, Atlanta, GA, 10 July 2017&lt;/em&gt;) The Georgia Institute of Technology and the University of Southern California Viterbi School of Engineering have been selected to receive &lt;a href=&#34;https://www.darpa.mil/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Department of Defense Research Projects Agency (DARPA)&lt;/a&gt; funding under the &lt;a href=&#34;https://www.darpa.mil/program/hierarchical-identify-verify-exploit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hierarchal Identify Verify Exploit (HIVE)&lt;/a&gt; program. Georgia Tech and USC are to receive total funding of $6.8 million over 4.5 years to develop a powerful new data analysis and computing platform.&lt;/p&gt;
&lt;p&gt;Many security and consumer applications – including identifying and zeroing in on erratic driving behavior of vehicles in real-time, recognizing terrorist cells through patterns of communication, or protecting critical infrastructure facilities such as power, communication and water grids, or even predicting the spread of a cyber attack – can be modeled using graph data-analysis formalisms envisioned in the HIVE program.&lt;/p&gt;
&lt;p&gt;Georgia Tech and USC will be responsible for developing a software toolkit to work on HIVE processors being developed by hardware vendors. The goal is to process data at a rate 1000 times faster than existing hardware and software techniques.&lt;/p&gt;
&lt;p&gt;Georgia Tech’s School of Computational Science and Engineering Chair &lt;strong&gt;David Bader&lt;/strong&gt; will lead the academic development project, and the two universities will co-develop software to quickly process the incredible amount of data from cellphones, social media and other sources, and demonstrate the relationships among data points in real-time.&lt;/p&gt;
&lt;p&gt;“With the research infrastructure and capabilities at Georgia Tech – including the Center for Research into Novel Computing Hierarchies – and at USC, we are well-poised to deliver a robust yet stable software platform that will allow future programmers to fully leverage the revolutionary capabilities and performance that HIVE processors are expected to provide,” Bader said.&lt;/p&gt;
&lt;p&gt;Viktor Prasanna, professor of electrical engineering and a professor of computer science at USC Viterbi School of Engineering whose research interests include high-performance computing (HPC), hardware-software co-design and data science, will head the program from USC’s end. Rajgopal Kannan, a research associate in USC Viterbi’s Department of Electrical Engineering will also collaborate on this project. With years of network, architecture, and HPC experience, Research Scientist Oded Green will also play a key role at Georgia Tech.&lt;/p&gt;
&lt;p&gt;The project will mine data to rapidly understand interactions at what Prasanna calls “the edge of the internet.” Computing and graph processing will move to the “edge” near the source of the data and enable real-time decision-making as the data arrives without relaying back to the data centers. The software will be the critical component of the 1000-fold speed-up envisioned in the program, said Prasanna.&lt;/p&gt;
&lt;p&gt;Although driven by national security goals, these technologies, said Kannan, will deliver practical benefits to consumers through “social media analysis to reduce traffic congestion, better product matching for online shopping, self-driving cars, and lower-cost electricity.”&lt;/p&gt;
&lt;p&gt;“Extremely high-speed graph analytics streamed through the processor will enable consumers to have a more enjoyable experience at LA theme parks with less waiting and allow faster domain-specific “vertical” internet search,” said Kannan.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/news/593388/georgia-tech-partners-usc-68-million-darpa-project&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/news/593388/georgia-tech-partners-usc-68-million-darpa-project&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Forward-Looking Panel on the Future of CSE</title>
      <link>http://localhost:1313/blog/20170303-siam/</link>
      <pubDate>Fri, 03 Mar 2017 07:27:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/20170303-siam/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Karthika Swamy Cohen&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Where is computational science and engineering (CSE) headed? What new &amp;ldquo;grand challenges&amp;rdquo; will stimulate progress? How will the field benefit from machine learning and scientific computing? How will it drive new application areas like computational biology and medicine, computational geoscience, and materials science? What opportunities and challenges may we encounter in extending CSE to new subjects such as social network analysis, cybersecurity and the social sciences?&lt;/p&gt;
&lt;p&gt;Experts in the community hailing from diverse application areas addressed these important questions at the forward-looking panel during the SIAM Conference on Computational Science and Engineering, happening in Atlanta, GA this week.&lt;/p&gt;
&lt;p&gt;As moderator George Karniadakis put it, &amp;ldquo;We have 75 minutes here to forecast the future of CS&amp;amp;E.”&lt;/p&gt;


















&lt;figure  id=&#34;figure-panelists-from-left-fariba-fahroo-omar-ghattas-david-bader-karen-willcox-and-horst-simon-at-the-forward-looking-panel-during-the-siam-conference-on-computational-science--engineering&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Panelists from left: Fariba Fahroo, Omar Ghattas, David Bader, Karen Willcox and Horst Simon at the Forward-Looking panel during the SIAM Conference on Computational Science &amp; Engineering.&#34; srcset=&#34;
               /blog/20170303-siam/620400cthumb7_hu_e82b29d4a55cf755.webp 400w,
               /blog/20170303-siam/620400cthumb7_hu_111e669636cce6d8.webp 760w,
               /blog/20170303-siam/620400cthumb7_hu_d6b513ed33e48921.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20170303-siam/620400cthumb7_hu_e82b29d4a55cf755.webp&#34;
               width=&#34;620&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Panelists from left: Fariba Fahroo, Omar Ghattas, David Bader, Karen Willcox and Horst Simon at the Forward-Looking panel during the SIAM Conference on Computational Science &amp;amp; Engineering.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Horst Simon, deputy director of Lawrence Berkeley National Laboratory (LBNL), emphasized the field’s universality by discussing its extensive utilization. “I encounter CSE problems almost on a daily basis,” he said.&lt;/p&gt;
&lt;p&gt;Simon went on to describe the federal Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative at LBNL. &amp;ldquo;We put electrodes in patients’ brains and see if we can actually see how someone speaks words,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;With a proposed budget of $4.5 billion over 12 years, the BRAIN Initiative aims to facilitate discoveries and breakthroughs in our understanding of how the brain works and develop tools and technologies to prevent, diagnose, and treat brain and neurological diseases.&lt;/p&gt;
&lt;p&gt;At the other end of the application spectrum, CSE helps address these challenges. &amp;ldquo;How can you squeeze water out of paper?&amp;rdquo; Simon asked.  &amp;ldquo;From the brain to the practical, CSE is great.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;He also talked about machine learning and artificial intelligence (AI), referencing the AI accomplishment about which nearly everyone has heard: the victory of DeepMind&amp;rsquo;s AlphaGo over human Go champion Lee Sedol.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Machine learning and AI are super hyped today, but it&amp;rsquo;s just another tool,&amp;rdquo; Simon said. Future growth will depend on performance improvements in hardware platforms, algorithms, and parallel implementations. He also made a plea to computational mathematicians. &amp;ldquo;Please embrace quantum computing,” he implored. “Don&amp;rsquo;t let physicists take over quantum computing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Simon then mentioned two Obama administration projects, which are unlikely to progress due to budget cuts in the Trump administration: the National Strategic Computing Initiative, which has no attached funding, and the Exascale Computing Project. He insisted that the CSE community and SIAM must embrace the latter, whose key objective is attaining a factor of 100 improvement for about 25 previously-selected applications. “This is true application work - it is real CSE,” Simon said.&lt;/p&gt;
&lt;p&gt;Omar Ghattas, a computational geoscientist from the University of Texas at Austin, spoke about the use of unprecedented amounts of data and computational models in the field of climate science. “How do we capitalize on this relentless growth of computing power?” he asked.
Ghattas added that with rapidly expanding volumes of observational data, models don’t get as much love as they used to. &amp;ldquo;There is this popular view that we don&amp;rsquo;t need mathematical models, and that big data and machine learning will obviate all of that,&amp;rdquo; he said. “I don&amp;rsquo;t think it needs to be said, but big data alone is not going to allow us to understand our planet and predict its behavior.”&lt;/p&gt;
&lt;p&gt;Science and technology advance through the interplay of data and models, each propelling the other. “Observations push theory, theory pushes observation, and we move forward,” Ghattas said.&lt;/p&gt;
&lt;p&gt;The modern approach of integrating data and models has been working well in climate science, he continued. This is why weather predictions have steadily improved over the past decade. Improved models improve data.&lt;/p&gt;
&lt;p&gt;Ghattas then explained how to integrate all of this current observational data into a model. “The answer is it&amp;rsquo;s an inverse problem,” he said.&lt;/p&gt;
&lt;p&gt;We need methods to extract knowledge from data while respecting known physical laws and constraints. The challenges to this strategy are manifold. Model parameters often represent infinitedimensional fields. Many inverse problems are ill-posed. Data tend to be noisy and sparse. Models are often inadequate and uncertainty becomes a fundamental feature of these problems.&lt;/p&gt;
&lt;p&gt;Enter the Bayesian inference framework for inverse problems. “Bayesian inference provides a coherent and systematic framework for addressing these challenges,” Ghattas said. “The holy grail I see [in this field] is that we will be able to take all this observational data with uncertainties and put it all together in a Bayesian framework.”&lt;/p&gt;
&lt;p&gt;Fariba Fahroo, program manager at the Defense Advanced Research Projects Agency (DARPA), spoke about the importance of CSE to DARPA projects.
“I can see CS&amp;amp;E being relevant to everything they&amp;rsquo;re trying to do,” Fahroo said. “What&amp;rsquo;s so unique about DARPA for me is that its mission is supposed to be creating surprises and preventing surprises in the areas of science and technology and national security.”&lt;/p&gt;
&lt;p&gt;She noted synthetic biology, biomedical devices, and neuroscience as specific fields that have much use for CSE tools.&lt;/p&gt;
&lt;p&gt;Fahroo echoed Simon’s thoughts on emerging areas like big data and machine learning. “The hype that they have received has puzzled a lot of people who have dealt with big data and machine learning,” she said. “They are very surprised that the field has become so hot.”&lt;/p&gt;
&lt;p&gt;However, she insisted that computational scientists should ignore the hype while simultaneously owning these areas. “This is something we should embrace, and just make the community aware that we have the skills for it,” Fahroo said.&lt;/p&gt;
&lt;p&gt;She mentioned that data-driven modeling as another very interesting area of growth for the DARPA community. Individuals with a background in computer science are embracing it, thus essentially taking traditional modelers, such as physicists and chemists, out of the modeling business.&lt;/p&gt;
&lt;p&gt;Fahroo then went on to explain a few DARPA programs of relevance to CSE. Enabling Quantification of Uncertainty in Physical Systems (EQUiPS) provides a rigorous mathematical framework for quantifying, advancing, and managing multiple sources of uncertainty in the modeling and design of complex physical and engineering systems of interest to the Department of Defense.&lt;/p&gt;
&lt;p&gt;The end-to-end uncertainty quantification process is one aspect in which to engage the CSE community, Fahroo said.&lt;/p&gt;
&lt;p&gt;Models, Dynamics and Learning (MoDyL) is another pertinent DARPA program that aims to build rigorous data-driven models for non-equilibrium dynamics, which can address challenges posed by complex, nonlinear, multiscale dynamical systems. These systems often evolve to a critical state due to irreversible and unexpected events, severely limiting mathematical model development and implementation for accurate prediction, formation, and pattern evolution.&lt;/p&gt;
&lt;p&gt;Karen Willcox, professor of aeronautics and astronautics at the Massachusetts Institute of Technology (MIT), spoke about how CSE helps build the next generation of aerospace vehicles.
Aerospace vehicles have increasing levels of autonomy. New technologies, data, and computational abilities inspire new ways of vehicle design for efficiency, cost, reliability, adaptability, and performance. Aerial vehicles are becoming interconnected and self-aware. “In a sense, vehicles will know about structural health as a human does,” Willcox said. “They’ll know how they feel in any given day and what that might mean to the way they fly.”&lt;/p&gt;
&lt;p&gt;Virtual clones have greatly advanced aerospace, and virtual models informed by physics-based models and system-specific lifecycle data are blurring the boundaries between design, manufacturing, and innovation. This fluidity allows researchers to model at various scales and assures new approaches for decision-making.&lt;/p&gt;
&lt;p&gt;“Even virtual models of ourselves change the way we think about healthcare and what we eat on a given day,” Willcox said.&lt;/p&gt;
&lt;p&gt;The next generation of engineering systems will need predictive models, predictive data surveying, scalable uncertainty quantification, scalable methods that exploit data and models, and robust and sustainable software tools.&lt;/p&gt;
&lt;p&gt;“If you take away the adjectives on this above list, we have all of these today,” Willcox said. “But we need to keep thinking about them.”&lt;/p&gt;
&lt;p&gt;In short, unprecedented sensing capabilities and onboard computational power are changing the way researchers think about aerospace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, professor and chair in the School of Computational Science and Engineering at the Georgia Institute of Technology, presented an overview of the school’s history and mission.&lt;/p&gt;
&lt;p&gt;With a focus on solving real-world challenges, the School of Computational Science and Engineering emphasizes advanced computational techniques. It promotes innovation in computational methods and data analysis practices that solve diverse problems in areas such as cancer and disease diagnosis and treatment, sustainability, transportation, social networks, national security, and defense. A main objective is the development of novel techniques for large-scale computation and massive data sets. The school often partners with leading industrial organizations and national labs, essential to progress in all research areas.&lt;/p&gt;
&lt;p&gt;“We put together the school that we all thought we wanted when we were kids,” Bader said.&lt;/p&gt;
&lt;p&gt;He talked about addressing globally-significant grand challenges and noted that emerging trends lead toward new architectural features.&lt;/p&gt;
&lt;p&gt;Possible applications of CSE range from healthcare, massive social networks, and intelligence to systems biology, the electrical power grid, and modeling and simulation, Bader said.&lt;/p&gt;
&lt;p&gt;The panel then took questions from the audience, addressing queries about CSE education, workforce training, and “Grand Challenge” issues.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sinews.siam.org/Details-Page/forward-looking-panel-on-the-future-of-cse&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sinews.siam.org/Details-Page/forward-looking-panel-on-the-future-of-cse&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Five GT Computing Research Papers Presented at International Algorithms Conference</title>
      <link>http://localhost:1313/blog/20170123-gatech/</link>
      <pubDate>Mon, 23 Jan 2017 15:55:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/20170123-gatech/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20170123-gatech/soda17_attendees_2_0_hu_1b6fb6030e71fca9.webp 400w,
               /blog/20170123-gatech/soda17_attendees_2_0_hu_307d3cf115b6a986.webp 760w,
               /blog/20170123-gatech/soda17_attendees_2_0_hu_28fe8dfeec3ad244.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20170123-gatech/soda17_attendees_2_0_hu_1b6fb6030e71fca9.webp&#34;
               width=&#34;99&#34;
               height=&#34;220&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A number of faculty and students represented the Georgia Institute of Technology’s College of Computing this week in Barcelona, Spain, presenting five research papers at the 2017 ACM-SIAM Symposium on Discrete Algorithms (SODA 2017).&lt;/p&gt;
&lt;p&gt;The papers featured contributions from faculty and students in the School of Computer Science (SCS) and the School of Computational Science and Engineering (CSE), both in the College of Computing.&lt;/p&gt;
&lt;p&gt;Ph.D. student Matthew Fahrbach presented one of the five papers, which he coauthored with fellow Ph.D. Student Ben Cousins and Former SCS Graduate Student Prateek Bhakta. SCS ADVANCE Professor and Institute for Data Engineering and Science (IDEaS) Co-Executive Director Dana Randall also made contributions to the paper. Other College of Computing faculty presenting at SODA 2017 included SCS Assistant Professor Richard Peng, who presented research done in conjunction with colleagues at three peer institutions, SCS Professor Santosh Vempala, and CSE Research Scientist Sharma Thankachan.&lt;/p&gt;
&lt;p&gt;Along with accepted research papers, Georgia Tech was represented at the planning level for the event, as well. Randall is in the second-term of a multiyear position with the SODA Steering Committee, helping to organize the conference. In addition to her steering committee role, Randall also aided the program committee for the annual Analytic Algorithmics and Combinatorics (ANALCO17). One of two satellite conferences collocated with SODA. Chair &lt;strong&gt;David Bader&lt;/strong&gt; served on the program committee for Algorithm Engineering and Experiments (ALENEX17), ANALCO’s sister event.&lt;/p&gt;
&lt;p&gt;Sponsored jointly by ACM Special Interest Group on Algorithms and Computation Theory and the SIAM Activity Group on Discrete Mathematics, SODA is one of the most prominent conferences on discrete algorithms. It focuses on research topics related to efficient algorithms and data structures for discrete problems. In addition to the design of such methods and structures, the scope of the conference also includes implementation, performance analysis, and mathematical problems related to the development and limitations of discrete algorithms.&lt;/p&gt;
&lt;p&gt;The SODA took place between Jan. 16-19.&lt;/p&gt;
&lt;h2 id=&#34;georgia-tech-college-of-computing-supported-papers-at-soda-2017&#34;&gt;Georgia Tech College of Computing Supported Papers at SODA 2017:&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;A Framework for Analyzing Resparsification Algorithms&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Rasmus Kyng  (Yale University); Jakub Pachocki (Harvard University); Richard Peng (Georgia Institute of Technology); Sushant Sachdeva (Google)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Parameterized Pattern Matching – Succinctly&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Arnab Ganguly (Louisiana State University); Rahul Shah (Louisiana State University); Sharma V. Thankachan (Georgia Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Statistical Query Algorithms for Mean Vector Estimation and Stochastic Convex Optimization&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Vitaly Feldman (IBM Research – Almaden); Cristobal Guzman (Escuela de Ingenier ́ıa Pontificia Universidad Cato ́lica de Chile); Santosh Vempala (Georgia Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Adaptive Matrix Vector Product&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Santosh Vempala (Georgia Institute of Technology); David P. Woodruff (IBM Research – Almaden)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Approximately Sampling Elements with Fixed Rank in Graded Posets&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Prateek Bhakta (University of Richmond); Ben Cousins (Georgia Institute of Technology); Matthew Fahrbach (Georgia Institute of Technology); Dana Randall (Georgia Institute of Technology)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/news/586315/five-gt-computing-research-papers-presented-international-algorithms-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cse.gatech.edu/news/586315/five-gt-computing-research-papers-presented-international-algorithms-conference&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader serves on 2017 IEEE Fellow Committee</title>
      <link>http://localhost:1313/blog/20170101-ieee-fellow/</link>
      <pubDate>Sun, 01 Jan 2017 15:39:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20170101-ieee-fellow/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20170101-ieee-fellow/pic_hu_2a3419a918e63979.webp 400w,
               /blog/20170101-ieee-fellow/pic_hu_7ee9fc0c35c342cc.webp 760w,
               /blog/20170101-ieee-fellow/pic_hu_5cec1b30e1e3ccb1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20170101-ieee-fellow/pic_hu_2a3419a918e63979.webp&#34;
               width=&#34;760&#34;
               height=&#34;609&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>New Approach Seeks to Automate Parallel Programming</title>
      <link>http://localhost:1313/blog/20161111-datanami/</link>
      <pubDate>Fri, 11 Nov 2016 23:30:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20161111-datanami/</guid>
      <description>&lt;p&gt;&lt;em&gt;by George Leopold&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Artificial intelligence researchers are leveraging an emerging divide-and-conquer computing approach called “dynamic programming” to greatly accelerate the process of solving problems ranging from genomic analysis to cyber security.&lt;/p&gt;
&lt;p&gt;Engineers at Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Stony Brook University in New York reported at a computing conference earlier this month that their approach can be used to “parallelize” algorithms that leverage dynamic programming. The resulting programs running on multicore chips were said to operate up to 11 times faster than previous parallel processing techniques.&lt;/p&gt;
&lt;p&gt;Just as important, AI-based dynamic programming approached the efficiency of parallel processing programs develop by computer scientists.&lt;/p&gt;
&lt;p&gt;The CSAIL researchers dubbed the new approach “Bellmania,” after Richard Bellman, the applied mathematician who pioneered dynamic programming. That approach is based on a parallel processing strategy called recursive divide-and-conquer.&lt;/p&gt;
&lt;p&gt;For certain classes of problems such as computational biology, the researchers said          dynamic programming offers much faster processing by storing the results of previous computations. In a form of computing compression, the scheme reuses those computations rather than re-computing them each time they are needed.&lt;/p&gt;
&lt;p&gt;The approach also seeks to address lingering memory management issues that have been complicated by the steady shift to multicore processing architectures. The researchers noted that a “hand-optimized” parallel version of a dynamic-programming algorithm is typically 10 times longer as a single-core version. Complicating matters further, individual lines of code also are more complex.&lt;/p&gt;
&lt;p&gt;Using traditional approaches requires “more memory, because you store the results of intermediate computations,” noted CSAIL researcher Shachar Itzhaky, an associate professor of electrical engineering and computer science at MIT. “When you come to implement it, you realize that you don’t get as much speedup as you thought you would, because the memory is slow.”&lt;/p&gt;
&lt;p&gt;The recursive divide-and-conquer approach breaks computing tasks in “subproblems” that lend themselves to parallelization. Applying the Bellmania framework, a user can describe the initial step in the computational process and the platform continues divvying a problem to maximize memory efficiency to speed up processing on a multicore platform.&lt;/p&gt;
&lt;p&gt;“The goal is to arrange the memory accesses such that when you read a cell [of the matrix], you do as much computation as you can with it, so that you will not have to read it again later,” Itzhaky explained.&lt;/p&gt;
&lt;p&gt;The work is seen as enabling new applications running on multicore and parallel processors. “One challenge has been to enable high-level writing of programs that work on our current multicore processors, and up to now doing that requires heroic, low-level manual coding to get performance,” said &lt;strong&gt;David Bader&lt;/strong&gt;, a professor of computational science and engineering at Georgia Tech.&lt;/p&gt;
&lt;p&gt;Dynamic programming “is a much simpler, high-level technique for some classes of programs that makes it very easy to write the program and have their system automatically figure out how to divide up the work to create codes that are competitive with hand-tuned, low-level coding,” Bader added.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2016/11/11/new-approach-automate-parallel-programming/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/2016/11/11/new-approach-automate-parallel-programming/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Extends Bader&#39;s Service on Advisory Committee for Cyberinfrastructure</title>
      <link>http://localhost:1313/blog/20160930-nsf-acci/</link>
      <pubDate>Fri, 30 Sep 2016 06:35:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20160930-nsf-acci/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160930-nsf-acci/letter_hu_add9ac40cb4ddce5.webp 400w,
               /blog/20160930-nsf-acci/letter_hu_8c15cacd3cdc8db8.webp 760w,
               /blog/20160930-nsf-acci/letter_hu_8ef3df29aa9ea62e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160930-nsf-acci/letter_hu_add9ac40cb4ddce5.webp&#34;
               width=&#34;588&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Dr. Bader&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;Thank you for your willingness to extend your service as a memmber of the Advsiory Committee on Cyberinfrastructure (ACCI) of the National Science Foundation (NSF). As you know, the ACCI is operationally managed by the Division of Advanced Cyberinfrastructure (ACI) and advises NSF on program management, overall program balance, and other aspects of program performance to develop and support state-of-the-art cyberinfrastructure that enables significant advances in all fields of science and engineering supported by the agency.&lt;/p&gt;
&lt;p&gt;This letter confirms your 1-year term extension beginning in 2017, so that your appointment ends on December 31, 2017. As you know, the ACCI meets twice per year for about one and a half days each.&lt;/p&gt;
&lt;p&gt;I would be pleased to answer any questions you might have about this extension. Thank you for your willingness to continue to serve on the ACCI. Your input makes a valuable contribution to NSF cyberinfrastructure goals.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;James Kurose&lt;br&gt;
Assistant Director&lt;/p&gt;
&lt;p&gt;Irene Qualters&lt;br&gt;
Division Director, ACI&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Professor Helps Set White House’s HPC Agenda</title>
      <link>http://localhost:1313/blog/20160802-nsci/</link>
      <pubDate>Tue, 02 Aug 2016 18:35:19 -0500</pubDate>
      <guid>http://localhost:1313/blog/20160802-nsci/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160802-nsci/nsci_july16__1_hu_5feaee3c5671cc41.webp 400w,
               /blog/20160802-nsci/nsci_july16__1_hu_23c536764ab44466.webp 760w,
               /blog/20160802-nsci/nsci_july16__1_hu_16005b93c9112b72.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160802-nsci/nsci_july16__1_hu_5feaee3c5671cc41.webp&#34;
               width=&#34;220&#34;
               height=&#34;165&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Georgia Tech Professor &lt;strong&gt;David Bader&lt;/strong&gt;, chair of the School of Computational Science and Engineering (CSE), participated in the &lt;a href=&#34;https://www.whitehouse.gov/the-press-office/2015/07/29/executive-order-creating-national-strategic-computing-initiative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Strategic Computing Initiative (NSCI)&lt;/a&gt; Anniversary Workshop in Washington D.C., held July 29. Created in 2015 via an Executive Order by President Barack Obama, the NSCI is responsible for ensuring the United States continues leading in high-performance computing (HPC) in coming decades.&lt;/p&gt;
&lt;p&gt;Bader, a renowned HPC expert, leads several initiatives and projects that connect to the NSCI’s core objectives. He attended the invitation-only meeting to offer his insights on the country’s current standing in HPC.&lt;/p&gt;
&lt;p&gt;“From improving health care to protecting our nation against a cyberattack, the United States must invest in high-performance computing,” said Bader. “Maintaining a competitive advantage in HPC requires strategic investment and coordination across government, industry, and academia.  Georgia Tech is a key university where we can make significant leaps in solving tomorrow’s computational and big data challenges.”&lt;/p&gt;
&lt;p&gt;The goal of the NSCI is to advance core technologies in order to solve difficult computational problems, and foster increased collaboration between the public and private sectors. It will enable new analytic methods requiring more extensive computer processing, such as emerging techniques that allow artificial intelligence to learn new skills from large number sets.&lt;/p&gt;
&lt;p&gt;There are also national security benefits, including using modeling and simulation to design vehicles resistant to improvised explosive devices (IEDs). This coordinated national HPC strategy draws on the strengths of departments and agencies to move the federal government into a position that sharpens, develops, and streamlines a wide range of new 21st century applications.&lt;/p&gt;
&lt;p&gt;For additional information about David Bader or the National Strategic Computing Initiative, please use the links provided.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/news/557541/georgia-tech-professor-helps-set-white-houses-hpc-agenda&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cse.gatech.edu/news/557541/georgia-tech-professor-helps-set-white-houses-hpc-agenda&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2016 IBM Faculty Awards</title>
      <link>http://localhost:1313/blog/20160608-ibm/</link>
      <pubDate>Wed, 08 Jun 2016 22:18:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/20160608-ibm/</guid>
      <description>&lt;p&gt;Congratulations to the following faculty who have been selected for IBM Faculty awards granted in 2016.&lt;/p&gt;
&lt;h2 id=&#34;big-data--analytics&#34;&gt;Big Data / Analytics&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bader, David&lt;/strong&gt;, Georgia Institute of Technology, Optimizing Graph Analytics for Cognitive Computing&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.research.ibm.com/university/awards/faculty_innovation_2016.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.research.ibm.com/university/awards/faculty_innovation_2016.shtml&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech recognizes David Bader for Ten Years of Dedicated Service</title>
      <link>http://localhost:1313/blog/20160425-gatech/</link>
      <pubDate>Mon, 25 Apr 2016 18:50:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20160425-gatech/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160425-gatech/award_hu_e8137af783102cbf.webp 400w,
               /blog/20160425-gatech/award_hu_a9c60967e37cf3aa.webp 760w,
               /blog/20160425-gatech/award_hu_757d24dc6d13866d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160425-gatech/award_hu_e8137af783102cbf.webp&#34;
               width=&#34;562&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160425-gatech/coc1_hu_88c273a9a5ec4325.webp 400w,
               /blog/20160425-gatech/coc1_hu_1f19b6c4651c15b3.webp 760w,
               /blog/20160425-gatech/coc1_hu_494efccf696881ff.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160425-gatech/coc1_hu_88c273a9a5ec4325.webp&#34;
               width=&#34;500&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160425-gatech/coc2_hu_54df437199523a9d.webp 400w,
               /blog/20160425-gatech/coc2_hu_a6d713ad1f504c39.webp 760w,
               /blog/20160425-gatech/coc2_hu_46a0924a7b44aab.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160425-gatech/coc2_hu_54df437199523a9d.webp&#34;
               width=&#34;485&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Georgia Institute of Technology presents this Certificate of Appreciation to &lt;strong&gt;David Bader&lt;/strong&gt; for Ten Years of Dedicated Service&lt;/p&gt;
&lt;p&gt;G.P. Peterson&lt;br&gt;
President&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2016 IEEE Computer Society Board of Governors</title>
      <link>http://localhost:1313/blog/20160205-ieee-cs-bog/</link>
      <pubDate>Fri, 05 Feb 2016 14:36:19 -0400</pubDate>
      <guid>http://localhost:1313/blog/20160205-ieee-cs-bog/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160205-ieee-cs-bog/photo_hu_fd5b368a45c48feb.webp 400w,
               /blog/20160205-ieee-cs-bog/photo_hu_6044a7e38262d755.webp 760w,
               /blog/20160205-ieee-cs-bog/photo_hu_92475a42d3d70774.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160205-ieee-cs-bog/photo_hu_fd5b368a45c48feb.webp&#34;
               width=&#34;760&#34;
               height=&#34;583&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Bader serves on 2016 IEEE Fellow Committee</title>
      <link>http://localhost:1313/blog/20160101-ieee-fellow/</link>
      <pubDate>Fri, 01 Jan 2016 15:34:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20160101-ieee-fellow/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20160101-ieee-fellow/pic_hu_b48421982bbf13fe.webp 400w,
               /blog/20160101-ieee-fellow/pic_hu_997c2616ce6dc65f.webp 760w,
               /blog/20160101-ieee-fellow/pic_hu_fc336b7e287af533.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20160101-ieee-fellow/pic_hu_b48421982bbf13fe.webp&#34;
               width=&#34;760&#34;
               height=&#34;646&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Latest Graph500 Ranking of Fastest Supercomputers Released by Leading Universities at SC15</title>
      <link>http://localhost:1313/blog/20151117-hpcwire-graph500/</link>
      <pubDate>Tue, 17 Nov 2015 16:47:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20151117-hpcwire-graph500/</guid>
      <description>&lt;p&gt;The eleventh Graph500 list was released today at the Supercomputing 2015 conference (SC’15), with Japan’s K-Computer maintaining its top spot for the second consecutive time.&lt;/p&gt;
&lt;p&gt;Fujitsu, IBM and China’s National University of Defense Technology dominated the top 10, with the BlueGene/Q architecture holding eight of the top 10 positions. Other notable entries are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Largest Problem: DOE/NNSA/LANL Sequoia at Scale 41&lt;/li&gt;
&lt;li&gt;Best single node performance: Institute of Statistical Mathematics ismuv2k (SGI UV 2000) ranking #40 on the list with 175 GE/s&lt;/li&gt;
&lt;li&gt;Largest single node problem: UV 2000 (#79), 19.6 GE/s&lt;/li&gt;
&lt;li&gt;Highest Performance with High Memory Utilization: TitanXforsite (#46), 132 GE/s&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Graph500 ranks the performance of more than 200 of the world’s most powerful supercomputers, which are used to analyze “big data” for cybersecurity, medical informatics, social networks and other scientific fields. The list is released twice each year to coincide with top high-performance computer (HPC) conferences, and it is the dominant international benchmark for analytics platforms. The Graph500 is compiled by the Georgia Institute of Technology, Indiana University, the University of Notre Dame and Boise State University.&lt;/p&gt;
&lt;p&gt;“It’s about the data movement,” said Richard Murphy, director of advanced computing solutions pathfinding at Micron Technology Inc. and affiliated faculty at Boise State University.&lt;/p&gt;
&lt;p&gt;“Supercomputers are built according to the jobs they will execute, and the bottleneck for analytics codes is often memory bandwidth rather than peak floating point capability,” said &lt;strong&gt;David Bader&lt;/strong&gt;, professor and chair of Georgia Tech’s School of Computational Science and Engineering in the College of Computing.&lt;/p&gt;
&lt;p&gt;The Graph500 executive committee also announced on Tuesday a major new initiative in analytics benchmarking—a new streaming analytics benchmark and an infrastructure to enable the incubation of new analytics benchmarks from the community. The effort will formalize its comparative methodology to begin the process of creating predictive analytics that map real application performance to benchmark measurements. The group believes this community-driven approach is essential to advancing the state of the art in data-intensive platforms.&lt;/p&gt;
&lt;p&gt;“A community-driven approach is essential to any benchmarking process,” said Jack Dongarra, professor at the University of Tennessee and creator of the Linpack Benchmark. “We need a set of metrics for the evaluation of any HPC system.”&lt;/p&gt;
&lt;p&gt;Of the upcoming advances, Peter Kogge of the University of Notre Dame says, “Just as we have learned a lot about dense regular computations from the Top500, the Graph500 has generated a knowledge on irregular structures. The upcoming additions to the Graph500 benchmark set should continue fostering such advances.”&lt;/p&gt;
&lt;p&gt;The next Graph500 list will be released in June 2016.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/latest-graph500-ranking-of-fastest-supercomputers-released-by-leading-universities-at-sc15/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/latest-graph500-ranking-of-fastest-supercomputers-released-by-leading-universities-at-sc15/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Students Named Finalists for Best Student Research Paper at SC15</title>
      <link>http://localhost:1313/blog/20151117-hpcwire/</link>
      <pubDate>Tue, 17 Nov 2015 14:54:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20151117-hpcwire/</guid>
      <description>&lt;p&gt;Four doctoral students comprising two research projects in the School of Computational Science &amp;amp; Engineering at the Georgia Institute of Technology are finalists for “Best Student Research Paper” at Supercomputing ’15, the International Conference for High Performance Computing, Networking, Storage and Analysis.&lt;/p&gt;
&lt;p&gt;For the first project, the finalists developed a fast algorithm; in the second, they created a GPU-based framework that can process large graphs exceeding a device’s internal memory capacity. Each demonstrably outperforms other common approaches in high-performance computing today. A winner will be announced at the conference in Austin, Texas.&lt;/p&gt;
&lt;p&gt;“Both of these are outstanding projects and evidence of the future leaders who are already keeping pace with and solving the most challenging problems in science, engineering, health and the social domain,” says &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor and chair of the School of Computational Science &amp;amp; Engineering.&lt;/p&gt;
&lt;h3 id=&#34;new-human-genome-indexing-algorithm-for-parallel-distributed-memory&#34;&gt;New Human Genome Indexing Algorithm for Parallel Distributed Memory&lt;/h3&gt;
&lt;p&gt;In his work, “Parallel Distributed Memory Construction of Suffix and Longest Common Prefix Arrays,” PhD Candidate Patrick Flick, working with Professor Srinivas Aluru, created parallel algorithms for distributed-memory construction that are 110x times faster than the best method running on a sequential, single computer. Flick, using the human genome as a racetrack to test his speed, indexed it in only 7.3 seconds using his distributed-memory algorithm running on 1024 Intel Xeon cores.&lt;/p&gt;
&lt;p&gt;“Bioinformatics is an example of a scientific field that is extremely data intensive; speed matters and speed helps,” he says. “We are not aware of any other parallel suffix array or suffix tree construction algorithms which achieve speedups this high.”&lt;/p&gt;
&lt;p&gt;It is believed to be the first algorithm and implementation that uses this approach for distributed-memory parallel systems.&lt;/p&gt;
&lt;p&gt;“At this stage the code is offered as an open-source library that can be used within parallel applications,” Fick adds. “We hope that it finds adaptation within bioinformatics research. We are now working on a user-friendly interface that can be used by bioinformaticians to replace older (and slower) tools.”&lt;/p&gt;
&lt;p&gt;Next, Flick is working on a journal paper that includes some more improvements and additional techniques, and further showcases their algorithms on real applications. Patrick also authored another paper at Supercomputing 2015 with fellow students Chirag Jain and Tony Pan about how to partition large graphs that arise in metagenomics, another data-intensive application area.&lt;/p&gt;
&lt;h3 id=&#34;processing-large-scale-graphs&#34;&gt;Processing Large-Scale Graphs&lt;/h3&gt;
&lt;p&gt;In their paper titled “GraphReduce: Processing Large-Scale Graphs on Accelerator-Based Systems,” PhD Candidates Dipanjan Sengupta, and Kapil Agarwal developed a scalable framework (dubbed “GraphReduce”) to process large graphs that exceed a device’s GPU memory.&lt;/p&gt;
&lt;p&gt;“GraphReduce can accelerate the analysis of graphs with billions of edges, operating at speeds much faster than similar operations on CPUs, and programmed in ways that are accessible to those who are not typically experts in GPU programming,” Sengupta says.&lt;/p&gt;
&lt;p&gt;It provides a logic for processing “shard stores” based on choice of interval, number and sizes of shards, and how to order the edges in each shard. A “graph layout engine” then defines the layout of the data by sorting in-edges by their destination and out-edges by their source.&lt;/p&gt;
&lt;p&gt;“One of the interesting results is that saturating the available bandwidth and overlapping data transfer with computation was able to hide a large amount of overhead, resulting in huge performance benefits,” Agarwal adds.&lt;/p&gt;
&lt;p&gt;Most methods process dynamic graphs (like those of a social network which are continually changing over time) by storing static versions of the graph and then repeatedly running analysis on them. To address this, Sengupta next is working on an open-source framework with a Fortune 500 company to boost processing.&lt;/p&gt;
&lt;h3 id=&#34;about-the-georgia-tech-college-of-computing&#34;&gt;About the Georgia Tech College of Computing&lt;/h3&gt;
&lt;p&gt;The Georgia Tech College of Computing is a national leader in the creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked 9th nationally by U.S. News and World Report, the College’s unconventional approach to education is expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human-centered solutions. For more information about the Georgia Tech College of Computing, its academic divisions and research centers, please visit &lt;a href=&#34;http://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/georgia-tech-students-named-finalists-for-best-student-research-paper-at-sc15/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/georgia-tech-students-named-finalists-for-best-student-research-paper-at-sc15/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cyberanalytics: Protecting us with high-performance computing</title>
      <link>http://localhost:1313/blog/20151028-gatech/</link>
      <pubDate>Wed, 28 Oct 2015 11:04:12 -0500</pubDate>
      <guid>http://localhost:1313/blog/20151028-gatech/</guid>
      <description>&lt;h2 id=&#34;surviving-cyberspace&#34;&gt;Surviving Cyberspace&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20151028-gatech/Bader_hu_eb9caf6e08c9c824.webp 400w,
               /blog/20151028-gatech/Bader_hu_561707cf891063be.webp 760w,
               /blog/20151028-gatech/Bader_hu_15fc489a62001229.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20151028-gatech/Bader_hu_eb9caf6e08c9c824.webp&#34;
               width=&#34;300&#34;
               height=&#34;290&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;When we think of cybersecurity, we often think of the outsider trying to hack into our computer systems. But, another challenge is how we identify and defend against an insider, oftentimes a lone wolf, who knows our procedures and safety precautions.&lt;/p&gt;
&lt;p&gt;If we want to protect ourselves from both scenarios, we must increase our reliance on high-performance computing, especially the graph analytic research we conduct at Georgia Tech, says David Bader, chair of the School of Computational Science and Engineering in the College of Computing.&lt;/p&gt;
&lt;p&gt;Graphs help us discover patterns and relationships hidden in massive amounts of data. These graphs are comprised of interconnected vertices (nodes) and lines (edges), and these graphs change over time.&lt;/p&gt;
&lt;p&gt;In the realm of cybersecurity, the vertices are people, places, and things, and the edges represent their interactions. By designing fast, using theoretic algorithms on large-scale graphs, we can produce insights in near-real time. This is crucial because cybersecurity analysts often are overwhelmed with thousands of alerts to review, and our algorithms may direct them immediately to the most important ones.&lt;/p&gt;
&lt;p&gt;We leave a digital trace every time we use a key card to get through a door, log in to a computer, or send an email. Security officers need to analyze this information so they can understand our patterns and identify potential threats.&lt;/p&gt;
&lt;p&gt;These massive-scale datasets are often unstructured and challenging to inspect. The emerging graph technology we are developing at Georgia Tech has the potential to be the best and most efficient way to prevent future attacks where we work and live, says Bader.&lt;/p&gt;
&lt;p&gt;—&lt;strong&gt;​​David Bader&lt;/strong&gt; chairs the School of Computational Science and Engineering in the College of Computing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/features/surviving-cyberspace&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/features/surviving-cyberspace&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC15 Panel Line-Up for November 18</title>
      <link>http://localhost:1313/blog/20151020-hpcwire/</link>
      <pubDate>Tue, 20 Oct 2015 16:51:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20151020-hpcwire/</guid>
      <description>&lt;p&gt;As data intensive science emerges, the need for high performance computing (HPC) to converge capacity and capabilities with Big Data becomes more apparent and urgent. Capacity requirements have stemmed from science data processing and the creation of large scale data products (e.g., earth observations, Large Hadron Collider, square-kilometer array antenna) and simulation model output (e.g., flight mission plans, weather and climate models).&lt;/p&gt;
&lt;p&gt;Capacity growth is further amplified by the need for more rapidly ingesting, analyzing, and visualizing voluminous data to improve understanding of known physical processes, discover new phenomena, and compare results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does HPC need to change in order to meet these Big Data needs?&lt;/li&gt;
&lt;li&gt;What can HPC and Big Data communities learn from each other?&lt;/li&gt;
&lt;li&gt;What impact will this have on conventional workflows, architectures, and tools?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An invited international panel of experts will examine these disruptive technologies and consider their long-term impacts and research directions.&lt;/p&gt;
&lt;h3 id=&#34;moderatorpanelist-details&#34;&gt;Moderator/Panelist Details:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;George O. Strawn (Moderator) – Networking and Information Technology Research and Development National Coordination Office&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt; – Georgia Institute of Technology&lt;/li&gt;
&lt;li&gt;Ian Foster – University of Chicago&lt;/li&gt;
&lt;li&gt;Bruce Hendrickson – Sandia National Laboratories&lt;/li&gt;
&lt;li&gt;Randy Bryant – Executive Office of the President, Office of Science and Technology Policy&lt;/li&gt;
&lt;li&gt;George Biros – The University of Texas at Austin&lt;/li&gt;
&lt;li&gt;Andrew W. Moore – Carnegie Mellon University&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;mentoring-undergraduates-through-competition&#34;&gt;Mentoring Undergraduates Through Competition&lt;/h3&gt;
&lt;p&gt;The next generation of HPC talent will face significant challenges to create software ecosystems and optimally use the next generation of HPC systems. The rapid advances in HPC make it difficult for academic institutions to keep pace.&lt;/p&gt;
&lt;p&gt;The Student Cluster Competition (SCC), now in its ninth year, was created to address this issue by immersing students into all aspects of HPC. This panel will examine the impact of the SCC on the students and schools that have participated.&lt;/p&gt;
&lt;p&gt;Representatives from five institutions from around the world will talk about their experiences with the SCC with regards to their students’ career paths, integration with curriculum and academic HPC computing centers.&lt;/p&gt;
&lt;p&gt;The panel will further discuss whether “extracurricular” activities, such as the SCC, provide sufficient return on investment and what activities could change or replace the competition to meet these goals more effectively.&lt;/p&gt;
&lt;h3 id=&#34;moderatorpanelist-details-1&#34;&gt;Moderator/Panelist Details:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Brent Gorda (Moderator) – Intel Corporation&lt;/li&gt;
&lt;li&gt;Jerry Chou – Tsinghua University&lt;/li&gt;
&lt;li&gt;Rebecca Hartman-Baker – Lawrence Berkeley National Laboratory&lt;/li&gt;
&lt;li&gt;Doug Smith – University of Colorado Boulder&lt;/li&gt;
&lt;li&gt;Xuanhua Shi – Huazhong University of Science and Technology&lt;/li&gt;
&lt;li&gt;Stephen Lien Harrell – Purdue University&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;programming-models-for-parallel-architectures-and-requirements-for-pre-exascale&#34;&gt;Programming Models for Parallel Architectures and Requirements for Pre-Exascale&lt;/h3&gt;
&lt;p&gt;Relying on domain scientists to provide programmer intervention to develop applications to emerging exascale platforms is a real challenge. A scientist prefers to express mathematics of the science, not describe the parallelism of the implementing algorithms.&lt;/p&gt;
&lt;p&gt;Do we expect too much of the scientist to code for high parallel performance given the immense capabilities of the platform. This ignores that the scientist may have a mandate to code for a new architecture, and yet preserve portability in their code.&lt;/p&gt;
&lt;p&gt;This panel will bring together user experience, programming model, architecture experts to discuss the pressing needs in finding the path forward to port scientific codes to such a platform. We hope to discuss the evolving programming stack, application-level requirements, and address the hierarchical nature of large systems in terms of different cores, memory levels, power consumption and the pragmatic advances of near term technology.&lt;/p&gt;
&lt;h3 id=&#34;moderatorpanelist-details-2&#34;&gt;Moderator/Panelist Details:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fernanda Foertter (Moderator) – Oak Ridge National Laboratory&lt;/li&gt;
&lt;li&gt;Barbara Chapman – University of Houston&lt;/li&gt;
&lt;li&gt;Steve Oberlin – NVIDIA Corporation&lt;/li&gt;
&lt;li&gt;Satoshi Matsuoka – Tokyo Institute of Technology&lt;/li&gt;
&lt;li&gt;Jack Wells – Oak Ridge National Laboratory&lt;/li&gt;
&lt;li&gt;Si Hammond – Sandia National Laboratories&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/sc15-panel-line-up-for-november-18/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/sc15-panel-line-up-for-november-18/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>White House National Strategic Computing Initiative Workshop Proceedings</title>
      <link>http://localhost:1313/blog/20151020-nsci/</link>
      <pubDate>Tue, 20 Oct 2015 07:53:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20151020-nsci/</guid>
      <description>&lt;p&gt;The White House National Strategic Computing Initiative Workshop Proceedings is a summary of the workshop held at the Hilton McLean Tysons Corner in McLean, Virginia on October 20-21, 2015. The Networking and Information Technology Research and Development (NITRD) program National Coordination Office (NCO) has edited and released this report in support of the White House National Strategic Computing Initiative (NSCI).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This workshop was made possible by the financial and organizational support of the U.S. Department of Energy on behalf of the NSCI Executive Council.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The second panel was moderated by Dona Crawford (Lawrence Livermore National Laboratory) and featured Chris Johnson (University of Utah), &lt;strong&gt;David Bader&lt;/strong&gt; (Georgia Tech), Peter Koenig (Procter and Gamble), Anna Michalak (Carnegie Institution for Science), and Fred Streitz (Lawrence Livermore National Laboratory) as panelists.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Dona Crawford initiated the discussion by asking how to mitigate risk involved in “bad analytics” or “bad data.” &lt;strong&gt;David Bader&lt;/strong&gt; remarked that traditionally, HPC is concerned more about accuracy, while data analytics is more focused on making decisions, which entails embracing some degree of noise and risk. Michalak commented that in environmental modeling research, all data is imperfect data, as surrogate data is often employed. When informational content is limited, it is tempting to incorporate all that was collected – the central challenge is deciding what should be excluded from the analysis.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nitrd.gov/nsci/files/NSCI2015WorkshopReport06142016.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nitrd.gov/nsci/files/NSCI2015WorkshopReport06142016.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Mourns Loss of Regents’ Professor Karsten Schwan</title>
      <link>http://localhost:1313/blog/20150929-gatech/</link>
      <pubDate>Tue, 29 Sep 2015 10:57:19 -0500</pubDate>
      <guid>http://localhost:1313/blog/20150929-gatech/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150929-gatech/karsten-schwan_0_2_hu_5a533e7492fe6b74.webp 400w,
               /blog/20150929-gatech/karsten-schwan_0_2_hu_c1cbffd941dc74c4.webp 760w,
               /blog/20150929-gatech/karsten-schwan_0_2_hu_a4b787af209f4fb4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150929-gatech/karsten-schwan_0_2_hu_5a533e7492fe6b74.webp&#34;
               width=&#34;200&#34;
               height=&#34;250&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Regents’ Professor Karsten Schwan, a prolific researcher and faculty leader in the School of Computer Science, passed away Sept. 28, following a battle with cancer.&lt;/p&gt;
&lt;p&gt;Schwan, who had taught in Georgia Tech’s College of Computing since 1988, leaves behind more than 70 active Ph.D. students, nine active research projects, 26 software systems, and a legacy of 276 published writings in books, journals, and conference proceedings. He was co-director of the Center for Experimental Research in Computer Systems (CERCS) and of the Interactive High-Performance Computation Laboratory at Georgia Tech, as well as associate editor of two professional journals.&lt;/p&gt;
&lt;p&gt;In recent years, much of Schwan’s work involved collaboration with colleagues Ada Gavrilovska, Matt Wolf, and Greg Eisenhauer, all research scientists in the School of Computer Science.&lt;/p&gt;
&lt;p&gt;“Karsten was an active and leading figure in a computing, not only through his own contributions, but also through the legacy of his many students who have careers throughout academia and industry,” Wolf said, describing Schwan as a font of boundless energy. “And although we talk of students, past and present, I think a more apt term was that he was developing his future collaborators and colleagues.  Hierarchies and boundaries didn’t matter much to him, but the people he knew and worked with certainly did.”&lt;/p&gt;
&lt;p&gt;Schwan earned his undergraduate degree at West Germany’s Christian-Albrechts Universitaet, then came to the United States to complete his master’s and Ph.D. in computer science at Carnegie-Mellon University as a Fulbright Scholar under the German Fulbright Commission. His thesis, “Tailoring Software for Multiple Processor Systems,” was selected as one of six theses in 1982 to be printed as a book in the “Computer Science Series” by UMI Research Press.&lt;/p&gt;
&lt;p&gt;Schwan immediately began teaching and also developed longstanding research relationships with Sandia National Laboratory, Oak Ridge National Labs, the U.S. Department of Energy, and corporations such as Intel. He was a sought-after consultant for companies such as IBM, Siemens, KSR Corp., and others. His more than 120 research projects throughout his career led to the creation of 26 software systems and honors such as the 2013 R&amp;amp;D 100 Award from Oak Ridge National Labs for ADIOS IO for high-performance computing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, chair of the School of Computational Science and Engineering, praised Schwan&amp;rsquo;s endeavors in high performance computing at Georgia Tech.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Karsten recognized the needs of scientists and engineers by improving high-end computing systems for their use in real applications,&amp;rdquo; Bader said. &amp;ldquo;He will be missed by our community.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Associate Professor Santosh Pande, who was recruited to Georgia Tech by Schwan, learned a great deal about systems research from Schwan&amp;rsquo;s unique approach to it.&lt;/p&gt;
&lt;p&gt;“In academia, it is especially true that one gets influenced a lot by one&amp;rsquo;s colleagues in terms of one&amp;rsquo;s research interests, philosophies and such in our evolution as teachers and researchers,” Pande said. “I learnt about many nuances of systems research from Karsten. His angle on what is important in a problem, systems research-wise, was very unique &amp;hellip; I will deeply miss him.”&lt;/p&gt;
&lt;p&gt;Professor and Chair Lance Fortnow called Schwan’s passing “a huge loss” for the School of Computer Science – “not just professionally but because Karsten had such an important influence in the School’s development and has been such a great friend and mentor to many faculty and students,” he said. “It’s very hard to imagine the School without him.”&lt;/p&gt;
&lt;p&gt;Dean Zvi Galil said Schwan was one of the College’s “foremost researchers” in high-performance computing. “He leaves behind a legacy of scholarly and academic achievement that would be difficult to match.”&lt;/p&gt;
&lt;p&gt;It was his personal character that Wolf will remember from 16 years of working together.&lt;/p&gt;
&lt;p&gt;“He was always one to refocus a conversation, not to dwell on where things had gone wrong, but on how to move forward and make them better,” Wolf added. “His vision, his insight, his sparkle, and his wit will all be keenly missed.”&lt;/p&gt;
&lt;p&gt;Details about a memorial service by the College of Computing will be forthcoming.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.gatech.edu/2015/09/28/college-computing-mourns-loss-regents-professor-karsten-schwan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.gatech.edu/2015/09/28/college-computing-mourns-loss-regents-professor-karsten-schwan&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Exactly Is a &#39;Flop,&#39; Anyway?</title>
      <link>http://localhost:1313/blog/20150801-vice/</link>
      <pubDate>Sat, 01 Aug 2015 07:36:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150801-vice/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Michael Byrne&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150801-vice/Pixabay_hu_84bdf90cacd8200f.webp 400w,
               /blog/20150801-vice/Pixabay_hu_3a094cf400ba37b4.webp 760w,
               /blog/20150801-vice/Pixabay_hu_d3c90d9bb6804c8a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150801-vice/Pixabay_hu_84bdf90cacd8200f.webp&#34;
               width=&#34;700&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Earlier this week, President Obama signed &lt;a href=&#34;http://motherboard.vice.com/read/obamas-new-executive-order-says-the-us-will-build-an-exascale-supercomputer?utm_source=mbfb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an executive order creating the National Strategic Computing Initiative&lt;/a&gt;, a vast effort at creating supercomputers at exaflop scales. Cool. An exaflop-scale supercomputer is capable of 1018 floating point operations (FLOPS) per second, which is a whole lot of FLOPS.&lt;/p&gt;
&lt;p&gt;But this raises the question: What&amp;rsquo;s a floating point operation and why does it matter so much to supercomputing when our mere mortals of computers are rated according to the simple-seeming notion of clock speed? I haven&amp;rsquo;t the faintest idea how many FLOPS this Lenovo might be capable of (though, as we&amp;rsquo;ll see, it can certainly be calculated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, chair of the Georgia Institute of Technology School of Computational Science and Engineering, suggests we &amp;ldquo;think of flops similar with horsepower to a car.&amp;rdquo; In buying the family Subaru (or whatever), you&amp;rsquo;re interested in a whole list of different metrics: gas mileage, safety rating, reliability. But, if it&amp;rsquo;s a racecar in question, e.g. a car that will do nothing but race around a track at top speed, the requirements are a lot more about horsepower, which is the thing wins races.&lt;/p&gt;
&lt;p&gt;So, the answer has to do with the difference in how your own computer and an extremely high-performance supercomputer are used. The latter is used for doing incredibly vast mathematical calculations while consumer computers are designed to run stuff like the browser you&amp;rsquo;re currently reading this in. The key thing is that the browser&amp;rsquo;s operation is all about your input and input from the internet itself. A browser spends a lot of time waiting around, in other words. And the same goes for most of the computing you do: it&amp;rsquo;s input/output based.&lt;/p&gt;
&lt;p&gt;The distinction might not be very intuitive. What it means is that the computer programs you&amp;rsquo;re used to running (or the sub-programs within those programs) are always stalling as they wait for other sub-programs (functions, methods, routines) and programs to finish or for some piece of input to arrive. We call this kind of computing sequential. Things happen only after other things have completed and the whole mess is characterized by complex inter-dependencies. What it looks like is logical decision making: &lt;em&gt;if &amp;hellip; then&lt;/em&gt;, &lt;em&gt;not&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt;, &lt;em&gt;or&lt;/em&gt;, etc.&lt;/p&gt;
&lt;p&gt;Mathematical computation is in general a different sort of beast, for a couple of connected reasons. The first is that it&amp;rsquo;s usually easy to do it using parallel, rather than sequential, processing. Big computations can often be broken down into smaller and smaller, easier and easier computations. Which means they can spread around to a bunch of different processors all working simultaneously. This is why GPUs are now becoming much more than graphics processing units—they exploit parallelism. Parallelism used to be useful largely for image rendering (where individual pixels can be operated on individually), but as computing becomes more and more data-focused, the idea has become much more general. So have GPUs.&lt;/p&gt;
&lt;p&gt;So, in a sense, you can imagine a GPU as a consumer-scale supercomputer. There&amp;rsquo;s another characteristic of mathematical/scientific computing that sets it apart, however. This is just the nature of the numbers themselves, or their precision. Computing numbers more precise than whole numbers is a very different process.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150801-vice/CPU_hu_76341af1fb335245.webp 400w,
               /blog/20150801-vice/CPU_hu_3734c6adf1e8f361.webp 760w,
               /blog/20150801-vice/CPU_hu_2c309095b73bfe5a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150801-vice/CPU_hu_76341af1fb335245.webp&#34;
               width=&#34;760&#34;
               height=&#34;550&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;This is so much so that it occurs on its very own dedicated hardware. A CPU is built to operate on integers—whole numbers. Which is fine because it&amp;rsquo;s not so much doing math as using numbers to model/implement the execution of a program that probably isn&amp;rsquo;t a scientific calculator. But, that said, most any general purpose computer is going to need to calculate non-integer numbers—more properly known as floating point numbers—on a regular basis.&lt;/p&gt;
&lt;p&gt;This can&amp;rsquo;t be accomplished on a CPU because floating point numbers are represented by computers in a very, very different way. An integer value described by a string of binary digits is just a simple base conversion. The numbers we use every day, which are somewhat confusingly referred to as decimal numbers even if they don&amp;rsquo;t employ a decimal point, are represented in a base-10 counting system. This &amp;ldquo;10&amp;rdquo; is actually where the &amp;ldquo;decimal&amp;rdquo; of a decimal number or decimal point comes from. Every digit in a decimal number corresponds to a value of that digit multiplied by 10 raised to the position of that digit. So, the number 12 is 2 * 100 + 1 * 101 and so forth. That&amp;rsquo;s the base-10 counting system. With binary, every digit is just 2 raised to the digit&amp;rsquo;s position times either a 1 or a 0. So, given a decimal number, I can turn it into a binary number and vise versa just through some simple arithmetic.&lt;/p&gt;
&lt;p&gt;If I tried the same thing with a floating point number, it wouldn&amp;rsquo;t work out at all. In a floating point number, digits mean very particular things. Some digits correspond to the actual digits of the number, some correspond to the base we&amp;rsquo;re using to represent that number, and some correspond to where exactly in the number we should place a decimal point. In a sense, every floating point number must be decoded according to a small set of rules. Treat it in any other way and the number is just nonsense.&lt;/p&gt;
&lt;p&gt;This is why floating point numbers are computed on different hardware, which is known as a floating-point unit. An FPU is very much so built for long mathematical operations. It&amp;rsquo;s possible to take the longest formula you can imagine, populated by the longest data set, and rearrange it a bit such that it can be fed through a floating-point unit in a really natural way. This is why when we talk about supercomputing, a land of deep mathematical problems and deep simulations based on mathematical problems, it&amp;rsquo;s useful to talk about floating-point speeds.&lt;/p&gt;
&lt;p&gt;Floating-point numbers and parallel computations come together in how FLOPS are calculated. The basic idea is to take the number of floating-point calculations that can be performed in a single processor clock cycle (four, usually) multiply it by the processor&amp;rsquo;s clock speed and then scale it by the number of available cores per socket connecting them. In the simplest case of a single core architecture operating at 2.5 GHz, we&amp;rsquo;ll wind up with 10 billion (2.5 GHz * 4) FLOPS.&lt;/p&gt;
&lt;p&gt;So, based on the calculation above, it&amp;rsquo;s possible to increase FLOPS without increasing processor clock speeds, which is good news for supercomputing if not for your MacBook. &amp;ldquo;You&amp;rsquo;ll notice that clock speed hasn&amp;rsquo;t been increasing significantly in the last decade on consumer machines,&amp;rdquo; Bader says, &amp;ldquo;because we&amp;rsquo;re hitting the limits of chip technology. Instead, we see chip makers like Intel, IBM, and NVIDIA, moving towards parallelism such as multicore and manycore processors.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Parallel programming hasn&amp;rsquo;t quite become mainstream, and it will always be constrained by the percentage of a given computer program that must execute sequentially, but FLOPS increasingly matter, if not always at exascales.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vice.com/en_us/article/vvbajx/what-exactly-is-a-flop-anyway&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.vice.com/en_us/article/vvbajx/what-exactly-is-a-flop-anyway&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Better Decisions through Big Data</title>
      <link>http://localhost:1313/blog/20150720-betterdecisions/</link>
      <pubDate>Mon, 20 Jul 2015 07:24:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150720-betterdecisions/</guid>
      <description>&lt;p&gt;by &lt;strong&gt;David Bader&lt;/strong&gt;, IEEE Computer Society member, and Chair, School of Computational Science &amp;amp; Engineering at Georgia Institute of Technology&lt;/p&gt;
&lt;p&gt;Companies and governments increasingly rely on ‘big data’ to operate efficiently and competitively. Analytics and security must
keep pace. What research underpins the latest big data-enabled
advances?&lt;/p&gt;
&lt;p&gt;Good decisions are well-informed
decisions, of late powered by a diversity of data. Big data is creating profound business and
social opportunities in nearly every field because of its ability to speed discovery and customize solutions. Yet without the means to
synthesize and protect data, we risk not being able to do what we intended: make the right decisions.&lt;/p&gt;
&lt;p&gt;This junction of big data and security shapes an increasingly important area of research that uses high-performance
cyber
analytics – led by research universities and industry in partnership.&lt;/p&gt;
&lt;p&gt;Everyday, enterprise systems create a deluge of data: power grid use across a metropolitan area, millions of credit card
transactions per hour, social network relationships, or the spread of contagious disease. One might think that so much data
requires more time to analyze and draw conclusions, but in practice, big data now allows us to make near realtime
responses. How
is this possible?&lt;/p&gt;
&lt;p&gt;Research universities act here, using graph analytics and by creating new visualization methods to give government and industry
actionable knowledge from growing mounds of data. Streaming graphs, for example, detect structural changes and flows, spot
clustering or key actors and highlight subtle anomalies. Graph analytics require large-scale,
high-performance
computers that can
trace trillions of interconnected vertices and edges that change over time. Projects such as Georgia Tech’s STINGER offers an
opensource
way to understand data with large, streaming graphs. Much university work is by nature open source and open to all –
providing a standard which others can improve upon without having to reinvent.&lt;/p&gt;
&lt;p&gt;Further, streaming graphs can be combined with techniques from machine learning to be more effective. A machine might be fed
information and trained to know how a well-behaved
employee normally operates, using information extracted from the masses of
data. Then, when an action deviates from the norm, an alert results. This is especially useful in business environments where
employees connect to proprietary data via numerous mobile devices. Even the best employees now must be monitored because
they may be unwittingly used in cybercrime.&lt;/p&gt;
&lt;p&gt;Contrary to common fears, the more data we share, the more secure we may actually become. The big data behind employee
behavior analysis, for example, enables new cyber security approaches that discover subtle, previously hidden suspicious
changes and guide quick responses before expensive damage is done. In the end, not only can that keep organizations safe; it too
can inform the right decisions.&lt;/p&gt;
&lt;p&gt;“Under the hood” fixes that don’t involve human actions will be another area of advancement in the realm of big data and cyber
security. Devices need smart tools for self-correction
and even the ability to clean up after a successful attack. Also, static security
defenses, such as firewalls and malware, have taxed computing power. Universities are studying how to increase computational
power, which will help future cyber security solutions. For example, leading universities and industry are working on a national need
to increase computational efficiency by 75-fold
over the best current systems.&lt;/p&gt;
&lt;p&gt;National laboratories and universities are environments of open innovation, where partnerships with industry permit computer
scientists to work with real problems and real data. This is what leads to true societal solutions that industry can deploy. Much of
today and tomorrow’s cyber security work will require heavy hitting computation, which industry usually cannot do alone. Therefore,
it begins with and revolves around sharing.&lt;/p&gt;
&lt;p&gt;Computing is, and always has been, about making better decisions faster. With every advance, technology solves some issues and
introduces others. There will be challenges. But big data-enabled
security keeps a nation devoted to knowledge-based
innovation
on the offensive. High-performance
cyber analytics – in partnership between industry and academia – is the next underpinning for
national progress and everyone’s security.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;David Bader is an IEEE Computer Society member, and professor and chair of the School of Computational Science &amp;amp; Engineering
and executive director of high performance computing at the Georgia Institute of Technology.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>World’s Best Supercomputers Ranked by Georgia Tech Expert</title>
      <link>http://localhost:1313/blog/20150716-gatech-graph500/</link>
      <pubDate>Thu, 16 Jul 2015 08:20:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150716-gatech-graph500/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150716-gatech-graph500/image_hu_bea843f16742aafe.webp 400w,
               /blog/20150716-gatech-graph500/image_hu_c19d7d5fd1c94a44.webp 760w,
               /blog/20150716-gatech-graph500/image_hu_4b9c2c58ce6be32e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150716-gatech-graph500/image_hu_bea843f16742aafe.webp&#34;
               width=&#34;140&#34;
               height=&#34;198&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Two highly anticipated lists of the world’s fastest supercomputers were released this week, and Georgia Tech’s &lt;strong&gt;David Bader&lt;/strong&gt; was behind one of them.&lt;/p&gt;
&lt;p&gt;The lists – Top 500 and Graph 500 – are published twice each year to coincide with supercomputer conferences, and are closely watched as indicators of development and investment into high-performance computing worldwide. The lists can indicate trends into which technologies are popular in the machines.&lt;/p&gt;
&lt;p&gt;Bader, chair of the School of Computational Science and Engineering, co-leads Graph 500 – a high-performance computing benchmark focused on data-intensive applications. He and others from Europe and North America evaluate supercomputers for their ability to solve data-intensive applications.&lt;/p&gt;
&lt;p&gt;In the latest Graph 500 list, released Monday, July 13, the top spot went to the K computer at Japan&amp;rsquo;s RIKEN Advanced Institute for Computational Science, while two IBM supercomputers in the United States came in second and third &amp;ndash; Sequoia at Lawrence Livermore in California and Mira at Argonne National Laboratory in Illinois.&lt;/p&gt;
&lt;p&gt;“The top of the list remained strong, but Japan’s K computer advanced from No. 2 to No.1 by significantly increasing performance,” Bader said. “No one knows yet how.”&lt;/p&gt;
&lt;p&gt;&amp;ldquo;All of these are impressive systems,&amp;rdquo; he added, &amp;ldquo;but our list was dominated by IBM&amp;rsquo;s Blue Gene/Q in terms of its new analytics processing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Whereas Top 500 evaluates machines based on how they solve a liner system of applications, Graph 500 seek to measure more of each supercomputer. Supercomputers were assessed against three kernels: Search, Optimization (single-source shortest path), and Edge-Oriented. Results are important for use in five business areas: cybersecurity, medical informatics, social networks, data enrichment, and symbolic networks of the human brain, for example.&lt;/p&gt;
&lt;p&gt;“We can ask you, how fast you can hit a nail on the head, or we can give you plans to build an entire house and ask how fast can you do that,” Bader says. “That’s the difference between Top 500 and Graph 500.”&lt;/p&gt;
&lt;p&gt;In the separate but similar Top 500 list, China received top honors for the world’s fastest supercomputer developed by China&amp;rsquo;s National Defense University. Its Tianhe-2 computer, housed in Guangzhou, China, boasts maximum performance of 33,863 teraflops per second. Still, the latest ranking was dominated by supercomputers in the United States, with 231 machines compared to 37 making the list from China. The U.S. remains the top country in terms of the number of supercomputers.&lt;/p&gt;
&lt;p&gt;The Top 500 list is compiled by supercomputing experts at the University of Mannheim, Germany; the University of Tennessee, Knoxville; and the Department of Energy&amp;rsquo;s Lawrence Berkeley National Laboratory.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://iisp.gatech.edu/news/worlds-best-supercomputers-ranked-georgia-tech-expert&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://iisp.gatech.edu/news/worlds-best-supercomputers-ranked-georgia-tech-expert&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM, Nvidia rev their HPC engines in next-gen supercomputer push</title>
      <link>http://localhost:1313/blog/20150713-ibm-nvidia/</link>
      <pubDate>Mon, 13 Jul 2015 07:04:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150713-ibm-nvidia/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Katherine Noyes, Senior U.S. Correspondent, IDG News Service&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hard on the heels of the publication of the latest &lt;a href=&#34;http://www.computerworld.com/article/2947452/data-center/china-retains-supercomputing-crown-in-latest-top-500-ranking.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Top 500 ranking&lt;/a&gt; of the world’s fastest supercomputers, IBM and Nvidia on Monday announced they have teamed up to launch two new supercomputer centers of excellence to develop the next generation of contenders.&lt;/p&gt;
&lt;p&gt;Created as part of IBM’s supercomputing contract with the U.S. Department of Energy, the new centers will be located at Lawrence Livermore National Laboratory and Oak Ridge National Laboratory and will focus on development of the forthcoming Summit and Sierra supercomputer systems, which are expected to be delivered in 2017. The Summit supercomputer will be housed at Oak Ridge, while the Sierra will be situated at Lawrence Livermore; both are due to become operational in 2018.&lt;/p&gt;
&lt;p&gt;The announcement was made at the International Supercomputing Conference going on this week in Frankfurt, Germany.&lt;/p&gt;
&lt;p&gt;“Clearly the United States is stepping things up with these new supercomputers,” said Steve Conway, a research vice president for high-performance computing with IDC. “The race is on to the exascale milestone.”&lt;/p&gt;
&lt;p&gt;Incorporating IBM’s advanced POWER processors along with Nvidia Tesla GPU accelerators and the Nvidia NVLink high-speed processor interconnect, Summit and Sierra will use a highly efficient, high-performance data-centric computing approach that minimizes data in motion, IBM said. That, in turn, will help optimize problem solving while also reducing overall energy consumption.&lt;/p&gt;
&lt;p&gt;The systems will also follow an OpenPOWER design concept, enabling collaborative innovation and sharing with other members of the &lt;a href=&#34;http://openpowerfoundation.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenPOWER Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Uses for the new machines will focus on scientific research in areas such as energy, climate research, cosmology, biophysics, astrophysics and medicine, as well as nuclear security and other national security interests.&lt;/p&gt;
&lt;p&gt;The first prototypes of the advanced supercomputers are expected to be available to system developers and application writers late this year.&lt;/p&gt;
&lt;p&gt;A third next-generation supercomputer is also forthcoming from Cray and Intel, Conway noted. It will be housed at Argonne National Laboratory and is expected to be even bigger than Summit or Sierra, he said.&lt;/p&gt;
&lt;p&gt;“Over the past several years, top national leaders have been recognizing that supercomputers are crucial not only for science but also for the economy and for industry,” Conway explained. “We’re seeing this happening all over the world.”&lt;/p&gt;
&lt;p&gt;Discussions of big data are also increasingly including high-performance computing as part of the package, further elevating the importance of supercomputing, he added.&lt;/p&gt;
&lt;p&gt;Whereas the high-performance computing market amounted to about US$2 billion worldwide in the early 1990s, it was valued last year at $21 billion, Conway noted.&lt;/p&gt;
&lt;p&gt;“It has now gotten to be of a size that’s really interesting to large companies,” he said, “especially as growth in the enterprise server market has flattened.”&lt;/p&gt;
&lt;p&gt;Today’s supercomputers can cost up to $550 million each, Conway said. Still, more than 100,000 of them are now sold each year, and because of their increasing use of commodity technologies, the vast majority of them cost less than $100,000.&lt;/p&gt;
&lt;p&gt;“They used to be like muscle cars with lots of custom technology, but today they’re more like family sedans that make use of a lot of commodity technologies from companies like Nvidia and Intel,” Conway said. “The good news is that because of these commodity technologies, sooner or later the work these guys are doing on the really big supercomputers is going to wash right down through the market.”&lt;/p&gt;
&lt;p&gt;Though China may be sitting in the No. 1 spot on the Top 500 list, “the United States still remains in a leadership position,” said &lt;strong&gt;David Bader&lt;/strong&gt;, a professor in the School of Computational Science and Engineering at Georgia Tech University.&lt;/p&gt;
&lt;p&gt;To wit: China’s leading Tianhe-2 contains more than 3 million cores, and all of them are made by Intel, he pointed out.&lt;/p&gt;
&lt;p&gt;The technologies in many or most of the world’s fastest machines, in fact, are made by U.S. companies including IBM, Cray, AMD and Nvidia, he said.&lt;/p&gt;
&lt;p&gt;Bader co-leads the &lt;a href=&#34;http://www.graph500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph 500&lt;/a&gt;, another high-performance computing benchmark that’s meant to complement the Top 500 by focusing on data-intensive applications. In the latest Graph 500 list, which was also released Monday, the No. 1 spot went to the K computer at Japan’s RIKEN Advanced Institute for Computational Science, while second place went to IBM’s Sequoia at Lawrence Livermore. Mira, another IBM Blue Gene/Q system at Argonne National Laboratory, came in third.&lt;/p&gt;
&lt;p&gt;The Graph 500’s top 10, in fact, currently includes several IBM systems.&lt;/p&gt;
&lt;p&gt;“All of these are impressive systems,” Bader said, “but our list was dominated by IBM’s Blue Gene/Q in terms of this new analytics processing.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2947832/ibm-and-nvidia-rev-their-hpc-engines-for-next-gen-supercomputer-push.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2947832/ibm-and-nvidia-rev-their-hpc-engines-for-next-gen-supercomputer-push.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pcworld.com/article/2947792/ibm-nvidia-rev-hpc-engines-in-nextgen-supercomputer-push.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.pcworld.com/article/2947792/ibm-nvidia-rev-hpc-engines-in-nextgen-supercomputer-push.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://computing.llnl.gov/news/ibm-nvidia-rev-hpc-engines-next-gen-supercomputer-push&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://computing.llnl.gov/news/ibm-nvidia-rev-hpc-engines-next-gen-supercomputer-push&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Katherine Noyes has been an ardent geek ever since she first conquered Pyramid of Doom on an ancient TRS-80. Today she covers enterprise software in all its forms, with an emphasis on cloud computing, big data, analytics and artificial intelligence.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accenture Awards 11 Research Grants to Leading Universities to Promote Greater R&amp;D Collaboration, Accelerate Pace of Innovation</title>
      <link>http://localhost:1313/blog/20150706-accenture/</link>
      <pubDate>Mon, 06 Jul 2015 07:18:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150706-accenture/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.accenture.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accenture&lt;/a&gt; (NYSE:ACN) has awarded 11 research grants to top universities around the world to significantly broaden and deepen the relationships between Accenture’s technology research and development (R&amp;amp;D) groups and leading university researchers.&lt;/p&gt;
&lt;p&gt;The grant program helps to support the ground-breaking efforts of leading university research teams, which will be invited to work in collaboration with researchers from the Accenture Technology Labs on R&amp;amp;D projects that are of strategic importance to the technology industry and Accenture’s enterprise clients.&lt;/p&gt;
&lt;p&gt;“Universities are a critical source of technical and scientific research that can deliver long-term business benefits for organizations around the world,” said Prith Banerjee, managing director of Global Technology R&amp;amp;D, Accenture. “We are thrilled that this grant program, in conjunction with the Accenture Open Innovation initiative, will enable Accenture Technology Labs researchers to translate a university’s conceptual research – which spans topics from cyber security to 3D printing and cognitive computing – into ideas that can ultimately help solve real business challenges for our enterprise clients.”&lt;/p&gt;
&lt;p&gt;The grant program is part of the Accenture Open Innovation initiative, in which Accenture functions as a bridgemaker between Accenture’s Global 2000 clients and the technology innovation ecosystem, which includes universities, top tier accelerators, start-ups, venture capitalists and corporate R&amp;amp;D labs from around the world. New technology research from Accenture Strategy found that four out of five enterprise respondents reported innovation was within their top three priorities and about one quarter placed innovation as the number one priority for their organization.&lt;/p&gt;
&lt;p&gt;The 2015 Accenture Open Innovation university grant program recipients and the R&amp;amp;D areas of focus include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prof. David Bader&lt;/strong&gt;, Georgia Institute of Technology &lt;br&gt;
This project is focused on accelerating anomaly detection of advanced persistent threats, zero-day malware, and other malicious cyber threats using graph analytics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Zhejing Bao, Zhejiang University, China &lt;br&gt;
This project will focus on the development of a prediction technology that provides short-term customer energy demand forecasting by utilizing real-time and historical sensor data of electricity consumption and other information, such as weather forecasting data and customer information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Michael Bernstein and Prof. Melissa Valentine, Stanford University &lt;br&gt;
Exploring tools and techniques needed to create a “Hybrid Virtual Workforce,” this project will look at how work traditionally carried out by static teams of enterprise employees can instead be carried out by computationally-guided, dynamically-assembled flash teams.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Pushpak Bhattacharyya, Indian Institute of Technology Bombay, India &lt;br&gt;
Focused on developing a text to universal language converter, this project aims to create more robust and accurate natural language processing capabilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Sutanu Chakraborti, Indian Institute of Technology Madras, Chennai, India &lt;br&gt;
This project will focus on the development of decision-making algorithms to conceptualize and develop cognitive learning systems. It will leverage a principled combination of bottom-up, data-driven techniques, such as machine learning, and top-down knowledge-based approaches classically used in planning and reasoning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Abhijit Deshmukh, Purdue University &lt;br&gt;
Focused on developing new approaches to understanding complex systems, this project will look at building more efficient systems by using next generation manufacturing and supply chains in a circular economy as a use case.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. F. Javier Heredia and Prof. Joaquim Minguella, Universitat Politecnica de Catalunya, BarcelonaTech &lt;br&gt;
Studying the advantages of ultra-postponement with 3D printing by using analytical tools and mathematical optimization models and algorithms, this project will explore how to transform supply chain management by allowing delayed and tailored production in the location where demand occurs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Michal Kosinski, Stanford University  &lt;br&gt;
This project aims to increase the understanding of customer behavior and design more psychologically matched customer interactions. It will explore approaches to building predictive models of a customer’s psychological profiles using machine learning in conjunction with shopping records and external social network data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dean Richard Lyons, University of California, Berkeley, Haas School of Business  &lt;br&gt;
A new multi-disciplinary experiential learning course for MBA students will focus on data strategy and data science curriculum. The goal is to create the next generation of business decision makers, develop and broaden data strategy skills and refine and improve this developing field of activity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Santonu Sarkar, Birla Institute of Technology and Sciences, Pilani, India  &lt;br&gt;
The goal of this project is to ensure dependability of software systems, making them reliable and available based on a recovery-oriented approach. The proposed approach is based on a near-real time assessment and remediation of dependability bottlenecks of operational systems and design strategies to build dependable software.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prof. Partha Pratim Talukdar, Indian Institute of Science, Bangalore, India  &lt;br&gt;
This project aims to harvest knowledge from large text datasets, using them to improve accuracy and coverage of cognitive computing systems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the monetary grant, the university researchers will benefit from the perspectives that Accenture Technology Labs researchers have to offer given their direct exposure to the enterprise business contexts in which the research would ultimately be applied. The program provides the opportunity for Accenture and university researchers to combine their expertise, learn from each other and achieve more than either could alone.&lt;/p&gt;
&lt;p&gt;“With cyber security threats rapidly evolving and becoming increasingly complex, we are pleased to have received this grant from Accenture to help us advance the research we’re doing in the critical area,” said Professor David Bader, Georgia Institute of Technology. “Collaboration with Accenture will provide access to data and expertise that will help us to build proper algorithms, allowing our research to push further and faster than previously possible.”&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I am delighted that we were amongst a select group of universities around the world to be awarded with a research grant from Accenture,” said Professor Pushpak Bhattacharyya, India Institute of Technology, Bombay. “The combination of IIT Bombay&amp;rsquo;s world renowned Natural Language Processing group with Accenture’s global consulting and technology services expertise will result in a mutually enriching experience, enabling us to work together on cutting edge cognitive computing problems.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;“We’re pleased to be a recipient of an Accenture grant to help further our research in the area of cognitive computing,” said Professor Partha Pratim Talukdar, Indian Institute of Science, Bangalore, India. “The opportunity to collaborate with Accenture researchers will provide us with valuable inputs into how our research can help address the real-world challenges of enterprise organizations.”&lt;/p&gt;
&lt;p&gt;For more information on Accenture Technology Labs, click &lt;a href=&#34;http://cts.businesswire.com/ct/CT?id=smartlink&amp;amp;url=http%3A%2F%2Fwww.accenture.com%2Fus-en%2Ftechnology%2Ftechnology-labs%2FPages%2Findex.aspx&amp;amp;esheet=51135719&amp;amp;newsitemid=20150706005082&amp;amp;lan=en-US&amp;amp;anchor=here&amp;amp;index=2&amp;amp;md5=3e2239b9cd8c81c47f78a66ebe34cfef&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more about the Accenture Open Innovation program, visit &lt;a href=&#34;https://www.accenture.com/openinnovation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.accenture.com/openinnovation&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;about-accenture&#34;&gt;About Accenture&lt;/h3&gt;
&lt;p&gt;Accenture is a global management consulting, technology services and outsourcing company, with more than 336,000 people serving clients in more than 120 countries. Combining unparalleled experience, comprehensive capabilities across all industries and business functions, and extensive research on the world’s most successful companies, Accenture collaborates with clients to help them become high-performance businesses and governments. The company generated net revenues of US$30.0 billion for the fiscal year ended Aug. 31, 2014. Its home page is &lt;a href=&#34;https://www.accenture.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.accenture.com&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;contacts&#34;&gt;Contacts&lt;/h3&gt;
&lt;p&gt;Accenture &lt;br&gt;
Hannah Unkefer, +1 415 537 4848 &lt;br&gt;
&lt;a href=&#34;mailto:hannah.m.unkefer@accenture.com&#34;&gt;hannah.m.unkefer@accenture.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.businesswire.com/news/home/20150706005082/en/Accenture-Awards-11-Research-Grants-Leading-Universities&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.businesswire.com/news/home/20150706005082/en/Accenture-Awards-11-Research-Grants-Leading-Universities&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader joins Accelogic&#39;s Advisory Board</title>
      <link>http://localhost:1313/blog/20150612-accelogic/</link>
      <pubDate>Fri, 12 Jun 2015 17:34:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150612-accelogic/</guid>
      <description>&lt;p&gt;To reach our goals, we have assembled a Top R&amp;amp;D Team with several of the world’s most renowned scientists in the fields of software engineering, high-performance computing, algorithm design, and hardware systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt; &lt;br&gt;
Professor and Chair &lt;br&gt;
School of Computational Science and Engineering &lt;br&gt;
Georgia Tech&lt;/p&gt;
&lt;p&gt;Dr. Bader is a world authority on graph-theoretic frameworks and author of STINGER, one of the most renowned graph platforms. He is Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director of High Performance Computing. Dr. Bader also serves as a board member of the Computing Research Association (CRA), on the NSF Advisory Committee on Cyberinfrastructure, on the Council on Competitiveness HPC Advisory Committee, on the IEEE Computer Society Board of Governors, and on the Steering Committees of the IPDPS and HiPC conferences.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20160221042649/Accelogic.com/our-advisory-board&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://Accelogic.com/our-advisory-board&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to find the needle in a big data haystack</title>
      <link>http://localhost:1313/blog/20150415-gatech/</link>
      <pubDate>Wed, 15 Apr 2015 07:32:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150415-gatech/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, professor and chair of the School of Computational Science and Engineering
at Georgia Tech, delivered the keynote address during the recent &amp;ldquo;International
Opportunities in Cloud Computing &amp;amp; Big Data&amp;rdquo; conference. Bader, executive director of High
Performance Computing, explains why graph analysis is crucial for big data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Social networks use big data to spot influencers and target advertising to
the right user. Biologists need data to understand common drug
interactions and design better medication. Utility providers use it to
monitor disruptions and improve resiliency. Graphs are the unifying motif
for data analysis, and as real-world
circumstances constantly change,
dynamic graphs are essential.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Businesses with global challenges need graph analysis of big data.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;At Georgia Tech, many globally significant challenges – across a variety
of industries – are being modeled by spatio-temporal
interaction networks
or graphs, which we call “STING” at the School of Computational Science
and Engineering. STING graphs are accessible through “STINGER”
(&lt;a href=&#34;http://www.stingergraph.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.stingergraph.com&lt;/a&gt;) – a freely available, open source software
project. STINGER – developed by Georgia Tech with colleagues in
academia, government and industry – reveals dynamic temporal and
semantic relationships between datasets in a way that makes unseen
activity, or hidden threats, observable.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;These streaming graphs solve problems that typically are hard to address
because of the massive amounts of data involved and the need for
supercomputers. The graphs are optimized to update large amounts of
constantly fluctuating data at rates of more than 3 million edges per
second on graphs of 1 billion edges.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;It can help businesses observe the number of users on a social networking
platform at a given time or the number of kilowatts in demand by a
neighborhood. For example, for social good, the U.S. Centers for Disease
Control and Prevention was able to observe public tweets on Twitter to
track flu outbreaks during the 2009-10
H1N1 health emergency.
Consulting firms, aeronautical engineers and energy researchers all have
found STINGER useful for their business.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The volume of data available to us is growing at an overwhelming rate.
Graph analytics can discover the value within massive datasets. It finds
the proverbial needle in a haystack.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPC and Big Data: A View from the Corner of Science and Industry</title>
      <link>http://localhost:1313/blog/20150302-cioreview/</link>
      <pubDate>Mon, 02 Mar 2015 08:03:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150302-cioreview/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, &lt;em&gt;Chair of the School - Computational Science and Engineering &amp;amp; Executive Director - HPC, Georgia Institute of Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With the last decade’s rapid advances in computational power and the resulting explosion of available data, two major areas of computational application—business and scientific research—are now converging. And this digitally fluid world of data science and application is providing remarkable value for industry.&lt;/p&gt;
&lt;p&gt;High performance computing, once an area reserved for technical or scientific application, has for some time now affected the realm of business and enterprise computing. What does this mean for business? It means improved knowledge of customers, more efficient operations, and accelerated response to changes or to problems, among other advantages.&lt;/p&gt;
&lt;p&gt;Many businesses have seen the light and are investing in personnel with expertise in analytics and data-specific algorithms like Hadoop. Smaller companies also now have the option of buying &lt;a href=&#34;http://www.ciowhitepapersreview.com/bigdata/building-successful-big-data-solutions-5.html?utm_source=clicktrack&amp;amp;utm_medium=hyperlink&amp;amp;utm_campaign=linkinnews&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;big data solutions&lt;/a&gt; for business intelligence and analytics from third-party companies.&lt;/p&gt;
&lt;p&gt;With a dual career in both industry and scientific research, I enjoy a broad and fascinating view of this ever-changing HPC terrain. Over in the research sciences, we are transforming methods for studying the very complex. This shift affects how scientists approach, for example, studies of genome-wide associations, the interplay of genes, and climate modeling.&lt;/p&gt;
&lt;p&gt;Science has traditionally operated by deciphering time-consuming cause-and-effect relationships. Meaningful associations are now emerging from heaps of various data types that can be merged and analyzed. Trends in large data sets can be searched, scrutinized and visualized—all techniques that dramatically speed up research time and allow us to tackle a huge range of problems at scales from tiny to enormous in terms of physical size, duration and complexity.&lt;/p&gt;
&lt;p&gt;HPC capabilities also permit powerful simulations that speed product development. From detergents to jet engines, products can now be developed without intermediate prototypes or several iterations of laboratory tests. All of these advances for science are also proportional advances for industry.&lt;/p&gt;
&lt;p&gt;Our world today is one of growing torrential streams of information that can provide valuable information to inform decisions related to business intelligence, market analysis and social trends using data from social media and user behaviors. At Georgia Tech, we are forging new ways to combine big data with HPC research to craft solutions to business and social problems.&lt;/p&gt;
&lt;p&gt;Take cybersecurity. Our work in cyberanalytics is an excellent example of applying these techniques at the intersection of HPC, big data, research, and real-world problems. Indeed, the word “cybersecurity” often evokes an image of outsiders trying to hack into our computer systems, particularly those of large corporations. But many incidents that go unreported in the media are insider events, which some reports say comprise up to a third of threats and can be more costly or damaging than those orchestrated by outsiders. We often wonder how these trusted individuals went about their malicious work unnoticed, but the key to detecting and preventing the breach was likely there, buried in data that could not—until recently—be interpreted and quickly acted upon.&lt;/p&gt;
&lt;p&gt;Every time we use a key card to open a door, send an email or invoke any number of computer actions, we leave a digital trace. Security officers analyze this information to determine our patterns and identify potential threats. But the massive scale datasets are often unstructured and challenging to inspect.&lt;/p&gt;
&lt;p&gt;At Georgia Tech, we conduct analytic research on graphs. Graphs are networks with up to trillions of connections, and they help us discover patterns and relationships hidden deep within massive amounts of data. These graphs are comprised of interconnected vertices and edges that change over time. In the realm of cybersecurity, the vertices may represent computers, and the edges represent their interactions. By designing fast theoretic algorithms on large-scale graphs, we can produce insights in near real time.&lt;/p&gt;
&lt;p&gt;Emerging graph technology at Georgia Tech has the potential to quickly sift massive amounts of data and correctly prioritize the most likely short list of results for a fast response. The aim is to develop the best and most efficient way to prevent future malicious activities where we work and live.&lt;/p&gt;
&lt;p&gt;The media notably cover large security breaches, but for each such report, going unnoticed by the public are numerous other data breaches in small companies, which are often less resilient and have a harder time recovering. Many of these companies fold permanently after an attack.&lt;/p&gt;
&lt;p&gt;The new way of thinking calls for businesses both large and small to deeply embed into their cultures the cybersecurity priorities, investments in expertise, and an increasing awareness of emerging technologies like those in use at Georgia Tech.&lt;/p&gt;
&lt;p&gt;Also, our work in graph analytics can lead to a suite of far-ranging and useful applications, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unraveling the driving forces in graphs that change as they grow, such as social networks. We are expanding what we know about scientific computing graphs and moving into large social and information networks, whose informatics are much more challenging.&lt;/li&gt;
&lt;li&gt;Analyzing massive streaming complex networks in real-time, with applications in public health, transportation, evacuation, security, drug design, water supplies and full-scale socioeconomic systems.&lt;/li&gt;
&lt;li&gt;Visualizing massive graphs. Growing amounts of data require new methods for meaningful interpretation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to advancing data analysis, we are also actively researching computer architectural requirements to maximize the performance of graph analyses across a variety of problem types. How can we integrate across different algorithms, programming models and architectures to address new challenges? The research includes exploring how best to combine cloud computing with in-memory parallel computing. This work lays a foundation to take on some of the most difficult problems in the world today, from computational biology and genomics, massive-scale data analytics with a focus on parallel algorithms, to combinatorial optimization, and vast social networks.&lt;/p&gt;
&lt;p&gt;Georgia Tech is playing its role to build a new computational future, one that includes creative new architectures and software approaches, improved energy efficiency, altered computing paradigms to ingest more information, and reshaped scientific and business workflows. In exchange, we will handle large scale, complex problems more quickly and accurately. We will have more targeted approaches to managing everything from malicious activities to public health and environmental concerns.&lt;/p&gt;
&lt;p&gt;The view from this corner is indeed compelling. Our basic assumptions change almost daily. What will tomorrow bring?&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150302-cioreview/Bader-CIOReview_hu_fdd088ad53bdcee.webp 400w,
               /blog/20150302-cioreview/Bader-CIOReview_hu_72d11a6a7f5241ee.webp 760w,
               /blog/20150302-cioreview/Bader-CIOReview_hu_7799a941a7e0a9f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150302-cioreview/Bader-CIOReview_hu_fdd088ad53bdcee.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A Bader&lt;/strong&gt; is a Full Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director of High Performance Computing. He received his Ph.D. in 1996 from The University of Maryland, and his research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE. Dr. Bader serves as a board member of the Computing Research Association (CRA), on the NSF Advisory Committee on Cyberinfrastructure, on the Council on Competitiveness High Performance Computing Advisory Committee, on the IEEE Computer Society Board of Governors, and on the Steering Committees of the IPDPS and HiPC conferences. He is the editor-in-chief of IEEE Transactions on Parallel and Distributed Systems (TPDS) and Program Chair for IPDPS 2014.&lt;/p&gt;
&lt;p&gt;He is also a leading expert on multicore, manycore, and multithreaded computing for data-intensive applications such as those in massive-scale graph analytics.&lt;/p&gt;
&lt;p&gt;Prof. Bader is a Fellow of the IEEE and AAAS, a National Science Foundation CAREER Award recipient.He has also served as Director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor. Bader is a co-founder of the Graph500 List for benchmarking &amp;ldquo;Big Data&amp;rdquo; computing platforms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://high-performance-computing.cioreview.com/cxoinsight/hpc-and-big-data-a-view-from-the-corner-of-science-and-industry-nid-5400-cid-84.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://high-performance-computing.cioreview.com/cxoinsight/hpc-and-big-data-a-view-from-the-corner-of-science-and-industry-nid-5400-cid-84.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader gives Triangle Computer Science Distinguished Lecture</title>
      <link>http://localhost:1313/blog/20150202-bader-triangle-distinguished-lecture/</link>
      <pubDate>Mon, 02 Feb 2015 06:58:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150202-bader-triangle-distinguished-lecture/</guid>
      <description>&lt;h2 id=&#34;triangle-computer-science-distinguished-lecturer-series&#34;&gt;Triangle Computer Science Distinguished Lecturer Series&lt;/h2&gt;
&lt;p&gt;The computer science departments at Duke University, North Carolina State University, and the University of North Carolina at Chapel Hill have joined forces to create the Triangle Computer Science Distinguished Lecturer Series. The series, which began in the 1995-1996 academic year, has been made possible with a number of grants from the U.S. Army Research Office, rotated between the departments.&lt;/p&gt;
&lt;h3 id=&#34;massive-scale-streaming-analytics&#34;&gt;Massive-Scale Streaming Analytics&lt;/h3&gt;
&lt;p&gt;Triangle Computer Science Distinguished Lecturer Series&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speaker Name&lt;/strong&gt;: &lt;strong&gt;David Bader&lt;/strong&gt; &lt;br&gt;
Georgia Institute of Technology &lt;br&gt;
&lt;strong&gt;Date and Time&lt;/strong&gt;: Monday, February 02, 2015 4:00 pm - 5:00 pm &lt;br&gt;
&lt;strong&gt;Location&lt;/strong&gt;: D106 LSRC, Duke &lt;br&gt;
&lt;strong&gt;Notes&lt;/strong&gt;: This talk will be videocast live to D106 from UNC.&lt;/p&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Emerging real-world graph problems include: detecting community structure in large social networks; improving the resilience of the electric power grid; and detecting and preventing disease in human populations. Unlike traditional applications in computational science and engineering, solving these problems at scale often raises new challenges because of the sparsity and lack of locality in the data, the need for additional research on scalable algorithms and development of frameworks for solving these problems on high performance computers, and the need for improved models that also capture the noise and bias inherent in the torrential data streams. In this talk, the speaker will discuss the opportunities and challenges in massive data-intensive computing for applications in computational science and engineering.&lt;/p&gt;
&lt;h3 id=&#34;short-biography&#34;&gt;Short Biography&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a full professor and chair of the School of Computational Science and Engineering, College of Computing at Georgia Institute of Technology and Executive Director of High Performance Computing. His research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE, and his main areas of research are in parallel algorithms, combinatorial optimization, massive-scale social networks, and computational biology and genomics. He received his Ph.D. from The University of Maryland, is a Fellow of the IEEE and AAAS, a National Science Foundation CAREER Award recipient, and has received numerous industrial awards from IBM, NVIDIA, Intel, Cray, Oracle/Sun Microsystems, and Microsoft Research. He serves as a board member of the Computing Research Association (CRA), on the NSF Advisory Committee on Cyberinfrastructure, on the Council on Competitiveness High Performance Computing Advisory Committee, on the IEEE Computer Society Board of Governors, and on the Steering Committees of the IPDPS and HiPC conferences and is the editor-in-chief of IEEE Transactions on Parallel and Distributed Systems (TPDS). Dr. Bader is a leading expert on multicore, manycore, and multithreaded computing for data-intensive applications such as those in massive-scale graph analytics and has co-authored over 130 articles in peer-reviewed journals and conferences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Host&lt;/strong&gt;: UNC, Prof. Ashok Krishnamurthy&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cs.duke.edu/events/node/1940&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cs.duke.edu/events/node/1940&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cs.unc.edu/tcsdls/bios-and-abstracts-14-15/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cs.unc.edu/tcsdls/bios-and-abstracts-14-15/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2015 IEEE Computer Society Board of Governors</title>
      <link>http://localhost:1313/blog/20150130-ieee-cs-bog/</link>
      <pubDate>Fri, 30 Jan 2015 14:26:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20150130-ieee-cs-bog/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20150130-ieee-cs-bog/photo_hu_eb7c37958326d3a2.webp 400w,
               /blog/20150130-ieee-cs-bog/photo_hu_fc67491eeee5312c.webp 760w,
               /blog/20150130-ieee-cs-bog/photo_hu_23b129df8855af69.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20150130-ieee-cs-bog/photo_hu_eb7c37958326d3a2.webp&#34;
               width=&#34;760&#34;
               height=&#34;612&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Surviving Cyberspace</title>
      <link>http://localhost:1313/blog/20141201-gatech-cyberspace/</link>
      <pubDate>Mon, 01 Dec 2014 11:56:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20141201-gatech-cyberspace/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/image_hu_527216ced634858f.webp 400w,
               /blog/20141201-gatech-cyberspace/image_hu_e6d45065f5b033b5.webp 760w,
               /blog/20141201-gatech-cyberspace/image_hu_2735e533a68b6c10.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/image_hu_527216ced634858f.webp&#34;
               width=&#34;760&#34;
               height=&#34;152&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;making-room-for-privacy-in-cybersecurity&#34;&gt;Making room for privacy in cybersecurity&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/ODa2I3SnBD8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In this video, Anton explains why she disagrees with the Director of the FBI and the Attorney General on the topic of cell phone encryption.&lt;/a&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-annie-anton&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Annie Anton&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/Annie-Ant%C3%B3n1_hu_93ba5a0b0accf4a3.webp 400w,
               /blog/20141201-gatech-cyberspace/Annie-Ant%C3%B3n1_hu_7b508cab585eef02.webp 760w,
               /blog/20141201-gatech-cyberspace/Annie-Ant%C3%B3n1_hu_4ab396f0bf4c1bd6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/Annie-Ant%C3%B3n1_hu_93ba5a0b0accf4a3.webp&#34;
               width=&#34;300&#34;
               height=&#34;274&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Annie Anton
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Balancing individual privacy with national security needs will continue to be a major challenge in the years to come. Annie Antón has been working on these dual objectives since the 1990s, when she was a graduate student at Georgia Tech.&lt;/p&gt;
&lt;p&gt;“I realized then that security and privacy were both essential goals for modern computing systems,” said Antón, who returned to Georgia Tech in 2012 as chair of the School of Interactive Computing.&lt;/p&gt;
&lt;p&gt;Antón, who presented her first privacy and security papers before the 9/11 attacks, is part of a National Research Council committee on Foundational Science in Cybersecurity. She is on security and privacy advisory boards for major corporations and has served on the Department of Homeland Security Data Privacy and Integrity Advisory Committee. She recently appeared on C-SPAN for her testimony on the role of technology and privacy in fighting terrorism.&lt;/p&gt;
&lt;p&gt;Antón’s current research focuses on how to create systems that actually comply with regulatory, security, and privacy requirements.&lt;/p&gt;
&lt;p&gt;This work is particularly relevant today, considering the rapid growth of the Internet of Things, which allows for communication among everyday devices such as Fitbit Trackers, smart thermostat systems, and washers/dryers that use Wi-Fi for remote monitoring.&lt;/p&gt;
&lt;p&gt;As information for these devices is collected and stored, big data analytics will process this information and infer things about us.&lt;/p&gt;
&lt;p&gt;“Security and privacy will need to be baked in from the start,” she said.&lt;/p&gt;
&lt;p&gt;—​​&lt;em&gt;Annie Antón chairs the School of Interactive Computing in the College of Computing.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;cyberanalytics-protecting-us-with-high-performance-computing&#34;&gt;Cyberanalytics: Protecting us with high-performance computing&lt;/h2&gt;


















&lt;figure  id=&#34;figure-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/Bader_hu_eb9caf6e08c9c824.webp 400w,
               /blog/20141201-gatech-cyberspace/Bader_hu_561707cf891063be.webp 760w,
               /blog/20141201-gatech-cyberspace/Bader_hu_15fc489a62001229.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/Bader_hu_eb9caf6e08c9c824.webp&#34;
               width=&#34;300&#34;
               height=&#34;290&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When we think of cybersecurity, we often think of the outsider trying to hack into our computer systems. But, another challenge is how we identify and defend against an insider, oftentimes a lone wolf, who knows our procedures and safety precautions.&lt;/p&gt;
&lt;p&gt;If we want to protect ourselves from both scenarios, we must increase our reliance on high-performance computing, especially the graph analytic research we conduct at Georgia Tech, says &lt;strong&gt;David Bader&lt;/strong&gt;, chair of the School of Computational Science and Engineering in the College of Computing.&lt;/p&gt;
&lt;p&gt;Graphs help us discover patterns and relationships hidden in massive amounts of data. These graphs are comprised of interconnected vertices (nodes) and lines (edges), and these graphs change over time.&lt;/p&gt;
&lt;p&gt;In the realm of cybersecurity, the vertices are people, places, and things, and the edges represent their interactions. By designing fast, using theoretic algorithms on large-scale graphs, we can produce insights in near-real time. This is crucial because cybersecurity analysts often are overwhelmed with thousands of alerts to review, and our algorithms may direct them immediately to the most important ones.&lt;/p&gt;
&lt;p&gt;We leave a digital trace every time we use a key card to get through a door, log in to a computer, or send an email. Security officers need to analyze this information so they can understand our patterns and identify potential threats.&lt;/p&gt;
&lt;p&gt;These massive-scale datasets are often unstructured and challenging to inspect. The emerging graph technology we are developing at Georgia Tech has the potential to be the best and most efficient way to prevent future attacks where we work and live, says Bader.&lt;/p&gt;
&lt;p&gt;—&lt;strong&gt;​​David Bader&lt;/strong&gt; &lt;em&gt;chairs the School of Computational Science and Engineering in the College of Computing.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;securing-the-new-cyberspace-revolution&#34;&gt;Securing the new cyberspace revolution&lt;/h2&gt;


















&lt;figure  id=&#34;figure-raheem-beyah&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Raheem Beyah&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/Beyah_hu_4a919ac4ee00e6da.webp 400w,
               /blog/20141201-gatech-cyberspace/Beyah_hu_670812b55bb0f654.webp 760w,
               /blog/20141201-gatech-cyberspace/Beyah_hu_688d8693d026147b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/Beyah_hu_4a919ac4ee00e6da.webp&#34;
               width=&#34;300&#34;
               height=&#34;265&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Raheem Beyah
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We are in the midst of a revolution in cyberspace that could be as transformative as the emergence of the Internet. The most significant difference between this revolution and that of the emergence of the Internet is that the current revolution gives computing systems an unprecedented amount of control over individuals and critical infrastructure.  As such, computer engineers play a key role in ensuring maximum benefit from technologies while also ensuring that systems operate safely.&lt;/p&gt;
&lt;p&gt;One goal of the Georgia Tech Communications Assurance and Performance Group is to develop algorithms that enable a more secure and efficient current and future network infrastructure, with computer systems that are more accountable and less vulnerable to attacks and abuse.&lt;/p&gt;
&lt;p&gt;Our efforts focus on areas that are critical to the success and security of the so-called Internet of Things (IoT) and cyber physical systems (CPS), says Raheem Beyah, an associate professor in the School of Electrical and Computer Engineering who leads the Communications Assurance and Performance Group.&lt;/p&gt;
&lt;p&gt;IoT is comprised of everyday devices — such as irons, toasters, and thermostats — that are wirelessly connected and work to improve our quality of life. CPS deals with the use of cyberspace to manage and monitor existing physical systems, such as power grids, oil and gas generation, and distribution systems.&lt;/p&gt;
&lt;p&gt;To improve security in these areas, we are working to understand the behavior of these systems to prevent misuse, secure the wireless networks IoT devices use to communicate, and ensure information is exchanged in a manner that preserves privacy, Beyah said. The impact of IoT and CPS on society will be tremendous, which is why we must keep it secure, he added.&lt;/p&gt;
&lt;p&gt;—&lt;em&gt;​Raheem Beyah, an associate professor in the School of Electrical and Computer Engineering, leads the Communications Assurance and Performance Group.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;defending-the-us-against-cyber-warriors&#34;&gt;Defending the U.S. against cyber warriors&lt;/h2&gt;


















&lt;figure  id=&#34;figure-andrew-howard&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Andrew Howard&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/IMG_6830_hu_581fef4156f49d9.webp 400w,
               /blog/20141201-gatech-cyberspace/IMG_6830_hu_2690cf1dd8488a3e.webp 760w,
               /blog/20141201-gatech-cyberspace/IMG_6830_hu_63f50f6962b91b3c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/IMG_6830_hu_581fef4156f49d9.webp&#34;
               width=&#34;300&#34;
               height=&#34;226&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Andrew Howard
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;A new generation of cyber warriors has suited up for battle and is targeting U.S. interests.&lt;/p&gt;
&lt;p&gt;The Georgia Tech Research Institute (GTRI) is a leader in developing the technologies that secure, defend, and respond to threats within our country’s information, distribution, and network systems on the virtual battlefield.&lt;/p&gt;
&lt;p&gt;The Cyber Technology and Information Security Laboratory (CTISL) conducts applied research focused on cyber threats and countermeasures, secure multilevel information sharing, resilient command and control network architectures, reverse engineering, vulnerability identification, and high-performance computing and analytics.&lt;/p&gt;
&lt;p&gt;CTISL has six strategic thrusts: Reverse Engineering, Vulnerability Identification, and Exploitation; Resilient Network Systems Engineering; Malicious Software Analysis, Threat Intelligence, and Penetration Testing; High-Performance Computing and Analytics; Multilevel, Secure Software Systems, and Collaboration Tools; and Professional Education, Outreach, and Awareness.&lt;/p&gt;
&lt;p&gt;CTISL engineers develop and apply cutting-edge technologies in computing, network architectures, signal and protocol analysis, network forensics, malware analysis, and reverse engineering (hardware and software) to solve tough problems, says Andrew Howard, senior research scientist at GTRI.&lt;/p&gt;
&lt;p&gt;Howard, along with other GTRI experts in his lab, is tackling tough security issues within military and non-military networks, developing new tools and methods for securing information, educating and increasing awareness in the cyber domain, and applying leading technologies in network design to keep us safe now — and in the future. CTISL brings this knowledge to the classroom by providing professional education offerings across the cyber landscape.&lt;/p&gt;
&lt;p&gt;—&lt;em&gt;​Andrew Howard directs the Cyber Technology and Information Security Laboratory at the Georgia Tech Research Institute.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;attacking-botnets-before-they-attack-the-internet&#34;&gt;Attacking botnets before they attack the Internet&lt;/h2&gt;


















&lt;figure  id=&#34;figure-wenke-lee&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Wenke Lee&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/wenke_lee_hu_2a07e95e0940eb1a.webp 400w,
               /blog/20141201-gatech-cyberspace/wenke_lee_hu_6f266b1971668180.webp 760w,
               /blog/20141201-gatech-cyberspace/wenke_lee_hu_90d3540731a1eac1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/wenke_lee_hu_2a07e95e0940eb1a.webp&#34;
               width=&#34;246&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Wenke Lee
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Large-scale attacks on the Internet are typically launched using a botnet, which is a large number of infected machines under the control of an attacker.&lt;/p&gt;
&lt;p&gt;Wenke Lee, who directs the Georgia Tech Information Security Center, and his research group are leaders in botnet detection, and were among the first to work on this problem starting in 2005.&lt;/p&gt;
&lt;p&gt;They have focused on the key characteristics of botnets — for example, the need for a command-and-control infrastructure — that separate them from the previous generations of Internet-based attacks. They have developed and deployed several algorithms and have demonstrated their effectiveness in early-warning, detection, and attribution of Internet-scale attacks.&lt;/p&gt;
&lt;p&gt;More importantly, their work has had a significant practical impact. In 2006, Lee co-founded Damballa Inc., which focuses on delivering anti-botnet technologies to enterprises, and now has about 90 employees. It counts all major Internet service providers in the U.S. and many Fortune 500 companies as its customers.&lt;/p&gt;
&lt;p&gt;Lee’s group was the first to conduct a systematic study of the security of iOS as well as Apple’s app review and management process. While many in the industry and academia believe that iOS is (almost) immune to malicious programs, their work showed that it is possible to inject malicious code on iOS devices, and even create an iOS botnet. Their work revealed a number of vulnerabilities in iOS and has led Apple to implement several security improvements.&lt;/p&gt;
&lt;p&gt;—&lt;em&gt;​Wenke Lee, a professor of Computer Science in the College of Computing, directs the Georgia Tech Information Security Center.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;adjusting-to-todays-cyber-realities&#34;&gt;Adjusting to today’s cyber realities&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/JXBxrAFHfWQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In this video, Swire talks about the importance of cybersecurity, and how individuals can protect themselves against potential attacks.&lt;/a&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-peter-swire&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Peter Swire&#34; srcset=&#34;
               /blog/20141201-gatech-cyberspace/swire_peter_profile_hu_6fc87535562f9997.webp 400w,
               /blog/20141201-gatech-cyberspace/swire_peter_profile_hu_37fa7a0da2c6382b.webp 760w,
               /blog/20141201-gatech-cyberspace/swire_peter_profile_hu_da4163504a075a45.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141201-gatech-cyberspace/swire_peter_profile_hu_6fc87535562f9997.webp&#34;
               width=&#34;300&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Peter Swire
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In August 2013, less than a week after joining the faculty at Georgia Tech, Peter Swire was tapped to serve on President Barack Obama’s Review Group on Intelligence and Communications Technology. Swire became one of the five authors of a major report that was issued that December.&lt;/p&gt;
&lt;p&gt;The work on the Review Group is part of Swire’s two decades of research and government service on issues of cybersecurity and privacy. He previously served as chief counselor for privacy under President Bill Clinton, where cybersecurity topics included encryption, intrusion detection for federal systems, and how to update wiretap laws for the Internet age. In the subsequent decade, Swire served on security advisory boards.&lt;/p&gt;
&lt;p&gt;This year, he is teaching a privacy course and one on “Information Security Strategies and Policy.” Among his multiple research projects, he’s looking at how to refine the debate about when information sharing should take place for cybersecurity.&lt;/p&gt;
&lt;p&gt;“As personal data flows everywhere, security issues are everywhere as well,” Swire said.&lt;/p&gt;
&lt;p&gt;He’s also looking at how well secrets will be kept in the future, noting the effect of the “declining half-life of secrets” on the workings of intelligence agencies.&lt;/p&gt;
&lt;p&gt;“Today’s cyber realities mean people and government on the outside can find out what the agencies are doing,” Swire said. “We’re going to have to get used to a world where we can’t keep things classified for 25 or 50 years and assume they are going to stay hidden.”&lt;/p&gt;
&lt;p&gt;—&lt;em&gt;​​Peter Swire is the Nancy J. and Lawrence P. Huang Professor of Law and Ethics in the Scheller College of Business.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/features/surviving-cyberspace&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/features/surviving-cyberspace&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why You’d Never Know Atlanta is Tops for Data Centers</title>
      <link>http://localhost:1313/blog/20141120-wabe/</link>
      <pubDate>Thu, 20 Nov 2014 12:33:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20141120-wabe/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jim Burress&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Office parks, like this one in DeKalb County, are one place data storage centers are housed. Chances are you&amp;rsquo;d never know it, though. Companies prefer to keep their data center locations top-secret.
Office parks, like this one in DeKalb County, are one place data storage centers are housed. Chances are you&amp;rsquo;d never know it, though. Companies prefer to keep their data center locations top-secret.&lt;/p&gt;
&lt;p&gt;Big data needs big closets.&lt;/p&gt;
&lt;p&gt;Turns out, Atlanta’s among the best when it comes to storing the stuff.&lt;/p&gt;
&lt;p&gt;Real estate giant CBRE recently looked at the top 23 markets for data hubs, and found Atlanta is among the most cost-effective.&lt;/p&gt;
&lt;p&gt;“You’ve got a combination of lower cost utilities and very competitive rental prices,” says Pat Lynch, CBRE’s manager of data center solutions. “The tax component is the third part.”&lt;/p&gt;
&lt;p&gt;Couple those with the metro area’s solid IT infrastructure and decent weather, and Lynch says Atlanta rises to the top for companies wanting to store and access information.&lt;/p&gt;
&lt;p&gt;So where are all these massive data storage centers?&lt;/p&gt;
&lt;p&gt;Lynch declined to give specifics, but says the heaviest concentration is in Atlanta’s suburbs. And there’s a good chance you’ll not see a data center concentration near Hartsfield-Jackson International Airport. “We tend to avoid any location that falls within the flight path of an airport,” he says.  (Sorry, Florida. Hurricane-prone areas are also shunned.)&lt;/p&gt;
&lt;p&gt;Companies big and small lease data center space here, says &lt;strong&gt;David Bader&lt;/strong&gt; of Georgia Tech’s College of Computing. “From telecommunications to finance to energy to health, we really are a unique hub of innovation,” he says.&lt;/p&gt;
&lt;p&gt;And while Atlanta might be among the cheapest for renting data center space, it’s far from cheap. CBRE’s Pat Lynch says rates here are well above the highest commercial office rents in cities like San Francisco and New York.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;20141120-WABE.mp3&#34;&gt;Listen now&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wabe.org/why-youd-never-know-atlanta-tops-data-centers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wabe.org/why-youd-never-know-atlanta-tops-data-centers/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google Can Now Describe Your Cat Photos</title>
      <link>http://localhost:1313/blog/20141118-wsj/</link>
      <pubDate>Tue, 18 Nov 2014 08:12:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/20141118-wsj/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Rolfe Winkler&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-googles-trained-computers-recognized-that-this-is-a-photo-of-two-pizzas-sitting-on-top-of-a-stove-top-oven-google&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Google’s trained computers recognized that this is a photo of “two pizzas sitting on top of a stove top oven” *Google*&#34; srcset=&#34;
               /blog/20141118-wsj/GoogleCats_hu_dcf952a1c1c39534.webp 400w,
               /blog/20141118-wsj/GoogleCats_hu_1b1da771aa11680d.webp 760w,
               /blog/20141118-wsj/GoogleCats_hu_630adf07ce28911e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20141118-wsj/GoogleCats_hu_dcf952a1c1c39534.webp&#34;
               width=&#34;553&#34;
               height=&#34;369&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Google’s trained computers recognized that this is a photo of “two pizzas sitting on top of a stove top oven” &lt;em&gt;Google&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;http://online.wsj.com/public/quotes/main.html?type=djn&amp;amp;symbol=GOOGL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt; &amp;rsquo;s computers learned to recognize cats in photos. Now, they’re learning to describe cats playing
with a ball of string.&lt;/p&gt;
&lt;p&gt;Computer scientists in the search giant’s research division, and a separate team working at Stanford
University, independently developed artificial-intelligence software that can decipher the action in a photo,
and write a caption to describe it. That’s a big advance over previous software that was mostly limited to
recognizing objects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://online.wsj.com/public/quotes/main.html?type=djn&amp;amp;symbol=GOOGL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;In a blog post&lt;/a&gt;, Google described how it is using advanced “machine-learning” techniques that mimic the
human brain to recognize a photo of “a person riding a motorcycle on a dirt road,” or “a herd of elephants
walking across a dry grass field.”
The new software can “capture the whole scene and generate corresponding natural-looking text,” says
Yoshua Bengio, a professor of computer science at the University of Montreal and a leading expert in the
field. That defies predictions that software would be limited to recognizing objects, he said.&lt;/p&gt;
&lt;p&gt;The new technology could lead to big improvements in the accuracy of Google’s image-search results,
which today often rely on text found near a photo on a web page. One day it might help people search
vast libraries of untagged photos or videos stored on smartphones, says &lt;strong&gt;David Bader&lt;/strong&gt;, a professor of
computer science at Georgia Tech. A startup called Viblio is using similar research out of Simon Fraser
University to automatically categorize videos.&lt;/p&gt;
&lt;p&gt;In 2012, a Google/Stanford team famously &lt;a href=&#34;http://arxiv.org/abs/1112.6209&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;taught a computer to learn how to recognize cats&lt;/a&gt;. The
computer was shown millions of images from YouTube videos, and used then-state-of-the-art machinelearning
algorithms to teach itself to spot felines.&lt;/p&gt;
&lt;p&gt;Similar advances are helping improve other Google services. Earlier this year, Google researchers
disclosed how its computers had &lt;a href=&#34;http://arxiv.org/pdf/1312.6082v4.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learned to read house numbers&lt;/a&gt; from images captured by its Street View
cars, making it quicker and easier to locate buildings in Google Maps, for instance.&lt;/p&gt;
&lt;p&gt;Google is making big bets on artificial-intelligence technology. Earlier this year, it paid hundreds of millions
of dollars to acquire Deep Mind Technologies, a London-based startup that employs many specialists in
advanced machine learning. Earlier, it bought DNNResearch, a small company started at the University of
Toronto, in order to hire a top academic in machine learning, Geoffrey Hinton.&lt;/p&gt;
&lt;p&gt;Artificial-intelligence research also helps speech-recognition software, used by smartphone assistants like
&lt;a href=&#34;http://online.wsj.com/public/quotes/main.html?type=djn&amp;amp;symbol=AAPL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple&lt;/a&gt; &amp;rsquo;s Siri or Google voice search.&lt;/p&gt;
&lt;p&gt;Others also are investing in the field. &lt;a href=&#34;http://online.wsj.com/public/quotes/main.html?type=djn&amp;amp;symbol=FB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook&lt;/a&gt; scooped up a top artificial-intelligence academic late last
year. Meanwhile, Chinese search engine &lt;a href=&#34;http://online.wsj.com/articles/SB10001424052702304908304579565950123054242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baidu has said it will invest $300 million&lt;/a&gt; in an artificial intelligence
lab in Silicon Valley. To lead the lab, Baidu hired the head of Stanford’s artificial-intelligence lab, Andrew
Ng, who helped build the computer that taught itself to recognize cats from YouTube videos.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blogs.wsj.com/digits/2014/11/18/google-can-now-describe-your-cat-photos/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blogs.wsj.com/digits/2014/11/18/google-can-now-describe-your-cat-photos/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic graph analytics tackle social media and other big data</title>
      <link>http://localhost:1313/blog/20121106-dynamicgraph/</link>
      <pubDate>Thu, 06 Nov 2014 07:50:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121106-dynamicgraph/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Rick Robinson&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-a-gtri-team-consisting-of-left-to-right-dan-campbell-rob-mccoll-jason-poovey-and-david-ediger-is-bringing-graph-analytics-to-bear-on-a-range-of-data-related-challenges-including-social-networks-surveillance-intelligence-computer-network-functionality-and-industrial-control-systems&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A GTRI team consisting of (left to right) Dan Campbell, Rob McColl, Jason Poovey, and David Ediger is bringing graph analytics to bear on a range of data-related challenges including social networks, surveillance intelligence, computer-network functionality, and industrial control systems.&#34; srcset=&#34;
               /blog/20121106-dynamicgraph/graphanalytics1_0_hu_e77a13f3cfb19263.webp 400w,
               /blog/20121106-dynamicgraph/graphanalytics1_0_hu_e9ba8dacf12560e5.webp 760w,
               /blog/20121106-dynamicgraph/graphanalytics1_0_hu_964c4f41d48bbead.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20121106-dynamicgraph/graphanalytics1_0_hu_e77a13f3cfb19263.webp&#34;
               width=&#34;740&#34;
               height=&#34;605&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A GTRI team consisting of (left to right) Dan Campbell, Rob McColl, Jason Poovey, and David Ediger is bringing graph analytics to bear on a range of data-related challenges including social networks, surveillance intelligence, computer-network functionality, and industrial control systems.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Today, petabytes of digital information are generated daily by such sources as social media, Internet activity, surveillance sensors, and advanced research instruments. The results are often referred to as “big data” – accumulations so huge that highly sophisticated computer techniques are required to identify useful information hidden within.&lt;/p&gt;
&lt;p&gt;Graph analysis is a prime tool for finding the needle in the data haystack. This potent technology – not to be confused with simple illustrations like bar graphs and pie charts – utilizes mathematical techniques that represent relationships in the data more efficiently than traditional statistical analyses.&lt;/p&gt;
&lt;p&gt;Researchers at the Georgia Tech Research Institute (GTRI) are bringing graph analytics to bear on a range of data-related challenges. They&amp;rsquo;re developing advanced technology that can help investigate social networks, surveillance intelligence, computer-network functionality, industrial control systems, and more.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our first task is to look at the interesting properties of a graph – to find the important questions we can ask of that graph,&amp;rdquo; said Dan Campbell, a GTRI principal research engineer who heads the High Performance Computing Branch. &amp;ldquo;The second task is to find the answers as quickly as possible, and then put them to practical use.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A graph is a type of data structure comprised of entities – meaning anything that can be represented digitally – and their relationships. In graph terminology, an entity is a vertex or a node; the connections between it and other vertices are edges or arcs. Graphs are constructed using software algorithms that represent both the data points and the relationships between them, and also enable computers to manipulate and analyze that information.&lt;/p&gt;
&lt;p&gt;GTRI researchers make extensive use of a graph-analysis framework called STINGER, built specifically to tackle dynamic, ever-changing applications such as social networks and Internet traffic. STINGER was created by a team led by &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor in the School of Computational Science and Engineering; key members of that team included David Ediger and Robert McColl, who are now part of Campbell&amp;rsquo;s GTRI group. STINGER, which is open-source software (STINGERgraph.com), continues to be developed at Georgia Tech and in the broader graph analytics community.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;ve done a great deal of work on analyzing openly available social media in real time,&amp;rdquo; said Ediger.&amp;ldquo;Social media analysis clearly has an important role to play in emergency response to both natural disasters like Hurricane Sandy and to potential terrorist attacks, and we&amp;rsquo;re actively researching applications in those areas, among others.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;STINGER helps support GTRI’s focus on streaming or dynamic-graph technology, which can store very large databases and then update them in real time as new data come in. This novel approach allows users to monitor social media on a massive scale, and can also be utilized to simulate very large networks.&lt;/p&gt;
&lt;p&gt;Georgia Tech researchers have presented this technology at several recent conferences including the 1st Workshop on Parallel Programming for Analytics Applications, which was held in February in Orlando, Fla., in conjunction with the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Unlike traditional graph databases, STINGER’s streaming-graph technology lets us store very big graphs and analyze them at high speed using fairly modest computing capability,&amp;rdquo; said Jason Poovey, a GTRI research scientist in Campbell&amp;rsquo;s group. &amp;ldquo;In half a terabyte of main memory – a pretty reasonable size today – we can handle billions of nodes and edges. Our benchmark tests show we can represent, update and analyze a graph in real time that&amp;rsquo;s essentially the size of all the data in Twitter.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;GTRI is focusing on multiple efforts in which graph analysis plays a key role.  These projects include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Behavioral Modeling and Computational Social Systems (BMCSS) Strategic Initiative – A GTRI team led by senior research scientist Erica Briscoe has used STINGER to study real-time social media analytics, as part of research aimed at predicting human behavior on a large scale.&lt;/li&gt;
&lt;li&gt;BlackForest – Members of Campbell&amp;rsquo;s group are using graph analytics to support the BlackForest project led by GTRI researcher Chris Smoak. The aim of this externally funded project involves forming coherent intelligence pictures from disparate types of data obtained from multiple sources.&lt;/li&gt;
&lt;li&gt;Nextcache – This externally funded project focuses on developing new CPU, cache and memory designs tailored for graph-based applications.&lt;/li&gt;
&lt;li&gt;Real-time Business Intelligence – Using streaming graph technology, members of Campbell’s group are working with GTRI researcher Erica Briscoe to develop a business-intelligence dashboard that monitors social media in real time and helps businesses gauge consumer sentiment.&lt;/li&gt;
&lt;li&gt;XDATA – Working with researchers from the School of Computational Science and Engineering, GTRI senior research scientists Barry Drake and Richard Boyd are helping to address big-data challenges by studying the computational demands of processing machine-learning algorithms.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Research News &lt;br&gt;
Georgia Institute of Technology &lt;br&gt;
177 North Avenue &lt;br&gt;
Atlanta, Georgia  30332-0181  USA&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media Relations Contacts&lt;/strong&gt;: John Toon (&lt;a href=&#34;mailto:jtoon@gatech.edu&#34;&gt;jtoon@gatech.edu&lt;/a&gt;) (404-894-6986) or Brett Israel (&lt;a href=&#34;mailto:brett.israel@comm.gatech.edu&#34;&gt;brett.israel@comm.gatech.edu&lt;/a&gt;) (404-385-1933).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://phys.org/news/2014-11-dynamic-graph-analytics-tackle-social.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://phys.org/news/2014-11-dynamic-graph-analytics-tackle-social.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2014/11/06/dynamic-graph-analytics-tackle-social-media-and-other-big-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2014/11/06/dynamic-graph-analytics-tackle-social-media-and-other-big-data&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>George Michael HPC Fellowships Announced</title>
      <link>http://localhost:1313/blog/20141014-hpcwire/</link>
      <pubDate>Tue, 14 Oct 2014 17:02:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20141014-hpcwire/</guid>
      <description>&lt;p&gt;ACM (the Association for Computing Machinery) and IEEE Computer Society have named Harshitha Menon of the University of Illinois, Urbana Champaign (UIUC) and Alexander Breuer of Technische Universität München (TUM) as recipients of 2014 ACM/IEEE Computer Society George Michael Memorial HPC Fellowships. Menon and Breuer will each receive a $5,000 honorarium, plus travel and registration to receive the award at SC14 during the awards ceremony in November.&lt;/p&gt;
&lt;p&gt;A doctoral candidate advised by Laxmikant V. Kale at UIUC, Harshitha Menon’s research focuses on scalable load-balancing algorithms and adaptive run-time techniques to improve the performance of large-scale dynamic applications. In addition, Menon works on optimizing performance for N-body codes, such as the cosmology simulation application ChaNGa. She won recognition for her paper “Scalable Load Balancing and Adaptive Run Time Techniques.”&lt;/p&gt;
&lt;p&gt;Menon is an exemplary contributor in HPC with her ability to bring to light HPC applications in science and improve research and work in HPC overall. “Menon has taken on two challenging HPC research problems—load balancing and code optimization for N-body simulations—and she is working tirelessly to solve them,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor and chair of Georgia Tech’s School of Computational Science and Engineering and chair of the 2014 Fellowship Committee. “Menon’s strong collaborative work and dedication to improving HPC applications is inspiring.”&lt;/p&gt;
&lt;p&gt;Alexander Breuer, doctoral student at TUM, actively works on improving performance for highly resolved dynamic rupture and seismic wave simulations. Breuer pushes time-to-solution for the SeisSol software package in terms of superior numeric and high-performance optimizations. He was a member of the team to receive the PRACE award at ISC14 and he is also part of the 2014 ACM Gordon Bell Prize submission for seismic simulations, “&lt;a href=&#34;https://web.archive.org/web/20140816235436/http://sc14.supercomputing.org/schedule/event_detail?evid=gb112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Petascale High Order Dynamic Rupture Earthquake Simulations on Heterogeneous Supercomputers.&lt;/a&gt;” He won recognition for his project, “Petascale High Order Earthquake Simulations.”&lt;/p&gt;
&lt;p&gt;“By researching and simulating seismic waves, we can better identify regions exposed to severe ground motions and support engineers in planning and constructing human developments,” said Trish Damkroger, SC14 Conference Chair and Lawrence Livermore National Laboratory’s Deputy Associate Director At-Large for Computation. “We can expect great things from Breuer in HPC. His strong integration of mathematics, software development, and HPC into his work in SeisSol is truly brilliant.”&lt;/p&gt;
&lt;p&gt;Endowed in memory of George Michael, a founding father of the SC Conference series, the ACM IEEE-CS George Michael Memorial Fellowships honor exceptional PhD students throughout the world whose research focus is on high-performance computing applications, networking, storage, or large-scale data analysis using the most powerful computers that are currently available.&lt;/p&gt;
&lt;p&gt;Fellowship recipients are selected based on their project’s research excellence, their technical interests’ alignment with the HPC community, academic progress, faculty advisors’ recommendations, and study plans to enhance HPC-related skills. For more information about the fellowship, visit &lt;a href=&#34;http://sc14.supercomputing.org/program/awards/acmieee-cs-george-michael-memorial-hpc-fellowship&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sc14.supercomputing.org/program/awards/acmieee-cs-george-michael-memorial-hpc-fellowship&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;about-acm&#34;&gt;About ACM&lt;/h3&gt;
&lt;p&gt;ACM, the Association for Computing Machinery &lt;a href=&#34;https://www.acm.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.acm.org&lt;/a&gt;, is the world’s largest educational and scientific computing society, uniting computing educators, researchers and professionals to inspire dialogue, share resources and address the field’s challenges. ACM strengthens the computing profession’s collective voice through strong leadership, promotion of the highest standards, and recognition of technical excellence. ACM supports the professional growth of its members by providing opportunities for life-long learning, career development, and professional&lt;/p&gt;
&lt;h3 id=&#34;about-ieee-computer-society&#34;&gt;About IEEE Computer Society&lt;/h3&gt;
&lt;p&gt;IEEE Computer Society is the world’s leading computing membership organization and the trusted information and career-development source for a global workforce of technology leaders including: professors, researchers, software engineers, IT professionals, employers, and students. The unmatched source for technology information, inspiration, and collaboration, the IEEE Computer Society is the source that computing professionals trust to provide high-quality, state-of-the-art information on an on-demand basis. The Computer Society provides a wide range of forums for top minds to come together, including technical conferences, publications, and a comprehensive digital library, unique training webinars, professional training, and the TechLeader Training Partner Program to help organizations increase their staff’s technical knowledge and expertise, as well as the personalized information tool myComputer. To find out more about the community for technology leaders, visit &lt;a href=&#34;http://www.computer.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;about-sc14&#34;&gt;About SC14&lt;/h3&gt;
&lt;p&gt;SC14, sponsored by IEEE Computer Society and ACM (Association for Computing Machinery) offers a complete technical education program and exhibition to showcase the many ways high performance computing, networking, storage and analysis lead to advances in scientific discovery, research, education and commerce. This premier international conference includes a globally attended technical program, workshops, tutorials, a world class exhibit area, demonstrations and opportunities for hands-on learning. For more information on SC14, please visit: &lt;a href=&#34;http://sc14.supercomputing.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sc14.supercomputing.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/george-michael-hpc-fellowships-announced/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/george-michael-hpc-fellowships-announced/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Researchers working on computers which can last 75 times longer</title>
      <link>http://localhost:1313/blog/20140705-darpa-perfect/</link>
      <pubDate>Fri, 04 Jul 2014 08:39:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140705-darpa-perfect/</guid>
      <description>&lt;p&gt;Researchers Georgia Tech are helping Defense Advanced Projects Research Agency (DARPA) to develop an energy efficient computer that can last 75 times longer than the present day computers.&lt;/p&gt;
&lt;p&gt;The computer is being developed as part of an initiative called Power Efficiency Revolution for Embedded Computing Technologies (PERFECT), which is still in the elementary stages.&lt;/p&gt;
&lt;p&gt;The success of this project could result in smaller and more efficient systems which could be used in aircraft and ground vehicles as well as used by soldiers on the ground.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, School of Computational Science and Engineering executive director of high-performance computing, said, &amp;ldquo;The program is looking at how do we come to a new paradigm of computing where running time isn&amp;rsquo;t necessarily the constraint, but how much power and battery that we have available is really the new constraint.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Georgia Tech&amp;rsquo;s part in DARPA-led PERFECT effort is called Graph Analysis Tackling power-Efficiency, Uncertainty and Locality (GRATEFUL), which focuses on algorithms that will create graphical representation out of large volumes of data in the most energy-efficient way.&lt;/p&gt;
&lt;p&gt;The main focus would be to reduce power consumption by cutting the level of data collection.&lt;/p&gt;
&lt;p&gt;The ultimate goal of the project is to get an algorithmic framework that would enable to create smaller devices with supercomputer like capabilities.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20140709052901/http://www.cbronline.com/news/tech/hardware/desktops/researchers-working-on-computers-which-can-last-75-times-longer-4311072&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cbronline.com/news/tech/hardware/desktops/researchers-working-on-computers-which-can-last-75-times-longer-4311072&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GT Computing Flexes Power at Parallel Computation Symposium</title>
      <link>http://localhost:1313/blog/20140520-ipdps2014/</link>
      <pubDate>Tue, 20 May 2014 07:38:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140520-ipdps2014/</guid>
      <description>&lt;p&gt;Georgia Tech is putting forth a dominating presence at one of the premier parallel computation symposia this week in Phoenix as it sends 30 of its professors and researchers to present nine papers, two of which earned “best paper” honors.&lt;/p&gt;
&lt;p&gt;The Institute of Electrical and Electronics Engineers (IEEE) International Parallel &amp;amp; Distributed Processing Symposium (IPDPS), held from May 19 to 23 in Phoenix, is the flagship activity of the IEEE Computer Society’s Technical Committee on Parallel Processing (TCPP), representing a unique international gathering of computer scientists from around the world.&lt;/p&gt;
&lt;p&gt;For IPDPS 2014, Georgia Tech participates in virtually every part of the technical program, where researchers from the College of Computing and the Institute for Data and High Performance Computing (IDH) have gathered to present their latest research findings in all aspects of parallel computation. They will take part in numerous paper presentations, workshops, and other parts of the 28th annual IPDPS program. &lt;strong&gt;David A. Bader&lt;/strong&gt; serves as the 2014 IPDPS program chair, and Srinivas Aluru has been tapped as the program chair for the 2015 symposium.&lt;/p&gt;
&lt;p&gt;This year, Georgia Tech researchers—CSE’s Associate Professor Edmond Chow and graduate students Xing Liu and Aftab Patel, and CS’s Associate Professor Santosh Pande and graduate student Kaushik Ravichandran—took home two of the four best paper awards given at IPDPS 2014.&lt;/p&gt;
&lt;p&gt;Georgia Tech’s participation at IPDPS 2014 includes:&lt;/p&gt;
&lt;h2 id=&#34;leadership&#34;&gt;Leadership&lt;/h2&gt;
&lt;h3 id=&#34;srinivas-aluru&#34;&gt;Srinivas Aluru&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;li&gt;IPDPS 2015 Program Chair&lt;/li&gt;
&lt;li&gt;HiCOMB Workshop Co-Chair&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;david-a-bader&#34;&gt;David A. Bader&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Chair&lt;/li&gt;
&lt;li&gt;HCW Panelist: Is the Amount of Heterogeneity Increasing in Future Computer Systems?&lt;/li&gt;
&lt;li&gt;Graph Algorithms Building Blocks (GABB) Workshop Co-organizer&lt;/li&gt;
&lt;li&gt;GABB Workshop Presenter: STINGER: Multi-threaded Graph Streaming&lt;/li&gt;
&lt;li&gt;High Performance Computational Biology (HiCOMB) Workshop Co-Chair&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;edmond-chow&#34;&gt;Edmond Chow&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;zhihui-du-visiting-professor&#34;&gt;Zhihui Du (visiting professor)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Parallel and Distributed Computing for Large Scale Machine Learning and Big Data Analytics (ParLearning) 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;david-ediger&#34;&gt;David Ediger&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MTAAP 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bo-hong&#34;&gt;Bo Hong&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PhD Forum Co-Chair&lt;/li&gt;
&lt;li&gt;Student Travel Chair&lt;/li&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;li&gt;MTAAP 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;santosh-pande&#34;&gt;Santosh Pande&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;jason-riedy&#34;&gt;Jason Riedy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;li&gt;MTAAP 2014 Program Committee&lt;/li&gt;
&lt;li&gt;GABB Workshop Presenter: STINGER: Multi-threaded Graph Streaming&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;karsten-schwan&#34;&gt;Karsten Schwan&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HPDIC2014 Keynote: pMem—Persistent Memory for Data-intensive Applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;magdalena-slawinska&#34;&gt;Magdalena Slawinska&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;IPDPS 2014 Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;jeffrey-s-vetter&#34;&gt;Jeffrey S. Vetter&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MTAAP 2014 Program Committee&lt;/li&gt;
&lt;li&gt;ASHES Workshop Keynote: Exploring Emerging Technologies in the HPC Co-Design Space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;richard-vuduc&#34;&gt;Richard Vuduc&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;High-Performance, Power-Aware Computing (HPPAC) Panelist: Emerging Challenges to Software Management of Power and Energy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;best-papers&#34;&gt;Best Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;A New Scalable Parallel Algorithm for Fock Matrix Construction&lt;/em&gt;&lt;br&gt;
Xing Liu (Georgia Institute of Technology, USA); Aftab Patel (Georgia Institute of Technology, USA); Edmond Chow (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;F2C2-STM: Flux-based Feedback-driven Concurrency Control for STMs&lt;/em&gt;&lt;br&gt;
Kaushik Ravichandran (Georgia Institute of Technology, USA); Santosh Pande (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Scibox: Online Sharing of Scientific Data via the Cloud&lt;/em&gt;&lt;br&gt;
Jian Huang (Georgia Institute of Technology, USA); Xuechen Zhang (Georgia Institute of Technology, USA); Greg Eisenhauer (Georgia Institute of Technology, USA); Karsten Schwan (Georgia Tech, USA); Matthew Wolf (Georgia Institute of Technology, USA); Stephane Ethier (PPPL, USA); Scott Klasky (Oak Ridge National Laboratory, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Parallel Mutual Information Based Construction of Whole-Genome Networks on the Intel(R) Xeon Phi(TM) Coprocessor&lt;/em&gt;&lt;br&gt;
Sanchit Misra (Intel Corporation, India); Kiran Pamnany (Intel Corporation, India); Srinivas Aluru (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Finding Motifs in Biological Sequences Using the Micron Automata Processor&lt;/em&gt;&lt;br&gt;
Indranil Roy (Georgia Institute of Technology, USA); Srinivas Aluru (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TBPoint: Reducing Simulation Time for Large Scale GPGPU Kernels&lt;/em&gt;&lt;br&gt;
Jen-Cheng Huang (Georgia Institute of Technology, USA); Lifeng Nai (Georgia Institute of Technology, USA); Hyesoon Kim (Georgia Tech, USA); Hsien-Hsin Lee (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Algorithmic Time, Energy, and Power on Candidate HPC Compute Building Blocks&lt;/em&gt;&lt;br&gt;
Jee Choi (Georgia Institute of Technology, USA); Marat Dukhan (Georgia Institute of Technology, USA); Xing Liu (Georgia Institute of Technology, USA); Richard W. Vuduc (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Large-scale Hydrodynamic Brownian Simulations on Multicore and Manycore Architectures&lt;/em&gt;&lt;br&gt;
Xing Liu (Georgia Institute of Technology, USA); Edmond Chow (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Interactive Program Debugging and Optimization for Directive-Based, Efficient GPU Computing&lt;/em&gt;&lt;br&gt;
Seyong Lee (Oak Ridge National Laboratory, USA); Dong Li (Oak Ridge National Laboratory, USA); Jeffrey S. Vetter (Oak Ridge National Laboratory &amp;amp; Georgia Tech, USA)&lt;/p&gt;
&lt;h2 id=&#34;workshop-papers&#34;&gt;Workshop Papers&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Presented in Conjunction with IPDPS 2014.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;hicomb-2014&#34;&gt;HiCOMB 2014&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Parallelization of the Trinity Pipeline for de Novo Transcriptome Assembly&lt;/em&gt;&lt;br&gt;
Vipin Sachdeva (Georgia Institute of Technology, USA); Chang-Sik Kim, Kirk Jordan; Martyn D. Winn&lt;/p&gt;
&lt;h3 id=&#34;mtaap-2014&#34;&gt;MTAAP 2014&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Revisiting Edge and Node Parallelism for Dynamic GPU Graph Analytics&lt;/em&gt;&lt;br&gt;
Adam McLaughlin (Georgia Institute of Technology, USA); &lt;strong&gt;David A. Bader&lt;/strong&gt; (Georgia Institute of Technology, USA)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ece.gatech.edu/news/298581/gt-computing-flexes-power-parallel-computation-symposium&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ece.gatech.edu/news/298581/gt-computing-flexes-power-parallel-computation-symposium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ECE Alumnus Bader Promoted to Chair at Georgia Tech</title>
      <link>http://localhost:1313/blog/20140415-maryland/</link>
      <pubDate>Tue, 15 Apr 2014 17:22:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140415-maryland/</guid>
      <description>

















&lt;figure  id=&#34;figure-professor-david-bader-ece-alumnus-and-founder-of-ece-gsa-is-named-chair-of-georgia-techs-cse-photo-courtesy-of-raftermen-photography&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Professor David Bader, ECE alumnus and founder of ECE GSA, is named Chair of Georgia Tech&amp;#39;s CSE. *Photo courtesy of Raftermen Photography*&#34; srcset=&#34;
               /blog/20140415-maryland/article8137.large_hu_fb4a962561c4987e.webp 400w,
               /blog/20140415-maryland/article8137.large_hu_a7ee898b514f194f.webp 760w,
               /blog/20140415-maryland/article8137.large_hu_d2195e463648c591.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20140415-maryland/article8137.large_hu_fb4a962561c4987e.webp&#34;
               width=&#34;155&#34;
               height=&#34;233&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Professor David Bader, ECE alumnus and founder of ECE GSA, is named Chair of Georgia Tech&amp;rsquo;s CSE. &lt;em&gt;Photo courtesy of Raftermen Photography&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;University of Maryland Electrical and Computer Engineering alumnus &lt;strong&gt;David Bader&lt;/strong&gt; (Ph.D., ’96) was recently promoted to Chair of Georgia Tech’s School of Computational Science and Engineering (CSE). He will assume his role beginning July 2014.&lt;/p&gt;
&lt;p&gt;“I’m thrilled that we found within our own ranks a candidate as viable as David to take the helm of the School of CSE,” said Zvi Galil, John P. Imlay Jr. Dean of Computing. “We expect great things to continue in the school under David’s watch.”&lt;/p&gt;
&lt;p&gt;During his time at UMD, Bader was advised by Professor Joseph JaJa (ECE/&lt;a href=&#34;http://www.umiacs.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UMIACS&lt;/a&gt;) and founded and served as president of the Electrical and Computer Engineering Graduate Student Association (&lt;a href=&#34;http://gsa.ece.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECEGSA&lt;/a&gt;). Since his graduation, Bader was named fellow of both IEEE and AAAS, and received a National Science Foundation (NSF) CAREER Award. Bader also received the ECE Distinguished Alumni Award in 2012.&lt;/p&gt;
&lt;p&gt;“I am amazed at the speed with which David has achieved national prominence in high performance computing and and big data analytics, and in leadership roles for major national and international professional organizations,” said Joseph JaJa. “I am confident that David with lead the Georgia Tech School of Computational Science and Engineering to higher levels of national prominence.”&lt;/p&gt;
&lt;p&gt;For more information about David Bader, visit his &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;biography page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://eng.umd.edu/news/story/ece-alumnus-bader-promoted-to-chair-at-georgia-tech&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eng.umd.edu/news/story/ece-alumnus-bader-promoted-to-chair-at-georgia-tech&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Chosen to Lead Georgia Tech&#39;s School of Computational Science and Engineering</title>
      <link>http://localhost:1313/blog/20140326-bader-cse/</link>
      <pubDate>Wed, 26 Mar 2014 21:54:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140326-bader-cse/</guid>
      <description>

















&lt;figure  id=&#34;figure-david-bader-the-new-chair-of-georgia-techs-school-of-computational-science-and-engineering&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader the new chair of Georgia Tech&amp;#39;s School of Computational Science and Engineering.&#34; srcset=&#34;
               /blog/20140326-bader-cse/image_hu_53f9ece5814fdd0e.webp 400w,
               /blog/20140326-bader-cse/image_hu_715c95a19b4312b9.webp 760w,
               /blog/20140326-bader-cse/image_hu_4ca82c860e238cd1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20140326-bader-cse/image_hu_53f9ece5814fdd0e.webp&#34;
               width=&#34;217&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader the new chair of Georgia Tech&amp;rsquo;s School of Computational Science and Engineering.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Following a national search for new leadership of its &lt;a href=&#34;http://www.cse.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;School of Computational Science and Engineering&lt;/a&gt; (CSE), Georgia Tech’s College of Computing has selected its own &lt;strong&gt;David A. Bader&lt;/strong&gt;, a renowned leader in high-performance computing, to chair the school.&lt;/p&gt;
&lt;p&gt;Bader, a professor in the School of CSE and executive director of the High-Performance Computing Lab, succeeds Regents’ Professor Richard Fujimoto, who has served in the role since 2007 and through CSE’s elevation to “school” status in 2010. Fujimoto has continued to serve as chair amid the search and will remain as a professor within the school.&lt;/p&gt;
&lt;p&gt;Bader will assume his new role in July 2014. His appointment is contingent upon approval by Georgia Tech President G.P. “Bud” Peterson and the Board of Regents of the University System of Georgia.&lt;/p&gt;
&lt;p&gt;“I’m thrilled that we found within our own ranks a candidate as viable as David to take the helm of the School of CSE,” said Zvi Galil, John P. Imlay Jr. Dean of Computing. “Not only does David bring a deep background of experiences and an ambitious agenda that will ensure our college remains at the forefront of high-performance computing and data analytics, he exudes a contagious level of energy for these areas of computing. We expect great things to continue in the school under David’s watch.”&lt;/p&gt;
&lt;p&gt;Bader earned his Ph.D. in electrical engineering from the University of Maryland, College Park, in 1996 after earning both a B.S. in computer engineering in 1990 and an M.S. in electrical engineering in 1991 from Lehigh University. During his doctoral research, he was a NASA graduate fellow and was awarded a National Science Foundation research role in experimental computer science after receiving his doctorate.&lt;/p&gt;
&lt;p&gt;In 1998, he became an assistant professor and Regents’ lecturer at the University of New Mexico. In 2005 he joined Georgia Tech’s College of Computing, where he became a full professor in 2008 and an integral part of the formation of the School of CSE.&lt;/p&gt;
&lt;p&gt;“I believe that the School of CSE has firmly established itself as the country’s premier department focused on solving real-world challenges through advanced computational techniques, thanks to a world-class faculty and dedicated students,” Bader said. “My plan as school chair is to accelerate our impactful research and gain recognition for further successes by solving grand challenges that make this world a better place for all.”&lt;/p&gt;
&lt;p&gt;While at Georgia Tech, Bader has established himself as an expert in the design and analysis of parallel and multicore algorithms for real-world applications, including computational biology and genomics and massive-scale data analytics.&lt;/p&gt;
&lt;p&gt;For his work, he has earned recognition as a fellow with both IEEE and AAAS, received a National Science Foundation CAREER award and numerous industry awards from IBM, NVIDIA, Intel, Cray, Oracle/Sun Microsystems and Microsoft Research. InsideHPC recognized him as a &amp;ldquo;RockStar&amp;rdquo; of high-performance computing, and HPCwire&amp;rsquo;s cited him among “People to Watch” in both 2012 and 2014.&lt;/p&gt;
&lt;p&gt;Bader serves as a board member of the Computing Research Association (CRA), on the NSF Advisory Committee on Cyberinfrastructure, on the Council on Competitiveness High Performance Computing Advisory Committee, on the IEEE Computer Society Board of Governors and on the steering committees of the IPDPS and HiPC conferences. He is editor-in-chief of IEEE Transactions on Parallel and Distributed Systems (TPDS) and program chair for IPDPS 2014.&lt;/p&gt;
&lt;p&gt;The School of Computational Science and Engineering is devoted to the advancement and promotion of the CSE discipline. Research focuses on making fundamental advances in the creation and application of new computational methods and techniques in order to enable breakthroughs in scientific discovery and engineering practice.&lt;/p&gt;
&lt;p&gt;About the Georgia Tech College of Computing: The Georgia Tech College of Computing is a national leader in the creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked ninth nationally by U.S. News and World Report, the College’s unconventional approach to education is defining the new face of computing by expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human-centered solutions. For more information about the Georgia Tech College of Computing, its academic divisions and research centers, please visit &lt;a href=&#34;http://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newswise.com//articles/david-bader-chosen-to-lead-georgia-tech-s-school-of-computational-science-and-engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newswise.com//articles/david-bader-chosen-to-lead-georgia-tech-s-school-of-computational-science-and-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2014/03/25/college-computing-picks-bader-lead-school-cse&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2014/03/25/college-computing-picks-bader-lead-school-cse&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Picks Bader to Lead School of CSE</title>
      <link>http://localhost:1313/blog/20140326-hpcwire/</link>
      <pubDate>Wed, 26 Mar 2014 07:09:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140326-hpcwire/</guid>
      <description>&lt;p&gt;Following a national search for new leadership of its &lt;a href=&#34;http://www.cse.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;School of Computational Science and Engineering&lt;/a&gt; (CSE), Georgia Tech’s College of Computing has selected its own &lt;strong&gt;David A. Bader&lt;/strong&gt;, a renowned leader in high-performance computing, to chair the school.&lt;/p&gt;
&lt;p&gt;Bader, a professor in the School of CSE and executive director of the High-Performance Computing Lab, succeeds Regents’ Professor Richard Fujimoto, who has served in the role since 2007 and through CSE’s elevation to “school” status in 2010. Fujimoto has continued to serve as chair amid the search and will remain as a professor within the school.&lt;/p&gt;
&lt;p&gt;Bader will assume his new role in July 2014. His appointment is contingent upon approval by Georgia Tech President G.P. “Bud” Peterson and the Board of Regents of the University System of Georgia.&lt;/p&gt;
&lt;p&gt;“I’m thrilled that we found within our own ranks a candidate as viable as David to take the helm of the School of CSE,” said Zvi Galil, John P. Imlay Jr. Dean of Computing. “Not only does David bring a deep background of experiences and an ambitious agenda that will ensure our college remains at the forefront of high-performance computing and data analytics, he exudes a contagious level of energy for these areas of computing. We expect great things to continue in the school under David’s watch.”&lt;/p&gt;
&lt;p&gt;Bader earned his Ph.D. in electrical engineering from the University of Maryland, College Park, in 1996 after earning both a B.S. in computer engineering in 1990 and an M.S. in electrical engineering in 1991 from Lehigh University. During his doctoral research, he was a NASA graduate fellow and was awarded a National Science Foundation research role in experimental computer science after receiving his doctorate.&lt;/p&gt;
&lt;p&gt;In 1998, he became an assistant professor and Regents’ lecturer at the University of New Mexico. In 2005 he joined Georgia Tech’s College of Computing, where he became a full professor in 2008 and an integral part of the formation of the School of CSE.&lt;/p&gt;
&lt;p&gt;“I believe that the School of CSE has firmly established itself as the country’s premier department focused on solving real-world challenges through advanced computational techniques, thanks to a world-class faculty and dedicated students,” Bader said. “My plan as school chair is to accelerate our impactful research and gain recognition for further successes by solving grand challenges that make this world a better place for all.”&lt;/p&gt;
&lt;p&gt;While at Georgia Tech, Bader has established himself as an expert in the design and analysis of parallel and multicore algorithms for real-world applications, including computational biology and genomics and massive-scale data analytics.&lt;/p&gt;
&lt;p&gt;For his work, he has earned recognition as a fellow with both IEEE and AAAS, received a National Science Foundation CAREER award and numerous industry awards from IBM, NVIDIA, Intel, Cray, Oracle/Sun Microsystems and Microsoft Research. InsideHPC recognized him as a &amp;ldquo;RockStar&amp;rdquo; of high-performance computing, and HPCwire&amp;rsquo;s cited him among “People to Watch” in both 2012 and 2014.&lt;/p&gt;
&lt;p&gt;Bader serves as a board member of the Computing Research Association (CRA), on the NSF Advisory Committee on Cyberinfrastructure, on the Council on Competitiveness High Performance Computing Advisory Committee, on the IEEE Computer Society Board of Governors and on the steering committees of the IPDPS and HiPC conferences. He is editor-in-chief of IEEE Transactions on Parallel and Distributed Systems (TPDS) and program chair for IPDPS 2014.&lt;/p&gt;
&lt;p&gt;The School of Computational Science and Engineering is devoted to the advancement and promotion of the CSE discipline. Research focuses on making fundamental advances in the creation and application of new computational methods and techniques in order to enable breakthroughs in scientific discovery and engineering practice.&lt;/p&gt;
&lt;h3 id=&#34;about-the-georgia-tech-college-of-computing&#34;&gt;About the Georgia Tech College of Computing&lt;/h3&gt;
&lt;p&gt;The Georgia Tech College of Computing is a national leader in the creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked ninth nationally by U.S. News and World Report, the College’s unconventional approach to education is defining the new face of computing by expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human-centered solutions. For more information about the Georgia Tech College of Computing, its academic divisions and research centers, please visit &lt;a href=&#34;http://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/college-computing-picks-bader-lead-school-cse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/college-computing-picks-bader-lead-school-cse/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumnus &#39;HPC rock star&#39; to lead Ga. Tech&#39;s School of CSE</title>
      <link>http://localhost:1313/blog/20140325-lehigh/</link>
      <pubDate>Tue, 25 Mar 2014 13:19:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140325-lehigh/</guid>
      <description>&lt;p&gt;Georgia Tech’s College of Computing has selected &lt;strong&gt;David A. Bader&lt;/strong&gt; &amp;lsquo;90 &amp;lsquo;91G, a renowned leader in high-performance computing, to chair its School of Computational Science and Engineering (CSE). A professor in the School of CSE and executive director of its High-Performance Computing Lab, Bader will assume his new role in July 2014.&lt;/p&gt;
&lt;p&gt;Bader, a 1987 graduate of &lt;a href=&#34;http://www-lhs.beth.k12.pa.us/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Liberty High School&lt;/a&gt; in Bethlehem, PA, attended Lehigh University, where he earned a bachelor&amp;rsquo;s in computer engineering in 1990 and  a master&amp;rsquo;s in electrical engineering in 1991. In 2005 he joined Georgia Tech’s College of Computing; he became a full professor in 2008 and played an integral role in the formation of the School of CSE.&lt;/p&gt;
&lt;p&gt;An expert in the design and analysis of parallel and multicore algorithms for real-world applications, including computational biology and genomics and massive-scale data analytics, Bader has won highly-competitive awards from the National Science Foundation (NSF), IBM, Microsoft, Sony, and Sun Microsystems. He has co-chaired a series of meetings, the IEEE International Workshop on High-Performance Computational Biology (HiCOMB), written several book chapters, and co-edited a special issue of the Journal of Parallel and Distributed Computing on High-Performance Computational Biology. He has co-authored over 100 articles in peer-reviewed journals and conferences.&lt;/p&gt;
&lt;p&gt;Bader has earned recognition as a fellow with both IEEE and AAAS, received a National Science Foundation CAREER award and numerous industry awards. InsideHPC recognized him as a &amp;ldquo;RockStar&amp;rdquo; of high-performance computing, and HPCwire cited him among &amp;ldquo;People to Watch&amp;rdquo; in both 2012 and 2014.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I believe that the School of CSE has firmly established itself as the country’s premier department focused on solving real-world challenges through advanced computational techniques, thanks to a world-class faculty and dedicated students,&amp;rdquo; says Bader. &amp;ldquo;My plan as school chair is to accelerate our impactful research and gain recognition for further successes by solving grand challenges that make this world a better place for all.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20150109185459/https://www.lehigh.edu/engineering/news/alumni/2014/alum_20140325_bader_georgia_tech.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lehigh webpage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://engineering.lehigh.edu/alumni/david-bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://engineering.lehigh.edu/alumni/david-bader&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>For Google, a leg up in the artificial intelligence arms race</title>
      <link>http://localhost:1313/blog/20140205-google/</link>
      <pubDate>Wed, 05 Feb 2014 09:35:08 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140205-google/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Verne Kopytoff&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Google’s executives have long dreamed of solving one of the technology industry’s biggest riddles. How do you predict what people want — hockey scores or new Ugg boots, for example — before they even ask for it? Reading user’s minds, or at least seeming to, would make Google’s products that much faster and more convenient. It could also help the company fend off rivals.&lt;/p&gt;
&lt;p&gt;Last week, Google (GOOG) took its biggest step yet to ramp up its predictive powers. It acquired &lt;a href=&#34;https://money.cnn.com/2014/01/27/technology/google-deepmind/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind Technologies&lt;/a&gt;, a British startup focused on artificial intelligence, for a reported $400 to $500 million. DeepMind develops algorithms that learn as they comb through mountains of data. The company had been working on technology for simulations, e-commerce, and video games, although the exact details were not disclosed.&lt;/p&gt;
&lt;p&gt;The addition could help Google’s perpetual efforts to improve its search results and make its ads more relevant. Helping tag photos and improve the accuracy of voice recognition are among the other possibilities.&lt;/p&gt;
&lt;p&gt;“Artificial intelligence can become part of the fabric for all of Google’s products,” said Colin Sebastian, an analyst with R.W. Baird. “Google has a knack for adapting and innovating. For a company its size, that’s a requirement — or you risk becoming a Yahoo or AOL,” two companies that are struggling because of an aversion to change.&lt;/p&gt;
&lt;p&gt;Artificial intelligence isn’t new. Researchers have worked on the technology for decades. But it has been slow-going because computers haven’t been powerful enough to handle the number-crunching that’s necessary. Recent advancements in technology have sped up the progress.&lt;/p&gt;
&lt;p&gt;“Powerful algorithms and computers are able to solve problems that were intractable just a few years ago,” said &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high performance computing at Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;Google has been working on artificial intelligence almost since its founding 16 years ago. Its efforts focused on improving search results, translation, and filtering spam. Google Now, a digital personal assistant within Google’s mobile search app, is among the most ambitious of its efforts to date. It guesses what information users want based on their past search history and location, and then gives it to them.&lt;/p&gt;
&lt;p&gt;“Google Now can do simple things like give you directions to work or figure out that you have a flight,” Bader said. “But to really predict what you’re really going to do or to understand your lifestyle, and be able to do that in multiple languages and cultures requires this new technology.”&lt;/p&gt;
&lt;p&gt;Google already collects a huge swath of personal data about its users including their search histories, locations when using their smartphones, and any purchase history made through Google products. Google’s engineers — or rather algorithms — have plenty of information to crunch so as to create user profiles and gauge user intent.&lt;/p&gt;
&lt;p&gt;DeepMind, which was founded four years ago, has been trying to get computers to learn much like the human brain does. Algorithms search data for patterns and then draw conclusions from it in what’s known as deep learning. The company, which has around 50 employees, has yet to come up with a commercialized product. But a recent paper published by some of its researchers provides a window into its progress.&lt;/p&gt;
&lt;p&gt;The paper describes a computer that learned to play seven Atari 2600 games. Feeding the computer with information about how to play specific games was unnecessary, the researchers said, a sign that the system learned to play from first-hand experience.&lt;/p&gt;
&lt;p&gt;Google’s rivals are also hard at work on artificial intelligence. Amazon (AMZN), Microsoft (MSFT), and Facebook (FB) all have big efforts underway. It’s a new arms race in the Internet industry. As the DeepMind acquisitions shows, companies are wiling to spend huge amounts of money to keep up.&lt;/p&gt;
&lt;p&gt;Analysts said it’s impossible to tell whether artificial intelligence will lift Google’s profits. Rather, they said any improved products that come from the technology would help the company defend its leadership position.&lt;/p&gt;
&lt;p&gt;“It’s all about staying ahead of the competition,” said Jan Dawson, an analyst with Jackdaw Research. “You won’t be able to point to a specific product and say ‘A.I. contributed this.’”&lt;/p&gt;
&lt;p&gt;Artificial intelligence could play an especially big role in Google’s new business lines. For example, earlier this month, Google said it would pay $3.2 billion to acquire Nest Labs, the maker of Internet-connected devices for the home. A thermostat the company already sells infers when homeowners come and go based on usage patterns. Any future appliances that come out of Nest may also depend on artificial intelligence — at least in part — to control the settings or, in the case of a smart television, recommend shows to watch.&lt;/p&gt;
&lt;p&gt;Google is also pushing into robotics through the acquisition of a number of companies including &lt;a href=&#34;https://money.cnn.com/2013/12/16/technology/google-boston-dynamics-robots/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boston Dynamics&lt;/a&gt;. A separate project to build self-driving cars is running parallel. Both initiatives require sophisticated technology to be able to maneuver independently of humans. Artificial intelligence is therefore a given, but it’s unclear whether it would involve DeepMind.&lt;/p&gt;
&lt;p&gt;Although artificial intelligence is a huge opportunity, Google will have to tread carefully. Privacy concerns and the “creep factor” may rule out certain uses. Imagine if people wearing &lt;a href=&#34;https://money.cnn.com/video/technology/2014/01/28/t-new-google-glasses.cnnmoney&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Glass&lt;/a&gt;, Google’s science fiction-inspired eyeglasses, could use facial recognition technology to identify everyone passing by them on the street and to pull up detailed information about them. Public opposition would be intense.&lt;/p&gt;
&lt;p&gt;Another potential problem is if an algorithm were to draw an inaccurate conclusion based on your personal data. Google might wrongly conclude that you’re seriously ill, for example. Or it could accurately deduce as much and then inadvertently share that information with someone who borrowed your computer by suggesting all sorts of drugs to take.&lt;/p&gt;
&lt;p&gt;“Artificial intelligence can be really scary when it crosses the line,” Dawson said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://fortune.com/2014/02/05/for-google-a-leg-up-in-the-artificial-intelligence-arms-race/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fortune.com/2014/02/05/for-google-a-leg-up-in-the-artificial-intelligence-arms-race/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPCwire Reveals the 2014 People to Watch</title>
      <link>http://localhost:1313/blog/20140127-hpcwire-people/</link>
      <pubDate>Mon, 27 Jan 2014 08:09:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140127-hpcwire-people/</guid>
      <description>&lt;p&gt;&lt;strong&gt;HPCwire&lt;/strong&gt;, the leader in world-class journalism covering high performance computing (HPC), announced its ‘HPCwire People to Watch 2014’ list today. The annual list is comprised of an elite group of some of the brightest minds in HPC. The annual selections are made following an extensive review process by the HPCwire editorial and executive staff along with guidance from industry analysts and luminaries across the HPC community.&lt;/p&gt;
&lt;p&gt;“It’s our great pleasure each year to step back for a moment and be truly inspired by the intriguing personalities in the high performance computing space, and to honor what they do,” said Alan El Faye, President of Tabor Communications, Inc., producers of HPCwire. “With help from the HPC community, including the former recipients of the recognition, we take great pride and pains in making the selections for the ‘People to Watch’ list each year. This year’s choices demonstrate the diversity of people within the field of high performance computing, and the important work they are doing to leverage technology, and make the world a better place.”&lt;/p&gt;
&lt;p&gt;The annual HPCwire People to Watch list has become a much anticipated feature highlighting the work and accomplishments of the people involved with the use, production, or proliferation of high performance computing technologies. This year readers will discover that the individuals selected to be profiled are not only forerunners in HPC, but are also skilled pilots, expert chefs, candle makers, single parents of many, avid skiers and racers, and Grateful Dead followers.&lt;/p&gt;
&lt;p&gt;The HPCwire People to Watch 2014 list can be found at the following URL: &lt;a href=&#34;http://www.hpcwire.com/people-watch-2014/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/people-watch-2014/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A new year has dawned, which means there are new stars on the rise in high performance computing. Every year, HPCwire’s People to Watch pays tribute to the best and brightest minds in HPC whose hard work, dedication, and contributions reach far beyond high performance computing and shape the direction that technology is taking us – not only as an industry, but humanity at large. This is the cutting edge, and these are the people who are pushing the envelope. Along with this year’s list sponsor, SGI, we present the HPCwire People to Watch 2014.&lt;/p&gt;
&lt;hr&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20140127-hpcwire-people/Bader-HPCwire_hu_4531525d9a12ab45.webp 400w,
               /blog/20140127-hpcwire-people/Bader-HPCwire_hu_534bee715eab7541.webp 760w,
               /blog/20140127-hpcwire-people/Bader-HPCwire_hu_8839cd15caa42e87.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20140127-hpcwire-people/Bader-HPCwire_hu_4531525d9a12ab45.webp&#34;
               width=&#34;120&#34;
               height=&#34;168&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;  &lt;br&gt;
&lt;strong&gt;Professor and Executive Director of High Performance Computing, Georgia Institute of Technology&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; is no stranger to the pages of HPCwire, having been recognized before as a person to watch – and yet his star continues to rise. With the rise of big data, Bader’s work at the intersection where high performance computing meets real-world applications is turning heads. Among other achievements, Bader was the principal investigator in an effort that led to a nearly $2 million dollar grant for researchers at the Georgia Institute of Technology and the University of Southern California to bring supercomputing capabilities into the grasp of tablets, smart phones and other internet-era devices. We caught up with and asked Bader his thoughts on the trends for 2014.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HPCwire: David, you’ve been very prolific in HPC for some time now, with your hand in many cookie jars across the HPC landscape. What can you tell us about the important things you’ll be working on for 2014?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;: It’s an exciting time in High Performance Computing, when we can design and implement real systems for massive data analytics for distilling big data into knowledge. Specifically, we’re excited about streaming graph analytics and their tech transfer to industrial-class problems, and new processors and memory technologies from the leading vendors. The opportunities are vast for moving our processing kernels into emerging memory technologies, for instance, with power-savings advantages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HPCwire: You’ve told us that you are excited by new accelerator and memory technologies that are coming on to the scene. Can you talk about your vision about the importance of these technologies and what they mean for the future of HPC?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;: In computing, we’ve always considered processors and memory as separate components. However, with accelerator technologies from vendors such as NVIDIA and Intel, and new memory technologies from Micron and IBM, among others, we are changing the fundamental costs in both time and energy for performing key data processing kernels.&lt;/p&gt;
&lt;p&gt;Still there will be a challenge of how to design efficient algorithms and program them, but on this novel and heterogeneous architecture. I truly believe we are entering a new age of high performance computing that will change our basic assumptions about the possible!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HPCwire: As you look down the road at 2014 and beyond, what do you see as the most important trends developing that will have an impact now and into the future?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;: The two big trends we hear about are Exascale Computing and Big Data. These terms are more than the current fad– they are trends that definitely impact our future. Exascale computing refers to several key research and engineering challenges in designing supercomputing systems with millions or more of cores, and orchestrating them in a reliable and scalable way to solve real problems.&lt;/p&gt;
&lt;p&gt;With Big Data, in addition to building a data warehouse that can hold this massive pile of data, we have to figure out how to change the computing paradigm to ingest this information in all of our applications and perform complex analytics. Both of these trends also require us to think seriously in the years to come about energy-efficiency as a primary constraint in high performance computing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HPCwire: On a personal note, can you talk about your personal life? Your family, background, any hobbies?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;: Thanks for asking me to tell you a little about myself, because after all, high performance computing is about the people and making our lives better. I’ve just tried to make the world a better place, end suffering, and be happy. All of the academic things that one measures follows instead of being a goal. I’ve just been blessed to have the right paths in life that got me here.&lt;/p&gt;
&lt;p&gt;At Georgia Tech, I enjoy working with colleagues and students to make a difference in this world. My daughter Sadie is almost a teenager, and my hobby had been taking her to supercomputing conferences when she was little. Now, I attend her youth circus arts events!&lt;/p&gt;
&lt;p&gt;My other hobbies include being vegan and traveling around the world to see other people and cultures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HPCwire: One last question – is there anything about yourself that you can share that you think your colleagues would be surprised to learn?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;: Maybe the fact my first computer in 1972 was a &lt;code&gt;PDP-11/45&lt;/code&gt;, or that I followed around the Grateful Dead.&lt;/p&gt;
&lt;h4 id=&#34;about-hpcwire&#34;&gt;About HPCwire&lt;/h4&gt;
&lt;p&gt;HPCwire is the leader in world-class journalism for HPC. With a legacy dating back to 1986, HPCwire is recognized worldwide for its breakthrough coverage of the fastest computers in the world and the people who run them. Science, business, and industry professionals worldwide have established HPCwire as the industry’s leading news authority for information and intelligence across a broad range of advanced computing technologies. For topics ranging from the latest trends and emerging technologies, to expert commentary, in-depth analysis, and original feature coverage, HPCwire is the HPC communities’ longest running and most reliable and trusted resource.&lt;/p&gt;
&lt;h4 id=&#34;about-tabor-communications-inc&#34;&gt;About Tabor Communications, Inc.&lt;/h4&gt;
&lt;p&gt;Tabor Communications Inc. (TCI) is a leading international digital media, advertising, and communications company. As publisher of a complete advanced computing portfolio that includes HPCwire, Datanami, Enterprise Tech, and HPCwire Japan, TCI is the market-leader in online journalism covering emerging technologies within the high-tech industry.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/off-the-wire/hpcwire-reveals-2014-people-watch/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/off-the-wire/hpcwire-reveals-2014-people-watch/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/people-watch-2014/david-bader-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/people-watch-2014/david-bader-2/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Selected as One of HPCwire’s “People to Watch” in 2014</title>
      <link>http://localhost:1313/blog/20140126-gatech-hpcwire/</link>
      <pubDate>Sun, 26 Jan 2014 08:15:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20140126-gatech-hpcwire/</guid>
      <description>

















&lt;figure  id=&#34;figure-david-bader-is-a-professor-of-computational-science-and-engineering-in-the-college-of-computing&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader is a professor of computational science and engineering in the College of Computing.&#34; srcset=&#34;
               /blog/20140126-gatech-hpcwire/image_hu_bea843f16742aafe.webp 400w,
               /blog/20140126-gatech-hpcwire/image_hu_c19d7d5fd1c94a44.webp 760w,
               /blog/20140126-gatech-hpcwire/image_hu_4b9c2c58ce6be32e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20140126-gatech-hpcwire/image_hu_bea843f16742aafe.webp&#34;
               width=&#34;140&#34;
               height=&#34;198&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader is a professor of computational science and engineering in the College of Computing.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The College of Computing at the Georgia Institute of Technology, one of only two major universities to house its computing program within a college of its own, today announced that &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor and executive director of High Performance Computing, has been selected as one of HPCWire’s “People to Watch” in 2014.&lt;/p&gt;
&lt;p&gt;The 2014 list is a compilation of the 16 best and brightest minds from academia, science, and technology whose contributions in high performance computing (HPC) have the potential to profoundly impact the world this year and beyond. Finalists are selected following an extensive review process by the HPCwire editorial and executive staff along with guidance from industry analysts and luminaries across the HPC community.&lt;/p&gt;
&lt;p&gt;Although HPCwire has recognized Bader as a “person to watch” before, he continues to garner attention and recognition for his work in big data at the intersection where high performance computing meets real-world applications.&lt;/p&gt;
&lt;p&gt;Among his many achievements, Bader led an effort that resulted in a nearly $2 million dollar grant for researchers at Georgia Tech and the University of Southern California to bring supercomputing capabilities into the grasp of tablets, smart phones, and other devices.&lt;/p&gt;
&lt;p&gt;Read more about Bader and his thoughts on the trends in HPC for 2014.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20140330225419/http://www.cc.gatech.edu/news/david-bader-selected-one-hpcwire%E2%80%99s-%E2%80%9Cpeople-watch%E2%80%9D-2014&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/david-bader-selected-one-hpcwire%E2%80%99s-%E2%80%9Cpeople-watch%E2%80%9D-2014&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader discusses High Performance Computing at Georgia Tech</title>
      <link>http://localhost:1313/blog/20131230-youtube/</link>
      <pubDate>Mon, 30 Dec 2013 06:25:08 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131230-youtube/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20131230-youtube/image_hu_78bdf854746aff71.webp 400w,
               /blog/20131230-youtube/image_hu_c77c8d462dd441ab.webp 760w,
               /blog/20131230-youtube/image_hu_927441926804ac4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20131230-youtube/image_hu_78bdf854746aff71.webp&#34;
               width=&#34;636&#34;
               height=&#34;323&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Executive Director of High Performance Computing at Georgia Tech, &lt;strong&gt;David Bader&lt;/strong&gt;, discusses High Performance Computing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/Jbd_fW5l6ls&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://youtu.be/Jbd_fW5l6ls&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Opening Up the Accelerator Advantage</title>
      <link>http://localhost:1313/blog/20131126-hpcwire/</link>
      <pubDate>Tue, 26 Nov 2013 07:34:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131126-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Tiffany Trader&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Researchers at Georgia Institute of Technology and University of Southern California will receive nearly $2 million in federal funding for the creation of tools that will help developers exploit hardware accelerators in a cost-effective and power-efficient manner. The purpose of this three-year &lt;a href=&#34;http://www.nsf.gov/awardsearch/showAward?AWD_ID=1339745&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NSF grant&lt;/a&gt; is to bring formerly niche supercomputing capabilities into the hands of a more general audience to help them achieve high-performance for applications that were previously deemed hard to optimize. The project will involve the use of tablets, smart phones and other Internet-era devices, according to &lt;strong&gt;David Bader&lt;/strong&gt;, the lead principal investigator.&lt;/p&gt;
&lt;p&gt;“We want to take science that used to be only available to elite scientists and bring that to everyone around the planet,” said Bader, a professor in Georgia Tech’s School of Computational Science and Engineering and executive director for High Performance Computing. “We are bringing supercomputing to the masses.”&lt;/p&gt;
&lt;p&gt;An article at Georgia Tech details the project’s two main focal points: XScala and XBazaar. XScala is a software framework for developing efficient accelerator kernels. The framework includes a number of design time and run-time performance optimization tools designed to handle data-intensive kernels, those bound by data movement. Examples include large dictionary string matching, dynamic programming, graph theory, and sparse matrix computations — commonly associated with biology, network security, and the social sciences workloads.&lt;/p&gt;
&lt;p&gt;Researchers will work on different types of optimizations over the three-year period. Power efficiency will receive a lot of attention, as will security and social network analysis. The researchers expect that algorithms developed from these areas can then be applied to the graph analytics domain.&lt;/p&gt;
&lt;p&gt;The second focus involves a public software repository and forum, called XBazaar. This is similar to an app store, like on the iPhone. “XBazaar will serve as a one-stop shop for high-performance algorithms and software for multi-core and many-core processors,” according to the announcement.&lt;/p&gt;
&lt;p&gt;It’s a place where developers can go to share best practices for using accelerators. It’s open source so anyone can use it to develop their own high-performance applications, even commercial applications. The idea is to create a tool that can benefit academic researchers and industry.&lt;/p&gt;
&lt;p&gt;Besides Bader, the team includes co-PIs: Viktor Prasanna, a professor of electrical engineering and professor of computer science at the University of Southern California; Rich Vuduc, an associate professor in the School of Computational Science and Engineering at Georgia Tech; and Jason Riedy, a research scientist.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2013/11/26/opening-accelerator-advantage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2013/11/26/opening-accelerator-advantage/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating an app store for multi-core</title>
      <link>http://localhost:1313/blog/20131119-appstore/</link>
      <pubDate>Tue, 19 Nov 2013 07:54:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131119-appstore/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20131119-appstore/image_hu_b253fde0abcf9ba7.webp 400w,
               /blog/20131119-appstore/image_hu_4a39022a45599035.webp 760w,
               /blog/20131119-appstore/image_hu_c96c2163426fce6a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20131119-appstore/image_hu_b253fde0abcf9ba7.webp&#34;
               width=&#34;760&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Until now the niche area of supercomputing was available only to those in national research laboratories or leading research universities.&lt;/p&gt;
&lt;p&gt;That’s about to change.&lt;/p&gt;
&lt;p&gt;Researchers at the Georgia Institute of Technology and University of Southern California recently received a nearly $2 million federal grant to develop tools to assist all developers in using hardware accelerators productively and effectively.&lt;/p&gt;
&lt;p&gt;The goal of the three-year grant from the National Science Foundation is to bring capability to a general audience, including those using tablets, smartphones and other ubiquitous devices, said &lt;strong&gt;David Bader&lt;/strong&gt;, the lead principal investigator.&lt;/p&gt;
&lt;p&gt;Imagine an elderly woman using her smartphone to understand her health informatics or farmers in developing nations using hardware accelerators to understand weather patterns so they know when to plant crops.&lt;/p&gt;
&lt;p&gt;“We want to take science that used to be only available to elite scientists and bring that to everyone around the planet,” said Bader, a professor in Georgia Tech’s School of Computational Science and Engineering and executive director for High Performance Computing. “We are bringing supercomputing to the masses.”&lt;/p&gt;
&lt;p&gt;Bader will collaborate with Viktor Prasanna, another principal investigator who is a professor of electrical engineering and professor of computer science at the University of Southern California. Prasanna is executive director of the USC-Infosys Center for Advanced Software Technologies.&lt;/p&gt;
&lt;p&gt;Rich Vuduc, an associate professor in the School of Computational Science and Engineering at Georgia Tech, and Jason Riedy, a research scientist, are co-principal investigators.&lt;/p&gt;
&lt;p&gt;The grant focuses on two key areas. The first is to develop XScala, a software framework for developing efficient accelerator kernels. Data-intensive kernels include large dictionary string matching, dynamic programming and graph theory.&lt;/p&gt;
&lt;p&gt;Over the course of three years researchers will focus on different types of optimizations. They will emphasize power efficiency – an area with little current research – and hope to make discoveries that will allow for the development of new types of algorithms for accelerators, Bader said.&lt;/p&gt;
&lt;p&gt;Early research efforts will focus on key areas such as security and social network analysis.  The impact of this early research will be felt in other areas because the algorithms from these domains can be applied to other science and technology segments that use graph analytics.&lt;/p&gt;
&lt;p&gt;The second focus calls for a public software repository and forum, called XBazaar. Similar to the iPhone App Store, XBazaar will serve as a one-stop shop for high-performance algorithms and software for multi-core and many-core processors.&lt;/p&gt;
&lt;p&gt;This open source will create a marketplace to develop and share best practices for using accelerators. People will be able to modify code or use ideas from the researchers’ code to develop their own high-performance applications that are cost-effective and power-efficient, Bader said.&lt;/p&gt;
&lt;p&gt;XBazaar can be used in commercial applications, allowing it to spur economic development and benefit industry.&lt;/p&gt;
&lt;p&gt;“Our goal is to build something larger than what we can do ourselves,” Bader said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2013/11/19/creating-app-store-multi-core&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2013/11/19/creating-app-store-multi-core&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader’s Love for Teaching Goes Way Back</title>
      <link>http://localhost:1313/blog/20131028-teaching/</link>
      <pubDate>Mon, 28 Oct 2013 17:31:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131028-teaching/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20131028-teaching/image_hu_4ee07d615a84f94b.webp 400w,
               /blog/20131028-teaching/image_hu_cd12edbb2b5711a4.webp 760w,
               /blog/20131028-teaching/image_hu_95d63c3d9a00e0d7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20131028-teaching/image_hu_4ee07d615a84f94b.webp&#34;
               width=&#34;760&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;It’s around 9:30 a.m. on a Thursday, and &lt;strong&gt;David Bader&lt;/strong&gt; is explaining how trick-or-treating is connected to graph theory.&lt;/p&gt;
&lt;p&gt;“For example, we get together with our friends to figure out which houses to go to for optimal candy rewards — and optimization is one component of graph theory,” said Bader, professor in the College of Computing and executive director for High Performance Computing. “And when you get home, you sort your candy — yet another concept. We should consider October graph theory month.”&lt;/p&gt;
&lt;p&gt;Welcome to Bader’s Computational Science and Engineering Algorithms class — and his approach to teaching.&lt;/p&gt;
&lt;p&gt;“When I prepare for class, I think about the story I’m going to tell and try to incorporate current events and real-world cases into the curriculum,” he said.&lt;/p&gt;
&lt;p&gt;But these skills didn’t develop overnight.&lt;/p&gt;
&lt;h4 id=&#34;from-techie-toddler-to-full-prof&#34;&gt;From Techie Toddler to Full Prof&lt;/h4&gt;
&lt;p&gt;It began in 1972, when Bader started playing around on a mainframe computer at the college where his dad worked. He was 3.&lt;/p&gt;
&lt;p&gt;“Then, when I was in elementary school in the late 1970s, I began holding computer retraining classes for the parents of my friends and neighbors who were being laid off from a nearby steel plant and were looking for a new profession,” he added. “Things just seemed to evolve from there.”&lt;/p&gt;
&lt;p&gt;After spending time at the University of Maryland as a National Science Foundation postdoctoral research associate, Bader began working his way through the professorial ranks at the University of New Mexico in the late 1990s. He was recruited to Georgia Tech in 2005 to launch the School of Computational Science and Engineering.&lt;/p&gt;
&lt;p&gt;Bader’s research aims to solve real-world problems by using computational and data-intensive solutions.&lt;/p&gt;
&lt;p&gt;For example, Bader might use massive data sets to help him figure out how to keep people safer during severe weather or how to design more personalized medications.&lt;/p&gt;
&lt;p&gt;“I love that my job is fun,” he said. “I can’t believe I get paid to collaborate with the most amazing students and colleagues, and solve grand challenge problems that have a lasting impact on society.”&lt;/p&gt;
&lt;h4 id=&#34;the-man-behind-the-monitor&#34;&gt;The Man Behind the Monitor&lt;/h4&gt;
&lt;p&gt;When Bader isn’t conducting research or teaching, you might find him enjoying one of his favorite spots on campus, the Klaus Advanced Computing Building.&lt;/p&gt;
&lt;p&gt;“It’s just a beautiful facility that inspires me to make contributions to computing and to think about the impact of my research on society,” he added.&lt;/p&gt;
&lt;p&gt;Or you might catch this vegan enjoying the various dining spots around campus to try their takes on international cuisines ranging from African to Asian.&lt;/p&gt;
&lt;p&gt;And one last fun fact about this computing wiz — he also has a strong interest in genomics (simply put, the study of genes and their functions).&lt;/p&gt;
&lt;p&gt;“Many people might not be aware that I have a twin sister — who happened to earn her doctorate in genetics,” he said. “My full mitochondrial genome is even available in GenBank, a database that contains publicly available DNA sequences.”&lt;/p&gt;
&lt;p&gt;Contact:  &lt;br&gt;
Amelia Pavlik &lt;br&gt;
Institute Communications&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2013/10/28/baders-love-teaching-goes-way-back&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2013/10/28/baders-love-teaching-goes-way-back&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David A. Bader elected to IEEE Computer Society&#39;s Board of Governors</title>
      <link>http://localhost:1313/blog/20131022-ieeecs-bog/</link>
      <pubDate>Tue, 22 Oct 2013 22:37:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131022-ieeecs-bog/</guid>
      <description>&lt;p&gt;DAVID ALAN GRIER&lt;br&gt;
2013 President&lt;br&gt;
George Washington University&lt;br&gt;
Elliott School of International Affairs&lt;br&gt;
1957 E Street NW- Suite 401&lt;br&gt;
Washington, DC 20052 USA&lt;/p&gt;
&lt;p&gt;G. P. &amp;ldquo;Bud&amp;rdquo; Peterson&lt;br&gt;
President&lt;br&gt;
Georgia Institute of Technology&lt;/p&gt;
&lt;p&gt;Dear Dr. Peterson,&lt;/p&gt;
&lt;p&gt;It is with great pleasure that I inform you that &lt;strong&gt;David A. Bader&lt;/strong&gt; has been elected by the IEEE
Computer Society membership to the Society&amp;rsquo;s Board of Governors. Dr. Bader will begin his 3-year term on 1 January 2014. A press release with further election details can be found at
&lt;a href=&#34;http://www.computer.org/portal/web/pressroom/2013ElectionResults&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org/portal/web/pressroom/2013ElectionResults&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The largest of the 38 societies of the Institute of Electrical and Electronic Engineers, the IEEE
Computer Society is dedicated to advancing the theory and application of computer and
information-processing technology, and is known globally for its computing standards activities.&lt;/p&gt;
&lt;p&gt;The IEEE Computer Society&amp;rsquo;s Board of Governors sets the direction and determines the strategy for the Computer Society, and provides guidance at the policy level to all Society organizational entities.&lt;/p&gt;
&lt;p&gt;We appreciate the Georgia Institute of Technology&amp;rsquo;s support of Dr. Bader as he assumes this
important Computer Society leadership role.&lt;/p&gt;
&lt;p&gt;Sincerely&lt;/p&gt;
&lt;p&gt;David Alan Grier &lt;br&gt;
2013 President, IEEE Computer Society&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Computer Society election results</title>
      <link>http://localhost:1313/blog/20131010-ieeecs-results/</link>
      <pubDate>Thu, 10 Oct 2013 22:45:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20131010-ieeecs-results/</guid>
      <description>&lt;p&gt;Thomas M. Conte, first vice president for Publications and professor of Computer Science and Electrical and Computer Engineering at Georgia Institute of Technology, has been voted IEEE Computer Society 2014 president-elect.&lt;/p&gt;
&lt;p&gt;Conte, who will serve as 2015 president, garnered 4,035 votes, compared with 2,418 cast for Roger Fujii, 2013-2014 IEEE Division VIII Director and President, Fujii Systems. The president oversees IEEE-CS programs and operations and is a nonvoting member of most IEEE-CS program boards and
committees. Of the 6,526 ballots cast with an 11.56 percent turnout, 6,350 were submitted online and 175 by mail.&lt;/p&gt;
&lt;p&gt;In balloting for first vice president, Elizabeth (Liz) Burd, Second Vice President, Member and Geographic Activities, professor and Pro-Vice Chancellor at the University of Newcastle, Australia, garnered 4,909 votes, compared with 1,542 cast for Paul Croll, vice president for Technical and Conference Activities and a retired Fellow at CSC.&lt;/p&gt;
&lt;p&gt;James W. Moore, 2012-2013 IEEE Division V Director, who recently retired from the MITRE Corporation, was elected second vice president, with 3,789 votes, compared with 2,546 cast for Charlene (&amp;ldquo;Chuck&amp;rdquo;) Walrad, vice president for Standards Activities.&lt;/p&gt;
&lt;p&gt;The seven highest vote-getters for the 2014-2016 terms on the Board of Governors were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christina M. Schober, a Honeywell staff engineer and IEEE-CS Vice Treasurer and former Board of Governors member (3,706 votes);&lt;/li&gt;
&lt;li&gt;Jill I. Gostin, senior research scientist at the Georgia Tech Research Institute, IEEE Atlanta Section Chair, and IEEE-CS Atlanta Chapter Chair (3,523 votes);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor in Georgia Institute of Technology&amp;rsquo;s School of Computational Science and Engineering and executive director of High-Performance Computing, and Chair of the IEEE- CS Transactions Operations Committee and 2012 Awards Committee (3,482 votes);&lt;/li&gt;
&lt;li&gt;Dennis Frailey, adjunct computer science and engineering professor at Southern Methodist University, and a retired Raytheon principal fellow; and an IEEE-CS Board of Governors member and Educational Activities Board Vice Chair (3,218 votes);&lt;/li&gt;
&lt;li&gt;Rob Reilly, IEEE-CS Awards Committee member and MGA Vice Chair of Awards and Recognition (3,094 votes);&lt;/li&gt;
&lt;li&gt;Pierre Bourque, a professor at l&amp;rsquo;École de technologie supérieure of the Université du Québec, and an IEEE-CS Board of Governors member and lead editor of Version 3 of the Guide to the Software Engineering Body of Knowledge (SWEBOK) (3,049 votes); and&lt;/li&gt;
&lt;li&gt;Atsuhiro Goto, professor at the Institute of Information Security&amp;rsquo;s Graduate School of Information Security, and member of the IEEE-CS Board of Governors and Audit Committee (2,886 votes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Results for other Board of Governors candidates were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Harold Javid, Director of Regional Programs at Microsoft Research and IEEE-CS Board of Governors member and past Industry Advisory Board chair (2,881 votes).&lt;/li&gt;
&lt;li&gt;Paolo Montuschi, a computer engineering professor at Politecnico di Torino and IEEE-CS Board of Governors member and Magazine Operations Committee Chair (2,793 votes).&lt;/li&gt;
&lt;li&gt;Cheng-Chung (William) Chu, a computer science professor and R&amp;amp;D office dean and director of Software Engineering and Technologies Center at Tunghai University (2,503 votes).&lt;/li&gt;
&lt;li&gt;Aditya Rao, Senior Development Manager at Oracle and IEEE-CS Vice Chair of Membership Development and former MGA R10 representative (2,431 votes).&lt;/li&gt;
&lt;li&gt;Jian Pei, a computer science professor at Simon Fraser University and Editor in Chief of IEEE Transactions on Knowledge and Data Engineering and the IEEE Transactions on Emerging Topics in Computing steering committee. (2,351 votes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The seven candidates who received the most votes will assume seats on the board starting in January 2014. The 21 members serve rotating three-year terms in groups of seven.&lt;/p&gt;
&lt;p&gt;Candidates on the ballot are selected by the IEEE-CS Nominations Committee or by petition. The Nominations Committee accepts nominations from members until April of the current year and presents their nominations to the Board of Governors for final slate approval.&lt;/p&gt;
&lt;p&gt;Results will also be published in the December issue of Computer.&lt;/p&gt;
&lt;h4 id=&#34;about-ieee-computer-society&#34;&gt;About IEEE Computer Society&lt;/h4&gt;
&lt;p&gt;IEEE Computer Society is the world&amp;rsquo;s leading computing membership organization and the trusted information and career-development source for a global workforce of technology leaders including: professors, researchers, software engineers, IT professionals, employers, and students. The unmatched source for technology information, inspiration, and collaboration, IEEE Computer Society is the source that computing professionals trust to provide high-quality, state-of-the-art information on an on-demand basis. The Computer Society provides a wide range of forums for top minds to come together, including technical conferences, publications, and a comprehensive digital library, unique training webinars,professional training, and the TechLeader Training Partner Program to help organizations increase their staff&amp;rsquo;s technical knowledge and expertise. The Computer Society is the producer of Rock Stars of Big Data, the must-attend big data event of the year, and the personalized information tool, myComputer, now available at an introductory price. To find out more about the community for technology leaders, visit &lt;a href=&#34;http://www.computer.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.computer.org/portal/web/pressroom/2013ElectionResults&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org/portal/web/pressroom/2013ElectionResults&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data for Computational Science and Engineering</title>
      <link>http://localhost:1313/blog/20130907-insidehpc/</link>
      <pubDate>Sat, 07 Sep 2013 10:51:50 -0500</pubDate>
      <guid>http://localhost:1313/blog/20130907-insidehpc/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Doug Black&lt;/em&gt;&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/_XPAV2EqGGE?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;In this video from the SIAM Conference on Computational Science and Engineering, &lt;strong&gt;David Bader&lt;/strong&gt; from Georgia Tech and Tamara Kolda from Sandia discuss the significance of Big Data and the importance of mathematical modeling to make sense of and interpret all that data in various fields from social networks and epidemiology to climatology.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2013/09/big-data-computational-science-engineering/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2013/09/big-data-computational-science-engineering/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader named Editor-in-Chief of the IEEE Transactions on Parallel and Distributed Systems</title>
      <link>http://localhost:1313/blog/20130726-ieee-tpds/</link>
      <pubDate>Fri, 26 Jul 2013 17:38:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130726-ieee-tpds/</guid>
      <description>&lt;h2 id=&#34;new-editors-in-chief&#34;&gt;New Editors-in-Chief&lt;/h2&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20130726-ieee-tpds/dbader2007-small_hu_e1e99b7010b4ab39.webp 400w,
               /blog/20130726-ieee-tpds/dbader2007-small_hu_f1cf876bb5290620.webp 760w,
               /blog/20130726-ieee-tpds/dbader2007-small_hu_e79d8fa08bfa24e3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20130726-ieee-tpds/dbader2007-small_hu_e1e99b7010b4ab39.webp&#34;
               width=&#34;186&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor in the College of Computing at the Georgia Institute of Technology, has been named the new Editor-in-Chief of the &lt;em&gt;IEEE Transactions on Parallel and Distributed Systems&lt;/em&gt; starting in 2014.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://news.computer.org/ef1/preview_campaign.php?lf1=755533786d987916105340c10442888&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://news.computer.org/ef1/preview_campaign.php?lf1=755533786d987916105340c10442888&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPCwire Live! Atlanta’s Big Data Kick Off Week Meets HPC: What does the future holds for HPC?</title>
      <link>http://localhost:1313/blog/20130614-datanami/</link>
      <pubDate>Fri, 14 Jun 2013 13:55:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130614-datanami/</guid>
      <description>&lt;p&gt;Join HPCwire Editor Nicole Hemsoth and &lt;strong&gt;Dr. David Bader&lt;/strong&gt; from Georgia Tech as they take center stage on opening night at Atlanta’s first Big Data Kick Off Week, filmed in front of a live audience. Nicole and David look at the evolution of HPC, today’s big data challenges, discuss real world solutions, and reveal their predictions. Exactly what does the future holds for HPC?&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;speech-transcript&#34;&gt;Speech Transcript&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Moderator:&lt;/strong&gt;
I am really pleased to introduce our next speaker tonight and she is Nicole Hemsoth. She’s the editor and chief of HPCwire and for those of you who don’t know HPCWire was started in 1986. It covers the fastest computers in the world and the people who run them. She is also former managing editor of Datanami and the theme really is big data research and development and how it affects companies big and small. I would like to invite Nicole up here to talk to you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth (Founder and Sr. Contributing Editor Datanami):&lt;/strong&gt;
Thanks a lot to the organizers for having me and for this event. This is very exciting, also thanks for the CDC for doing that presentation. I have a little bit of a confession to make when I was invited here it was to present on trends in big data so a sort of macro view of all the different applications, infrastructure systems and what not. I went through and made a list of who are the vendors are and such. I realized how boring that is first of all you can get that information from any analysis firm and secondly the trend in big data if there is any such word is really converging. So I changed my entire presentation at the last minute, all of this is handwritten in two hours or so and partially because I met with a number of city leaders and others who talked about what big data means to the city, what it means to research institutions, like projects you are working on at the high performing learning center and so its too multilevel to offer one cohesive sense or view.&lt;/p&gt;
&lt;p&gt;What I do want to talk about is this trend of convergence and I want to start at the very top, which is where I generally live which is in high performance computing or supercomputing. Unless you are involved with it supercomputers sound like this esoteric thing, off limit technology that who actually uses supercomputing. In fact some of the most exciting data is from supercomputing so to bring this topic of convergence together, I’ll invite Dr. Bader back up to talk more about data intensive computing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
So high performance computing is as Nicole said use to be a niche of computing that where we design machines that would take up the size of this center. They would focus on niche problems from tracking severe storms, to securing our nuclear weapons, to designing energy efficient and safer cars. Now all of that super computing has been shrunk down to machines that either fit a cabinet, for example the size of your refrigerator at home or down into our laptops where we have technology such as multicore CPUs and GPUs. What has this done? It has created an exciting age where we have supercomputers that we can harness on our desktops and now as you know we have this data tsunami that we can process and mold together. So we’ve seen these machines that were once supercomputers really only accessible by the masses now being geared towards these data intensive problems. I know that is why many of you are here in the audience. I think it is quite an exciting time to look at these data intensive problems and found out how do these capabilities solve the real world challenges that we have day to day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
Thank you very much Dr. Bader. Dr. Bader is very well known in high performance computing. This year in June, You and I will be in Germany for the conference where the fastest computer is expected to be the Titan supercomputer, which is clocking in at around 17 pads. You have this amazing amount of processing power but processing power when it comes to data isn’t the whole story so you have these giant machines that are designed for capacity, also storage, and memory has been optimized for big data. The convergence from supercomputing is trickling down to mainstream computing but at the same time enterprise and web 2.0 driven technology like Facebook and Google like map reduce are finding their way up into supercomputing. So for the first time supercomputing isn’t telling all of technology what to do on the high end, there is a bottom up democratization of technology in a totally new way and it’s totally exciting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
I totally agree and we are seeing it at these exhibits like supercomputing shows where it is not only countries trying to harness these computers but businesses both large and small try to keep our credit transactions safe, how to make it a smarter planet, how to really go about our daily lives in a much more efficient way. It’s really a new set of problems that we have to solve.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
Again, it’s the reason why I changed my presentation again last night was listening to people who live in the computing world at different levels of the spectrum. People who work at Georgia Tech, people who work on some high level cloud systems, massive servers, and host applications, people who have smaller startups that do consumer services. One of the things that struck me was that Atlanta represents that pyramid from the highest level of computer use to that middle layer where fortune 500 companies exist to at the bottom where start ups use some of that new technology that uses mobile application. Atlanta serves all ends of computing where the highest levels and smallest levels of computing converge. I know that you are working on some really neat applications many that you cannot talk about, what do you think best represents a data intensive application? How it represents both of those worlds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
We do a lot of research here at GA Tech some of their projects support National Science Foundation endeavors in trying to understanding biology GENOME, other projects are geared towards trying to understand how social networks change over time so I think these types of applications are really a new view into a new application space and we have data sets available to sift against the machines that are coming out of IBM, Cray, Microprocessor designers, Intel and Nvidia. All have new technologies that allow us to get new information from that data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
Absolutely, I imagine that there may be a few questions. We may take two or three. Keep it focused on the highest level of computers and where it is filtering down to servers, cloud service companies and Telco’s. I am happy to field them.&lt;/p&gt;
&lt;p&gt;That’s an interested point so I believe that commodity vanilla hardware is the wave of the future and that is boring to an HPC person who gets bored over accelerators and GPUs but unfortunately we are moving towards these massive data centers that are basic community clusters that will be able to handle a broad range of applications because a lot of them are built on open source hardware/ software stack, all integrated stack that you can do anything with from mobile to advance life science applications, financial applications all on the same hardware. Hosted or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
A little bit future technology we are going to see a shift in the upcoming year are systems that are now more energy efficient. Moving everything up close to the processor and further away from the disk which gives us more energy efficiency and if we can reduce that memory so that that data and there are new memory technology so my view is that we look at 3 to 5 years. Keep an eye on what is happening on high bird memory consortium and other groups that are looking at how memory systems change and I think that it will greatly impact the big data solutions we see– as I said, 3 to 5 years from now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
I think to give a little bit of context on efficiency; this is the missing part of the conversation on big data. We just talked about the Cray Titan, we are talking about petascale the next step up is xscale. At this point with current technology even if you factor in 3d memory you are looking at needing a nuclear powered center to run these data centers. Clearly something needs to change.&lt;/p&gt;
&lt;p&gt;So these commercialization and privatization of HPC, I can point some awesome stuff if I had a Cray so I do believe that it is a long way away. One of the cool parts of that is that if we all have our cabinet in our kitchen or wherever we want to keep it, we can re-route the system so that it can warm your home. There is a well-known study where people were using their own in home data center to heat their home. There are ways to work that in there are ways to work that in. The affordability is there you can get an Intel 5 for an affordable price and put together your own research center in your home or turn to amazon or rent out some high level hardware in the cloud.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
Maybe we have time for one more question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
So HPC there are always debates about how you define HPC: do you define it by the applications, by the pure floating point performance and the way HPCWire as a magazine looks at it what is the purpose is, what degree is performance critical there. For a lot of big data applications it is a bad thing and performance would be perfect and despite every vendor telling you it is real time, this is probably not the case so there’s some merging of technology that needs to happen before they come together.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
It’s a moving target what I do is refer you to an analysis called the IDC that tracks the market and there’s some movement as to what defines an HPC resource vs. Data resource. I would point you there. When we talk about HPC we are normally talking about bleeding systems in computer technology and if you look at HPC what was a supercomputer one year and wait 7 to 10 years it would be the laptop on your desk. Think about it another way, how are you using your computer that you have maybe your iPod if you are to go back in time 10 years you would have had a super computer. So that’s the thought experiment what can you do today with the capabilities you have on your desk.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nicole Hemsoth:&lt;/strong&gt;
In terms of convergence HPC in big data pushing together are building that foundation for the new layer of supercomputers that will eventually be the next generation. That is why having this pure from the ground up where the top and bottom are working together to created this solid foundation technology is critical and such an exciting field. All aside big data big data is exciting for what it means for everyone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader:&lt;/strong&gt;
Thank you Nicole and thank you for listening to those presentations.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/multimedia/hpcwire_live_atlanta_s_big_data_kick_off_week_meets_hpc_what_does_the_future_holds_for_hpc_-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bit.ly/30EeycJ&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Type &#39;S&#39; for Suspicious</title>
      <link>http://localhost:1313/blog/20130613-foreignpolicy/</link>
      <pubDate>Thu, 13 Jun 2013 22:55:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130613-foreignpolicy/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Joshua E. Keating&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-jung-yeon-je--afp--getty-images&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;*Jung Yeon-Je / AFP / Getty Images*&#34; srcset=&#34;
               /blog/20130613-foreignpolicy/FP-image_hu_59a3038526b2ade7.webp 400w,
               /blog/20130613-foreignpolicy/FP-image_hu_f050b14f70c56c0e.webp 760w,
               /blog/20130613-foreignpolicy/FP-image_hu_bc0b830ce13a3d3c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20130613-foreignpolicy/FP-image_hu_59a3038526b2ade7.webp&#34;
               width=&#34;625&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;em&gt;Jung Yeon-Je / AFP / Getty Images&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Government-funded trolls. Decoy documents. Software that identifies you by how you type. Those are just a few of the methods the Pentagon has pursued in order to find the next Edward Snowden &lt;em&gt;before&lt;/em&gt; he leaks. The small problem, military-backed researchers tell &lt;code&gt;Foreign Policy&lt;/code&gt;, is that every spot-the-leaker solution creates almost as many headaches as it’s supposed to resolve.&lt;/p&gt;
&lt;p&gt;With more than &lt;a href=&#34;http://www.bloomberg.com/news/2013-06-12/top-secret-crate-packers-among-legions-hired-with-leaker.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1.4 million Americans&lt;/a&gt; holding top-secret clearance throughout a complex network of military, government, and private agencies, rooting out the next Snowden or Bradley Manning is a daunting task. But even before last week’s National Security Agency (NSA) revelations, the government was funding research to see whether there are telltale signs in the mountains of data that can help detect internal threats in advance.&lt;/p&gt;
&lt;p&gt;In the months following the WikiLeaks revelations, the Defense Advanced Research Projects Agency (DARPA) — the U.S. military’s &lt;a href=&#34;https://foreignpolicy.com/articles/2012/05/31/battleship_earth&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;far-out tech&lt;/a&gt; arm — put out a number of &lt;a href=&#34;http://www.wired.com/dangerroom/2010/05/darpa-wants-code-to-spot-anomalous-behavior-on-the-job/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;requests for research&lt;/a&gt; on methods to detect suspicious behavior in large datasets as a way to root out rogue actors like Manning (or in more extreme cases, ones like Fort Hood shooter Nidal Malik Hasan.)&lt;/p&gt;
&lt;p&gt;The most ambitious of these is known as Anomaly Detection at Multiple Scales (ADAMS), a program that as an October 2010 &lt;a href=&#34;https://www.fbo.gov/download/2f6/2f6289e99a0c04942bbd89ccf242fb4c/DARPA-BAA-11-04_ADAMS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;research request&lt;/a&gt; put it, is meant &amp;ldquo;to create, adapt and apply technology to the problem of anomaly characterization and detection in massive data sets.&amp;rdquo; The hope is that ADAMS would develop computers that could analyze a large set of user-generated data — the emails and data requests passing through an NSA office in Honolulu for instance — and learn to detect abnormal behavior in the system.&lt;/p&gt;
&lt;p&gt;The tricky part of this kind of analysis is not so much training a computer to detect aberrant behavior — there’s plenty of that going around on any large network — it’s training a computer what to ignore.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I like to use the example of learning to recognize the difference between reindeer and elk,&amp;rdquo; wrote Oregon State University computer scientist Tom Dietterich, who worked on developing &lt;a href=&#34;http://web.engr.oregonstate.edu/~tgd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;anomaly detection methods for ADAMS&lt;/a&gt;, in an email to &lt;strong&gt;Foreign Policy&lt;/strong&gt;. &amp;ldquo;If all I need to do is tell these species apart, I can focus on the size [of] their antlers and whether the antlers have velvety fur, and I don’t need to consider color. But if I only focus on these features, I won’t notice that Rudolph the Red-Nosed Reindeer is anomalous, because I’m ignoring color (and noses, for that matter). So in an anomaly detection system, it is important to consider any attribute (or behavior) that might possibly be relevant rather than trying to focus on a very few specific characteristics.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Over the past three years, DARPA has &lt;a href=&#34;http://www.dod.mil/pubs/foi/Science_and_Technology/DARPA/12-F-1039_2012-DARPA-Funding-List.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shelled out millions&lt;/a&gt; of dollars on efforts to learn how to root out Rudolphs from the rest of the reindeer and find out exactly what these red noses look like. This includes a &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$9 million award to &lt;strong&gt;Georgia Tech&lt;/strong&gt;&lt;/a&gt; to coordinate research on developing anomaly detection algorithms. You can peruse much of the research funded through ADAMS online. For instance, &lt;a href=&#34;http://info.publicintelligence.net/DARPA-ADAMS.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a proposal&lt;/a&gt; by the New York-based firm Allure Security Technology, &lt;a href=&#34;http://www.wired.com/dangerroom/2012/07/fog-computing/all/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;founded by&lt;/a&gt; a Columbia University computer science professor, calls for seeding government systems with &amp;ldquo;honeypot servers&amp;rdquo; and decoy documents meant to entice potential leakers to subversives. The files would alert administrators when accessed and allow the system to develop models for suspicious behavior. The company cheekily refers to this technique as &amp;ldquo;&lt;a href=&#34;http://ids.cs.columbia.edu/sites/default/files/Fog_Computing_Position_Paper_WRIT_2012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fog computing&lt;/a&gt;.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Another &lt;a href=&#34;http://reports-archive.adm.cs.cmu.edu/anon/2012/CMU-CS-12-100.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ADAMS-funded paper&lt;/a&gt; by Carnegie Mellon University computer scientist Kevin Killourhy looks at systems to &amp;ldquo;distinguish people based on their typing.&amp;rdquo; For instance, Killourhy explains, when three typists are asked to type the password &amp;ldquo;.tie5Roanl,&amp;rdquo; the three users can be easily identified by how long they hold down the &amp;ldquo;t&amp;rdquo; key. The paper suggests such technologies &amp;ldquo;could revolutionize insider-threat detection,&amp;rdquo; though unfortunately even the best systems can have an error rate of up to 63 percent, and detection can apparently be thrown off if the person just isn’t a very good typist. (Note to prospective whistle-blowers: Try two-finger typing.)&lt;/p&gt;
&lt;p&gt;Under the fairly obtuse title &amp;ldquo;&lt;a href=&#34;http://www.cs.cmu.edu/~htong/pdfs/sdm11_tong.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Non-Negative Residual Matrix Factorization with Application to Graph Anomaly Detection&lt;/a&gt;,&amp;rdquo; two DARPA-supported IBM researchers attempted to identify the kind of behaviors that might indicate suspicious behavior in a large network. These included &amp;ldquo;a connection between two nodes which belong to two remotely connected communities,&amp;rdquo; such as an author publishing a paper on a topic not normally associated with his or her research; &amp;ldquo;port-scanning like behavior,&amp;rdquo; which is when a particular IP address is receiving information from an unusually high number of other addresses; and &amp;ldquo;collusion,&amp;rdquo; such as a &amp;ldquo;group of users who always give good ratings to another group of users in order to artificially boost the reputation of the target group.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The thinking has gone somewhat beyond the theoretical level. &lt;a href=&#34;http://www.sei.cmu.edu/community/writ2013/writ2013-program.cfm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;At a conference in May&lt;/a&gt;, researchers from defense firm SAIC presented results from the &lt;strong&gt;PRODIGAL (Proactive Discovery of Insider Threats Using Graph Analysis and Learning) research team&lt;/strong&gt; — part of the overall ADAMS initiative — which tested a series of anomaly detection methods on an organization of approximately 5,500 users over the course of two months. &amp;ldquo;Red teams&amp;rdquo; were inserted into the data simulating characters such as a &amp;ldquo;saboteur,&amp;rdquo; an intellectual property thief, and a &amp;ldquo;rager&amp;rdquo; — someone prone to &amp;ldquo;strong, vociferous, abusive, and threatening language in email/Webmail/instant messages.&amp;rdquo; The detection methods varied widely in effectiveness.&lt;/p&gt;
&lt;p&gt;Such systems are clearly not yet up to the task of identifying a leaker before he or she strikes, and Dietterich, the Oregon State computer scientist, was cautious when asked whether they ever would be. &amp;ldquo;Anything I would say here would just be speculation, and artificial intelligence researchers have learned the painful lesson that we are very bad at predicting when, if, or how the methods we develop will be useful,&amp;rdquo; he stated.&lt;/p&gt;
&lt;p&gt;ADAMS may still be in the trial stage, but &amp;ldquo;insider threat&amp;rdquo; detection was clearly a major priority for the U.S. government even before last week. In October 2011, for instance, President Barack Obama signed an &lt;a href=&#34;http://www.whitehouse.gov/the-press-office/2011/10/07/executive-order-structural-reforms-improve-security-classified-networks-&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;executive order&lt;/a&gt; calling for the creation of an interagency Insider Threat Task Force charged with the &amp;ldquo;safeguarding of classified information from exploitation, compromise, or other unauthorized disclosure.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Snowden affair will no doubt only accelerate efforts t
o combat these threats. Of course, if these efforts included high-tech initiatives along the lines of ADAMS, it would be somewhat ironic, as this type of big-data analysis is a &lt;a href=&#34;http://ideas.foreignpolicy.com/posts/2013/06/11/the_academic_paper_that_predicted_the_nsa_scandal?_ga=2.215071831.1715373072.1566269586-2062637566.1566269586&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;not-so-distant cousin&lt;/a&gt; of the much larger surveillance programs that Snowden sought to expose. This type of analysis may alarm &lt;a href=&#34;http://www.theregister.co.uk/2010/05/19/darpa_smite/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;privacy advocates&lt;/a&gt;, but in the end, the idea of intelligence agencies developing a vast high-tech data-surveillance program to prevent anyone from learning about an even vaster high-tech data-surveillance program feels a little more &lt;em&gt;Catch-22&lt;/em&gt; than &lt;em&gt;1984&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://foreignpolicy.com/2013/06/13/type-s-for-suspicious/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://foreignpolicy.com/2013/06/13/type-s-for-suspicious/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Six Can’t Miss Sessions for ISC’13</title>
      <link>http://localhost:1313/blog/20130613-hpcwire/</link>
      <pubDate>Thu, 13 Jun 2013 17:01:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130613-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Nicole Hemsoth&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Outside of the main attractions, including the keynote sessions, vendor showdowns, Think Tank panels, BoFs, and tutorial elements, the International Supercomputing Conference has balanced its five-day agenda with some striking panels, discussions and topic areas that are worthy of some attention.&lt;/p&gt;
&lt;p&gt;We scoured the agenda in search of the sessions we thought would strike the most resonant chords with the diverse mix of user, vendor and researcher attendees and compiled them here. Even if you’re not able to make the trip to Germany next week, the list offers a sense of what the community has found valuable enough to dedicate exclusive attention to.&lt;/p&gt;
&lt;p&gt;For those of you who are heading out, stop by and see us – and use this guide to help you wrangle some last-minute schedule scratching. Let’s begin – in no particular order….&lt;/p&gt;
&lt;h3 id=&#34;solving-hpcs-energy-challenges-no-stone-left-unturned&#34;&gt;Solving HPC’s Energy Challenges: No Stone Left Unturned&lt;/h3&gt;
&lt;p&gt;The growing energy demands of high performance computing systems require new approaches across the energy ecosystem. From data center design, to system cooling and power distribution, all the way down to the individual HPC applications – researchers and engineers are bringing a wealth of new ideas and technologies to help the address these challenges.&lt;/p&gt;
&lt;p&gt;This hour-long session will bring together a number of experts in the field of energy efficient HPC, including researchers from Hewlett-Packard, NREL, NCAR and UC San Diego, all of whom will address individual topics under this umbrella with time at the end for questions.&lt;/p&gt;
&lt;p&gt;More info here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=361&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=361&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supercomputing-and-the-human-brain-project&#34;&gt;Supercomputing and the Human Brain Project&lt;/h3&gt;
&lt;p&gt;Some of the most recognized leaders working on the vast Human Brain Project will be onhand at ISC 13 to discuss the many computational challenges such an undertaking involves. From defining a roadmap to looking at new hardware and software technologies powering the effort, this hour-long session is sure to draw some serious attention.&lt;/p&gt;
&lt;p&gt;The goal of the Human Brain Project is to pull together all our existing knowledge about the human brain and to reconstruct the brain, piece by piece, in supercomputer-based models and simulations. The models offer the prospect of a new understanding of the human brain and its diseases and of completely new computing and robotic technologies. Central to the Human Brain Project is Information and Computing Technology (ICT).&lt;/p&gt;
&lt;p&gt;The project will develop ICT platforms for neuroinformatics, brain simulation and supercomputing that will make it possible to federate neuroscience data from all over the world, to integrate the data in unifying models and simulations of the brain, to check the models against data from biology and to make them available to the world scientific community. Exploiting the resulting knowledge on the architecture and circuitry of the brain will allow researchers to develop new computing systems..&lt;/p&gt;
&lt;p&gt;Starting in 2013, the European Commission will support this vision through the new FET Flagship Program. Federating more than 80 European and international research institutions, the Human Brain Project is planned to last ten years and estimated to cost an estimated EUR 1,190 million. The series of yearly sessions at ISC aims at presenting the challenge and the progress and to provide a platform for engaging the community of High Performance Computing and beyond.&lt;/p&gt;
&lt;p&gt;More info about this session here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=362&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=362&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;panel-file-systems-for-supercomputers-challenges-for-the-future&#34;&gt;Panel: File Systems for Supercomputers: Challenges for the Future&lt;/h3&gt;
&lt;p&gt;Some of the foremost leaders in high performance computing file system development from organizations like Intel-Whamcloud, EPFL, IBM, Xyratex and CEA will spend an hour discussing the next generation of file systems for top systems.&lt;/p&gt;
&lt;p&gt;As the organizers note, parallel file systems are an indispensable part of the operation of modern Supercomputers where hundreds of thousands of processor cores work on the same (simulation) challenges. At the same time they are probably the most complicated pieces of system software for High-Performance Systems and often a single point of failure when any part of the system hardware does not quite work as expected (especially the internal network or storage controllers).&lt;/p&gt;
&lt;p&gt;The I/O procedures have to be very carefully planned by the developers and designers of such file systems as well as by the users who must deal with total different means as before to get reasonable performance. The panel will address these and other issues in file system development for the era of exascale.&lt;/p&gt;
&lt;p&gt;More info about this file systems panel here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=359&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=359&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;overview-of-the-sme-market&#34;&gt;Overview of the SME Market&lt;/h3&gt;
&lt;p&gt;Earl Joseph from IDC and Max Lemke from the European Commission will provide two presentations in their hour long-session around the potential for HPC to infiltrate into the “missing middle” markets.&lt;/p&gt;
&lt;p&gt;Joseph will focus on the market and its needs as a whole while Max Lemke will talk about how to bring simulation and HPC into broader reach in manufacturing.&lt;/p&gt;
&lt;p&gt;The HPC system vendors refer to the SME/SMB HPC market as the “missing middle” and public funding bodies like the European Commission have setup specific programs for increasing the use of HPC in small and medium businesses. They are doing so because it is well recognized that innovation and competitiveness cannot be maintained in most markets without intensifying the use of HPC and because the bulk of industrial research and production actually comes from small and medium businesses.&lt;/p&gt;
&lt;p&gt;This panel will question why is it not evident for SMEs/SMBs to to invest in HPC, what keeps them from doing so even when they recognize the need. They will also question public funding is required and why HPC system vendors have apparent difficulties in penetrating this market.&lt;/p&gt;
&lt;p&gt;More info can be found here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=379&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=379&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;the-role-of-hpc-in-the-oil--gas-industry&#34;&gt;The Role of HPC in the Oil &amp;amp; Gas Industry&lt;/h3&gt;
&lt;p&gt;What better way to understand the needs and concerns of the oil and gas industry than to gather some of its technology end users together to discuss in a series of presentations? This session will bring together Keith Gray, from BP Upstream IT&amp;amp;S, Muhammad Soofi from Saudi Aramco, and Detlef Hohl from Shell International Exploration and Production.&lt;/p&gt;
&lt;p&gt;The presenters will explore a number of topics, including how HPC operations function at a large oil company (BP), how new technologies are being used (Saudi Aramco) and the numerous computational challenges of the industry, particularly in terms of seismic processing and simulation (Shell).&lt;/p&gt;
&lt;p&gt;More information about this Wednesday session can be found here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=404&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=404&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;better-understanding-brains-genomes--life-using-hpc-systems&#34;&gt;Better Understanding Brains, Genomes &amp;amp; Life Using HPC Systems&lt;/h3&gt;
&lt;p&gt;One of the most important real-world grand challenges is the understanding of complex biological systems. New technologies have allowed the sequencing of genomes at unprecedented rates and volumes and new sensors allow for the scanning and imaging of living beings. The challenges are immense, in terms of both computational requirements and data intensity, and require the use of high-performance computing and big data technologies for tackling these challenges. This session will explore three areas in medical and life sciences; i.e., analyzing massive collections of genomic sequences, scalable life simulation, and brain-scale neuronal networks. The session will encourage the dialog between the HPC and Big Data technologists and the life sciences application developers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; from Georgia Institute of Technology will chair the session and oversee presentations from some of the leading genetic researchers and organizations, including experts from BGI, ScalaLife and INM/IAS Julich and RWTH.&lt;/p&gt;
&lt;p&gt;More information about this session can be found here: &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=407&amp;amp;a=select&amp;amp;ra=byday&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;o=407&amp;a=select&amp;ra=byday&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2013/06/13/six_can_t_miss_sessions_for_isc_13/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2013/06/13/six_can_t_miss_sessions_for_isc_13/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Human Condition with Big Data and HPC</title>
      <link>http://localhost:1313/blog/20130528-isc13/</link>
      <pubDate>Tue, 28 May 2013 13:31:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130528-isc13/</guid>
      <description>&lt;p&gt;In this guest feature from &lt;a href=&#34;http://www.scientific-computing.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scientific Computing World&lt;/a&gt;, Georgia Institute of Technology’s &lt;strong&gt;David A. Bader&lt;/strong&gt; discusses his upcoming &lt;a href=&#34;http://www.isc-events.com/isc13/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ISC’13&lt;/a&gt; session, &lt;a href=&#34;http://www.isc-events.com/isc13_ap/sessiondetails.php?t=event&amp;amp;o=407&amp;amp;a=select&amp;amp;ra=index&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Better Understanding Brains, Genomes &amp;amp; Life Using HPC Systems&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Supercomputing at ISC has traditionally focused on problems in areas such as the simulation space for physical phenomena. Manufacturing, weather simulations and molecular dynamics have all been popular topics, but an emerging trend is the examination of how we use high-end computing to solve some of the most important problems that affect the human condition.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Prompted by &lt;a href=&#34;http://en.wikipedia.org/wiki/Hans_Meuer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hans Meuer&lt;/a&gt;, general chairman of ISC, we decided to put together a session that asks how supercomputing is enabling the study of life, and by this we mean answering questions such as how life evolves, how we go from genotype to phenotype, and how our brains function. At the moment, brain function, for example, remains a mystery but I do believe that we will begin to make progress in the next 10 to 20 years. It truly is an exciting time for life sciences research, and we have brought together three speakers that will explore three different axes of computational biology and genomics.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The first speaker, &lt;a href=&#34;http://www.isc-events.com/isc13_ap/speakerdetails.php?t=speaker&amp;amp;o=12512&amp;amp;a=select&amp;amp;ra=sessiondetails&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr BingQiang Wang&lt;/a&gt;, is head of High Performance Computing, BGI Research at the Beijing Genome Institute, and he will begin the session by talking about bioinformatics and how we take the massive scale sequencing of genomes and use that intensive architecture to do the problems related to aligning sequences at extreme scale. In addition, it will focus on giving tools to the analysts and accelerate computations using GPUs. Essentially, his is a Big Data talk on the first aspect of biology, the study of sequences.&lt;/p&gt;
&lt;p&gt;Our second speaker, &lt;a href=&#34;http://www.isc-events.com/isc13_ap/speakerdetails.php?t=speaker&amp;amp;o=12513&amp;amp;a=select&amp;amp;ra=sessiondetails&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr Rossen Apostolov&lt;/a&gt;, comes from the KTH Royal Institute of Technology, Sweden, and is technical director of the EU-funded project &lt;a href=&#34;http://www.scalalife.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ScalaLife&lt;/a&gt;. We thought that ScalaLife was a very interesting project to preview at ISC because it’s taking a lot of simulation aspects in life sciences and then bringing HPC to bear in tackling these problems. The project brings together multi-physics simulations between molecular dynamics, quantum mechanics, and discrete optimisation, and then uses these different computational disciplines to solve the simulation of life itself. This will be a fascinating talk because it’s going to connect these unique types of simulation into a unified framework, again supported by the EU, for the simulation of living organisms – an impossible task without the use of supercomputing resources.&lt;/p&gt;
&lt;p&gt;The third presentation will be given by &lt;a href=&#34;http://www.isc-events.com/isc13_ap/speakerdetails.php?t=speaker&amp;amp;o=12538&amp;amp;a=select&amp;amp;ra=sessiondetails&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Dr Markus Diesmann&lt;/a&gt;, director of the Institute of Neuroscience and Medicine, Computational and Systems Neuroscience (INM-6) and the Institute for Advanced Simulation, Theoretical Neuroscience (IAS-6) at the Jülich Research Centre, and Professor for Computational Neuroscience at the Medical Faculty of RWTH Aachen University. His talk will focus on some of the most challenging simulations of the collection of neurons that make up the brain, and will fit in with the need for massive supercomputers that can do these full-scale simulations. Diesmann leads a European effort to develop the algorithms, the simulation technology, the validation for both the brain imaging and scanning, along with the computational methods to aid our understanding of how the brain works. This is so critical because if we knew how the brain functioned we’d be able to better help the population in dealing with some of issues that affect it.&lt;/p&gt;
&lt;p&gt;By the end of the session we hope to have informed the international supercomputing community that biology and molecular science are hot topics for applications. The most important point is that it will be a two-way conversation, bringing these communities together and giving system designers and vendors a better idea of how to make supercomputing systems that will be more efficient for tackling these types of challenges.&lt;/p&gt;
&lt;p&gt;The hallmark of all the problems we will be exploring is that their efficient solutions require high-end computing systems and Big Data platforms. We really need to bring together supercomputers and Big Data storage systems to tackle what I feel is the holy grail of computing: the understanding of life.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.isc-events.com/isc13_ap/eventdetails.php?t=event&amp;amp;o=407&amp;amp;a=select&amp;amp;ra=index&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Better Understanding Brains, Genomes &amp;amp; Life Using HPC Systems&lt;/a&gt;  &lt;br&gt;
Wednesday, 19 June 2013 &lt;br&gt;
9am – 10.30am &lt;br&gt;
Hall 2, CCL – Congress Center Leipzig&lt;/p&gt;
&lt;p&gt;This story appears here as part of a &lt;a href=&#34;http://insidehpc.com/2012/02/16/scientific-computing-world-and-insidehpc-announce-cross-publishing-agreement-2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cross-publishing agreement&lt;/a&gt; with &lt;a href=&#34;http://www.scientific-computing.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scientific Computing World&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2013/05/understanding-the-human-condition-with-big-data-and-hpc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2013/05/understanding-the-human-condition-with-big-data-and-hpc/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech has High Participation in IPDPS2013 Technical Program</title>
      <link>http://localhost:1313/blog/20130524-ipdps/</link>
      <pubDate>Fri, 24 May 2013 15:26:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130524-ipdps/</guid>
      <description>&lt;p&gt;Georgia Tech&amp;rsquo;s leadership in education and research came through clearly at the 27th IEEE International Parallel &amp;amp; Distributed Processing Symposium (IPDPS2013) in Cambridge, MA the week of May 19-24, 2013. IPDPS accepted only 22% of submissions, and Georgia Tech&amp;rsquo;s Schools of Computational Science and Engineering, Computer Science, and Electrical and Computer Engineering participated in nine of the total 108 papers and one of the 23 PhD posters. Four of the associated and well-known workshops contained Georgia Tech research. One graduating student presented his thesis research at the IPDPS PhD forum. In addition, &lt;strong&gt;David A. Bader&lt;/strong&gt; was recognized as the recipient of this year’s IEEE Computer Society’s Technical Committee on Parallel Processing (TCPP) Outstanding Service Award.&lt;/p&gt;
&lt;h3 id=&#34;papers-9-papers-of-the-108-with-220-acceptance-rate&#34;&gt;Papers: (9 papers of the 108, with 22.0% acceptance rate)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Optimizing Checkpoints Using NVM as Virtual Memory. Sudarsun Kannan (Georgia Institute of Technology, USA); Ada Gavrilovska (Georgia Institute of Technology, USA); Karsten Schwan (Georgia Tech, USA); Dejan Milojicic (HP Labs, USA)&lt;/li&gt;
&lt;li&gt;FlexIO: I/O Middleware for Location-Flexible Scientific Data Analytics. Fang Zheng (Georgia Tech, USA); Hongbo Zou (Georgia Institute of Technology, USA); Greg Eisenhauer (Georgia Institute of Technology, USA); Karsten Schwan (Georgia Tech, USA); Matthew Wolf (Georgia Institute of Technology, USA); Jai Dayal (Georgia Institute of Technology, USA); Tuan-Anh Nguyen (Georgia Institute of Technology, USA); Jianting Cao (Beihang University, P.R. China); Mohammad Abbasi (Georgia Insitute of Technology, USA); Scott Klasky (Oak Ridge National Laboratory, USA); Norbert Podhorszki (Oak Ridge National Laboratory, USA); Hongfeng Yu (Sandia National Laborotories, USA)&lt;/li&gt;
&lt;li&gt;iBridge: Improving Unaligned Parallel File Access with Solid-State Drives. Xuechen Zhang (Georgia Institute of Technology, USA); Ke Liu (Wayne State University, USA); Kei Davis (Los Alamos National Laboratory, USA); Song Jiang (Wayne State University, USA)&lt;/li&gt;
&lt;li&gt;Energy-Efficient Scheduling for Best-Effort Interactive Services to Achieve High Response Quality. Zhihui Du (Tsinghua University, P.R. China); Hongyang Sun (Nanyang Technological University, Singapore); Yuxiong He (Microsoft Research, USA); Yu He (Tsinghua Univiversity, P.R. China); &lt;strong&gt;David A. Bader&lt;/strong&gt; (Georgia Institute of Technology, USA); Huangzhe Zhang (Beijing University of Post and Telecommunication, P.R. China)&lt;/li&gt;
&lt;li&gt;A roofline model of energy. Jee Choi (Georgia Institute of Technology, USA); Richard W Vuduc (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;li&gt;A theoretical framework for algorithm-architecture co-design. Kenneth Czechowski (Georgia Institute of Technology, USA); Richard W Vuduc (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;li&gt;Best Paper Finalist: Extending the Generality of Molecular Dynamics Simulations on a Special-Purpose Machine. Daniele Scarpazza (D. E. Shaw Research, USA); Douglas Ierardi (D. E. Shaw Research, USA); Adam Lerer (D. E. Shaw Research, USA); Kenneth Mackenzie (D. E. Shaw Research, USA); Albert Pan (D. E. Shaw Research, USA); Joseph Bank (D. E. Shaw Research, USA); Edmond Chow (Georgia Institute of Technology, USA); Ron Dror (D. E. Shaw Research, USA); Jp Grossman (D. E. Shaw Research, USA); Daniel Killebrew (D. E. Shaw Research, USA); Mark Moraes (D. E. Shaw Research, USA); Cristian Predescu (D. E. Shaw Research, USA); John Salmon (D. E. Shaw Research, USA); David Shaw (D. E. Shaw Research, USA)&lt;/li&gt;
&lt;li&gt;Cura: A Cost-optimized Model for MapReduce in a Cloud. Balaji Palanisamy (Georgia Institute of Technology, USA); Aameek Singh (IBM Almaden Research Center, USA); Ling Liu (Georgia Tech, USA); Bryan Langston (IBM Research, Almaden, USA)&lt;/li&gt;
&lt;li&gt;Optimizing Checkpoints Using NVM as Virtual Memory. Sudarsun Kannan (Georgia Institute of Technology, USA); Ada Gavrilovska (Georgia Institute of Technology, USA); Karsten Schwan (Georgia Tech, USA); Dejan Milojicic (HP Labs, USA)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ipdps-2013-panel-on-big-data-in-10-years&#34;&gt;IPDPS 2013 Panel on Big Data in 10 Years&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Panelist&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;phd-forum&#34;&gt;Ph.D. Forum&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Designing Hybrid Architectures for Massive-Scale Graph Analysis. David Ediger; &lt;strong&gt;David A. Bader&lt;/strong&gt; (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;keynote&#34;&gt;Keynote:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Rich Vuduc, &amp;ldquo;What first principles of algorithms and architectures says about heterogeneity.&amp;rdquo; The Third International Workshop on Accelerators and Hybrid Exascale Systems (AsHES)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ipdps-2013-organization&#34;&gt;IPDPS 2013 Organization:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Steering Committee&lt;/li&gt;
&lt;li&gt;Bo Hong, PhD Forum Co-Chair&lt;/li&gt;
&lt;li&gt;Program Committee for IPDPS:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Bo Hong&lt;/li&gt;
&lt;li&gt;Santosh Pande&lt;/li&gt;
&lt;li&gt;Jason Riedy&lt;/li&gt;
&lt;li&gt;Jeffrey Vetter&lt;/li&gt;
&lt;li&gt;Rich Vuduc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; and Srinivas Aluru, Co-Chairs, 12th IEEE International Workshop on High Performance Computational Biology (HiCOMB 2013)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Program Committee, The Third International Workshop on Accelerators and Hybrid Exascale Systems (AsHES)&lt;/li&gt;
&lt;li&gt;Program Committee for the Workshop on Multithreaded Architectures and Applications (MTAAP 2013):
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Bo Hong&lt;/li&gt;
&lt;li&gt;Jeffrey Vetter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;workshops&#34;&gt;Workshops:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;HCW 2013: 22nd International Heterogeneity in Computing Workshop
&lt;ul&gt;
&lt;li&gt;Brawny vs. Wimpy: Evaluation and Analysis of Modern Workloads on Heterogeneous Processors. Vishal Gupta (Georgia Institute of Technology, USA), Karsten Schwan (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;12th IEEE International Workshop on High Performance Computational Biology (HiCOMB 2013)
&lt;ul&gt;
&lt;li&gt;HPC Software Libraries for Next-Gen Sequencing Analytics. Srinivas Aluru.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The Ninth Workshop on High-Performance, Power-Aware Computing (HPPAC)
&lt;ul&gt;
&lt;li&gt;PowerTune: Differentiated Power Allocation in Over-provisioned Multicore Systems. Vishal Gupta and Karsten Schwan&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Workshop on Multithreaded Architectures and Applications (MTAAP 2013)
&lt;ul&gt;
&lt;li&gt;CHiP: A Profiler to Measure the effect of Cache Contention on Scalability. Bevin Brett (Intel Corporation, USA); Pranith Kumar (Georgia Institute of Technology, USA); Minjang Kim (Georgia Institute of Technology, USA); Hyesoon Kim (Georgia Tech, USA)&lt;/li&gt;
&lt;li&gt;Investigating Graph Algorithms in the BSP Model on the Cray XMT. David Ediger (Georgia Institute of Technology, USA); &lt;strong&gt;David A. Bader&lt;/strong&gt; (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;li&gt;Multithreaded Community Monitoring for Massive Streaming Graph Data. Jason Riedy (Georgia Institute of Technology, USA); &lt;strong&gt;David A. Bader&lt;/strong&gt; (Georgia Institute of Technology, USA)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://hg.gatech.edu/node/215031&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hg.gatech.edu/node/215031&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The human condition</title>
      <link>http://localhost:1313/blog/20130524-isc13/</link>
      <pubDate>Fri, 24 May 2013 13:49:59 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130524-isc13/</guid>
      <description>&lt;p&gt;Georgia Institute of Technology’s &lt;strong&gt;David A. Bader&lt;/strong&gt; discusses his upcoming ISC’13 session, &lt;em&gt;Better Understanding Brains, Genomes &amp;amp; Life Using HPC Systems&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Supercomputing at ISC has traditionally focused on problems in areas such as the simulation space for physical phenomena. Manufacturing, weather simulations and molecular dynamics have all been popular topics, but an emerging trend is the examination of how we use high-end computing to solve some of the most important problems that affect the human condition.&lt;/p&gt;
&lt;p&gt;Prompted by Hans Meuer, general chairman of ISC, we decided to put together a session that asks how supercomputing is enabling the study of life, and by this we mean answering questions such as how life evolves, how we go from genotype to phenotype, and how our brains function. At the moment, brain function, for example, remains a mystery but I do believe that we will begin to make progress in the next 10 to 20 years. It truly is an exciting time for life sciences research, and we have brought together three speakers that will explore three different axes of computational biology and genomics.&lt;/p&gt;
&lt;p&gt;The first speaker, Dr BingQiang Wang, is head of High Performance Computing, BGI Research at the Beijing Genome Institute, and he will begin the session by talking about bioinformatics and how we take the massive scale sequencing of genomes and use that intensive architecture to do the problems related to aligning sequences at extreme scale. In addition, it will focus on giving tools to the analysts and accelerate computations using GPUs. Essentially, his is a Big Data talk on the first aspect of biology, the study of sequences.&lt;/p&gt;
&lt;p&gt;Our second speaker, Dr Rossen Apostolov, comes from the KTH Royal Institute of Technology, Sweden, and is technical director of the EU-funded project ScalaLife. We thought that ScalaLife was a very interesting project to preview at ISC because it’s taking a lot of simulation aspects in life sciences and then bringing HPC to bear in tackling these problems. The project brings together multi-physics simulations between molecular dynamics, quantum mechanics, and discrete optimisation, and then uses these different computational disciplines to solve the simulation of life itself. This will be a fascinating talk because it’s going to connect these unique types of simulation into a unified framework, again supported by the EU, for the simulation of living organisms – an impossible task without the use of supercomputing resources.&lt;/p&gt;
&lt;p&gt;The third presentation will be given by Prof. Dr Markus Diesmann, director of the Institute of Neuroscience and Medicine, Computational and Systems Neuroscience (INM-6) and the Institute for Advanced Simulation, Theoretical Neuroscience (IAS-6) at the Jülich Research Centre, and Professor for Computational Neuroscience at the Medical Faculty of RWTH Aachen University. His talk will focus on some of the most challenging simulations of the collection of neurons that make up the brain, and will fit in with the need for massive supercomputers that can do these full-scale simulations. Diesmann leads a European effort to develop the algorithms, the simulation technology, the validation for both the brain imaging and scanning, along with the computational methods to aid our understanding of how the brain works. This is so critical because if we knew how the brain functioned we’d be able to better help the population in dealing with some of issues that affect it.&lt;/p&gt;
&lt;p&gt;By the end of the session we hope to have informed the international supercomputing community that biology and molecular science are hot topics for applications. The most important point is that it will be a two-way conversation, bringing these communities together and giving system designers and vendors a better idea of how to make supercomputing systems that will be more efficient for tackling these types of challenges.&lt;/p&gt;
&lt;p&gt;The hallmark of all the problems we will be exploring is that their efficient solutions require high-end computing systems and Big Data platforms. We really need to bring together supercomputers and Big Data storage systems to tackle what I feel is the holy grail of computing: the understanding of life.&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Better Understanding Brains, Genomes &amp;amp; Life Using HPC Systems&lt;/em&gt; &lt;br&gt;
Wednesday, 19 June 2013 &lt;br&gt;
9am - 10.30am &lt;br&gt;
Hall 2, CCL - Congress Center Leipzig&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.scientific-computing.com/analysis-opinion/human-condition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.scientific-computing.com/analysis-opinion/human-condition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TCPP Outstanding Service Award</title>
      <link>http://localhost:1313/blog/20130522-tcpp/</link>
      <pubDate>Wed, 22 May 2013 15:20:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130522-tcpp/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20130522-tcpp/award_hu_c6a66bca31d16e2e.webp 400w,
               /blog/20130522-tcpp/award_hu_b1f51e0acd3f7ef1.webp 760w,
               /blog/20130522-tcpp/award_hu_dc9a6f4151c687ad.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20130522-tcpp/award_hu_c6a66bca31d16e2e.webp&#34;
               width=&#34;722&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;IEEE Computer Society Technical Committee on Parallel Processing (TCPP) presents the 2013 TCPP Outstanding Service Award to &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Jose Rolim&lt;br&gt;
Chair, Award Committee&lt;/p&gt;
&lt;p&gt;Ajay Gupta&lt;br&gt;
Chair, IEEE-CS TCPP&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;This is to recognize those individuals in the broader community who have had major professional service roles in conferences (TCPP and others), journals, various committees, major events, community resources, and international outreach, and those who have had a major impact on the community at large in possibly other ways.&lt;/p&gt;
&lt;p&gt;2013 Winner: &lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Institute of Technology&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://tc.computer.org/tcpp/awards/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://tc.computer.org/tcpp/awards/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CRA Announces Four New Board Members</title>
      <link>http://localhost:1313/blog/20130201-cra-board/</link>
      <pubDate>Fri, 01 Feb 2013 06:51:25 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130201-cra-board/</guid>
      <description>&lt;p&gt;There are four new additions to the CRA Board of Directors. &lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Institute of Technology, is the new IEEE-CS Representative. Julia Hirschberg, Columbia University, and  P. Takis Metaxas, Wellesley College, replace members who resigned, and their terms end June 30, 2014.
Henry Kautz, University of Rochester, is the new AAAI representative.&lt;/p&gt;
&lt;h4 id=&#34;david-bader&#34;&gt;David Bader&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Full Professor in the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director for High Performance Computing. Dr. Bader is a lead scientist in the DARPA Ubiquitous High Performance Computing (UHPC) program. He received his Ph.D. in 1996 from The University of Maryland, and his research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE. He was elected as chair of the IEEE Computer Society Technical Committee on Parallel Processing (TCPP) and as chair of the SIAM Activity Group in Supercomputing (SIAG/SC). Bader is a Fellow of the IEEE and AAAS, a National Science Foundation CAREER Award recipient, and has received numerous industrial awards. Bader is a co-founder of the Graph500 List for benchmarking “Big Data” computing platforms. Bader is recognized as a “RockStar” of High Performance Computing by InsideHPC and as HPCwire’s People to Watch in 2012&lt;/p&gt;
&lt;h4 id=&#34;julia-hirschberg&#34;&gt;Julia Hirschberg&lt;/h4&gt;
&lt;p&gt;Julia Hirschberg is a professor in the Department of Computer Science at Columbia University. She received her Ph.D. in Computer Science from the University of Pennsylvania, after completing a Ph.D. in sixteenth-century Mexican social history at the University of Michigan and teaching history at Smith. She worked at Bell Laboratories and AT&amp;amp;T Laboratories — Research from 1985-2003, creating the Human-Computer Interface Research Department there. She served as editor-in-chief of Computational Linguistics and  Speech Communication. She was on the Executive Board of the Association for Computational Linguistics (ACL), has been on the Permanent Council of International Conference on Spoken Language Processing (ICSLP) since 1996, and served on the board of the International Speech Communication Association (ISCA) from. She is currently on the board of the CRA-W and has been active in working for diversity at AT&amp;amp;T and at Columbia. She is a Fellow of the American Association for Artificial Intelligence, an ISCA Fellow, and an ACL Fellow. She received an Honorary Doctorate (Hedersdoktor) from KTH, a Columbia Engineering School Alumni Association (CESAA) Distinguished Faculty Teaching Award in 2009, the IEEE James L. Flanagan Speech and Audio Processing Award and the ISCA Medal for Scientific Achievement in 2011.&lt;/p&gt;
&lt;h4 id=&#34;henry-kautz&#34;&gt;Henry Kautz&lt;/h4&gt;
&lt;p&gt;Henry Kautz is Professor Chair of the Department of Computer Science at the University of Rochester. He performs research in knowledge representation, machine learning, pervasive computing, and assistive technology. His academic degrees include an A.B. in mathematics from Cornell University, an M.A. in Creative Writing from the Johns Hopkins University, an M.Sc. in Computer Science from the University of Toronto, and a Ph.D. in computer science from the University of Rochester. He was a researcher and department head at Bell Labs and AT&amp;amp;T Laboratories until becoming a Professor in the Department of Computer Science and Engineering of the University of Washington in 2000. He joined University of Rochester in 2006. He was President (2010-2012) of the Association for the Advancement of Artificial Intelligence, and is a Fellow of the AAAI, a Fellow of the American Association for the Advancement of Science, a recipient of the IJCAI Computers and Thought Award.&lt;/p&gt;
&lt;h4 id=&#34;p-takis-metaxas&#34;&gt;P. Takis Metaxas&lt;/h4&gt;
&lt;p&gt;P. Takis Metaxas studied Mathematics at the University of Athens and Computer Science at Brown University. He received his Masters of Science and Ph. D. in Computer Science from Dartmouth, and has been a visiting scientist at MIT and the Sydney University in Australia. He is currently a Computer Science professor at Wellesley College, where he founded the Media Arts and Sciences program, and also at Harvard, where he is a Visiting Scholar and Affiliate at the Center for Research on Computation and Society (CRCS).  Metaxas is a senior member of the ACM, and a member of LACS, IEEE Computer Society, SIGWEB, SIGCSE and SIGACT’s Electronic Publication Board. He is on the Advisory Board of XRDS and a Guest Editor for the special issue of the Internet Research Journal on “The Power of Prediction with Social Media” and of the Special Issue of the Journal “Kunstliche Intelligenz KI” on Social Media.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cra.org/crn/2013/02/cra_announces_four_new_board_members/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cra.org/crn/2013/02/cra_announces_four_new_board_members/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Era of Big Analytics</title>
      <link>http://localhost:1313/blog/20130101-lehigh-resolve/</link>
      <pubDate>Tue, 01 Jan 2013 06:25:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20130101-lehigh-resolve/</guid>
      <description>&lt;h3 id=&#34;did-you-know&#34;&gt;Did You Know&lt;/h3&gt;
&lt;p&gt;Lehigh alumni are engaged in high performance computing and data analytics in a variety of ways. Here are a few examples:&lt;/p&gt;
&lt;h4 id=&#34;david-a-bader&#34;&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&amp;lsquo;90, &amp;lsquo;92G is executive director of high performance computing at Georgia Tech, conducting research at the intersection of high performance computing, computational biology and genomics, and social network analysis.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://engineering.lehigh.edu/research/resolve/volume-2-2013&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://engineering.lehigh.edu/research/resolve/volume-2-2013&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emcien Corp. Taps Big Data Thought Leader Dr. David A. Bader for Advisory Board</title>
      <link>http://localhost:1313/blog/20121204-emcien/</link>
      <pubDate>Tue, 04 Dec 2012 08:07:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121204-emcien/</guid>
      <description>&lt;p&gt;Emcien Corp., a leading provider of pattern-based
analytics solutions, announced today the addition of
internationally renowned High-Performance
Computing luminary &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt; to the Emcien
Advisory Board, thereby lending his expertise to
Emcien’s next generation of solutions designed for
massive-scale analytics in real time in multiple
sectors, including government, financial services,
healthcare, medical research, and insurance. Dr.
Bader is a recognized leader in designing large-scale
parallel algorithms for data-intensive problems, such
as social network analysis, as well as for his
decades-long research and innovation in dataintensive
computing.&lt;/p&gt;
&lt;p&gt;“This technology is uniquely positioned at the
intersection of high-performance computing and
graph analytics, so I am pleased to join Emcien’s
Advisory Board,” said Dr. Bader. “Emcien’s
technological approach tackles one of the biggest
challenges with Big Data, namely what are the right
questions one should be asking? Graph analytics
quickly discovers the value within massive datasets
by making connections between disparate, seemingly
unrelated bits of information, and by finding the
highest-ranked of these connections to focus on for
critical insights.”&lt;/p&gt;
&lt;p&gt;Dr. Bader is a professor in the School of
Computational Science and Engineering, College of Computing and Executive Director for High Performance
Computing at Georgia Institute of Technology. His interests are at the intersection of high-performance computing
and real-world applications, including data analytics.&lt;/p&gt;
&lt;p&gt;“Companies are facing a data overload, and solutions that combine graph analytics with combinatorial
optimization are critical to delivering the true value hidden in an organization’s Big Data,” said Radhika
Subramanian, CEO, Emcien. “Therefore, we are pleased Dr. Bader is joining the Emcien Advisory Board. His
expertise in High Performance Computing adds to our relentless focus on developing the best-in-class suite of
patented pattern-based analytics solutions and enables us to deliver significant value in the form of operational
efficiency, new sources of revenue, and an unsurpassed competitive advantage to organizations across sectors.”&lt;/p&gt;
&lt;p&gt;Dr. Bader received his Ph.D. in 1996 from The University of Maryland, is an IEEE and AAAS Fellow, and his
research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE. Dr.
Bader serves on the Steering Committees of the IPDPS and HiPC conferences. He has served on the Research
Advisory Council for Internet2 and has chaired several of the flagship professional meetings in parallel and highperformance
computing. He is an associate editor-in-chief of the Journal of Parallel and Distributed Computing
(JPDC) and serves as an associate editor for several high impact publications, including ACM Journal of
Experimental Algorithmics (JEA), Parallel Computing, and Journal of Computational Science, and has been an
associate editor for the IEEE Transactions on Parallel and Distributed Systems (TPDS). He was elected as chair of
the IEEE Computer Society Technical Committee on Parallel Processing (TCPP) and as chair of the SIAM Activity
Group in Supercomputing (SIAG/SC). He has co-authored over 100 articles in peer-reviewed journals and
conferences, and his main areas of research are in parallel algorithms, combinatorial optimization, and massivescale
social networks.&lt;/p&gt;
&lt;h3 id=&#34;about-emcien-corp&#34;&gt;About Emcien Corp.&lt;/h3&gt;
&lt;p&gt;Emcien is a first-in-class provider of pattern-based analytics solutions purpose-built for converting an
organization’s complex, multi-dimensional data into actionable intelligence. Emcien’s suite of solutions delivers
significant business value in the forms of operational efficiency, new sources of revenue, increased profits, and
enhanced competitive advantage. For more details, visit emcien.com, contact us at sales(at)emcien(dot)com or
call 404-961-6360.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.prweb.com/releases/2012/12/prweb10200887.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.prweb.com/releases/2012/12/prweb10200887.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emcien Taps Big Data Leader for Advisory Board</title>
      <link>http://localhost:1313/blog/20121204-hpcwire/</link>
      <pubDate>Tue, 04 Dec 2012 07:04:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121204-hpcwire/</guid>
      <description>&lt;p&gt;Emcien Corp., a leading provider of pattern-based
analytics solutions, announced today the addition of
internationally renowned High-Performance
Computing luminary &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt; to the Emcien
Advisory Board, thereby lending his expertise to
Emcien’s next generation of solutions designed for
massive-scale analytics in real time in multiple
sectors, including government, financial services,
healthcare, medical research, and insurance. Dr.
Bader is a recognized leader in designing large-scale
parallel algorithms for data-intensive problems, such
as social network analysis, as well as for his
decades-long research and innovation in dataintensive
computing.&lt;/p&gt;
&lt;p&gt;“This technology is uniquely positioned at the
intersection of high-performance computing and
graph analytics, so I am pleased to join Emcien’s
Advisory Board,” said Dr. Bader. “Emcien’s
technological approach tackles one of the biggest
challenges with Big Data, namely what are the right
questions one should be asking? Graph analytics
quickly discovers the value within massive datasets
by making connections between disparate, seemingly
unrelated bits of information, and by finding the
highest-ranked of these connections to focus on for
critical insights.”&lt;/p&gt;
&lt;p&gt;Dr. Bader is a professor in the School of
Computational Science and Engineering, College of Computing and Executive Director for High Performance
Computing at Georgia Institute of Technology. His interests are at the intersection of high-performance computing
and real-world applications, including data analytics.&lt;/p&gt;
&lt;p&gt;“Companies are facing a data overload, and solutions that combine graph analytics with combinatorial
optimization are critical to delivering the true value hidden in an organization’s Big Data,” said Radhika
Subramanian, CEO, Emcien. “Therefore, we are pleased Dr. Bader is joining the Emcien Advisory Board. His
expertise in High Performance Computing adds to our relentless focus on developing the best-in-class suite of
patented pattern-based analytics solutions and enables us to deliver significant value in the form of operational
efficiency, new sources of revenue, and an unsurpassed competitive advantage to organizations across sectors.”&lt;/p&gt;
&lt;p&gt;Dr. Bader received his Ph.D. in 1996 from The University of Maryland, is an IEEE and AAAS Fellow, and his
research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE. Dr.
Bader serves on the Steering Committees of the IPDPS and HiPC conferences. He has served on the Research
Advisory Council for Internet2 and has chaired several of the flagship professional meetings in parallel and highperformance
computing. He is an associate editor-in-chief of the Journal of Parallel and Distributed Computing
(JPDC) and serves as an associate editor for several high impact publications, including ACM Journal of
Experimental Algorithmics (JEA), Parallel Computing, and Journal of Computational Science, and has been an
associate editor for the IEEE Transactions on Parallel and Distributed Systems (TPDS). He was elected as chair of
the IEEE Computer Society Technical Committee on Parallel Processing (TCPP) and as chair of the SIAM Activity
Group in Supercomputing (SIAG/SC). He has co-authored over 100 articles in peer-reviewed journals and
conferences, and his main areas of research are in parallel algorithms, combinatorial optimization, and massivescale
social networks.&lt;/p&gt;
&lt;h3 id=&#34;about-emcien-corp&#34;&gt;About Emcien Corp.&lt;/h3&gt;
&lt;p&gt;Emcien is a first-in-class provider of pattern-based analytics solutions purpose-built for converting an
organization’s complex, multi-dimensional data into actionable intelligence. Emcien’s suite of solutions delivers
significant business value in the forms of operational efficiency, new sources of revenue, increased profits, and
enhanced competitive advantage.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20121211093343/http://www.hpcwire.com/hpcwire/2012-12-04/emcien_taps_big_data_leader_for_advisory_board.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/hpcwire/2012-12-04/emcien_taps_big_data_leader_for_advisory_board.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequoia HPC tops new Graph 500 ranking of Big Data machines</title>
      <link>http://localhost:1313/blog/20121119-techworld/</link>
      <pubDate>Mon, 19 Nov 2012 17:48:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121119-techworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Joab Jackson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Graph 500 ranking of the most powerful supercomputers at handling Big Data workloads has been introduced to complement the Top500 HPC list.&lt;/p&gt;
&lt;p&gt;So while a new Cray supercomputer took first place on the Top500, it was another machine, Lawrence Livermore National Laboratory&amp;rsquo;s Sequoia, that proved to be the most adept at processing data intensive workloads on the Graph 500.&lt;/p&gt;
&lt;p&gt;Such differences in ranking between the two scales highlight the changing ways in which the world&amp;rsquo;s most powerful supercomputers are being used. An increasing number of high performance computing (HPC) machines are being put to work on data analysis, rather than the traditional duties of modeling and simulation.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I look around the exhibit floor of the Supercomputing 2012 conference, and I&amp;rsquo;m hard-pressed to find a booth that is not doing Big Data or analytics. Everyone has recognised that data is a new workload for HPC,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, a computational science professor at the Georgia Institute of Technology who helps oversee the Graph 500.&lt;/p&gt;
&lt;p&gt;The Graph 500 was created to chart how well the world&amp;rsquo;s largest computers handle such data intensive workloads. The latest edition of the list was released at the SC12 supercomputing conference, being held this week in Salt Lake City.&lt;/p&gt;
&lt;p&gt;In a nutshell, the Graph 500 benchmark looks at &amp;ldquo;how fast a system can trace through random memory addresses,&amp;rdquo; Bader said. With data intensive workloads, &amp;ldquo;the bottleneck in the machine is often your memory bandwidth rather than your peak floating point processing rate,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.techworld.com/news/apps-wearables/sequoia-hpc-tops-new-graph-500-ranking-of-big-data-machines-3411706/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.techworld.com/news/apps-wearables/sequoia-hpc-tops-new-graph-500-ranking-of-big-data-machines-3411706/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>World&#39;s Most Powerful Big Data Machines Charted on Graph 500</title>
      <link>http://localhost:1313/blog/20121116-computerworld/</link>
      <pubDate>Fri, 16 Nov 2012 09:05:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121116-computerworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Joab Jackson, U.S. Correspondent, IDG News Service&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.top500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Top500&lt;/a&gt; is no longer the only ranking game in town: make way for the &lt;a href=&#34;http://www.graph500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph 500&lt;/a&gt;, which tracks how well supercomputers handle big-data-styled workloads.&lt;/p&gt;
&lt;p&gt;So while a new Cray supercomputer took &lt;a href=&#34;https://www.computerworld.com/article/2493517/cray-bumps-ibm-from-top500-supercomputer-top-spot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first place on the Top500&lt;/a&gt;, it was another machine, Lawrence Livermore National Laboratory&amp;rsquo;s Sequoia, that proved to be the most adept at processing data intensive workloads on the Graph 500.&lt;/p&gt;
&lt;p&gt;Such differences in ranking between the two scales highlight the changing ways in which the world&amp;rsquo;s most powerful supercomputers are being used. An increasing number of high performance computing (HPC) machines are being put to work on data analysis, rather than the traditional duties of modeling and simulation.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I look around the exhibit floor [of the Supercomputing 2012 conference], and I&amp;rsquo;m hard-pressed to find a booth that is not doing big data or analytics. Everyone has recognized that data is a new workload for HPC,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, a computational science professor at the Georgia Institute of Technology who helps oversee the Graph 500.&lt;/p&gt;
&lt;p&gt;The Graph 500 was created to chart how well the world&amp;rsquo;s largest computers handle such data intensive workloads. The latest edition of the list was released at the SC12 supercomputing conference, being held this week in Salt Lake City.&lt;/p&gt;
&lt;p&gt;In a nutshell, the Graph 500 benchmark looks at &amp;ldquo;how fast [a system] can trace through random memory addresses,&amp;rdquo; Bader said. With data intensive workloads, &amp;ldquo;the bottleneck in the machine is often your memory bandwidth rather than your peak floating point processing rate,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;The approach is markedly different than Top500. The well-known Top500 list relies on the &lt;a href=&#34;http://www.netlib.org/linpack/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linpack&lt;/a&gt; benchmark, which was created in 1974. Linpack measures how effectively a supercomputer executes floating point operations, which are used for mathematically intensive computations such as weather modeling or other three dimensional simulations.&lt;/p&gt;
&lt;p&gt;The Graph 500, in contrast, places greater emphasis on how well a computer can search through a large data set. &amp;ldquo;Big data has a lot of irregular and unstructured data sets, irregular accesses to memory, and much more reliance on memory bandwidth and memory transactions than on floating point performance,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;For the Graph 500 benchmark, the supercomputer is given a large set of data, called a graph. A graph is an interconnected set of data, such as a group of connected friends on a social network like Facebook. A graph consists of a set of vertices and edges, and in the social media context a vertex would be a person and the edge that person&amp;rsquo;s connection to another person. Some vertices have many connections while many others have fewer. The computer is given a single vertex and is timed on how quickly it discovers all the other vertices in a graph, namely by following the edges.&lt;/p&gt;
&lt;p&gt;Currently, IBM&amp;rsquo;s BlueGene/Q systems dominate this edition of the Graph 500. Nine out of the top 10 systems on the list are BlueGene/Q models &amp;ndash; compared to four BlueGene/Q systems on the November 2011 compilation. For Bader, this is proof that IBM is becoming more sensitive to current data processing needs. IBM&amp;rsquo;s previous BlueGene system, BlueGene/L, was geared more towards floating point operations, and does not score as highly on the list.&lt;/p&gt;
&lt;p&gt;Like the Top500, each successive edition of the Graph 500 shows steady performance gains among its participants. The top machine on the new list, Sequoia, traversed 15,363 billion edges per second. In contrast, the top entrant of the first list, compiled in 2010, followed only 7 billion edges per second. This jump of four orders of magnitude is &amp;ldquo;staggering,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;The Graph500 list is compiled twice a year, and, like the Top500, the results are announced at the Supercomputing conference, usually held in November, or the International Supercomputing Conference, usually held in June. Participation is voluntary: entrants will run either the reference implementations, or their own implementations, of the benchmark and submit the results.&lt;/p&gt;
&lt;p&gt;Despite its name, the Graph 500 has yet to attract 500 submissions, though the numbers are improving with each edition. The first contest garnered 9 participants, and this latest edition has 124 entrants.&lt;/p&gt;
&lt;p&gt;Bader is quick to point out that the Graph 500 is not a replacement for the Top500 but rather a complementary benchmark. Still, the data intensive benchmark could help answer &lt;a href=&#34;http://www.computerworld.com/s/article/9197198/IBM_Intel_question_key_Top500_supercomputer_metric&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;some of the criticisms&lt;/a&gt; around the Top500&amp;rsquo;s use of the Linpack benchmark.&lt;/p&gt;
&lt;p&gt;Jack Dongarra, who helped create Linpack and now maintains the Top500, admitted &lt;a href=&#34;http://www.computerworld.com/s/article/9233639/SC2012_Top500_expects_exascale_computing_by_2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;during a discussion&lt;/a&gt; about the latest results of the Top500 at SC12 that Linpack does not measure all aspects of a computer&amp;rsquo;s performance. He pointed to projects like Graph 500, the &lt;a href=&#34;http://www.green500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Green500&lt;/a&gt; and the HPC Challenge that measure other aspects of supercomputer performance.&lt;/p&gt;
&lt;p&gt;At least one system, the &lt;a href=&#34;http://www.ncsa.illinois.edu/BlueWaters/system.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Center for Supercomputing Applications&amp;rsquo; Blue Waters&lt;/a&gt;, was not entered in the Top500, because its keepers did not feel Linpack would adequately convey the true power of the machine.&lt;/p&gt;
&lt;p&gt;Supercomputers are built according to the jobs they will execute, not to an arbitrary benchmark, Bader pointed out.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At the end of the day, you are going to want the machine that does best for your workload,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Joab Jackson covers enterprise software and general technology breaking news for The IDG News Service. Follow Joab on Twitter at @Joab_Jackson. Joab&amp;rsquo;s e-mail address is &lt;a href=&#34;mailto:Joab_Jackson@idg.com&#34;&gt;Joab_Jackson@idg.com&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2493162/world-s-most-powerful-big-data-machines-charted-on-graph-500.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2493162/world-s-most-powerful-big-data-machines-charted-on-graph-500.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>World&#39;s Most Powerful Big Data Machines Charted on Graph 500</title>
      <link>http://localhost:1313/blog/20121116-graph500/</link>
      <pubDate>Fri, 16 Nov 2012 09:05:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121116-graph500/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Joab Jackson, U.S. Correspondent, IDG News Service&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.top500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Top500&lt;/a&gt; is no longer the only ranking game in town: make way for the &lt;a href=&#34;http://www.graph500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph 500&lt;/a&gt;, which tracks how well supercomputers handle big-data-styled workloads.&lt;/p&gt;
&lt;p&gt;So while a new Cray supercomputer took &lt;a href=&#34;https://www.computerworld.com/article/2493517/cray-bumps-ibm-from-top500-supercomputer-top-spot.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first place on the Top500&lt;/a&gt;, it was another machine, Lawrence Livermore National Laboratory&amp;rsquo;s Sequoia, that proved to be the most adept at processing data intensive workloads on the Graph 500.&lt;/p&gt;
&lt;p&gt;Such differences in ranking between the two scales highlight the changing ways in which the world&amp;rsquo;s most powerful supercomputers are being used. An increasing number of high performance computing (HPC) machines are being put to work on data analysis, rather than the traditional duties of modeling and simulation.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I look around the exhibit floor [of the Supercomputing 2012 conference], and I&amp;rsquo;m hard-pressed to find a booth that is not doing big data or analytics. Everyone has recognized that data is a new workload for HPC,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, a computational science professor at the Georgia Institute of Technology who helps oversee the Graph 500.&lt;/p&gt;
&lt;p&gt;The Graph 500 was created to chart how well the world&amp;rsquo;s largest computers handle such data intensive workloads. The latest edition of the list was released at the SC12 supercomputing conference, being held this week in Salt Lake City.&lt;/p&gt;
&lt;p&gt;In a nutshell, the Graph 500 benchmark looks at &amp;ldquo;how fast [a system] can trace through random memory addresses,&amp;rdquo; Bader said. With data intensive workloads, &amp;ldquo;the bottleneck in the machine is often your memory bandwidth rather than your peak floating point processing rate,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;The approach is markedly different than Top500. The well-known Top500 list relies on the Linpack benchmark, which was created in 1974. Linpack measures how effectively a supercomputer executes floating point operations, which are used for mathematically intensive computations such as weather modeling or other three dimensional simulations.&lt;/p&gt;
&lt;p&gt;The Graph 500, in contrast, places greater emphasis on how well a computer can search through a large data set. &amp;ldquo;Big data has a lot of irregular and unstructured data sets, irregular accesses to memory, and much more reliance on memory bandwidth and memory transactions than on floating point performance,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;For the Graph 500 benchmark, the supercomputer is given a large set of data, called a graph. A graph is an interconnected set of data, such as a group of connected friends on a social network like Facebook. A graph consists of a set of vertices and edges, and in the social media context a vertex would be a person and the edge that person&amp;rsquo;s connection to another person. Some vertices have many connections while many others have fewer. The computer is given a single vertex and is timed on how quickly it discovers all the other vertices in a graph, namely by following the edges.&lt;/p&gt;
&lt;p&gt;Currently, IBM&amp;rsquo;s BlueGene/Q systems dominate this edition of the Graph 500. Nine out of the top 10 systems on the list are BlueGene/Q models &amp;ndash; compared to four BlueGene/Q systems on the November 2011 compilation. For Bader, this is proof that IBM is becoming more sensitive to current data processing needs. IBM&amp;rsquo;s previous BlueGene system, BlueGene/L, was geared more towards floating point operations, and does not score as highly on the list.&lt;/p&gt;
&lt;p&gt;Like the Top500, each successive edition of the Graph 500 shows steady performance gains among its participants. The top machine on the new list, Sequoia, traversed 15,363 billion edges per second. In contrast, the top entrant of the first list, compiled in 2010, followed only 7 billion edges per second. This jump of four orders of magnitude is &amp;ldquo;staggering,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;The Graph500 list is compiled twice a year, and, like the Top500, the results are announced at the Supercomputing conference, usually held in November, or the International Supercomputing Conference, usually held in June. Participation is voluntary: entrants will run either the reference implementations, or their own implementations, of the benchmark and submit the results.&lt;/p&gt;
&lt;p&gt;Despite its name, the Graph 500 has yet to attract 500 submissions, though the numbers are improving with each edition. The first contest garnered 9 participants, and this latest edition has 124 entrants.&lt;/p&gt;
&lt;p&gt;Bader is quick to point out that the Graph 500 is not a replacement for the Top500 but rather a complementary benchmark. Still, the data intensive benchmark could help answer some of the criticisms around the Top500&amp;rsquo;s use of the Linpack benchmark.&lt;/p&gt;
&lt;p&gt;Jack Dongarra, who helped create Linpack and now maintains the Top500, admitted during a discussion about the latest results of the Top500 at SC12 that Linpack does not measure all aspects of a computer&amp;rsquo;s performance. He pointed to projects like Graph 500, the Green500 and the HPC Challenge that measure other aspects of supercomputer performance.&lt;/p&gt;
&lt;p&gt;At least one system, the National Center for Supercomputing Applications&amp;rsquo; Blue Waters, was not entered in the Top500, because its keepers did not feel Linpack would adequately convey the true power of the machine.&lt;/p&gt;
&lt;p&gt;Supercomputers are built according to the jobs they will execute, not to an arbitrary benchmark, Bader pointed out.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At the end of the day, you are going to want the machine that does best for your workload,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Joab Jackson covers enterprise software and general technology breaking news for The IDG News Service. Follow Joab on Twitter at @Joab_Jackson. Joab&amp;rsquo;s e-mail address is &lt;a href=&#34;mailto:Joab_Jackson@idg.com&#34;&gt;Joab_Jackson@idg.com&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cio.com/article/2390304/world-s-most-powerful-big-data-machines-charted-on-graph-500.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cio.com/article/2390304/world-s-most-powerful-big-data-machines-charted-on-graph-500.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DARPA awards Georgia Tech energy-efficient high-performance computing contract</title>
      <link>http://localhost:1313/blog/20121113-darpa-perfect/</link>
      <pubDate>Tue, 13 Nov 2012 07:41:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121113-darpa-perfect/</guid>
      <description>&lt;p&gt;Georgia Tech has received $561,130 for the first phase of a negotiated three-phase $2.9 million cooperative agreement contract from the U.S. Defense Advanced Projects Research Agency (DARPA) to create the algorithmic framework for supercomputing systems that require much less energy than traditional high-speed machines, enabling devices in the field to perform calculations that currently require room-sized supercomputers.&lt;/p&gt;
&lt;p&gt;Awarded under DARPA&amp;rsquo;s Power Efficiency Revolution for Embedded Computing Technologies (PERFECT) program, the negotiated cooperative agreement contract (with options out to five years) is one piece of a national effort to increase the computational power efficiency of &amp;ldquo;embedded systems&amp;rdquo; by 75-fold over the best current computing performance in areas extending beyond traditional scientific computing. Professor &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing in the School of Computational Science &amp;amp; Engineering, is principal investigator on the Georgia Tech cooperative agreement, along with research scientist and co-PI Jason Riedy.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Power efficiency is one of the greatest challenges confronting the designer of any computing system, much less one that&amp;rsquo;s capable of this kind of speed,&amp;rdquo; Bader said. &amp;ldquo;We could build this system today, but it would require megawatts of electricity&amp;ndash;enough to power a medium-sized city. Our goal is to deliver the same graph analytic capabilities on platforms that require only watts or kilowatts.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Such a system would have benefits in energy conservation, of course, but it could also save lives. The tactical advantages of supercomputing in military situations&amp;ndash;quickly and comprehensively mapping individual or group social-media activity, for example&amp;ndash;are becoming more critical every day, and the capacity simply doesn&amp;rsquo;t exist to deliver massive amounts of data from the field to a central computing system. Georgia Tech&amp;rsquo;s objective is to bring supercomputer graph-analysis capabilities where they&amp;rsquo;re needed, from vehicles to field hospitals and beyond. The project bears the acronym GRATEFUL: &amp;ldquo;Graph Analysis Tackling power-Efficiency, Uncertainty and Locality.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition to power efficiency, the second priority is to maximize computational resiliency, meaning the product algorithms will be able to withstand errors at the application and even hardware level that could result from input error or environmental factors (such as weather and hardware damage).&lt;/p&gt;
&lt;p&gt;Bader and Riedy&amp;rsquo;s task is to develop the algorithmic framework upon which these new embedded systems will operate, and they will consciously remain &amp;ldquo;architecture-agnostic&amp;rdquo; so that the end product can be applied as widely as possible. Finally, like all programs funded under DARPA PERFECT, research and testing will be done in simulation rather than on actual embedded systems. GRATEFUL will be broken up into three stages: research &amp;amp; startup (18 months), risk mitigation (18 months) and prototyping (two years).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our goal is to make sure we have graph-analysis algorithms that can manage issues across architectures,&amp;rdquo; Riedy said. &amp;ldquo;And we&amp;rsquo;ll be looking at all the issues that concern hardware designers. Today&amp;rsquo;s platforms maximize the number of operations running at once, while these new platforms consider the most power-efficient levels of that concurrency. These are not new concerns, but our job is to find new ways to deal with them.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/pub_releases/2012-11/giot-dag111312.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eurekalert.org/pub_releases/2012-11/giot-dag111312.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DARPA Awards GA Tech Energy-Efficient HPC Contract</title>
      <link>http://localhost:1313/blog/20121112-hpcwire/</link>
      <pubDate>Mon, 12 Nov 2012 06:59:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20121112-hpcwire/</guid>
      <description>&lt;p&gt;Georgia Tech has received $561,130 for the first phase of a negotiated three-phase $2.9 million cooperative agreement contract from the U.S. Defense Advanced Projects Research Agency (DARPA) to create the algorithmic framework for supercomputing systems that require much less energy than traditional high-speed machines, enabling devices in the field to perform calculations that currently require room-sized supercomputers.&lt;/p&gt;
&lt;p&gt;Awarded under DARPA&amp;rsquo;s Power Efficiency Revolution for Embedded Computing Technologies (PERFECT) program, the negotiated cooperative agreement contract (with options out to five years) is one piece of a national effort to increase the computational power efficiency of &amp;ldquo;embedded systems&amp;rdquo; by 75-fold over the best current computing performance in areas extending beyond traditional scientific computing. Professor &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing in the School of Computational Science &amp;amp; Engineering, is principal investigator on the Georgia Tech cooperative agreement, along with research scientist and co-PI Jason Riedy.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Power efficiency is one of the greatest challenges confronting the designer of any computing system, much less one that&amp;rsquo;s capable of this kind of speed,&amp;rdquo; Bader said. &amp;ldquo;We could build this system today, but it would require megawatts of electricity&amp;ndash;enough to power a medium-sized city. Our goal is to deliver the same graph analytic capabilities on platforms that require only watts or kilowatts.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Such a system would have benefits in energy conservation, of course, but it could also save lives. The tactical advantages of supercomputing in military situations&amp;ndash;quickly and comprehensively mapping individual or group social-media activity, for example&amp;ndash;are becoming more critical every day, and the capacity simply doesn&amp;rsquo;t exist to deliver massive amounts of data from the field to a central computing system. Georgia Tech&amp;rsquo;s objective is to bring supercomputer graph-analysis capabilities where they&amp;rsquo;re needed, from vehicles to field hospitals and beyond. The project bears the acronym GRATEFUL: &amp;ldquo;Graph Analysis Tackling power-Efficiency, Uncertainty and Locality.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition to power efficiency, the second priority is to maximize computational resiliency, meaning the product algorithms will be able to withstand errors at the application and even hardware level that could result from input error or environmental factors (such as weather and hardware damage).&lt;/p&gt;
&lt;p&gt;Bader and Riedy&amp;rsquo;s task is to develop the algorithmic framework upon which these new embedded systems will operate, and they will consciously remain &amp;ldquo;architecture-agnostic&amp;rdquo; so that the end product can be applied as widely as possible. Finally, like all programs funded under DARPA PERFECT, research and testing will be done in simulation rather than on actual embedded systems. GRATEFUL will be broken up into three stages: research &amp;amp; startup (18 months), risk mitigation (18 months) and prototyping (two years).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our goal is to make sure we have graph-analysis algorithms that can manage issues across architectures,&amp;rdquo; Riedy said. &amp;ldquo;And we&amp;rsquo;ll be looking at all the issues that concern hardware designers. Today&amp;rsquo;s platforms maximize the number of operations running at once, while these new platforms consider the most power-efficient levels of that concurrency. These are not new concerns, but our job is to find new ways to deal with them.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20121116094027/http://www.hpcwire.com/hpcwire/2012-11-12/darpa_awards_ga_tech_energy-efficient_hpc_contract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/hpcwire/2012-11-12/darpa_awards_ga_tech_energy-efficient_hpc_contract.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Do Super Computers Use Linux?</title>
      <link>http://localhost:1313/blog/20121105-unixmen/</link>
      <pubDate>Mon, 05 Nov 2012 14:42:47 -0500</pubDate>
      <guid>http://localhost:1313/blog/20121105-unixmen/</guid>
      <description>&lt;p&gt;In our last few posts we discussed the fact that &lt;a href=&#34;https://www.unixmen.com/linux-share-in-supercomputer-os/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;over 90% supercomputers (94.2% to be precise)&lt;/a&gt; employ Linux as their operating system. In this post, a sequel to our last posts, we shall attempt to investigate the potentials of Linux which make it suitable and perhaps the best choice for supercomputers OS.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20121105-unixmen/THUMB-150x150_hu_639370e397b69089.webp 400w,
               /blog/20121105-unixmen/THUMB-150x150_hu_994076802f7d734b.webp 760w,
               /blog/20121105-unixmen/THUMB-150x150_hu_3d116541c38d0dbe.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20121105-unixmen/THUMB-150x150_hu_639370e397b69089.webp&#34;
               width=&#34;150&#34;
               height=&#34;150&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Indeed no one can highlight the capability of Linux as a supercomputer OS better than the team responsible for deploying and maintaining supercomputers. Before we enlist and discuss notable features of Linux, let’s have a look at what key persons in supercomputer domain say about Linux; and what reasons they give when they are asked about selecting Linux as their supercomputer OS.&lt;/p&gt;
&lt;p&gt;Mark Seager serves as the assistant department head for advanced technologies at the Lawrence Livermore National Laboratory in Livermore, Calif. The site (Lawrence Livermore National Laboratory) operates ten machines on the Top 500 list, including Blue Gene/L, the world’s most powerful supercomputer, and Thunder, which ranks fifth. The supercomputers, of course, use Linux as an OS. Mark Seager says “Linux has dominated the marketplace for &lt;strong&gt;high-performance&lt;/strong&gt; computing”. High-performance Linux clustering is a two-part series providing background on high-performance computing that is only possible with Linux.&lt;/p&gt;
&lt;p&gt;Scott Gnau is the chief development officer at Teradata Corporation. The corporation handles several mission critical tasks and deploys supercomputers for computation intensive tasks. The supercomputers run on Linux. Scott says “Our solutions are designed to drive powerful business intelligence and real-time decisioning applications, including fraud detection and prevention, customer segmentation, human resources and forecasting. Our solutions run on SUSE Linux Enterprise Server from Novell, which, thanks to its extreme &lt;strong&gt;scalability&lt;/strong&gt;, &lt;strong&gt;reliability&lt;/strong&gt;, &lt;strong&gt;flexibility&lt;/strong&gt; and &lt;strong&gt;ease of use&lt;/strong&gt;, is the optimal Linux operating system for our customers’ businesses. We selected Novell and its SUSE Linux Enterprise platform because of its full range of industry-leading Linux services to support large-scale, mission-critical enterprises”.&lt;/p&gt;
&lt;p&gt;Eddie Epstein on System Administration of the Watson Supercomputer when asked about their choice of Linux for the supercomputer said “the project started with x86-based blades, and the researchers responsible for admin were very familiar with Linux.” Indicating &lt;strong&gt;Linux is already famous with researchers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Alejandro Ramirez was the leader of the team at the Barcelona Supercomputing Center (BSC) developing the world’s first ARM-based supercomputer. He says about the choice of Linux for supercomputers “the &lt;strong&gt;support&lt;/strong&gt; of the Linux operating system is one of the fundamental elements” He further added “&lt;strong&gt;ARM supports Linux well…&lt;/strong&gt;”&lt;/p&gt;
&lt;p&gt;Irene Qualters is SGI senior vice president of software at SGI. Irene says about their selection of Linux for their , “At SGI, our focus is on &lt;strong&gt;high-performance computing&lt;/strong&gt; and &lt;strong&gt;robust scalability&lt;/strong&gt;, and SUSE Linux Enterprise Server is the operating system of choice for many of our Altix and Altix XE customers”&lt;/p&gt;
&lt;p&gt;Back in 2000, &lt;strong&gt;University of New Mexico&lt;/strong&gt; built a virtual supercomputer Los Lobos employing Red Hat Linux with the initial investment of 1.5 Million dollars (then). Dr. Frank Gilfeather was appointed as the executive director of high-performance computing at the University of New Mexico then. Gilfeather said: “We have several customers that would benefit from high-end computing on a Linux cluster. Down the road, we believe that this will be important for e-business as well–as more and more customers deploy middleware and message queueing, they’ll need the kind of power you’ll find in a Linux cluster: indicating the high performance provide by Linux OS. He further says “the evolution of large Linux superclusters emerges from the proliferation of commodity components such as PCs, the development of high-speed COTS networks, such as Myrinet, and rapid expansion of the open software movement,”…. “Thus, true supercomputers can be created at an extremely &lt;strong&gt;reasonable cost&lt;/strong&gt; in comparison to traditional supercomputers.”&lt;/p&gt;
&lt;p&gt;Stephen Scott a research scientist at Oak Ridge National Lab’s computer-science division (2000) was among the early users/researchers of Linux supercomputer. He says “The scientific world likes Linux because &lt;strong&gt;it’s close to standard Unix&lt;/strong&gt;,” Scott added. “Most high-performance environments are Unix, but all of the free GNU tools make it much easier and cheaper to deploy Linux&lt;/p&gt;
&lt;p&gt;Having looked at the expert view lets elaborate features of Linux that makes Linux the best choice for supercomputers:&lt;/p&gt;
&lt;h2 id=&#34;1--modular-nature-of-linux&#34;&gt;1- Modular nature of Linux&lt;/h2&gt;
&lt;p&gt;A layman can think of typical Linux as being made up of small building blocks or modules. Each module performs distinct dedicated utilities. These building blocks work together to make the OS running. This modular nature of Linux facilitates anyone, may they be average Linux users or Supercomputer administrators to modify the OS to suit their requirements. No other operating system, specifically Windows gives freedom of customization to this extent. As a consequence Linux can be modified to be used on supercomputers and archive dedicated goals, particularly enhance performance or energy efficiency etc. Today most supercomputers employ a modified Linux kernel.&lt;/p&gt;
&lt;h2 id=&#34;2-generic-nature-of-linux-kernel&#34;&gt;2-Generic Nature of Linux Kernel&lt;/h2&gt;
&lt;p&gt;Linux kernel is generic, as much as possible. This implies that single source code can be written to run on large supercomputers and also on small even hand-held gadgets; this is entirely upto user how one uses Linux, either on giant systems or smaller systems. There is no need to add fundamental and large changes to the kernel in order to run on larger or smaller systems. Typically Linux kernel can be configured to be as small as 2Mb or as large as 1G or 1T without impending time and effort.&lt;/p&gt;
&lt;h2 id=&#34;3-scalability&#34;&gt;3-Scalability&lt;/h2&gt;
&lt;p&gt;Scalability can be defined as the ability of the server to adapt to larger loads. Scalability can be directly thought of as a measure of efficiency, performance. System must be such that addition of new server should be painless. Linux has tremendous scalability as it can accommodate the new and higher loads rather easily. This why you can find Linux run supercomputers and Android (using Linux kernel) on mobile phones, refrigerators and even microwaveovens!&lt;/p&gt;
&lt;h2 id=&#34;4-open-source-nature&#34;&gt;4-Open Source Nature&lt;/h2&gt;
&lt;p&gt;Linux is entirely Open source and free software with complete source code available. This implies that supercomputer administrators can customize the OS to any level. Additionally, in case of performance glitches, security loopholes etc. found on supercomputers administrators can alter the code anytime to attain max performance and security (or for that matter any goal); rather than waiting for security updates from proprietary companies.&lt;/p&gt;
&lt;p&gt;Supercomputers seek to maximize performance. Usually supercomputers are assigned with jobs that require computation at a very high speed. When compared to Windows, Windows has number of extraneous processes that are unnecessary and only degrade the supercomputer performance  However as Windows is proprietary the code cannot be altered to cut off unnecessary processes. With Linux it is certainly possible which gives a performance boost to computers.&lt;/p&gt;
&lt;h2 id=&#34;5--community-support&#34;&gt;5- Community Support&lt;/h2&gt;
&lt;p&gt;Linux being Open source has immense community support that is unparalleled on any other operating system.&lt;/p&gt;
&lt;h2 id=&#34;6--cost&#34;&gt;6- Cost&lt;/h2&gt;
&lt;p&gt;Cost can be of major concern when it comes to huge devices, one like supercomputers. Deploying Linux on supercomputers is cost effective as Linux is completely royalty free.&lt;/p&gt;
&lt;p&gt;Other reasons of using Linux as OS is Linux nice networking support. It is relatively easier to add or remove any experimental networking device. No reboots required! Linux is reliable and stable OS that can be run on large costly servers and computers without having to worry alot. Finally, Linux is more secure.&lt;/p&gt;
&lt;p&gt;What do you think are factors that make Linux as most suitable choice for Supercomputers? Add your expert comments to make the content richer!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.unixmen.com/why-do-super-computers-use-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.unixmen.com/why-do-super-computers-use-linux/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who’s the Most Influential in a Social Graph?</title>
      <link>http://localhost:1313/blog/20120910-cacm/</link>
      <pubDate>Mon, 10 Sep 2012 08:08:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120910-cacm/</guid>
      <description>&lt;p&gt;Georgia Tech researchers say they have developed an algorithm that quickly determines betweenness centrality for streaming graphs.&lt;/p&gt;
&lt;p&gt;They say the algorithm also can identify influencers as information changes within a network. &amp;ldquo;Our algorithm stores the graph’s prior centrality data and only does the bare minimal computations affected by the inserted edges,&amp;rdquo; says Georgia Tech professor &lt;strong&gt;David Bader&lt;/strong&gt;. In some situations, Bader says the software can compute betweenness centrality more than 100 times faster than conventional methods.&lt;/p&gt;
&lt;p&gt;He notes advertisers could use the software to identify which celebrities are most influential on social media during product launches. &amp;ldquo;Despite a fragmented social media landscape, data analysts would be able to use the algorithm to look at each social media network and mark inferences about a single influencer across these different platforms,&amp;rdquo; Bader says.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=152431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From Georgia Tech News&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cacm.acm.org/news/155107-whos-the-most-influential-in-a-social-graph/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cacm.acm.org/news/155107-whos-the-most-influential-in-a-social-graph/fulltext&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Develops New Graph Algorithm</title>
      <link>http://localhost:1313/blog/20120910-hpcwire/</link>
      <pubDate>Mon, 10 Sep 2012 06:52:54 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120910-hpcwire/</guid>
      <description>&lt;p&gt;At an airport, many people are essential for planes to take off. Gate staffs, refueling crews, flight attendants and pilots are in constant communication with each other as they perform required tasks. But it’s the air traffic controller who talks with every plane, coordinating departures and runways. Communication must run through her in order for an airport to run smoothly and safely.&lt;/p&gt;
&lt;p&gt;In computational terms, the air traffic controller is the “betweenness centrality,” the most connected person in the system. In this example, finding the key influencer is easy because each departure process is nearly the same.&lt;/p&gt;
&lt;p&gt;Determining the most influential person on a social media network (or, in computer terms, a graph) is more complex. Thousands of users are interacting about a single subject at the same time. New people (known computationally as edges) are constantly joining the streaming conversation.&lt;/p&gt;
&lt;p&gt;Georgia Tech has developed a new algorithm that quickly determines betweenness centrality for streaming graphs. The algorithm can identify influencers as information changes within a network. The first-of-its-kind streaming tool was presented this week by Computational Science and Engineering Ph.D. candidate Oded Green at the Social Computing Conference in Amsterdam.&lt;/p&gt;
&lt;p&gt;“Unlike existing algorithms, our system doesn’t restart the computational process from scratch each time a new edge is inserted into a graph,” said College of Computing Professor &lt;strong&gt;David Bader&lt;/strong&gt;, the project’s leader. “Rather than starting over, our algorithm stores the graph’s prior centrality data and only does the bare minimal computations affected by the inserted edges.”&lt;/p&gt;
&lt;p&gt;In some cases, betweenness centrality can be computed more than 100 times faster using the Georgia Tech software. The open source software will soon be available to businesses.&lt;/p&gt;
&lt;p&gt;Bader, the Institute’s executive director for high performance computing, says the technology has wide-ranging applications. For instance, advertisers could use the software to identify which celebrities are most influential on Twitter or Facebook, or both, during product launches.&lt;/p&gt;
&lt;p&gt;“Despite a fragmented social media landscape, data analysts would be able to use the algorithm to look at each social media network and mark inferences about a single influencer across these different platforms,” said Bader.&lt;/p&gt;
&lt;p&gt;As another example, the algorithm could be used for traffic patterns during a wreck or traffic jam. Transportation officials could quickly determine the best new routes based on gradual side-street congestion.&lt;/p&gt;
&lt;p&gt;The accepted paper was co-authored by Electrical and Computer Engineering Ph.D. candidate Rob McColl.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20120916071239/http://www.hpcwire.com/hpcwire/2012-09-10/georgia_tech_develops_new_graph_algorithm.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/hpcwire/2012-09-10/georgia_tech_develops_new_graph_algorithm.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who’s the Most Influential in a Social Graph?</title>
      <link>http://localhost:1313/blog/20120907-gatech/</link>
      <pubDate>Fri, 07 Sep 2012 08:11:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120907-gatech/</guid>
      <description>&lt;p&gt;At an airport, many people are essential for planes to take off. Gate staffs, refueling crews, flight attendants and pilots are in constant communication with each other as they perform required tasks. But it’s the air traffic controller who talks with every plane, coordinating departures and runways. Communication must run through her in order for an airport to run smoothly and safely.&lt;/p&gt;
&lt;p&gt;In computational terms, the air traffic controller is the “betweenness centrality,” the most connected person in the system. In this example, finding the key influencer is easy because each departure process is nearly the same.&lt;/p&gt;
&lt;p&gt;Determining the most influential person on a social media network (or, in computer terms, a graph) is more complex. Thousands of users are interacting about a single subject at the same time. New people (known computationally as edges) are constantly joining the streaming conversation.&lt;/p&gt;
&lt;p&gt;Georgia Tech has developed a new algorithm that quickly determines betweenness centrality for streaming graphs. The algorithm can identify influencers as information changes within a network. The first-of-its-kind streaming tool was presented this week by Computational Science and Engineering Ph.D. candidate Oded Green at the Social Computing Conference in Amsterdam.&lt;/p&gt;
&lt;p&gt;“Unlike existing algorithms, our system doesn’t restart the computational process from scratch each time a new edge is inserted into a graph,” said College of Computing Professor &lt;strong&gt;David Bader&lt;/strong&gt;, the project’s leader. “Rather than starting over, our algorithm stores the graph’s prior centrality data and only does the bare minimal computations affected by the inserted edges.”&lt;/p&gt;
&lt;p&gt;In some cases, betweenness centrality can be computed more than 100 times faster using the Georgia Tech software. The open source software will soon be available to businesses.&lt;/p&gt;
&lt;p&gt;Bader, the Institute’s executive director for high performance computing, says the technology has wide-ranging applications. For instance, advertisers could use the software to identify which celebrities are most influential on Twitter or Facebook, or both, during product launches.&lt;/p&gt;
&lt;p&gt;“Despite a fragmented social media landscape, data analysts would be able to use the algorithm to look at each social media network and mark inferences about a single influencer across these different platforms,” said Bader.&lt;/p&gt;
&lt;p&gt;As another example, the algorithm could be used for traffic patterns during a wreck or traffic jam. Transportation officials could quickly determine the best new routes based on gradual side-street congestion.&lt;/p&gt;
&lt;p&gt;The accepted paper was co-authored by Electrical and Computer Engineering Ph.D. candidate Rob McColl.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This project is supported by the National Science Foundation (NSF) (Award Number CNS-0708307). The content is solely the responsibility of the principal investigators and does not necessarily represent the official views of the NSF.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2012/09/07/who%E2%80%99s-most-influential-social-graph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2012/09/07/who%E2%80%99s-most-influential-social-graph&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Analysts Seek to Make Social Media More Useful</title>
      <link>http://localhost:1313/blog/20120907-neo4j/</link>
      <pubDate>Fri, 07 Sep 2012 07:25:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120907-neo4j/</guid>
      <description>&lt;p&gt;It’s not easy turning the Mayberry Police Department into the team from CSI, or turning an idea for a new type of social network analysis into something like Klout on steroids, but those types of transformations are becoming ever more realistic. The world’s universities and research institutions are hard at work figuring out ways to make the mountains of social data generated every day more useful and, hopefully, make us realize there’s more to social data than just figuring out whose digital voice is the loudest. Aspiring heirs to the Klout throne, for example, might look to a project called Stinger now under development at Georgia Institute of Technology. Stinger, which stands for Spatio-Temporal Interaction Networks and Graphs Extensible Representation, is a graph-processing engine that project lead &lt;strong&gt;David Bader&lt;/strong&gt; says is bigger, faster, and more flexible than anything currently in use for analyzing social media connections. You provide a shared-memory computing system, and it provides an open-source tool that can help detect relationships between billions of people, places, and things as those relationships change over time—even in real time. Someone using Facebook (FB) data, for example, might write an algorithm where people or pages would be the vertices and actions (likes, shares, wall posts, etc.) would be the graph’s edges. One relatively easy application, Bader explains, would be to analyze how activity around particular people is increasing, decreasing, or changing, therefore indicating changes in their importance or the growth of new communities. Writing an algorithm to perform that kind of analysis isn’t really the problem, though—it’s writing one that can scale into the billions of vertices and edges and still perform quickly enough to be useful. An algorithm that generates one false positive in a million isn’t so bad when you’re dealing with tens of thousands of items, Bader says, but it gets to be a big problem when you’re talking about billions of items against which it’s running. There are dozens of open-source graph databasesavailable, including popular offerings such as Neo4j andInfiniteGraph. But, Bader says, “our lab focuses on algorithms that run fast on massive data sets and that are more accurate than what is traditionally done in social media.” Bader’s team recently presented a paper detailing a social media algorithm running atop Stinger that ran 100 times faster than some previous approaches because the system stores the graph’s previous state and performs only the minimal amount of processing necessary as new edges are inserted. This is in contrast to traditional approaches that reprocess the entire graph every time there’s a change.
That being said, Georgia Tech isn’t alone in analyzing massive amounts of social data with graph databases.Google’s (GOOG) Pregel had already scaled to billions of vertices and edges as of 2009, and Facebook is currently analyzing more than a billion edges using Apache Giraph(an open-source, Hadoop-based Pregel implementation). But those cases—both companies are loaded with smart engineers, data scientists, and powerful infrastructure—just underscore the importance of what researchers like Bader are building and releasing as open source.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neo4j.com/news/data-analysts-seek-to-make-social-media-more-useful/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://neo4j.com/news/data-analysts-seek-to-make-social-media-more-useful/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How researchers are letting us uncover secrets in social data</title>
      <link>http://localhost:1313/blog/20120907-gigaom/</link>
      <pubDate>Fri, 07 Sep 2012 07:23:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120907-gigaom/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Derrick Harris&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It’s not easy work turning the Mayberry Police Department into the team from &lt;em&gt;C.S.I.&lt;/em&gt;, or turning an idea for a new type of social network analysis into something like Klout on steroids, but those types of transformations are becoming increasingly more possible. The world’s universities and research institutions are hard at work figuring out ways to make the mountains of social data generated every day more useful and, hopefully, to make us realize there’s more to social data than &lt;a href=&#34;http://gigaom.com/cloud/why-klout-really-matters-money-money-money/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;just figuring out whose digital voice is the loudest&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Aspiring heirs to the Klout throne, for example, might look to a project called &lt;a href=&#34;http://www.cc.gatech.edu/stinger/index.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;STINGER&lt;/a&gt; currently under development at Georgia Tech University. STINGER, which stands for Spatio-Temporal Interaction Networks and Graphs Extensible Representation, is a graph-processing engine that project lead &lt;strong&gt;David Bader&lt;/strong&gt; says is bigger, faster and more flexible than anything currently in use for analyzing social media connections. You provide a shared-memory computing system, and it provides an open-source tool that can help detect relationships between billions of people, places and things as those relationships change over time — even in real time.&lt;/p&gt;
&lt;p&gt;Someone using Facebook data, for example, might write an algorithm using where people or pages would be the vertices and actions (likes, shares, wall posts, etc.) would be the graph’s edges. One relatively easy application, Bader explained, would be to analyze how activity around particular people is increasing, decreasing or changing, therefore indicating changes in their importance or the growth of new communities.&lt;/p&gt;
&lt;h3 id=&#34;well-do-the-hard-work&#34;&gt;We’ll do the hard work&lt;/h3&gt;
&lt;p&gt;Writing an algorithm to perform that kind of analysis isn’t really the problem, though — it’s writing one that can scale into the billions of vertices and edges and &lt;a href=&#34;http://highscalability.com/blog/2010/3/30/running-large-graph-algorithms-evaluation-of-current-state-o.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;still perform quickly enough to be useful&lt;/a&gt;. An algorithm that generates one false positive in a million isn’t so bad when you’re dealing with tens of thousands of items, Bader explained, but it gets to be a big problem when you’re talking about billions of items against which it’s running.&lt;/p&gt;
&lt;p&gt;There are &lt;a href=&#34;http://en.wikipedia.org/wiki/Graph_database&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dozens of open source graph databases available&lt;/a&gt;, including popular offerings &lt;a href=&#34;http://gigaom.com/cloud/springsource-links-up-with-neo-technology-on-nosql/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;such as Neo4j&lt;/a&gt; and &lt;a href=&#34;http://gigaom.com/cloud/twitters-success-pulls-23-year-old-objectivity-into-nosql/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InfiniteGraph&lt;/a&gt;, but he said, “Our lab focuses on algorithms that run fast on massive data sets and that are more accurate than what is traditionally done in social media.”&lt;/p&gt;


















&lt;figure  id=&#34;figure-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader&#34; srcset=&#34;
               /blog/20120907-gigaom/dbader2007-small_hu_dc7db990452a4cf4.webp 400w,
               /blog/20120907-gigaom/dbader2007-small_hu_99001d446291c383.webp 760w,
               /blog/20120907-gigaom/dbader2007-small_hu_28270e1d9b94ec96.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120907-gigaom/dbader2007-small_hu_dc7db990452a4cf4.webp&#34;
               width=&#34;186&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Bader’s team recently presented a paper detailing a social media algorithm running atop STINGER that ran 100 times faster than some previous approaches because the system stores the graph’s previous state and only performs the minimal amount of processing necessary as new edges are inserted. This is in contrast to traditional approaches that re-process the entire graph every time there’s a change.&lt;/p&gt;
&lt;p&gt;That being said, Georgia Tech isn’t entirely alone analyzing massive amounts of social data with graph databases. Google’s (s goog) Pregel had &lt;a href=&#34;http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;already scaled to billions of vertices and edges&lt;/a&gt; as of 2009, and Facebook (s fb) is currently &lt;a href=&#34;http://www.slideshare.net/Hadoop_Summit/processing-edges-on-apache-giraph&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analyzing more than a billion edges&lt;/a&gt; using &lt;a href=&#34;http://incubator.apache.org/giraph/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apache Giraph&lt;/a&gt; (an open source, Hadoop-based Pregel implementation). But those cases — both companies are loaded with smart engineers, data scientists and powerful infrastructure — just underscore the importance of what researchers like Bader are building and releasing as open source.&lt;/p&gt;
&lt;h3 id=&#34;forget-social-media-solve-real-problems&#34;&gt;Forget social media, solve real problems&lt;/h3&gt;
&lt;p&gt;But social data isn’t just useful for figuring out who’s influential on Twitter or Facebook — it also can be used to solve some real problems. Bader said he’s already used graph processing with Twitter data to determine who was leading resistance units during Egypt’s recent revolution. “Anywhere I can look at connections between entities,” he said, “these approaches are available.”&lt;/p&gt;
&lt;p&gt;Indeed. On Wednesday afternoon, for example, a group of researchers from the University of Alberta, University of Connecticut and University of California-Merced unveiled a new data-based method that could make it faster, easier and less expensive to root out culprits in fraud cases.&lt;/p&gt;
&lt;p&gt;The technique uses a method called the Steiner tree to analyze the connections –social, business, familial, etc. — between the people involved in a given case of fraud. The algorithm is able to determine the shortest path between two objects, which the researchers posit is especially applicable to fraud investigations — the person with the shortest path between himself and the crime is probably the culprit (or at least a solid suspect).&lt;/p&gt;
&lt;p&gt;The fraud researchers’ paper follows the publication in August of a &lt;a href=&#34;http://gigaom.com/data/an-algorithm-for-tracking-viruses-and-twitter-rumors-to-their-source/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;method for determining the source of everything&lt;/a&gt; from a disease outbreak to a Twitter rumor by tracking its spread across a complex network over time. Their algorithm, the paper’s authors claim, could be particularly effective for combating cybercrime by tracking computer viruses back to their sources. The more connections (in the case of social data), or observers, a particular point has, the fewer that are needed to track down the source point.&lt;/p&gt;
&lt;p&gt;However, all the algorithms and data frameworks in the world probably won’t make too big a difference until they’re turned into products that actually work on real-world situations. As the University of Alberta’s Ray Patterson pointed out in a &lt;a href=&#34;http://www.news.ualberta.ca/article.aspx?id=598C1DAC742446ED84B477CB8FA05324&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release detailing the fraudster-detection algorithm&lt;/a&gt;, “It might take several years or many years before anyone picks it up. But it’s a good thing if we can point people towards what’s useful.”&lt;/p&gt;
&lt;p&gt;Georgia Tech’s Bader said DARPA, Intel (s intc), Sandia National Laboratory and other research institutions have already used STINGER to tackle some complex data sets, and he suspects a strong commercial interest, as well. If a company is willing to take STINGER from a project into a product, it could bring the project’s scale and speed to everything from analyzing customer interactions to monitoring the changing nature of criminal networks, Bader said. Considering the desire from companies of all types to extract some meaning from social data, I have to think someone will give it a shot.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gigaom.com/2012/09/07/as-social-data-grows-researchers-want-to-uncover-its-secrets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gigaom.com/2012/09/07/as-social-data-grows-researchers-want-to-uncover-its-secrets/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Analysts Seek to Make Social Media More Useful</title>
      <link>http://localhost:1313/blog/20120907-dataanalysts/</link>
      <pubDate>Fri, 07 Sep 2012 07:15:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120907-dataanalysts/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Derrick Harris&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It’s not easy turning the Mayberry Police Department into the team from CSI, or turning an idea for a
new type of social network analysis into something like Klout on steroids, but those types of
transformations are becoming ever more realistic. The world’s universities and research institutions
are hard at work figuring out ways to make the mountains of social data generated every day more
useful and, hopefully, make us realize there’s more to social data than just figuring out whose digital
voice is the loudest.&lt;/p&gt;
&lt;p&gt;Aspiring heirs to the Klout throne, for example, might look to a project called Stinger now under
development at Georgia Institute of Technology. Stinger, which stands for Spatio-Temporal
Interaction Networks and Graphs Extensible Representation, is a graph-processing engine that project
lead &lt;strong&gt;David Bader&lt;/strong&gt; says is bigger, faster, and more flexible than anything currently in use for analyzing
social media connections. You provide a shared-memory computing system, and it provides an opensource
tool that can help detect relationships between billions of people, places, and things as those
relationships change over time—even in real time.&lt;/p&gt;
&lt;p&gt;Someone using Facebook (FB) data, for example, might write an algorithm where people or pages
would be the vertices and actions (likes, shares, wall posts, etc.) would be the graph’s edges. One
relatively easy application, Bader explains, would be to analyze how activity around particular people
is increasing, decreasing, or changing, therefore indicating changes in their importance or the growth
of new communities.&lt;/p&gt;
&lt;p&gt;Writing an algorithm to perform that kind of analysis isn’t really the problem, though—it’s writing
one that can scale into the billions of vertices and edges and still perform quickly enough to be useful.
An algorithm that generates one false positive in a million isn’t so bad when you’re dealing with tens
of thousands of items, Bader says, but it gets to be a big problem when you’re talking about billions
of items against which it’s running.&lt;/p&gt;
&lt;p&gt;There are dozens of open-source graph databases available, including popular offerings such as Neo4j
and InfiniteGraph. But, Bader says, “our lab focuses on algorithms that run fast on massive data sets
and that are more accurate than what is traditionally done in social media.”&lt;/p&gt;
&lt;p&gt;Bader’s team recently presented a paper detailing a social media algorithm running atop Stinger that
ran 100 times faster than some previous approaches because the system stores the graph’s previous
state and performs only the minimal amount of processing necessary as new edges are inserted. This
is in contrast to traditional approaches that reprocess the entire graph every time there’s a change.&lt;/p&gt;
&lt;p&gt;That being said, Georgia Tech isn’t alone in analyzing massive amounts of social data with graph
databases. Google’s (GOOG) Pregel had already scaled to billions of vertices and edges as of 2009,
and Facebook is currently analyzing more than a billion edges using Apache Giraph (an open-source,
Hadoop-based Pregel implementation). But those cases—both companies are loaded with smart
engineers, data scientists, and powerful infrastructure—just underscore the importance of what
researchers like Bader are building and releasing as open source.&lt;/p&gt;
&lt;p&gt;But social data isn’t just useful for figuring out who’s influential on Twitter or Facebook—it also can
be used to solve some real problems. Bader says he’s already used graph processing with Twitter data
to determine who was leading resistance units during Egypt’s recent revolution. “Anywhere I can
look at connections between entities,” he says, “these approaches are available.”&lt;/p&gt;
&lt;p&gt;Indeed. On Wednesday afternoon, for example, a group of researchers from the University of Alberta,
University of Connecticut, and University of California at Merced unveiled a new data-based method
that could make it faster, easier, and less expensive to root out culprits in fraud cases.&lt;/p&gt;
&lt;p&gt;The technique uses a method called the Steiner tree to analyze the connections—social, business,
familial, etc.—between the people involved in a given case of fraud. The algorithm is able to
determine the shortest path between two objects, which the researchers posit is especially applicable
to fraud investigations—the person with the shortest path between himself and the crime is probably
the culprit (or at least a solid suspect).&lt;/p&gt;
&lt;p&gt;The fraud researchers’ paper follows the publication in August of a method for determining the source
of everything from a disease outbreak to a Twitter rumor by tracking its spread across a complex
network over time. Their algorithm, the paper’s authors claim, could be particularly effective for
combating cybercrime by tracking computer viruses back to their sources. The more connections (in
the case of social data), or observers, a particular point has, the fewer that are needed to track down
the source point.&lt;/p&gt;
&lt;p&gt;All the algorithms and data frameworks in the world, however, probably won’t make too big a
difference until they’re turned into products that actually work in real-world situations. As the
University of Alberta’s Ray Patterson pointed out in a press release detailing the fraudster-detection
algorithm: “It might take several years or many years before anyone picks it up. But it’s a good thing
if we can point people towards what’s useful.”&lt;/p&gt;
&lt;p&gt;Georgia Tech’s Bader says Darpa, Intel (INTC), Sandia National Laboratories, and other research
institutions have already used Stinger to tackle some complex data sets, and he suspects the project
has a strong commercial interest, as well. If a company is willing to take Stinger from a project into a
product, it could bring the project’s scale and speed to everything from analyzing customer
interactions to monitoring the changing nature of criminal networks, Bader says.&lt;/p&gt;
&lt;p&gt;Considering the desire from companies of all types to extract some meaning from social data, I have
to think someone will give it a shot.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bloomberg.com/news/articles/2012-09-07/data-analysts-seek-to-make-social-media-more-useful&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.bloomberg.com/news/articles/2012-09-07/data-analysts-seek-to-make-social-media-more-useful&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How DARPA Does Big Data</title>
      <link>http://localhost:1313/blog/20120815-datanami/</link>
      <pubDate>Wed, 15 Aug 2012 16:43:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120815-datanami/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Nicole Hemsoth&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The world lost one of its &lt;a href=&#34;http://www.philipkdick.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;most profound&lt;/a&gt; science fiction authors in the early eighties, long before the flood of data came down the vast virtual mountain.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/darpacover_hu_3ef7fe25a19a2a8e.webp 400w,
               /blog/20120815-datanami/darpacover_hu_9221fa6b3e9fb5a5.webp 760w,
               /blog/20120815-datanami/darpacover_hu_6d047c022f164073.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/darpacover_hu_3ef7fe25a19a2a8e.webp&#34;
               width=&#34;600&#34;
               height=&#34;603&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;It was a sad loss for literature, but it also bore a devastating hole in the hopes of those seeking a modern fortuneteller who could so gracefully grasp the impact of the data-humanity dynamic.  Dick foresaw a massively connected society—and all of the challenges, beauties, frights and potential for invasion or (or safety, depending on your outlook).&lt;/p&gt;
&lt;p&gt;Without dwelling too long in the realm of the fantastic since we’re focusing today on the tangible projects powering the next generation of needs for the wired military and security complex, it’s worth saying now that had he lived long enough, Dick could have seen his world of dreams, data and domination come startlingly to life.&lt;/p&gt;
&lt;p&gt;All of the darkness and damnation of the tales aside, the technological messages about the promises of massive, diverse data continue to resonate with eerie accuracy. On the cusp of this real-time data stream reality, we are seeing the possibilities of stitching together new governments, societies, militaries and economies through data and imagination resonates still. Projects underway now at government and military agencies like the Defense Advanced Research Projects Agency (DARPA) are highlighting these possibilities—and keeping the imaginations of those inclined to wonder what is next for society at large—keenly tuned-in.&lt;/p&gt;
&lt;p&gt;DARPA, like other government agencies worldwide, is struggling to keep up with its lava flow of hot military intellignce data. Research and public sector organizations have become experts at finding new ways to create data, so the challenge has been keeping up with it—effectively running fast enough to stay just ahead of the heat with the hopes of being able to understand its source before the stream hardens and becomes static, useless.&lt;/p&gt;
&lt;p&gt;As many are already aware, these challenges were at the heart of the U.S. &lt;a href=&#34;https://www.datanami.com/datanami/2012-04-05/7_big_winners_in_u.s._big_data_drive.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;government’s recent big data drive&lt;/a&gt;, where funding was doled out to address barriers to making use of the flood of intelligence, research and military data.&lt;/p&gt;
&lt;p&gt;This week we wanted to take a step back and look at how a defense-oriented intelligence and research organization is trying to capture, handle and make the best use of its data flows by highlighting select projects.&lt;/p&gt;
&lt;p&gt;Without further delay, let’s begin with the first big intel data project–&lt;/p&gt;
&lt;h3 id=&#34;who-needs-precogs-when-you-have-adams&#34;&gt;Who Needs Precogs When You Have ADAMS?&lt;/h3&gt;
&lt;p&gt;It’s a sad but relatively commonplace surprise when a solider or government agent whom others might have thought to be in good mental health suddenly begins making bad decisions—either to the detriment of national security or those around him. When this happens, the first reaction is often one of awe, “how could something like this happen—how couldn’t someone know that there was a problem before it got to such a point?”&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/textanalytics_hu_d87d76863fa89d6b.webp 400w,
               /blog/20120815-datanami/textanalytics_hu_35a968eae7830d43.webp 760w,
               /blog/20120815-datanami/textanalytics_hu_a4ef55c8d3e4be71.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/textanalytics_hu_d87d76863fa89d6b.webp&#34;
               width=&#34;319&#34;
               height=&#34;306&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In other words, in the case of a government that has some of the most sophisticated intelligence-gathering and analysis capabilities, how could anything slip through the cracks?&lt;/p&gt;
&lt;p&gt;DARPA is seeking to snag this problem by understanding operative and soldier patterns via network activity and large volumes of data with a $35 million project that has been underway since late 2010.&lt;/p&gt;
&lt;p&gt;According to DARPA, the Anomaly Detection at Multiple Scales (ADAMS) program has been designed to “create, adapt and apply technology to anomaly characterization and detection in massive data sets.” The agency says that triggers in the large data would tip them off to possible “insider threats” in which “malevolent (or possibly inadvertent) actions by a trusted individual are detected against a background of everyday network activity.”&lt;/p&gt;
&lt;p&gt;DARPA says that the importance of anomaly detection is cemented in the “fact that anomalies in data translate to significant, and often critical actionable information.” They claim that operators in the counter-intelligence community are the target end users for ADAMS insider threat detection technology.&lt;/p&gt;
&lt;p&gt;While there are not many details about the actual algorithms or systems used to handle this information, when the project was first announced the agency was seeking an “automated and integrated modeling, correlation, exploitation, prediction, and resource management” system to handle the needs.&lt;/p&gt;
&lt;p&gt;Researchers from Georgia Tech are among those who are helping DARPA with its insider threat detection project. Under the leadership of computer scientist Dr. &lt;strong&gt;David Bader&lt;/strong&gt;, the team has been in the midst of a $9 million, 2-year project to create a suite of algorithms that can scan for such anomalies across a diverse pool of data, including email, text messages, file transfers and other forms of data.&lt;/p&gt;
&lt;p&gt;To develop new approaches for identifying “insider threats” before an incident occurs, Georgia Tech researchers will have access to massive data sets collected from operational environments where individuals have explicitly agreed to be monitored. The information will include electronically recorded activities, such as computer logins, emails, instant messages and file transfers.&lt;/p&gt;
&lt;p&gt;The ADAMS system will be capable of pulling these terabytes of data together and using novel algorithms to quickly analyze the information to discover anomalies.&lt;/p&gt;
&lt;p&gt;“We need to bring together high-performance computing, algorithms and systems on an unprecedented scale because we’re collecting a massive amount of information in real time for a long period of time,” explained Bader. “We are further challenged because we are capturing the information at different rates — keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.”&lt;/p&gt;
&lt;p&gt;We will hold off for now speculating about a massive-scale (okay, let’s say nuclear powered exascale-type facility) that crunches this kind of data on a national scale—and we certainly won’t mention the possibility of being nabbed for a future crime, but needless to say, this type of data mining has significant value for just about every business in existence—not to mention every government.&lt;/p&gt;
&lt;h3 id=&#34;darpas-insight-into-data-oversight&#34;&gt;DARPA’s Insight into Data Oversight&lt;/h3&gt;
&lt;p&gt;There are endless tales from the application and algorithm front to be told, but before we address any more of those, a brief description of systems to handle the DARPA data deluge is in order first.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/darpa2_hu_b32e244c02f6e647.webp 400w,
               /blog/20120815-datanami/darpa2_hu_4c78531dc8f32735.webp 760w,
               /blog/20120815-datanami/darpa2_hu_45f5b488c68afcc8.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/darpa2_hu_b32e244c02f6e647.webp&#34;
               width=&#34;540&#34;
               height=&#34;360&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;DARPA’s Insight Program addresses the critical challenges of working with projects like ADAMS or any other for that matter. After all, without adequate high performance hardware and software systems working in concert across massive, diverse and performance-craving data sets little is possible. Over the last decade, and especially now in the age of nanomachines and “super-sensors” governments have become adept at creating data-generating wonders—but there is a hefty analytics cost involved.&lt;/p&gt;
&lt;p&gt;The agency’s Insight Program seeks to mitigate those virtual costs to handling the waves of data, especially for the benefit of soldiers in need of real-time sensemaking from it. DARPA is working toward the development of “an adaptable, integrated human-machine Exploitation and Resource Management System (E&amp;amp;RM) System.” They describe this more specifically as a “next generation intelligence, surveillance and reconnaissance (ISR) system that, through the development of semi- and fully automated technologies, can provide real-time or near real-time capabilities in direct support of tactical users on the battlefield.”&lt;/p&gt;
&lt;p&gt;This is no small task, of course. For instance, real-time intelligence for soldiers on the field means gathering, meshing and analyzing data from multiple sources of diverse data types (from all those many new, complex sensors, text, image or video, etc) in addition to melding that data with the info from other systems, including for example, behavioral discovery and prediction algorithm-derived intelligence.&lt;/p&gt;
&lt;p&gt;As Henry Kenyon detailed upon the first word of the Insight Program goas, “The shortcomings of current ISR platforms and systems include a lack of automated tools to interpret, edit and weave data streams into a form useful to human analysts. According to the solicitation, vital information is often lost or overlooked due to the overwhelming flow of incoming data. A lack of integrated human-machine reasoning tools limits the ability of system to use operators’ knowledge and ability to understand complex data.”&lt;/p&gt;
&lt;p&gt;If successful, the program will achieve a number of DARPA’s data handling goals, including the ability to replace existing stovepipes with an integrated system that operates across national, theatre and lower-level tactical intelligence systems. This means they would be able to ultimately create a mission and sensor-agnostic system that would work across different theatres of operation and promote greater collaboration between different intelligence and military analyst communities and agencies.&lt;/p&gt;
&lt;p&gt;The most recent word is that the program, following a field test earlier this year, showed full functionality of the E&amp;amp;RM System to perform sequence-neutral (i.e., out-of-order) fusion of data from multi-INT sources, as well as graph-based multi-INT fusion.&lt;/p&gt;
&lt;p&gt;According to DARPA, “The field test also produced a unique, multi-modality, high-fidelity truthed data set which is available to ISR researchers across the Department of Defense and Intelligence Community.  This data set, combined with the foundational data set collected in the Fall of 2010, provides an unparalleled 135-terabyte resource to more than 240 users across government, industry and academia.”&lt;/p&gt;
&lt;h3 id=&#34;its-all-in-the-minds-eye&#34;&gt;It’s All in the Mind’s Eye&lt;/h3&gt;
&lt;p&gt;One of the more prominent big data projects that has rolled out of DARPA in recent years has been the Mind’s Eye project. Despite is creepy, Big Brother-esque moniker, the project could potentially save the lives of soldiers via the use of a truly smart “camera” system that can use vast amounts of diverse data to describe a landscape or situation—without putting human lives at risk.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/CONNECTOME3_hu_99d6792894e31e0d.webp 400w,
               /blog/20120815-datanami/CONNECTOME3_hu_e46cd0f34f231edb.webp 760w,
               /blog/20120815-datanami/CONNECTOME3_hu_3058ae99dcf37a47.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/CONNECTOME3_hu_99d6792894e31e0d.webp&#34;
               width=&#34;270&#34;
               height=&#34;270&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The technology behind this smart camera hails from machine-based visual intelligence. The program will be able to use these cameras to develop the capability for remote visual intelligence by automating the ability to learn “generally applicable and generative representations of action between objects in a scene directly from visual inputs, and then reason over those learned representations.”&lt;/p&gt;
&lt;p&gt;The smart cameras would replace the traditional mode of surveillance, which required dangerous missions that necessitated temporary observation post set-up and constant monitoring of the site. These cameras could take data from visual scenarios and describe in vivid textual detail what it sees, what is obstructing its view, and what it is able to reason algorithmically from what it is before it. Further, the agency could train these cameras to report only on a select set of activities or flagged actions to minimize the flood of data.&lt;/p&gt;
&lt;p&gt;DARPA says this project differs from of the other commercial and security applications of machine vision that have found their way to market in that is their system has made continual progress in recognizing a wide range of objects and their properties—what might be thought of as the nouns in the description of a scene.  The agency claims that the focus of Mind’s Eye is to add the perceptual and cognitive underpinnings for recognizing and reasoning about the verbs in those scenes, enabling a more complete narrative of action in the visual experience.&lt;/p&gt;
&lt;p&gt;The most recent update on the success of the endeavor claims that in the first 18 months of the program, Mind’s Eye “demonstrated fundamentally new capabilities in visual intelligence, including the ability of automated systems to recognize actions they had never seen, describe observed events using simple text messages, and flag anomalous behaviors.” DARPA is looking ahead to new possibilities, noting that precision and accuracy tweaks, filling temporal gaps (answering “What just happened?” and “What might happen next?”), and answering questions about events in a scene are on their bucket list of improvements.&lt;/p&gt;
&lt;p&gt;As one might imagine, one of the biggest hurdles to the program is on the computational horsepower side; the team says they need to lower the computational requirements of visual intelligence to address operational use constraints, such as power requirements for unmanned ground vehicles.&lt;/p&gt;
&lt;h3 id=&#34;the-coming-of-xdata&#34;&gt;The Coming of XDATA&lt;/h3&gt;
&lt;p&gt;One of the most highly-publicized examples of DARPA’s focus on big data comes in the form of the XDATA program, which is a broad-based initiative to put the best minds in applied mathematics, computer science and visualization to work to fine-tune and create new tools for managing massive military and intelligence data.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/DARPA_2_hu_58204899b4476d1a.webp 400w,
               /blog/20120815-datanami/DARPA_2_hu_37a111478ef6c0ca.webp 760w,
               /blog/20120815-datanami/DARPA_2_hu_7ffaf78a394785e1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/DARPA_2_hu_58204899b4476d1a.webp&#34;
               width=&#34;465&#34;
               height=&#34;349&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The needs that created the program are simple in theory, and we’ve already touched on the massive sensor problem (many sensors, but without enough mature ways to handle all the diverse data they are feeding). DARPA says that the Department of Defense has been challenged in how it uses, fuses and analyzes all of the data from the many military networks.&lt;/p&gt;
&lt;p&gt;The DoD openly said during the launch of the XDATA program that the systems they had in place for processing, handling and analyzing all of the information from multiple intelligence networks was not scaling to fit their needs. Additionally, they noted that the “volume and characteristics of the data and the range of applications for data analysis require a fundamentally new approach to data science, analysis and incorporation into mission planning on timelines consistent with operational tempo.”&lt;/p&gt;
&lt;p&gt;DARPA began the XDATA program to develop computational techniques and software tools for processing and analyzing the vast amount of mission-oriented information for Defense activities.  As part of this exploration, XDATA aims to address the need for scalable algorithms for processing and visualization of imperfect and incomplete data.  And because of the variety of DoD users, XDATA leaders says they anticipate the creation of human-computer interaction tools that could be easily customized for different missions.&lt;/p&gt;
&lt;h3 id=&#34;military-reading-machines&#34;&gt;Military Reading Machines&lt;/h3&gt;
&lt;p&gt;Examples of machine reading programs are not unique to large-scale military agencies, but DARPA has high hopes for its program, which takes a different approach. While traditional text processing research has emphasized locating specific text and transforming into other forms of text (via translation or summaries) the goal has always been to make it readable by humans.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120815-datanami/predictive_analytics1_hu_da96b99dc64db799.webp 400w,
               /blog/20120815-datanami/predictive_analytics1_hu_c1f310aefbc624ec.webp 760w,
               /blog/20120815-datanami/predictive_analytics1_hu_9d3b8ca32ecfbb81.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120815-datanami/predictive_analytics1_hu_da96b99dc64db799.webp&#34;
               width=&#34;413&#34;
               height=&#34;413&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Their program, on the other hand, will involve little to no human interpretation—machines will “learn to read from a few examples and will read to learn what they need in order to answer questions or perform some reasoning task.”&lt;/p&gt;
&lt;p&gt;DARPA says that when it comes to operational warfighters, the amount of textual information from reports, email and other communications and the rapid processing required to make it useful is a strain on internal systems. The agency claims that when it comes to the need for this Machine Reading program, “AI offers a promising approach to this problem, however the cost of handcrafting information within the narrow confines of first order logic or other AI formalisms is currently prohibitive for many applications.&lt;/p&gt;
&lt;p&gt;The Machine Reading program seeks to address these challenges by replacing “knowledge engineers” with “self-supervised learning systems” that are able to understand natural text and dump into the proper AI holding tank for more refined processing and machine reasoning.&lt;/p&gt;
&lt;p&gt;The team behind the project has been working on building universal text engines that will capture information from text, then transform it into the formal representations used by artificial intelligence applications. This involves a number of specific elements, including designing a system that can select and annotate text, the creation of model reasoning systems that the reading systems will interact with, and the formulation of question and answer sets with the appropriate protocols to determine progress.&lt;/p&gt;
&lt;p&gt;For those interested in the specifics of the algorithms and applications, there is a detailed paper that describes &lt;a href=&#34;http://hnk.ffzg.hr/bibl/lrec2010/pdf/862_Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the concepts&lt;/a&gt; available.&lt;/p&gt;
&lt;h3 id=&#34;on-the-darpa-data-horizon&#34;&gt;On the DARPA Data Horizon&lt;/h3&gt;
&lt;p&gt;As we have noted in the past, the government as a whole is putting &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFEQFjAA&amp;amp;url=http%3A%2F%2Fwww.datanami.com%2Fdatanami%2F2012-04-05%2F7_big_winners_in_u.s._big_data_drive.html&amp;amp;ei=8wQsUJq0JpDC9QTfz4HwDg&amp;amp;usg=AFQjCNEN31cjfxkebZ9ESEYaluq2OWhzOw&amp;amp;sig2=XOio5nIZdMPiv7pQ27GANQ&amp;amp;cad=rja&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;significant emphasis (and money)&lt;/a&gt; on the possibilities of big data analytics and systems.&lt;/p&gt;
&lt;p&gt;Among a few of the projects that are included in DARPA’s big data portfolio are other notworthy additions, including:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Programming Computation on Encrypted Data (PROCEED)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PROCEED is a research effort that seeks to develop methods that allow computing with encrypted data without first decrypting it, making it more difficult for malware programmers to write viruses.  The Video and Image Retrieval and Analysis Tool (VIRAT) program aims to develop a system to provide military imagery analysts with the capability to exploit the vast amount of overhead video content being collected. If successful, VIRAT will enable analysts to establish alerts for activities and events of interest as they occur. VIRAT also seeks to develop tools that would enable analysts to rapidly retrieve, with high precision and recall, video content from extremely large video libraries.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cyber-Insider Threat (CINDER) Program&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This effort is looking for new approaches to detect activities consistent with cyber espionage in military computer networks. As a means to expose hidden operations, CINDER will apply various models of adversary missions to “normal” activity on internal networks. CINDER also aims to increase the accuracy, rate and speed with which cyber threats are detected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Mission-oriented Resilient Clouds Program&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The goal of this effort is to address security challenges inherent in cloud computing by developing technologies to detect, diagnose and respond to attacks, effectively building a “community health system” for the cloud. The program also aims to develop technologies to enable cloud applications and infrastructure to continue functioning while under attack. The loss of individual hosts and tasks within the cloud ensemble would be allowable as long as overall mission effectiveness was preserved.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Related Stories&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFEQFjAA&amp;amp;url=http%3A%2F%2Fwww.datanami.com%2Fdatanami%2F2012-04-05%2F7_big_winners_in_u.s._big_data_drive.html&amp;amp;ei=8wQsUJq0JpDC9QTfz4HwDg&amp;amp;usg=AFQjCNEN31cjfxkebZ9ESEYaluq2OWhzOw&amp;amp;sig2=XOio5nIZdMPiv7pQ27GANQ&amp;amp;cad=rja&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Seven Big Winners in the U.S. Big Data Drive&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/datanami/2012-06-22/world_s_top_data-intensive_systems_unveiled.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World’s Top Data-Intensive Systems Unveiled&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/datanami/2012-03-30/government_puts_$200_million_behind_big_data_initiative.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Government Puts $200 Million Behind Big Data Initiative&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datanami.com/2012/08/15/how_darpa_does_big_data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.datanami.com/2012/08/15/how_darpa_does_big_data/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your Laptop Can Now Analyze Big Data</title>
      <link>http://localhost:1313/blog/20120717-mit-technology-review/</link>
      <pubDate>Tue, 17 Jul 2012 17:42:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120717-mit-technology-review/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Pavlus&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Computer scientists from Carnegie Mellon University&lt;/strong&gt; have devised a framework for running large-scale computations for tasks such as social network or Web search analysis efficiently on a single personal computer.&lt;/p&gt;
&lt;p&gt;The software could help developers working on many modern tasks: for example, designing a new recommendation engine using social network connections. In order to make effective recommendations—“your friends liked this movie, so here is another movie that you haven’t seen yet, but you will probably like”—the software has to be able to analyze the connections between the members of a social network. This type of task is called graph computation, and it is increasingly common. But working with large-scale data sets (such as online social networks) usually requires the processing horsepower of many computers clustered together, such as those offered by Amazon’s cloud-based EC2 service.&lt;/p&gt;
&lt;p&gt;The new software, called GraphChi, exploits the capacious hard drives that are becoming ever more common in personal computers. A graph would normally be stored in temporary memory (RAM) for analysis. With GraphChi, the hard drive performs this task instead.&lt;/p&gt;
&lt;p&gt;“PCs don’t have enough RAM to hold an entire Web graph, but they do have hard drives, which can hold a lot of information,” says Carlos Guestrin, codirector of Carnegie Mellon’s Select Lab, where GraphChi was developed. But hard drives are slow compared to RAM for reading and writing data, which tends to slow down computation. So Guestrin’s student Aapo Kyrola designed a faster, less random method of accessing the hard drive.&lt;/p&gt;
&lt;p&gt;According to Guestrin, a Mac Mini running GraphChi can analyze Twitter’s social graph from 2010—which contains 40 million users and 1.2 billion connections—in 59 minutes. “The previous published result on this problem took 400 minutes using a cluster of about 1,000 computers,” Guestrin says.&lt;/p&gt;
&lt;p&gt;As technology gets more networked, and data sets get larger, graph computation is becoming more and more relevant in many domains, says &lt;strong&gt;David A. Bader&lt;/strong&gt;, a graph computation expert at Georgia Tech. “Trying to understand how the human brain works or trying to make sense of medical patient records involve graph computing,” he says.&lt;/p&gt;
&lt;p&gt;Graph analysis also drives the development of new web products, says Jeremy Kepner, a researcher at MIT. “Document search, ad placement, route planning, travel reservations, and cyber security all rely on graph analysis,” he says. “Enabling web developers to construct these analyses on their desktop computers catalyzes these industries and accelerates product development.”&lt;/p&gt;
&lt;p&gt;Guestrin adds that GraphChi can handle “streaming graphs,” which more accurately model large networks by showing how relationships change over time. Bader and others at Georgia Tech have created a graph computation framework, called Stinger, that’s optimized for supercomputers working with massive streaming graphs.&lt;/p&gt;
&lt;p&gt;“The scales of these problems will obviously keep growing,” says Guestrin. But he says GraphChi is capable of effectively handling many large-scale graph-computing problems without resorting to cloud-based solutions or supercomputers.&lt;/p&gt;
&lt;p&gt;“A researcher in computational biology could do large-scale computations on their PC; a developer working on a data-center algorithm can test it on their laptop before pushing it to the cloud,” Guestrin says. “Big data is everywhere now, but some big data isn’t as big as it once was, relatively speaking. Tools like GraphChi will let many companies and startups solve all their graph-computing needs on a single machine. It’s cost effective, and it drives innovation, too.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technologyreview.com/s/428497/your-laptop-can-now-analyze-big-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technologyreview.com/s/428497/your-laptop-can-now-analyze-big-data/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-messenger Astrophysics: Challenges and Opportunities in Big data and Computing</title>
      <link>http://localhost:1313/blog/20120714-gatech/</link>
      <pubDate>Sat, 14 Jul 2012 18:39:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120714-gatech/</guid>
      <description>&lt;h2 id=&#34;agenda&#34;&gt;AGENDA&lt;/h2&gt;
&lt;p&gt;1:30-2:00 coffee&lt;/p&gt;
&lt;p&gt;2:00-3:00 Ed Seidel’s talk title &amp;ldquo;Computing and Data Challenges for Multi-Messenger Astronomy”&lt;/p&gt;
&lt;p&gt;3:00-3:15 Bread for bagels&lt;/p&gt;
&lt;p&gt;3:15-4:00 Panel Discussion “Big Data and the National Data Service”&lt;/p&gt;
&lt;p&gt;Panelists: Gab Allen, &lt;strong&gt;David Bader&lt;/strong&gt;, Ed Seidel, John Wise and Srinivas Aluru&lt;/p&gt;
&lt;h2 id=&#34;talk-information&#34;&gt;TALK INFORMATION&lt;/h2&gt;
&lt;p&gt;SPEAKER:  H. Edward Seidel, NCSA director&lt;/p&gt;
&lt;p&gt;TITLE:  Computing and Data Challenges for Multi-Messenger Astronomy&lt;/p&gt;
&lt;p&gt;ABSTRACT:  Multi-messenger astronomy, that is, the ability to view and understand the universe though multiple &amp;ldquo;messengers&amp;rdquo; (e.g., gravitational waves, electromagnetic (including optical, infrared and radio), neutrinos, and cosmic rays), along with models and simulations will bring a revolution in our understanding of the universe and its constituents.  Each one of these fields is itself undergoing a revolution. In isolation, each may lead to Nobel Prize caliber discoveries in the coming decade; combining them for comprehensive understanding of great mysteries is where the true Grand Challenges of the Universe lie &amp;mdash; to answer fundamental questions such as what is a gamma-ray burst, or what is dark energy. I will describe basic concepts of multi-messenger approaches to astronomical observation and simulation, and describe the tremendous computing challenges these approaches bring.&lt;/p&gt;
&lt;p&gt;BIO:  NCSA director H. Edward Seidel is a distinguished researcher in high-performance computing and relativity and astrophysics with an outstanding track record as a researcher and administrator. In addition to leading NCSA, he is also a Founder Professor in the University of Illinois Department of Physics and a professor in the Department of Astronomy.&lt;/p&gt;
&lt;p&gt;His previous leadership roles include serving as the senior vice president of research and innovation at the Skolkovo Institute of Science and Technology in Moscow, directing the Office of Cyberinfrastructure and serving as assistant director for Mathematical and Physical Sciences at the U.S. National Science Foundation, and leading the Center for Computation &amp;amp; Technology at Louisiana State University.&lt;/p&gt;
&lt;p&gt;Seidel is a fellow of the American Physical Society and of the American Association for the Advancement of Science, as well as a member of the Institute of Electrical and Electronics Engineers, and the Society for Industrial and Applied Mathematics. His research has been recognized by a number of awards, including the 2006 IEEE Sidney Fernbach Award.&lt;/p&gt;
&lt;p&gt;He earned a master’s degree in physics at the University of Pennsylvania in 1983 and a doctorate in relativistic astrophysics at Yale University in 1988.&lt;/p&gt;
&lt;h2 id=&#34;panel-information&#34;&gt;PANEL INFORMATION&lt;/h2&gt;
&lt;p&gt;PANELISTS:  Gab Allen, &lt;strong&gt;David Bader&lt;/strong&gt;, Ed Seidel, John Wise and Srinivas Aluru&lt;/p&gt;
&lt;p&gt;TITLE:  Big Data and National Data Center Panel&lt;/p&gt;
&lt;p&gt;DESCRIPTION:  a  discussion about opportunities for partnering on multi-messenger astronomy and other drivers of big data and high performance computing, including their impact on a National Data Service.&lt;/p&gt;
&lt;h2 id=&#34;panel-bios&#34;&gt;PANEL BIOS:&lt;/h2&gt;
&lt;h3 id=&#34;gabrielle-allen&#34;&gt;Gabrielle Allen&lt;/h3&gt;
&lt;p&gt;Gabrielle Allen is a  full professor of Astronomy at the University of Illinois Urbana-Champaign and the Associate Director for Computational Research and Education Programs at the National Center for Supercomputing Applications.  She received her Ph.D.  in computational astrophysics from Cardiff University in 1993.&lt;/p&gt;
&lt;h3 id=&#34;david-bader&#34;&gt;David Bader&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is a Full Professor and Chair of the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director of High Performance Computing. He received his Ph.D. in 1996 from The University of Maryland, and his research is supported through highly-competitive research awards, primarily from NSF, NIH, DARPA, and DOE&lt;/p&gt;
&lt;h3 id=&#34;ed-seidel&#34;&gt;Ed Seidel&lt;/h3&gt;
&lt;p&gt;NCSA director H. Edward Seidel is a distinguished researcher in high-performance computing and relativity and astrophysics with an outstanding track record as a researcher and administrator. In addition to leading NCSA, he is also a Founder Professor in the University of Illinois Department of Physics and a professor in the Department of Astronomy.&lt;/p&gt;
&lt;h3 id=&#34;john-wise&#34;&gt;John Wise&lt;/h3&gt;
&lt;p&gt;John Wise is an assistant professor in the School of Physics at Georgia Institute of Technology.  He received his Ph.D. from Stanford University in 2007.  His research is in computational cosmology including high performance computing to understanding the first stars and galaxies.&lt;/p&gt;
&lt;h3 id=&#34;srinivas-aluru&#34;&gt;Srinivas Aluru&lt;/h3&gt;
&lt;p&gt;Srinivas Aluru is a professor in the School of Computational Science and Engineering within the College of Computing at Georgia Institute of Technology. He conducts research in high performance computing, bioinformatics and systems biology, combinatorial scientific computing, and applied algorithms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ideas.gatech.edu/hg/item/307241&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ideas.gatech.edu/hg/item/307241&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feds Look to Fight Leaks with &#39;Fog of Disinformation&#39;</title>
      <link>http://localhost:1313/blog/20120703-wired/</link>
      <pubDate>Tue, 03 Jul 2012 18:02:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120703-wired/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Noah Shachtman&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-air-force-one-waits-for-us-president-barack-obama-in-the-fog-at-londons-stansted-airport-friday-april-3-2009-photo-ap--kirsty-wigglesworth&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Air Force One waits for U.S. President Barack Obama in the fog at London&amp;#39;s Stansted Airport, Friday, April 3, 2009. PHOTO: AP / KIRSTY WIGGLESWORTH&#34; srcset=&#34;
               /blog/20120703-wired/AP090103035458_hu_1ca0c0d12abb44df.webp 400w,
               /blog/20120703-wired/AP090103035458_hu_c4486ecd86e6542f.webp 760w,
               /blog/20120703-wired/AP090103035458_hu_c54dadeea9736a0b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120703-wired/AP090103035458_hu_1ca0c0d12abb44df.webp&#34;
               width=&#34;582&#34;
               height=&#34;364&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Air Force One waits for U.S. President Barack Obama in the fog at London&amp;rsquo;s Stansted Airport, Friday, April 3, 2009. PHOTO: AP / KIRSTY WIGGLESWORTH
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Pentagon-funded researchers have&lt;/strong&gt; come up with a new plan for busting leakers: Spot them by how they search, and then entice the secret-spillers with decoy documents that will give them away.&lt;/p&gt;
&lt;p&gt;Computer scientists call it it &amp;ldquo;Fog Computing&amp;rdquo; – a play on today&amp;rsquo;s cloud computing craze. And in a &lt;a href=&#34;http://dsearch.dtic.mil/search?q=cache:FsgWcwTFmzAJ:www.dtic.mil/dtic/tr/fulltext/u2/a552461.pdf&amp;#43;%22Allure&amp;#43;Security%22&amp;amp;site=tr_all&amp;amp;client=dticol_frontend&amp;amp;proxystylesheet=dticol_frontend&amp;amp;ie=UTF-8&amp;amp;access=p&amp;amp;oe=UTF-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt; for Darpa, the Pentagon&amp;rsquo;s premiere research arm, researchers say they&amp;rsquo;ve built &amp;ldquo;a prototype for automatically generating and distributing believable misinformation &amp;hellip; and then tracking access and attempted misuse of it. We call this &amp;lsquo;disinformation technology.&amp;rsquo;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Two small problems: Some of the researchers&amp;rsquo; techniques are barely distinguishable from spammers&amp;rsquo; tricks. And they could wind up undermining trust among the nation&amp;rsquo;s secret-keepers, rather than restoring it.&lt;/p&gt;
&lt;p&gt;The Fog Computing project is part of a &lt;a href=&#34;https://www.wired.com/dangerroom/2011/11/darpa-trap-wikileaks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;broader assault&lt;/a&gt; on so-called &amp;ldquo;&lt;a href=&#34;https://www.wired.com/dangerroom/2010/08/darpas-star-hacker-looks-to-wikileak-proof-the-pentagon/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;insider threats&lt;/a&gt;,&amp;rdquo; launched by Darpa in 2010 after the WikiLeaks imbroglio. Today, Washington is gripped by another frenzy over leaks – this time over disclosures about U.S. cyber sabotage and drone warfare programs. But the reactions to these leaks has been schizophrenic, to put it generously. The nation&amp;rsquo;s top spy says America&amp;rsquo;s intelligence agencies will be strapping suspected leakers to lie detectors – even though the polygraph machines are famously flawed. An investigation into who spilled secrets about the &lt;a href=&#34;https://www.wired.com/threatlevel/2012/06/obama-ordered-stuxnet-continued/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stuxnet cyber weapon&lt;/a&gt; and the drone &amp;ldquo;&lt;a href=&#34;https://www.wired.com/dangerroom/2012/06/obama-yemen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kill list&lt;/a&gt;&amp;rdquo; has &lt;a href=&#34;http://thecable.foreignpolicy.com/posts/2012/06/26/hundreds_of_officials_summoned_for_justice_department_national_security_leak_invest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;already ensnared hundreds of officials&lt;/a&gt; – even though the reporters who disclosed the info patrolled the halls of power with the White House&amp;rsquo;s blessing.&lt;/p&gt;
&lt;p&gt;That leaves &lt;a href=&#34;http://www.washingtonian.com/blogs/capitalcomment/print/2012/06/14/the-obama-administrations-war-on-information-leaks.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;electronic tracking as the best means of shutting leakers down&lt;/a&gt;. And while you can be sure that counterintelligence and Justice Department officials are going through the e-mails and phone calls of suspected leakers, such methods have their limitations. Hence the interest in Fog Computing.&lt;/p&gt;


















&lt;figure  id=&#34;figure-an-air-force-poster-warning-troops-to-maintain-operational-security-or-opsec-courtesy-usaf-screen-shot-of-program&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An Air Force poster, warning troops to maintain operational security, or &amp;#34;OPSEC.&amp;#34; Courtesy USAF *SCREEN SHOT OF PROGRAM*.&#34; srcset=&#34;
               /blog/20120703-wired/ScreenShot010_hu_995a4a99804fcdc4.webp 400w,
               /blog/20120703-wired/ScreenShot010_hu_fa939f524d9ba33c.webp 760w,
               /blog/20120703-wired/ScreenShot010_hu_877a812ac78e6231.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120703-wired/ScreenShot010_hu_995a4a99804fcdc4.webp&#34;
               width=&#34;289&#34;
               height=&#34;161&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An Air Force poster, warning troops to maintain operational security, or &amp;ldquo;OPSEC.&amp;rdquo; Courtesy USAF &lt;em&gt;SCREEN SHOT OF PROGRAM&lt;/em&gt;.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The first goal of Fog Computing is to bury potentially valuable information in a pile of worthless data, making it harder for a leaker to figure out what to disclose.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Imagine if some chemist invented some new formula for whatever that was of great value, growing hair, and they then placed the true [formula] in the midst of a hundred bogus ones,&amp;rdquo; explains &lt;a href=&#34;http://www.cs.columbia.edu/~sal/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Salvatore Stolfo&lt;/a&gt;, the Columbia University computer science professor who coined the Fog Computing term. &amp;ldquo;Then anybody who steals the set of documents would have to test each formula to see which one actually works. It raises the bar against the adversary. They may not really get what they&amp;rsquo;re trying to steal.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The next step: Track those decoy docs as they cross the firewall. For that, Stolfo and his colleagues embed documents with covert beacons called &amp;ldquo;&lt;a href=&#34;http://en.wikipedia.org/wiki/Web_bug&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;web bugs&lt;/a&gt;,&amp;rdquo; which can monitor users&amp;rsquo; activities without their knowledge. They&amp;rsquo;re popular with online ad networks. &amp;ldquo;When rendered as HTML, a web bug triggers a server update which allows the sender to note when and where the web bug was viewed,&amp;rdquo; the researchers write. &amp;ldquo;Typically they will be embedded in the HTML portion of an email message as a non-visible white on white image, but they have also been demonstrated in other forms such as Microsoft Word, Excel, and PowerPoint documents.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Unfortunately, they have been most closely associated with unscrupulous operators, such as spammers, virus writers, and spyware authors who have used them to violate users privacy,&amp;rdquo; the researchers admit. &amp;ldquo;Our work leverages the same ideas, but extends them to other document classes and is more sophisticated in the methods used to draw attention. In addition, our targets are insiders who should have no expectation of privacy on a system they violate.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Steven Aftergood, who studies classification policies for the &lt;a href=&#34;http://www.fas.org/blog/secrecy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federation of American Scientists&lt;/a&gt;, wonders whether the whole approach isn&amp;rsquo;t a little off base, given Washington&amp;rsquo;s funhouse system for determining what should be secret. In June, for example, the National Security Agency refused to disclose how many Americans it had wiretapped without a warrant. The reason? &lt;a href=&#34;https://www.wired.com/dangerroom/2012/06/nsa-spied/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;It would violate Americans&amp;rsquo; privacy to say so&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If only researchers devoted as much ingenuity to combating spurious secrecy and needless classification. Shrinking the universe of secret information would be a better way to simplify the task of securing the remainder,&amp;rdquo; Aftergood tells Danger Room in an e-mail. &amp;ldquo;The Darpa approach seems to be based on an assumption that whatever is classified is properly classified and that leaks may occur randomly throughout the system. But neither of those assumptions is likely to be true.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Stolfo, for his part, insists that he&amp;rsquo;s merely doing &amp;ldquo;basic research,&amp;rdquo; and nothing Pentagon-specific. What Darpa, the Office of Naval Research, and other military technology organizations do with the decoy work is &amp;ldquo;not my area of expertise,&amp;rdquo; he adds. However, Stolfo has set up a firm, &lt;a href=&#34;http://www.alluresecurity.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allure Security Technology Inc.&lt;/a&gt;, &amp;ldquo;to create industrial strength software a company can actually use,&amp;rdquo; as he puts it. That software should be ready to implement by the end of the year.&lt;/p&gt;
&lt;p&gt;It will include more than bugged documents. Stolfo and his colleagues have also been working on what they call a &amp;ldquo;misbehavior detection&amp;rdquo; system. It includes some standard network security tools, like an intrusion detection system that watches out for unauthorized exfiltration of data. And it has some rather non-standard components – like an alert if a person searches his computer for something surprising.&lt;/p&gt;


















&lt;figure  id=&#34;figure-pfc-bradley-manning-is-escorted-to-a-courthouse-in-december-2011-his-alleged-disclosures-to-wikileaks-kickstarted-pentagon-interest-in-catching-so-called-insider-threats-photo-patrick-semanskyap&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Pfc. Bradley Manning is escorted to a courthouse in December 2011. His alleged disclosures to WikiLeaks kickstarted Pentagon interest in catching so-called &amp;#34;insider threats.&amp;#34; Photo: Patrick Semansky/AP.&#34; srcset=&#34;
               /blog/20120703-wired/manning_three_hu_c8bba6dd1fe8545a.webp 400w,
               /blog/20120703-wired/manning_three_hu_8bcc836bd3c2bbf5.webp 760w,
               /blog/20120703-wired/manning_three_hu_c2bab892ce815d87.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120703-wired/manning_three_hu_c8bba6dd1fe8545a.webp&#34;
               width=&#34;532&#34;
               height=&#34;350&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pfc. Bradley Manning is escorted to a courthouse in December 2011. His alleged disclosures to WikiLeaks kickstarted Pentagon interest in catching so-called &amp;ldquo;insider threats.&amp;rdquo; Photo: Patrick Semansky/AP.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&amp;ldquo;Each user searches their own file system in a unique manner. They may use only a few specific system functions to find what they are looking for. Furthermore, it is unlikely a masquerader will have full knowledge of the victim user&amp;rsquo;s file system and hence may search wider and deeper and in a less targeted manner than would the victim user. Hence, we believe search behavior is a viable indicator for detecting malicious intentions,&amp;rdquo; Stolfo and his colleagues write.&lt;/p&gt;
&lt;p&gt;In their initial experiments, the researchers claim, they were about to &amp;ldquo;model all search actions of a user&amp;rdquo; in a mere 10 seconds. They then gave 14 students unlimited access to the same file system for 15 minutes each. The students were told to comb the machine for anything that might be used to financial gain. The researchers say they caught all 14 searchers. &amp;ldquo;We can detect all masquerader activity with 100 percent accuracy, with a false positive rate of 0.1 percent.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Grad students may be a little easier to model than national security professionals, who have to radically alter their search patterns in the wake of major events. Consider the elevated interest in al-Qaida after 9 / 11, or the desire to know more about WikiLeaks after Bradley Manning allegedly disclosed hundreds of thousands of documents to the group.&lt;/p&gt;
&lt;p&gt;Other Darpa-backed attempts to find a signature for squirrely behavior are either just getting underway, or haven&amp;rsquo;t fared particularly well. In December, the agency recently handed out $9 million to a &lt;strong&gt;Georgia Tech-led consortium&lt;/strong&gt; with the goal of mining &lt;a href=&#34;https://www.wired.com/dangerroom/2011/12/darpa-email/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;250 million e-mails, IMs and file transfers a day&lt;/a&gt; for potential leakers. The following month, a &lt;a href=&#34;http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA557327&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pentagon-funded research paper&lt;/a&gt; (.pdf) noted the promise of &amp;ldquo;keystroke dynamics – technology to distinguish people based on their typing rhythms – [which] could revolutionize insider-threat detection. &amp;quot; Well, in theory. In practice, such systems&amp;rsquo; &amp;ldquo;error rates vary from 0 percent to 63 percent, depending on the user. Impostors triple their chance of evading detection if they touch type.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;For more reliable results, Stolfo aims to marry his misbehavior-modeling with the decoy documents and with other so-called &amp;ldquo;enticing information.&amp;rdquo; Stolfo and his colleagues also use &amp;ldquo;honeytokens&amp;rdquo; – small strings of tempting information, like online bank accounts or server passwords – as bait. They&amp;rsquo;ll get a one-time credit card number, link it to a PayPal account, and see if any charges are mysteriously rung up. They&amp;rsquo;ll generate a Gmail account, and see who starts spamming.&lt;/p&gt;
&lt;p&gt;Most intriguingly, perhaps, is Stolfo&amp;rsquo;s suggestion in a &lt;a href=&#34;http://www.ieee-security.org/TC/SPW2012/proceedings/4740a125.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;separate paper&lt;/a&gt; (.pdf) to fill up social networks with decoy accounts – and inject poisonous information into people&amp;rsquo;s otherwise benign social network profiles.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Think of &lt;a href=&#34;https://www.wired.com/business/2010/05/facebook-debuts-simplified-privacy-settings/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;advanced privacy settings&lt;/a&gt; [in sites like Facebook] where I choose to include my real data to my closest friends [but] everybody else gets access to a different profile with information that is bogus. And I would be alerted when bad guys try to get that info about me,&amp;rdquo; Stolfo tells Danger Room. &amp;ldquo;This is a way to create fog so that now you no longer know the truth abut a person through this artificial avatars or artificial profiles.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;So sure, Fog Computing could eventually become a way to keep those Facebooked pictures of your cat free from prying eyes. If you&amp;rsquo;re in the U.S. government, on the other hand, the system could be a method for hiding the truth about something far more substantive.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wired.com/2012/07/fog-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wired.com/2012/07/fog-computing/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputer Learns How to Recognize Cats</title>
      <link>http://localhost:1313/blog/20120628-hpcwire/</link>
      <pubDate>Thu, 28 Jun 2012 17:04:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120628-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Robert Gelber&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Search giant Google along with researchers from Stanford University have made an interesting discovery based on an X labs project. After being fed 10 million images from YouTube, a 16,000-core cluster learned how to recognize various objects, including cats. Earlier this week, the New York Times &lt;a href=&#34;https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html?pagewanted=1&amp;amp;_r=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;detailed&lt;/a&gt; the program, explaining its methods and potential use cases.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120628-hpcwire/Cat_6_28_hu_7f2e9f9ea85991b6.webp 400w,
               /blog/20120628-hpcwire/Cat_6_28_hu_a8b2c756e61829d6.webp 760w,
               /blog/20120628-hpcwire/Cat_6_28_hu_d75323ddbc2822c4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120628-hpcwire/Cat_6_28_hu_7f2e9f9ea85991b6.webp&#34;
               width=&#34;200&#34;
               height=&#34;220&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The project began a few years ago when Google researchers planned to make a human brain simulation. The compute cluster acted as the brain’s neural network, tasked with learning on its own and using the Internet as a source of information. After processing millions of unlabeled YouTube thumbnails, the system taught itself how to recognize a cat.  While the video website is known for its &lt;a href=&#34;https://www.youtube.com/results?search_query=cats&amp;amp;oq=cats&amp;amp;gs_l=youtube.3..0l10.4047.4455.0.4566.4.4.0.0.0.0.65.213.4.4.0...0.0.HBHyajSkWPI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;comprehensive collection&lt;/a&gt; of user submitted feline antics, project researchers were focused on the simulation’s ability to learn objects without human input.&lt;/p&gt;
&lt;p&gt;A 1,000-node cluster was the basis for the neural network, representing more than 1 billion connections. The unlabeled images were collected randomly and processed by machine-learning algorithms. Similar to IBM’s Watson, the technology relies on “deep learning” techniques, using previous outcomes to inform future decisions. Machine learning has also become integral in other applications, including speech recognition.&lt;/p&gt;
&lt;p&gt;The researchers removed all identifying labels from the images because they wanted to see if the network was able to create the concept of an object. Dr. Jeff Dean of Stanford University explained how the project differs from other recognition technologies. “We never told it during the training, ‘This is a cat,’ ” he said “It basically invented the concept of a cat. We probably have other ones that are side views of cats.”&lt;/p&gt;
&lt;p&gt;As a result, the software generated a vague image of a cat on its own. The simulation has also introduced possible evidence of “grandmother neurons”, which some believe are specialized cells, trained to recognize an individual face or concept.&lt;/p&gt;
&lt;p&gt;It’s not all about cats, of course. The system was also able to identify human faces and bodies to some degree. Compared to previous attempts to identify unlabeled images, the simulation fared much better at learning and recognition. According to the researchers’ &lt;a href=&#34;https://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/unsupervised_icml2012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;findings&lt;/a&gt;, they were able to deliver 15.8 percent accuracy in recognizing 20,000 object categories.  They claim that’s 70 percent better than what had been achieved previously.&lt;/p&gt;
&lt;p&gt;According to &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing at the Georgia Tech College of Computing, the simulation represents an improvement of “an order of magnitude over previous efforts.” He believes this work could lead to a complete model of the human visual cortex before the end of the decade.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2012/06/28/supercomputer_learns_how_to_recognize_cats/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2012/06/28/supercomputer_learns_how_to_recognize_cats/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KOMO News Radio Interview of David Bader on Google&#39;s Neural Network Experiment</title>
      <link>http://localhost:1313/blog/20120626-komo-radio/</link>
      <pubDate>Tue, 26 Jun 2012 17:05:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120626-komo-radio/</guid>
      <description>&lt;p&gt;[KOMO NewsRadio Interview](KOMO Radio&amp;ndash;GOOGLE NEURAL NETWORK EXPERIMENT.mp3)&lt;/p&gt;
&lt;p&gt;In this clip, Seattle&amp;rsquo;s &lt;a href=&#34;https://komonews.com/live/komo-4-newsradio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KOMO NewsRadio&lt;/a&gt; interviews &lt;strong&gt;David Bader&lt;/strong&gt; on Google&amp;rsquo;s neural network experiment that recognizes cats in YouTube videos.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph500 adds new measurement of supercomputing performance</title>
      <link>http://localhost:1313/blog/20120626-graph500/</link>
      <pubDate>Tue, 26 Jun 2012 09:32:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120626-graph500/</guid>
      <description>&lt;p&gt;Supercomputing performance is getting a new measurement with the Graph500 executive committee’s announcement of specifications for a more representative way to rate the large-scale data analytics at the heart of high-performance computing.&lt;/p&gt;
&lt;p&gt;An international team that includes Sandia National Laboratories announced the single-source shortest-path specification to assess computing performance on Tuesday at the International Supercomputing Conference in Hamburg, Germany.&lt;/p&gt;
&lt;p&gt;The latest benchmark “highlights the importance of new systems that can find the proverbial needle in the haystack of data,” said Graph500 executive committee member &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor in the School of Computational Science and Engineering and executive director of High-Performance Computing at the Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;The new specification will measure the closest distance between two things, said Sandia National Laboratories researcher Richard Murphy, who heads the executive committee. For example, it would seek the smallest number of people between two people chosen randomly in the professional network LinkedIn, finding the fewest friend of a friend of a friend links between them, he said.&lt;/p&gt;
&lt;p&gt;Graph500 already gauges two computational techniques, called kernels: a large graph that links huge numbers of participants and a parallel search of that graph. The first two kernels were relatively easy problems; this third one is harder, Murphy said. Once it’s been tested, the next kernel will be harder still, he said.&lt;/p&gt;
&lt;p&gt;The rankings are oriented toward enormous graph-based data problems, a core part of most analytics workloads. Graph500 rates machines on their ability to solve complex problems that have seemingly infinite numbers of components, rather than ranking machines on how fast they solve those problems.&lt;/p&gt;
&lt;p&gt;Big data problems represent a $270 billion market and are increasingly important for businesses such as Google, Facebook and LexisNexis, Murphy said.&lt;/p&gt;
&lt;p&gt;Large data problems are especially important in cybersecurity, medical informatics, data enrichment, social networks and symbolic networks. Last year, the Obama administration announced a push to develop better big data systems.&lt;/p&gt;
&lt;p&gt;Problems that require enormously complex graphs include correlating medical records of millions of patients, analyzing ever-growing numbers of electronically related participants in social media and dealing with symbolic networks, such as tracking tens of thousands of shipping containers of goods roaming the world’s oceans.&lt;/p&gt;
&lt;p&gt;Medical-related data alone could potentially overwhelm all of today’s high-performance computing, Murphy said.&lt;/p&gt;
&lt;p&gt;Graph500’s steering committee is made up of more than 30 international experts in high-performance computing who work on what benchmarks supercomputers should meet in the future. The executive committee, which implements changes in the benchmark, includes Sandia, Argonne National Laboratory, Georgia Institute of Technology and Indiana University.&lt;/p&gt;
&lt;p&gt;Bader said emerging applications in healthcare informatics, social network analysis, web science and detecting anomalies in financial transactions “require a new breed of data-intensive supercomputers that can make sense of massive amounts of information.”&lt;/p&gt;
&lt;p&gt;But performance can’t be improved without a meaningful benchmark, Murphy said.&lt;/p&gt;
&lt;p&gt;“The whole goal is to spur industry to do something harder” as they jockey for top rankings, he said.&lt;/p&gt;
&lt;p&gt;“If there’s a change in the list over time — and there should be — it’s a big deal,” he added.&lt;/p&gt;
&lt;p&gt;Murphy sees Graph500 as a complementary performance yardstick to the well-known Top 500 rankings of supercomputer performance, based on speed processing the Linpack code. Nine computers made the first Graph500 list in November 2010; by last November, the number had grown to 50. Its fourth list, released at the conference in Germany, ranked 88. Rankings are released twice a year at the Supercomputing Conference in November and the International Supercomputing Conference in June.&lt;/p&gt;
&lt;p&gt;“A machine on the top of this list may analyze huge quantities of data to provide better and more personalized health care decisions, improve weather and climate prediction, improve our cybersecurity and better integrate our online social networks with our personal lives,” Bader said.&lt;/p&gt;
&lt;p&gt;Provided by &lt;a href=&#34;https://phys.org/partners/sandia-national-laboratories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sandia National Laboratories&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://phys.org/news/2012-06-graph500-supercomputing.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://phys.org/news/2012-06-graph500-supercomputing.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://share-ng.sandia.gov/news/resources/news_releases/graph_500/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://share-ng.sandia.gov/news/resources/news_releases/graph_500/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cacm.acm.org/news/152574-graph500-adds-new-measurement-of-supercomputing-performance/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cacm.acm.org/news/152574-graph500-adds-new-measurement-of-supercomputing-performance/fulltext&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cse.gatech.edu/news/137431/graph500-adds-new-measurement-supercomputing-performance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cse.gatech.edu/news/137431/graph500-adds-new-measurement-supercomputing-performance&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sandia Labs Details Update to Graph500 Benchmark</title>
      <link>http://localhost:1313/blog/20120626-hpcwire/</link>
      <pubDate>Tue, 26 Jun 2012 06:41:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120626-hpcwire/</guid>
      <description>&lt;p&gt;Supercomputing performance is getting a new measurement with the Graph500 executive committee’s announcement of specifications for a more representative way to rate the large-scale data analytics at the heart of high-performance computing.&lt;/p&gt;
&lt;p&gt;An international team that includes Sandia National Laboratories announced the single-source shortest-path specification to assess computing performance on Tuesday at the International Supercomputing Conference in Hamburg, Germany.&lt;/p&gt;
&lt;p&gt;The latest benchmark “highlights the importance of new systems that can find the proverbial needle in the haystack of data,” said Graph500 executive committee member &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor in the School of Computational Science and Engineering and executive director of High-Performance Computing at the Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;The new specification will measure the closest distance between two things, said Sandia National Laboratories researcher Richard Murphy, who heads the executive committee. For example, it would seek the smallest number of people between two people chosen randomly in the professional network LinkedIn, finding the fewest friend of a friend of a friend links between them, he said.&lt;/p&gt;
&lt;p&gt;Graph500 already gauges two computational techniques, called kernels: a large graph that links huge numbers of participants and a parallel search of that graph. The first two kernels were relatively easy problems; this third one is harder, Murphy said. Once it’s been tested, the next kernel will be harder still, he said.&lt;/p&gt;
&lt;p&gt;The rankings are oriented toward enormous graph-based data problems, a core part of most analytics workloads. Graph500 rates machines on their ability to solve complex problems that have seemingly infinite numbers of components, rather than ranking machines on how fast they solve those problems.&lt;/p&gt;
&lt;p&gt;Big data problems represent a $270 billion market and are increasingly important for businesses such as Google, Facebook and LexisNexis, Murphy said.&lt;/p&gt;
&lt;p&gt;Large data problems are especially important in cybersecurity, medical informatics, data enrichment, social networks and symbolic networks. Last year, the Obama administration announced a push to develop better big data systems.&lt;/p&gt;
&lt;p&gt;Problems that require enormously complex graphs include correlating medical records of millions of patients, analyzing ever-growing numbers of electronically related participants in social media and dealing with symbolic networks, such as tracking tens of thousands of shipping containers of goods roaming the world’s oceans.&lt;/p&gt;
&lt;p&gt;Medical-related data alone could potentially overwhelm all of today’s high-performance computing, Murphy said.&lt;/p&gt;
&lt;p&gt;Graph500’s steering committee is made up of more than 30 international experts in high-performance computing who work on what benchmarks supercomputers should meet in the future. The executive committee, which implements changes in the benchmark, includes Sandia, Argonne National Laboratory, Georgia Institute of Technology and Indiana University.&lt;/p&gt;
&lt;p&gt;Bader said emerging applications in healthcare informatics, social network analysis, web science and detecting anomalies in financial transactions “require a new breed of data-intensive supercomputers that can make sense of massive amounts of information.”&lt;/p&gt;
&lt;p&gt;But performance can’t be improved without a meaningful benchmark, Murphy said.&lt;/p&gt;
&lt;p&gt;“The whole goal is to spur industry to do something harder” as they jockey for top rankings, he said.&lt;/p&gt;
&lt;p&gt;“If there’s a change in the list over time — and there should be — it’s a big deal,” he added.&lt;/p&gt;
&lt;p&gt;Murphy sees Graph500 as a complementary performance yardstick to the well-known Top 500 rankings of supercomputer performance, based on speed processing the Linpack code. Nine computers made the first Graph500 list in November 2010; by last November, the number had grown to 50. Its fourth list, released at the conference in Germany, ranked 88. Rankings are released twice a year at the Supercomputing Conference in November and the International Supercomputing Conference in June.&lt;/p&gt;
&lt;p&gt;“A machine on the top of this list may analyze huge quantities of data to provide better and more personalized health care decisions, improve weather and climate prediction, improve our cybersecurity and better integrate our online social networks with our personal lives,” Bader said.&lt;/p&gt;
&lt;p&gt;Sandia National Laboratories is a multi-program laboratory operated by Sandia Corporation, a wholly owned subsidiary of
Lockheed Martin company, for the U.S. Department of Energy’s National Nuclear Security Administration. With main facilities
in Albuquerque, N.M., and Livermore, Calif., Sandia has major R&amp;amp;D responsibilities in national security, energy and
environmental technologies and economic competitiveness.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20120629100425/http://www.hpcwire.com/hpcwire/2012-06-26/sandia_labs_details_update_to_graph500_benchmark.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/hpcwire/2012-06-26/sandia_labs_details_update_to_graph500_benchmark.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Many Computers to Identify a Cat? 16,000</title>
      <link>http://localhost:1313/blog/20120625-nytimes-cat/</link>
      <pubDate>Mon, 25 Jun 2012 21:39:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120625-nytimes-cat/</guid>
      <description>

















&lt;figure  id=&#34;figure-an-image-of-a-cat-that-a-neural-network-taught-itself-to-recognize-credit-jim-wilsonthe-new-york-times&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An image of a cat that a neural network taught itself to recognize. *Credit Jim Wilson/The New York Times*&#34; srcset=&#34;
               /blog/20120625-nytimes-cat/20120625-NYTimes_hu_761d5eb387a7942b.webp 400w,
               /blog/20120625-nytimes-cat/20120625-NYTimes_hu_503eee556d4a1380.webp 760w,
               /blog/20120625-nytimes-cat/20120625-NYTimes_hu_ddf6ff2664200e2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120625-nytimes-cat/20120625-NYTimes_hu_761d5eb387a7942b.webp&#34;
               width=&#34;760&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An image of a cat that a neural network taught itself to recognize. &lt;em&gt;Credit Jim Wilson/The New York Times&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;By John Markoff, June 25, 2012&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;MOUNTAIN VIEW, Calif. — Inside &lt;a href=&#34;http://topics.nytimes.com/top/news/business/companies/google_inc/index.html?inline=nyt-org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google&lt;/a&gt;’s secretive X laboratory, known for inventing self-driving cars and augmented reality glasses, a small group of researchers began working several years ago on a simulation of the human brain.&lt;/p&gt;
&lt;p&gt;There Google scientists created one of the largest neural networks for machine learning by connecting 16,000 computer processors, which they turned loose on the Internet to learn on its own.&lt;/p&gt;
&lt;p&gt;Presented with 10 million digital images found in YouTube videos, what did Google’s brain do? What millions of humans do with YouTube: looked for cats.&lt;/p&gt;
&lt;p&gt;The neural network taught itself to recognize cats, which is actually no frivolous activity. This week the researchers will present &lt;a href=&#34;http://arxiv.org/abs/1112.6209&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the results of their work&lt;/a&gt; at a conference in Edinburgh, Scotland. The Google scientists and programmers will note that while it is hardly news that the Internet is full of cat videos, the simulation nevertheless surprised them. It performed far better than any previous effort by roughly doubling its accuracy in recognizing objects in a challenging list of 20,000 distinct items.&lt;/p&gt;
&lt;p&gt;The research is representative of a new generation of computer science that is exploiting the falling cost of computing and the availability of huge clusters of computers in giant data centers. It is leading to significant advances in areas as diverse as machine vision and perception, speech recognition and language translation.&lt;/p&gt;
&lt;p&gt;Although some of the computer science ideas that the researchers are using are not new, the sheer scale of the software simulations is leading to learning systems that were not previously possible. And Google researchers are not alone in exploiting the techniques, which are referred to as “deep learning” models. Last year Microsoft scientists presented research showing that the techniques could be applied equally well to build computer systems to understand human speech.&lt;/p&gt;
&lt;p&gt;“This is the hottest thing in the speech recognition field these days,” said Yann LeCun, a computer scientist who specializes in machine learning at the Courant Institute of Mathematical Sciences at New York University.&lt;/p&gt;
&lt;p&gt;And then, of course, there are the cats.&lt;/p&gt;
&lt;p&gt;To find them, the Google research team, led by the Stanford University computer scientist Andrew Y. Ng and the Google fellow Jeff Dean, used an array of 16,000 processors to create a neural network with more than one billion connections. They then fed it random thumbnails of images, one each extracted from 10 million YouTube videos.&lt;/p&gt;
&lt;p&gt;The videos were selected randomly and that in itself is an interesting comment on what interests humans in the Internet age. However, the research is also striking. That is because the software-based neural network created by the researchers appeared to closely mirror theories developed by biologists that suggest individual neurons are trained inside the brain to detect significant objects.&lt;/p&gt;
&lt;p&gt;Currently much commercial machine vision technology is done by having humans “supervise” the learning process by labeling specific features. In the Google research, the machine was given no help in identifying features.&lt;/p&gt;
&lt;p&gt;“The idea is that instead of having teams of researchers trying to find out how to find edges, you instead throw a ton of data at the algorithm and you let the data speak and have the software automatically learn from the data,” Dr. Ng said.&lt;/p&gt;
&lt;p&gt;“We never told it during the training, ‘This is a cat,’ ” said Dr. Dean, who originally helped Google design the software that lets it easily break programs into many tasks that can be computed simultaneously. “It basically invented the concept of a cat. We probably have other ones that are side views of cats.”&lt;/p&gt;
&lt;p&gt;The Google brain assembled a dreamlike digital image of a cat by employing a hierarchy of memory locations to successively cull out general features after being exposed to millions of images. The scientists said, however, that it appeared they had developed a cybernetic cousin to what takes place in the brain’s visual cortex.&lt;/p&gt;
&lt;p&gt;Neuroscientists have discussed the possibility of what they call the “grandmother neuron,” specialized cells in the brain that fire when they are exposed repeatedly or “trained” to recognize a particular face of an individual.&lt;/p&gt;
&lt;p&gt;“You learn to identify a friend through repetition,” said Gary Bradski, a neuroscientist at Industrial Perception, in Palo Alto, Calif.&lt;/p&gt;
&lt;p&gt;While the scientists were struck by the parallel emergence of the cat images, as well as human faces and body parts in specific memory regions of their computer model, Dr. Ng said he was cautious about drawing parallels between his software system and biological life.&lt;/p&gt;
&lt;p&gt;“A loose and frankly awful analogy is that our numerical parameters correspond to synapses,” said Dr. Ng. He noted that one difference was that despite the immense computing capacity that the scientists used, it was still dwarfed by the number of connections found in the brain.&lt;/p&gt;
&lt;p&gt;“It is worth noting that our network is still tiny compared to the human visual cortex, which is a million times larger in terms of the number of neurons and synapses,” the researchers wrote.&lt;/p&gt;
&lt;p&gt;Despite being dwarfed by the immense scale of biological brains, the Google research provides new evidence that existing machine learning algorithms improve greatly as the machines are given access to large pools of data.&lt;/p&gt;
&lt;p&gt;“The Stanford/Google paper pushes the envelope on the size and scale of neural networks by an order of magnitude over previous efforts,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, executive director of high-performance computing at the Georgia Tech College of Computing. He said that rapid increases in computer technology would close the gap within a relatively short period of time: “The scale of modeling the full human visual cortex may be within reach before the end of the decade.”&lt;/p&gt;
&lt;p&gt;Google scientists said that the research project had now moved out of the Google X laboratory and was being pursued in the division that houses the company’s search business and related services. Potential applications include improvements to image search, speech recognition and machine language translation.&lt;/p&gt;
&lt;p&gt;Despite their success, the Google researchers remained cautious about whether they had hit upon the holy grail of machines that can teach themselves.&lt;/p&gt;
&lt;p&gt;“It’d be fantastic if it turns out that all we need to do is take current algorithms and run them bigger, but my gut feeling is that we still don’t quite have the right algorithm yet,” said Dr. Ng.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A version of this article appears in print on June 26, 2012, on Page B1 of the New York edition with the headline: How Many Computers to Identify a Cat? 16,000.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sandia Labs Details Update to Graph500 Benchmark</title>
      <link>http://localhost:1313/blog/20120625-neo4j/</link>
      <pubDate>Mon, 25 Jun 2012 06:49:01 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120625-neo4j/</guid>
      <description>&lt;p&gt;Supercomputing performance is getting a new measurement with the Graph500 executive committee’s announcement of specifications for a more representative way to rate the large-scale data analytics at the heart of high-performance computing. An international team that includes Sandia National Laboratories announced the single-source shortest-path specification to assess computing performance on Tuesday at the International Supercomputing Conference in Hamburg, Germany. The latest benchmark “highlights the importance of new systems that can find the proverbial needle in the haystack of data,” said Graph500 executive committee member &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor in the School of Computational Science and Engineering and executive director of High-Performance Computing at the Georgia Institute of Technology. The new specification will measure the closest distance between two things, said Sandia National Laboratories researcher Richard Murphy, who heads the executive committee. For example, it would seek the smallest number of people between two people chosen randomly in the professional network LinkedIn, finding the fewest friend of a friend of a friend links between them, he said. &lt;a href=&#34;http://www.hpcwire.com/hpcwire/2012-06-26/sandia_labs_details_update_to_graph500_benchmark.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Read the full article.&lt;/a&gt; &lt;a href=&#34;http://www.graph500.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learn more about Graph500 Benchmark.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neo4j.com/news/sandia-labs-details-update-to-graph500-benchmark/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://neo4j.com/news/sandia-labs-details-update-to-graph500-benchmark/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech takes leading role in IPDPS 2012</title>
      <link>http://localhost:1313/blog/20120622-gatech-ipdps/</link>
      <pubDate>Fri, 22 Jun 2012 14:45:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120622-gatech-ipdps/</guid>
      <description>&lt;p&gt;The 26th IEEE International Parallel &amp;amp; Distributed Processing Symposium (IPDPS) took place May 21-25, 2012, in Shanghai, China. Georgia
Tech’s participation in the technical program included 23 faculty and students presenting six accepted papers, eight workshops, two Ph.D. Forum
research posters, as well as roles in the sessions and invited panel talks. IPDPS, which drew more than 600 participants this year, is an
international forum for engineers and scientists from around the world to present their latest research findings in all aspects of parallel computation.
In addition to technical sessions of submitted paper presentations, the meeting offers workshops, tutorials, and commercial presentations &amp;amp;
exhibits.&lt;/p&gt;
&lt;p&gt;Below is a breakdown of Georgia Tech’s activities in the technical program.&lt;/p&gt;
&lt;h2 id=&#34;symposium-leadership&#34;&gt;SYMPOSIUM LEADERSHIP:&lt;/h2&gt;
&lt;h3 id=&#34;ipdps-2012-technical-program-committee&#34;&gt;IPDPS 2012 Technical Program Committee&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Richard Vuduc and George Biros, Computational Science and Engineering&lt;/li&gt;
&lt;li&gt;Bo Hong, Electrical and Computer Engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;steering-committee&#34;&gt;Steering Committee&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;papers&#34;&gt;PAPERS:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Improving the Performance of Dynamical Simulations Via Multiple Right-Hand Sides&lt;br&gt;
Xing Liu and Edmond Chow, Computational Science and Engineering, Georgia Tech
Karthikeyan Vaidyanathan and Mikhail Smelyanskiy, Parallel Computing Lab, Intel Corporation&lt;/li&gt;
&lt;li&gt;Efficient Quality Threshold Clustering for Parallel Architectures&lt;br&gt;
Anthony Danalis, University of Tennessee; Collin McCurdy, Oak Ridge National Laboratory; and Jeffrey S. Vetter, Oak Ridge National
Laboratory/Georgia Tech (Computational Science and Engineering)&lt;/li&gt;
&lt;li&gt;Identifying Opportunities for Byte-Addressable Non-Volatile Memory in Extreme-Scale Scientific Applications&lt;br&gt;
Dong Li, Oak Ridge National Laboratory; Jeffrey Vetter, Oak Ridge National Laboratory/Georgia Tech; Gabriel Marin, Oak Ridge National
Laboratory; Collin McCurdy, Oak Ridge National Laboratory; Cristian Cira, Auburn University; Zhuo Liu, Auburn University; Weikuan Yu, Auburn
University&lt;/li&gt;
&lt;li&gt;Hybrid Transactions: Lock Allocation and Assignment for Irrevocability&lt;br&gt;
Jaswanth Sreeram, Intel Labs
Santosh Pande, College of Computing, Georgia Tech&lt;/li&gt;
&lt;li&gt;Profiling-based Adaptive Contention Management for Software Transactional Memory&lt;br&gt;
Zhengyu He, Xiao Yu and Bo Hong, Electrical and Computer Engineering, Georgia Tech&lt;/li&gt;
&lt;li&gt;Predicting Potential Speedup of Serial Code via Lightweight Profiling and Emulations with Memory Performance Model&lt;br&gt;
Minjang Kim, Pranith Kumar, Hyesoon Kim, Computer Science, Georgia Tech
Bevin Brett, Software and Services Group, Intel Corporation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sessions&#34;&gt;SESSIONS:&lt;/h2&gt;
&lt;h3 id=&#34;parallel-graph-algorithms-ii&#34;&gt;Parallel Graph Algorithms II&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Edmond Chow, Computational Science and Engineering&lt;br&gt;
Chair&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;scientific-applications&#34;&gt;Scientific Applications&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Rich Vuduc, Computational Science and Engineering&lt;br&gt;
Chair&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multicore-algorithms&#34;&gt;Multicore Algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bo Hong, Electrical and Computer Engineering&lt;br&gt;
Chair&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;panels&#34;&gt;PANELS:&lt;/h2&gt;
&lt;p&gt;Plenary Session Panel Discussion:&lt;br&gt;
Will exascale computing really require new algorithms and programming models?&lt;br&gt;
Richard Vuduc, Computational Science and Engineering&lt;/p&gt;
&lt;h2 id=&#34;workshops&#34;&gt;WORKSHOPS:&lt;/h2&gt;
&lt;h3 id=&#34;presentations&#34;&gt;Presentations:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;21st International Heterogeneity in Computing Workshop&lt;br&gt;
Analyzing Massive Data using Heterogeneous Computing&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering&lt;/li&gt;
&lt;li&gt;Workshop on Multithreaded Architectures and Applications&lt;br&gt;
Merge Path - Parallel Merging Made Simple&lt;br&gt;
Saher Odeh, Technion; Oded Green, Computational Science and Engineering, Georgia Tech; Zahi Mwassi, Technion; Oz Shmueli, Technion;
Yitzhak Birk, Technion&lt;/li&gt;
&lt;li&gt;Scalable Multi-threaded Community Detection in Social Networks&lt;br&gt;
Jason Riedy, Computational Science and Engineering, Georgia Tech; &lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering, Georgia Tech;
Henning Meyerhenke, Karlsruhe Institute of Technology&lt;/li&gt;
&lt;li&gt;PMU-guided Priority Adjustment to Guarantee Thread Performance on IBM POWER SMT Processor&lt;br&gt;
Zhengyu He, Electrical and Computer Engineering, Georgia Tech; Bo Hong, Electrical and Computer Engineering, Georgia Tech&lt;/li&gt;
&lt;li&gt;Workshop on Large-Scale Parallel Processing&lt;br&gt;
Mesh Interface Resolution and Ghost Exchange in a Parallel Mesh Representation&lt;br&gt;
T. Tautges, J. Kraftcheck, N. Bertram, Vivin Sachdeva, J. Magerlein, Argonne National Laboratory, University of Wisconsin-Madison, Electrical and
Computer Engineering, Georgia Tech, IBM T. J. Watson&lt;/li&gt;
&lt;li&gt;16th Workshop on Job Scheduling Strategies for Parallel Processing&lt;br&gt;
Dynamic Kernel/Device Mapping Strategies for GPU-assisted HPC Systems&lt;br&gt;
Jiadong Wu, Weiming Shi and Bo Hong, Electrical and Computer Engineering, Georgia Tech&lt;/li&gt;
&lt;li&gt;2nd NSF/TCPP Workshop on Parallel and Distributed Computing Education&lt;br&gt;
Courses in High-Performance Computing for Scientists and Engineers&lt;br&gt;
Richard Vuduc, Kenneth Czechowski and Aparna Chandramowlishwaran, Computational Science and Engineering; and Jee Whan Choi, Electrical
and Computer Engineering, Georgia Tech&lt;/li&gt;
&lt;li&gt;Workshop on Parallel and Distributed Computing for Machine Learning and Inference Problems&lt;br&gt;
A GPU-accelerated Approximate Algorithm for Incremental Learning of Gaussian Mixture Model&lt;br&gt;
Chunlei Chen, Dejun Mu and Huixiang Zhang, Northwestern Polytechnical University of China; and Bo Hong, Electrical and Computer Engineering,
Georgia Tech&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Committee Appointments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;11th IEEE International Workshop on High Performance Computational Biology&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering&lt;br&gt;
Co-Chair&lt;/li&gt;
&lt;li&gt;2nd Workshop on Communication Architecture for Scalable Systems&lt;br&gt;
Ada Gavrilovska, Computer Science&lt;br&gt;
Program Committee&lt;/li&gt;
&lt;li&gt;2nd NSF/TCPP Workshop on Parallel and Distributed Computing Education&lt;br&gt;
Matthew Wolf, Computer Science&lt;br&gt;
Program Committee&lt;/li&gt;
&lt;li&gt;Workshop on Multithreaded Architectures and Applications&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering&lt;br&gt;
Program Committee&lt;/li&gt;
&lt;li&gt;2nd International Workshop on Accelerators and Hybrid Exascale Systems&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering&lt;br&gt;
Technical Program Committee&lt;/li&gt;
&lt;li&gt;Workshop on Parallel and Distributed Computing for Machine Learning and Inference Problems&lt;br&gt;
Edmond Chow, Computational Science and Engineering&lt;br&gt;
Program Committee&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;phd-forum&#34;&gt;Ph.D. FORUM:&lt;/h2&gt;
&lt;p&gt;Twenty-four students total were selected to display a poster describing their dissertation research.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Communication-Optimal Parallel N-body Solvers&lt;br&gt;
Aparna Chandramowlishwaran, Computational Science and Engineering, Georgia Tech&lt;/li&gt;
&lt;li&gt;Modeling and Analysis for Performance and Power&lt;br&gt;
Jee Choi, Electrical and Computer Engineering; Richard W Vuduc, Computational Science and Engineering, Georgia Tech&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Committee Appointment:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bo Hong, Electrical and Computer Engineering, Georgia Tech&lt;br&gt;
Co-Chair&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Recognized as Charter Member of New HPC500 Group at ISC ’12</title>
      <link>http://localhost:1313/blog/20120622-gatech-hpc500/</link>
      <pubDate>Fri, 22 Jun 2012 14:39:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120622-gatech-hpc500/</guid>
      <description>&lt;p&gt;Georgia Institute of Technology will be recognized as one the 50 charter
members of the HPC500, an exclusive community of High-Performance
Computing user organizations at the vanguard of their areas of
specialization, during the International Supercomputing conference,
ISC&#39;12, in Hamburg, Germany, June 17-21.&lt;/p&gt;
&lt;p&gt;Research in computational science and engineering at Georgia Tech spans many areas ranging from the development of new computational
methods that may be applied to one or more fields in science and engineering to novel computational approaches specific to a particular domain
such as biology or aerospace engineering.&lt;/p&gt;
&lt;p&gt;Because Georgia Tech views computation as the driver of future advances in science and engineering, the School of Computational Science and
Engineering was created to be a truly interdisciplinary unit that crosses the conventional academic boundaries found between research disciplines.
Faculty from all walks of computing, sciences, and engineering collaborate within six core areas: High-Performance Computing; Data Analytics,
Machine Learning and Visualization; Modeling and Simulation; Computational Mathematics; Computational Science; and Computational
Engineering.&lt;/p&gt;
&lt;p&gt;The HPC500 is comprised of a representative cross-section of academic, government, and commercial organizations across all budgets,
applications, and geographic areas, including users in both High Performance Technical Computing (HPTC) and High Performance Business
Computing (HPBC). The charter members are listed at the HPC500 Website (&lt;a href=&#34;http://www.hpc500.com/member-directory/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpc500.com/member-directory/)&lt;/a&gt;.
Of the first fifty members:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;20 are commerical organizations (13 with HPTC application, 7 HPBC), 19 are academic or non-for-profit, and 11 are government
organizations.&lt;/li&gt;
&lt;li&gt;25 are based in the U.S. or Canada; 14 are based in Europe, Middle East, or Africa (EMEA); nine are based in Asia Pacific (including Japan,
Australia, and New Zealand); and two are based in Latin America (including Mexico).&lt;/li&gt;
&lt;li&gt;Nine have supercomputing budgets of over $5.0 million per year; 15 have high-end HPC budgets of $1.0 million to $4.9 million per year; 15
have mid-range HPC budgets of $100,000 to $999,999 per year, and 11 have entry-level HPC budgets under $100,000 per year.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information about High Performance Computing at Georgia Tech, please contact &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor in the School of
Computational Science and Engineering and executive director of High Performance Computing, at &lt;a href=&#34;mailto:bader@cc.gatech.edu&#34;&gt;bader@cc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE to Award 14 Industry Professionals</title>
      <link>http://localhost:1313/blog/20120613-hpcwire/</link>
      <pubDate>Wed, 13 Jun 2012 06:36:59 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120613-hpcwire/</guid>
      <description>&lt;p&gt;The IEEE Computer Society will honor 14 prominent technologists at its annual awards dinner in Seattle, including the inventor of MATLAB, two dedicated computer science educators, a parallel programming languages expert, and innovators in the fields of data mining, distributed computing, database theory, computer standards, and other technologies.&lt;/p&gt;
&lt;p&gt;The ceremony will take place at 6 p.m. PT on Wednesday, June 13 at the Renaissance Hotel in Seattle, Washington.&lt;/p&gt;
&lt;p&gt;“For the IEEE Computer Society, the awards ceremony represents an opportunity to acknowledge these innovators for their sizable contributions to the field of computing,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, chair of the IEEE Computer Society Awards Committee and professor in the School of Computational Science and Engineering at Georgia Institute of Technology. “This year’s honorees come from diverse backgrounds, and include pioneers in parallel processing, data-mining, database theory, Web applications, computer standards, and many other specialties that are central to the further advancement of computing technology. On behalf of the IEEE Computer Society, I applaud them for their accomplishments.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cleve Moler&lt;/strong&gt;, co-founder, chairman, and chief mathematician of MathWorks, is being recognized with the 2012 Computer Pioneer Award for his invention of MATLAB, a well-known programming environment that allows for much faster solutions to technical computing problems.&lt;/p&gt;
&lt;p&gt;Massachusetts Institute of Technology Professor &lt;strong&gt;Arvind&lt;/strong&gt;, a world-renowned leader in computer languages for parallel processing, will receive the 2012 Harry H. Goode Memorial Award. Arvind has contributed to the development of dynamic dataflow architectures, the implicitly parallel programming languages Id and pH.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ronald Fagin&lt;/strong&gt;, an IBM Fellow at the IBM Almaden Research Center, is receiving the W. Wallace McDowell Award for his contributions to database theory.&lt;/p&gt;
&lt;p&gt;In addition, two computer science professors are being honored for their innovation and dedication to teaching the next generation of technology leaders. &lt;strong&gt;Mark Guzdial&lt;/strong&gt;, a professor in Georgia Institute of Technology’s School of Interactive Computing, is being recognized with a 2012 Computer Science and Engineering Undergraduate Teaching Award for his innovative teaching methods, including the Media Computation approach. Stanford University Computer Science Professor &lt;strong&gt;Eric Roberts&lt;/strong&gt;, principal architect of the introductory programming sequence, will receive the 2012 Taylor L. Booth Education Award.&lt;/p&gt;
&lt;p&gt;Other honorees include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Carl K. Chang, computer science professor and chair at Iowa State University, 2012 Richard E. Merwin Distinguished Service Award for exemplary leadership and service to the IEEE Computer Society and the profession;&lt;/li&gt;
&lt;li&gt;Paul Croll, IEEE Computer Society Vice President for Technical and Conference Activities, 2012 Hans Karlsson Award for achievement in computer standards;&lt;/li&gt;
&lt;li&gt;Michael Franz, professor of computer science at the University of California Irvine, 2012 Technical Achievement Award “for pioneering contributions to just-in-time compilation and optimization, significantly advancing web application technology.;&lt;/li&gt;
&lt;li&gt;Johannes Gehrke, computer science professor at Cornell University, 2011 Technical Achievement Award, for pioneering contributions to data mining and distributed query processing techniques;&lt;/li&gt;
&lt;li&gt;Ling Liu, computer science professor at Georgia Institute of Technology, 2012 Technical Achievement Award for pioneering contributions to Internet data management and decentralized trust management;&lt;/li&gt;
&lt;li&gt;Klara Nahrstedt, computer science professor at the University of Illinois at Urbana-Champaign, 2012 Technical Achievement Award for contributions to end-to-end quality of service and resource management in wired and wireless networks;&lt;/li&gt;
&lt;li&gt;Beng-Chin-Ooi, professor of computer science and dean of the School of Computing at National University of Singapore, Tsutomu Kanai Award, for pioneering research in distributed database management and peer-to-peer-based enterprise quality management;&lt;/li&gt;
&lt;li&gt;Mei-Ling Shyu, an associate electrical and computer engineering professor at University of Miami, 2012 Technical Achievement Award for pioneering contributions to multimedia data mining, management, and retrieval; and&lt;/li&gt;
&lt;li&gt;Xindong Wu, a professor of computer science at the University of Vermont, Technical Achievement Award for pioneering contributions to data mining and applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The IEEE Computer Society awards program recognizes outstanding work by computer professionals who advance the field through exceptional technical achievement and service to the profession and to society. For more information, visit &lt;a href=&#34;http://www.computer.org/awards&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org/awards&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h3&gt;
&lt;p&gt;The IEEE Computer Society is the world’s leading computing membership organization and the trusted information and career-development source for a global workforce of technology leaders including: professors, researchers, software engineers, IT professionals, employers, and students. The unmatched source for technology information, inspiration, and collaboration, the IEEE Computer Society is the source that computing professionals trust to provide high-quality, state-of-the-art information on an on-demand basis. The Computer Society provides a wide range of forums for top minds to come together, including technical conferences, publications, and a comprehensive digital library, unique training webinars, professional training, and a Corporate Affiliate Program to help organizations increase their staff’s technical knowledge and expertise. To find out more about the community for technology leaders, visit &lt;a href=&#34;http://www.computer.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20120619015950/https://www.hpcwire.com/hpcwire/2012-06-13/ieee_to_award_14_industry_professionals.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/hpcwire/2012-06-13/ieee_to_award_14_industry_professionals.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Computer Society to Honor 14 Technologists at Awards Dinner in Seattle</title>
      <link>http://localhost:1313/blog/20120611-ieee-cs-awards/</link>
      <pubDate>Mon, 11 Jun 2012 09:45:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120611-ieee-cs-awards/</guid>
      <description>&lt;p&gt;The IEEE Computer Society will honor 14 prominent technologists at its annual awards dinner in Seattle, including the inventor of MATLAB, two dedicated computer science educators, a parallel programming languages expert, and innovators in the fields of data mining, distributed computing, database theory, computer standards, and other technologies.&lt;/p&gt;
&lt;p&gt;The ceremony will take place at 6 p.m. PT on Wednesday, June 13 at the Renaissance Hotel in Seattle, Washington.&lt;/p&gt;
&lt;p&gt;“For the IEEE Computer Society, the awards ceremony represents an opportunity to acknowledge these innovators for their sizable contributions to the field of computing,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, chair of the IEEE Computer Society Awards Committee and professor in the School of Computational Science and Engineering at Georgia Institute of Technology. “This year’s honorees come from diverse backgrounds, and include pioneers in parallel processing, data-mining, database theory, Web applications, computer standards, and many other specialties that are central to the further advancement of computing technology. On behalf of the IEEE Computer Society, I applaud them for their accomplishments.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cleve Moler&lt;/strong&gt;, co-founder, chairman, and chief mathematician of MathWorks, is being recognized with the 2012 Computer Pioneer Award for his invention of MATLAB, a well-known programming environment that allows for much faster solutions to technical computing problems.&lt;/p&gt;
&lt;p&gt;Massachusetts Institute of Technology Professor &lt;strong&gt;Arvind&lt;/strong&gt;, a world-renowned leader in computer languages for parallel processing, will receive the 2012 Harry H. Goode Memorial Award. Arvind has contributed to the development of dynamic dataflow architectures, the implicitly parallel programming languages Id and pH.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ronald Fagin&lt;/strong&gt;, an IBM Fellow at the IBM Almaden Research Center, is receiving the W. Wallace McDowell Award for his contributions to database theory.&lt;/p&gt;
&lt;p&gt;In addition, two computer science professors are being honored for their innovation and dedication to teaching the next generation of technology leaders. &lt;strong&gt;Mark Guzdial&lt;/strong&gt;, a professor in Georgia Institute of Technology’s School of Interactive Computing, is being recognized with a 2012 Computer Science and Engineering Undergraduate Teaching Award for his innovative teaching methods, including the Media Computation approach. Stanford University Computer Science Professor &lt;strong&gt;Eric Roberts&lt;/strong&gt;, principal architect of the introductory programming sequence, will receive the 2012 Taylor L. Booth Education Award.&lt;/p&gt;
&lt;p&gt;Other honorees include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Carl K. Chang, computer science professor and chair at Iowa State University, 2012 Richard E. Merwin Distinguished Service Award for exemplary leadership and service to the IEEE Computer Society and the profession;&lt;/li&gt;
&lt;li&gt;Paul Croll, IEEE Computer Society Vice President for Technical and Conference Activities, 2012 Hans Karlsson Award for achievement in computer standards;&lt;/li&gt;
&lt;li&gt;Michael Franz, professor of computer science at the University of California Irvine, 2012 Technical Achievement Award “for pioneering contributions to just-in-time compilation and optimization, significantly advancing web application technology.;&lt;/li&gt;
&lt;li&gt;Johannes Gehrke, computer science professor at Cornell University, 2011 Technical Achievement Award, for pioneering contributions to data mining and distributed query processing techniques;&lt;/li&gt;
&lt;li&gt;Ling Liu, computer science professor at Georgia Institute of Technology, 2012 Technical Achievement Award for pioneering contributions to Internet data management and decentralized trust management;&lt;/li&gt;
&lt;li&gt;Klara Nahrstedt, computer science professor at the University of Illinois at Urbana-Champaign, 2012 Technical Achievement Award for contributions to end-to-end quality of service and resource management in wired and wireless networks;&lt;/li&gt;
&lt;li&gt;Beng-Chin-Ooi, professor of computer science and dean of the School of Computing at National University of Singapore, Tsutomu Kanai Award, for pioneering research in distributed database management and peer-to-peer-based enterprise quality management;&lt;/li&gt;
&lt;li&gt;Mei-Ling Shyu, an associate electrical and computer engineering professor at University of Miami, 2012 Technical Achievement Award for pioneering contributions to multimedia data mining, management, and retrieval; and&lt;/li&gt;
&lt;li&gt;Xindong Wu, a professor of computer science at the University of Vermont, Technical Achievement Award for pioneering contributions to data mining and applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The IEEE Computer Society awards program recognizes outstanding work by computer professionals who advance the field through exceptional technical achievement and service to the profession and to society. For more information, visit &lt;a href=&#34;http://www.computer.org/awards&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org/awards&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h3&gt;
&lt;p&gt;The IEEE Computer Society is the world’s leading computing membership organization and the trusted information and career-development source for a global workforce of technology leaders including: professors, researchers, software engineers, IT professionals, employers, and students. The unmatched source for technology information, inspiration, and collaboration, the IEEE Computer Society is the source that computing professionals trust to provide high-quality, state-of-the-art information on an on-demand basis. The Computer Society provides a wide range of forums for top minds to come together, including technical conferences, publications, and a comprehensive digital library, unique training webinars, professional training, and a Corporate Affiliate Program to help organizations increase their staff’s technical knowledge and expertise. To find out more about the community for technology leaders, visit &lt;a href=&#34;http://www.computer.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.computer.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.prweb.com/releases/2012/6/prweb9593831.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.prweb.com/releases/2012/6/prweb9593831.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>University of Maryland Distinguished Alumni Award</title>
      <link>http://localhost:1313/blog/20120511-maryland-ece/</link>
      <pubDate>Fri, 11 May 2012 08:18:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120511-maryland-ece/</guid>
      <description>&lt;p&gt;In 2011, &lt;a href=&#34;https://www.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The University of Maryland&lt;/a&gt;&amp;rsquo;s Department of Electrical and Computer Engineering established the &lt;strong&gt;Distinguished Alumni Award&lt;/strong&gt; to recognize alumni who have made significant and meritorious contributions to their fields. Alumni are nominated by their advising professors or the department chair, and the Department Council then approves their selection. In early May, the faculty and staff gather to honor the recipients at a luncheon.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt; was inducted in the inaugural 2012 class of distinguished alumni.&lt;/p&gt;
&lt;p&gt;2012: &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, Professor&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/alumni/award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/alumni/award&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120511-maryland-ece/ECE1_hu_48c8e29df3c049c9.webp 400w,
               /blog/20120511-maryland-ece/ECE1_hu_c155c9d141c70b4d.webp 760w,
               /blog/20120511-maryland-ece/ECE1_hu_6f0bf4c42f7f5975.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120511-maryland-ece/ECE1_hu_48c8e29df3c049c9.webp&#34;
               width=&#34;476&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120511-maryland-ece/ECE2_hu_b1efb033614f8299.webp 400w,
               /blog/20120511-maryland-ece/ECE2_hu_c657b956ffdf70e.webp 760w,
               /blog/20120511-maryland-ece/ECE2_hu_9b174e65ec2b1cac.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120511-maryland-ece/ECE2_hu_b1efb033614f8299.webp&#34;
               width=&#34;479&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120511-maryland-ece/ECE3_hu_67fac39c1576bd16.webp 400w,
               /blog/20120511-maryland-ece/ECE3_hu_f0663fdcacc715a.webp 760w,
               /blog/20120511-maryland-ece/ECE3_hu_eed555e64544a031.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120511-maryland-ece/ECE3_hu_67fac39c1576bd16.webp&#34;
               width=&#34;523&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120511-maryland-ece/plaque_hu_ce938967b4cc7f30.webp 400w,
               /blog/20120511-maryland-ece/plaque_hu_c690c493c628acc6.webp 760w,
               /blog/20120511-maryland-ece/plaque_hu_8ce8ecbd4dc4d919.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120511-maryland-ece/plaque_hu_ce938967b4cc7f30.webp&#34;
               width=&#34;760&#34;
               height=&#34;573&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SIAG/SC Prizes Awarded in Savannah</title>
      <link>http://localhost:1313/blog/20120414-siam/</link>
      <pubDate>Sat, 14 Apr 2012 16:11:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120414-siam/</guid>
      <description>&lt;p&gt;On the program of this year&amp;rsquo;s SIAM Conference on Parallel Processing in Scientific Computing, held February 15-17 in Savannah, Georgia, was the awarding of two prizes: the SIAG/SC Career Prize, to Rob Schreiber of the Exascale Computing Lab at Hewlett Packard Laboratories, and the SIAG/SC Junior Scientist Prize, to Torsten Hoefler of the National Center for Supercomputing Applications at the University of Illinois, Urbana-Champaign. Created in 2009 by the SIAM Activity Group on Supercomputing, the prizes were presented by the inaugural (2010) recipients, Jack Dongarra (University of Tennessee, Knoxville) and Kamesh Madduri (Pennsylvania State University), along with SIAG/SC chair &lt;strong&gt;David Bader&lt;/strong&gt; of the Georgia Institute of Technology.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120414-siam/img1_hu_312247ab00bde59a.webp 400w,
               /blog/20120414-siam/img1_hu_8f2a4338d23cf23d.webp 760w,
               /blog/20120414-siam/img1_hu_125194fe586545f7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120414-siam/img1_hu_312247ab00bde59a.webp&#34;
               width=&#34;200&#34;
               height=&#34;142&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&amp;ldquo;For his research spanning a number of areas in high-performance computing, from low-level systems issues to sophisticated scientific applications,&amp;rdquo; Torsten Hoefler (center) accepted the SIAG/SC Junior Scientist Prize in Savannah from Kamesh Madduri (left) and &lt;strong&gt;David Bader&lt;/strong&gt;.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120414-siam/img2_hu_5ba44413b008d9a8.webp 400w,
               /blog/20120414-siam/img2_hu_62134df01f45d87b.webp 760w,
               /blog/20120414-siam/img2_hu_51138e05e214542.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120414-siam/img2_hu_5ba44413b008d9a8.webp&#34;
               width=&#34;196&#34;
               height=&#34;200&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;SIAG/SC Career Prize recipient Rob Schreiber (right) was cited &amp;ldquo;for his contributions to numerical linear algebra and parallel computing; for the breadth, depth, and transferability of his research to industry; and for his service to the community.&amp;rdquo; In a prize lecture titled &amp;ldquo;It Seemed Like a Good Idea at the Time,&amp;rdquo; Schreiber, who is shown here with Jack Dongarra, reflected on his career-long effort &amp;ldquo;to make very fast, very parallel computing work for solving the most difficult problems science poses.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://archive.siam.org/news/news.php?id=1969&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://archive.siam.org/news/news.php?id=1969&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Accelerating drug discovery</title>
      <link>http://localhost:1313/blog/20120401-scw/</link>
      <pubDate>Sun, 01 Apr 2012 19:59:20 -0600</pubDate>
      <guid>http://localhost:1313/blog/20120401-scw/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120401-scw/apr12hpcpill_1_hu_a2d82691dfa8d1e4.webp 400w,
               /blog/20120401-scw/apr12hpcpill_1_hu_bf691485394287f3.webp 760w,
               /blog/20120401-scw/apr12hpcpill_1_hu_a25d8e19364b11ca.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120401-scw/apr12hpcpill_1_hu_a2d82691dfa8d1e4.webp&#34;
               width=&#34;240&#34;
               height=&#34;160&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Professor David A. Bader&lt;/strong&gt;, executive director of High-Performance Computing, School of Computational Science and Engineering, Georgia Institute of Technology
As part of an award from the US National Science Foundation, we are undertaking a project to accelerate computational discoveries at the petascale. The project is a collaboration between three research groups – myself, Professor Jijun Tang at the University of South Carolina and Stephen Schaeffer, a Professor at Penn State – and our focus is on trying to understand the evolutionary histories of multi chromosome organisms. The outcome will prove very useful for the pharmaceutical industry in terms of developing drug targets and understanding the evolutionary process of plants; particularly useful for determining their medicinal properties.&lt;/p&gt;
&lt;p&gt;We are developing a new algorithm and parallel program called COGNAC (Comparing Orders of Genes using Novel Algorithms and high-performance Computers) which reconstructs phylogenetic trees using gene order data. COGNAC is a follow-on from our previous code, dubbed GRAPPA, which dealt with the evolutionary histories of single chromosome organisms. Gaining an understanding of how species evolve is, as you can imagine, computationally difficult and this is where the use of HPC allows us to solve the issues efficiently and reduce the development time for pharmaceutical companies. Being able to assess human populations may also lead us to personalised medicine by offering insight into how a drug may effect one group of individuals over another.&lt;/p&gt;
&lt;p&gt;The use of a code such as COGNAC will enable us to differentiate people into sub groups based on their evolutionary history in order to fully know which treatments would be the most effective. Furthermore, this will help us to reduce health costs by speeding diagnosis and taking a preventative approach to disease. The code itself is continually being developed in order to make it more useful on real data sets and for it to run faster on new HPC platforms, including multi-core Intel processors and Nvidia GPUs. By keeping up with and harnessing these capabilities, we can attempt to solve problems that just a few years previously would have been considered intractable.&lt;/p&gt;
&lt;p&gt;Here at Georgia Tech we’re focusing on this area with a new academic programme in computational science and engineering that addresses both computing and discipline areas such as biology and precise medicine. This combining of disciplines is really where we see the solution to real-world problems such as drug design and discovery, and we’re optimistic that we have the right tools and are training the best people. The challenges lie in determining the effectiveness of these approaches and our ability to innovate with the right algorithms. Our preliminary results are good, however, and as computers become even more capable and push towards exascale, the resources will be there to make significant progress in the study and understanding of evolutionary histories.&lt;/p&gt;
&lt;h2 id=&#34;tom-messina-it-manager-high-performance--scientific-computing-application-services-rd-strategy--delivery-johnson-and-johnson&#34;&gt;Tom Messina, IT manager, High Performance &amp;amp; Scientific Computing, Application Services R&amp;amp;D Strategy &amp;amp; Delivery, Johnson and Johnson&lt;/h2&gt;
&lt;p&gt;As the IT team responsible for delivering high performance and scientific computing solutions to all sectors of Johnson and Johnson R&amp;amp;D, the primary demand we see our business partners facing is constrained compute capacity. This results in lengthy job queues during times of peak demand and filtering activity to determine which simulations they have capacity to run at all. These challenges lead directly to our team’s mission of providing a highly reusable platform offering limitless compute capacity to enable new opportunities and experiments that could not have been done before.&lt;/p&gt;
&lt;p&gt;Computer-aided models becoming both more critical and more complex, in combination with the technical advancements of modelling software, has exponentially increased computational demand. While many companies and institutions have purchased very high-end compute machines and farms, many others have not. Regardless, the on-demand elasticity of well-engineered cloud models is one of the best use cases for us to meet this HPC capacity challenge.&lt;/p&gt;
&lt;p&gt;Among the demands that our business partners face, particularly as they pertain to cloud computing, are understanding how cloud technology affects the deployment of GxP solutions, overcoming long data transfer times for larger datasets, being comfortable with storing data in the public cloud and working with licensing models that don’t port nicely to the elasticity of cloud platforms.&lt;/p&gt;
&lt;p&gt;One of the key requirements we have for the evolution of our own HPC platform is to integrate our disparate internal compute clusters with the cloud environment. Like other organisations, we have made significant investment in building up local compute environments. Many of these environments continue to have decent life left in them. To continue utilising that investment while taking advantage of cloud elasticity, we’re enabling our scientists and engineers to submit jobs to a decision engine. This engine will apply intelligent business rules to determine whether that job should be executed in a local cluster or dynamically provisioned to a cloud cluster, all without the user ever needing to worry about where the job will run.&lt;/p&gt;
&lt;p&gt;We have also started to dig into where HPC can add further value. Modelling, simulation, image processing and statistical processing will all be in our portfolio for some time and there are still many areas of J&amp;amp;J we have yet to tap into. However, we have also started to look into how HPC can benefit various areas within business intelligence.&lt;/p&gt;
&lt;p&gt;Datasets have reached the point that traditional mining and warehousing approaches either don’t work or are too complex or costly to support. Technologies like Hadoop have already begun to make big strides in this space and it is now on our radar to start taking a look at this opportunity.&lt;/p&gt;
&lt;h2 id=&#34;jason-stowe-ceo-at-cycle-computing&#34;&gt;Jason Stowe, CEO at Cycle Computing&lt;/h2&gt;
&lt;p&gt;Pharmaceutical workloads are pleasantly parallel in that they don’t depend on having access to lots of nodes running the exact same job at the same time. We are now in the era of utility supercomputing where much of the computational work can be done on demand infrastructures such as data centres, virtualisation environment and public clouds, which are all particularly suitable for pleasantly parallel workloads.&lt;/p&gt;
&lt;p&gt;Amazon web services is one such example being used by pharma researchers due to the fact not only can the service create 30,000 processors to run 30,000 individual analyses at scale, it can do so affordably.&lt;/p&gt;
&lt;p&gt;The impact of this is significant; researchers no longer have to wait weeks or months to get results back from certain classes of analysis. When you take into account the fact that there is a cost associated to each day it takes to bring a drug to market, the benefit of speeding up the process is clear. Beyond that, however, the scale of computational possibilities means that not only do jobs get completed faster, but researchers have the time to ask new questions.&lt;/p&gt;
&lt;p&gt;Companies will often have an archive of historical data, such as information gathered from mass spectrometers, that would have been analysed at the time but can now have newer algorithms applied if the appropriate level of resources are available. The internal cluster of that company may be perfectly adequate for running the daily analyses, but not for going through that past data. This is where on-demand services can be invaluable.&lt;/p&gt;
&lt;p&gt;When there is a significant investment in cluster technology, several generations of hardware will often be deployed and so regular evaluations should be performed on the cluster’s utilisation. This allows users to determine whether they should take advantage of ancillary sources for burst, rather than trying to buy for peak, and ensures that resources aren’t ever lacking or being wasted.&lt;/p&gt;
&lt;h2 id=&#34;sumit-gupta-director-of-tesla-product-marketing-at-nvidia&#34;&gt;Sumit Gupta, director of Tesla Product Marketing at Nvidia&lt;/h2&gt;
&lt;p&gt;The definition of high-performance computing has changed. There was a time that the term would only be applied to supercomputers but now even workstations fall into that category and this new definition means that every stage of the pharmaceutical process uses HPC in some way.&lt;/p&gt;
&lt;p&gt;The genomics work being done in the earlier stages of the drug lifecycle involves researchers drilling down to a finer level of detail in order to gain an understanding of the genetic structure of living things and how biomolecules behave with certain drug candidates. The molecular structure of the biomolecule/protein may have previously been done through methods such as X-ray crystallography, but genomics and high-speed sequencing machines mean that the genes of individual people can be sequenced to aid understanding of the genetic origin of specific diseases. A great amount of HPC focus is being directed in this area because while clinical trials or drug application reviews at the FDA can’t be accelerated, the discovery phase can.&lt;/p&gt;
&lt;p&gt;The discovery phase of drug manufacture can take up to five years and often the results will be a drug that looks promising but that when tested in animal studies or clinical trials is found to have too many side effects. The solution is to narrow down the candidates. Going over millions of compounds in a laboratory would be far too costly in terms of both human labour and time, however computational methods can reduce the field of prospects, which can then be taken to preclinical and clinical studies.&lt;/p&gt;
&lt;p&gt;We recently worked with BGI, the world’s largest genomics institute, who bought 300 high-speed sequencers in order to address the problem of China’s aging population. The result was that the institute was soon drowning in petabytes of data. Tesla GPUs accelerated the software so that they could crunch through the data more effectively and they soon began to make interesting discoveries that would not have previously been possible. That’s the main impact high-performance computing has on the pharmaceutical industry.&lt;/p&gt;
&lt;h2 id=&#34;geoffrey-noer-senior-director-of-product-marketing-at-panasas&#34;&gt;Geoffrey Noer, senior director of Product Marketing at Panasas&lt;/h2&gt;
&lt;p&gt;The workload we see far more than any other in life sciences is next-generation sequencing. This has driven a tremendous explosion in need for data storage and has been at the forefront of the demand for HPC resources in the bio-pharmaceutical space. Sequencers are far less expensive than they used to be and as a result institutions will often have multiple sequencers, with each one churning out vast amounts of data. There is a broad movement to compute clusters of x86 servers to do the processing of that data, which in turn drives the need for more storage – not only to house the initial results, but to act as an on-going repository. Scientists will often want to go back and do further analysis on the data and so it needs to be readily available; ensuring that happens typically falls to the IT department.&lt;/p&gt;
&lt;p&gt;I would say that the most difficult aspects of being an IT professional in this space are the resource planning and being able to act fast enough. Having spoken with many IT directors in the past, news of a new sequencer being installed is often given with very little advance notice, which leaves them in the position of having to figure out how to immediately provide tens or even hundreds of terabytes of additional storage. Under these circumstances, having a single file system that can be scaled up incrementally as needed is crucial.&lt;/p&gt;
&lt;p&gt;The term Big Data is being used to describe the phenomenon of the quantum shift in the amount of data being generated and there’s no doubt that genomics and bio-pharma are part of that trend. The key advice is to thoroughly evaluate the type of architecture that will best suit long-term needs and solve any scalability problems, not only for capacity and performance but in a way that maintains ease of use, manageability and reliability.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.scientific-computing.com/feature/accelerating-drug-discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.scientific-computing.com/feature/accelerating-drug-discovery&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CHASM Team Tackling Scalable Capability on Defense Application Requirements</title>
      <link>http://localhost:1313/blog/20120320-gatech-chasm/</link>
      <pubDate>Tue, 20 Mar 2012 15:00:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120320-gatech-chasm/</guid>
      <description>&lt;p&gt;Project CHASM, funded by the Defense Advanced Research Projects Agency (DARPA) as part of its
Ubiquitous High Performance Computing (UHPC) program announced in 2010, is focusing UHPC
architecture designs on achieving scalable capability on defense application requirements.&lt;/p&gt;
&lt;p&gt;The primary milestones in the project have been the creation of several forward-looking large scale
benchmarks or “challenge problems” that model difficult computational problems that the Department
of Defense will face in the upcoming decade, said Dan Campbell, the project’s research lead and a
principal research engineer in the Georgia Tech Research Institute.&lt;/p&gt;
&lt;p&gt;Project CHASM, which stands for Challenge Applications and Scalable Metrics identified five problem
areas: Streaming Sensors, Dynamic Graph Analysis, Decision/Search, Lagrangian Shock
Hydrodynamics and Molecular Dynamics.&lt;/p&gt;
&lt;p&gt;Team members are currently focused on the first two. Streaming Sensors involves modeling the
computational needs of a radar performing persistent ground imaging and change detection over a
very wide area, from an airborne platform. The problem requires up to a half petaflop of arithmetic per
second, and works with large, dense linear algebra operations.&lt;/p&gt;
&lt;p&gt;The Dynamic Graph Analysis challenge problem models the computing required to maintain an understanding of communities and complex
relationships in an environment where connections between individual entities are changing constantly and rapidly. This problem stresses the
ability of a computing system to access large amounts of data in unpredictable patterns.&lt;/p&gt;
&lt;p&gt;“Our primary goal will be to deliver high quality problem definitions that model the computing complexity of important DoD missions, along with
reference implementations of those problems, in C, C++, and other languages,” Campbell said.&lt;/p&gt;
&lt;p&gt;To meet the escalating demands for greater processing performance, CHASM’s work will advance the development of future computer system
designs that can support new generations of advanced DoD systems and enable new computing application code.&lt;/p&gt;
&lt;p&gt;The team is focused on delivering capability-centric benchmarks and metrics for DoD computing systems.&lt;/p&gt;
&lt;p&gt;“Most benchmarks report scores for fairly low-level aspects of a computer’s capabilities - such as peak theoretical flops throughput - but the
performance of complicated applications is rarely captured by those benchmarks.”&lt;/p&gt;
&lt;p&gt;With the complexity of modern computers, performance of a task can be bottlenecked on any of the many scarce resources on the
system. Benchmarks and metrics that are focused on low-level and granular capabilities lead to system designs that maximize those granular
capabilities, without necessarily focusing on overall application performance, Campbell says.&lt;/p&gt;
&lt;p&gt;“By elevating the level of abstraction to mission-performance-centric measurements, we hope to encourage the development of complete
computing systems that truly meet the DoD’s mission needs.”&lt;/p&gt;
&lt;p&gt;The CHASM team members at Georgia Tech include Campbell, &lt;strong&gt;David A. Bader&lt;/strong&gt; (Computational Science and Engineering), Mark Richards
(Electrical and Computer Engineering), and Jeffrey Vetter (Oak Ridge National Laboratory and CSE, Georgia Tech).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader selected as recipient of University of Maryland&#39;s Inaugural ECE Distinguished Alumni Award</title>
      <link>http://localhost:1313/blog/20120315-maryland/</link>
      <pubDate>Thu, 15 Mar 2012 20:43:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120315-maryland/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120315-maryland/letter_hu_66cbf8a6553eadd0.webp 400w,
               /blog/20120315-maryland/letter_hu_162d3f19bd5e6a44.webp 760w,
               /blog/20120315-maryland/letter_hu_1990808c328ae29f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120315-maryland/letter_hu_66cbf8a6553eadd0.webp&#34;
               width=&#34;590&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;David&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;On behalf of the Department of Electrical &amp;amp; Computer Engineering, I am honored to announce that you have been selected as a recipient of the Inaugural ECE Distinguished Alumni Award. We are pleased to inform you that you were nominated by ECE faculty member, Joseph JaJa, who shared your accomplishments and impressive contributions to the field, which exemplify this award. This prestigious award will be presented annually to alumni that have provided leadership and meritorious contributions in the broad field of engineering.  We are delighted to have someone of your stature as both an alumnus of our department and a 2012 Distinguished Alumni honoree.&lt;/p&gt;
&lt;p&gt;The conferring of this award will take place on May 11, 2012. We understand that you will not be able to join us and would like to invite you to speak in our Booz Allen Hamilton Distinguished Colloquium Series during the Fall 2012 semester. We would like to honor you at that time and present your award.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Again, congratulations on your selection. We are pleased to celebrate your achievements at this premiere department event, and look forward to having you join us in the fall.&lt;/p&gt;
&lt;p&gt;Most Sincerely,&lt;/p&gt;
&lt;p&gt;Rama Chellappa&lt;br&gt;
Professor and Interim Chair&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader to speak in University of Delaware&#39;s Distingushed Lecture Series</title>
      <link>http://localhost:1313/blog/20120222-delaware/</link>
      <pubDate>Wed, 22 Feb 2012 17:53:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120222-delaware/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20120222-delaware/baderdavid_hu_73d55b066c5cc6f8.webp 400w,
               /blog/20120222-delaware/baderdavid_hu_986c560e1191d21e.webp 760w,
               /blog/20120222-delaware/baderdavid_hu_e9942a76c08068.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20120222-delaware/baderdavid_hu_73d55b066c5cc6f8.webp&#34;
               width=&#34;261&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Can diseases in human populations be detected and prevented? How many currents can one electrical power grid handle? Who detects the community structure of large social networks?&lt;/p&gt;
&lt;p&gt;These are just a few of the emerging real-world graph problems that engineers currently examine.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, from Georgia Institute of Technology, is a leading expert on massive-scale social networks, combinatorial optimization and parallel algorithms.&lt;/p&gt;
&lt;p&gt;He will discuss the real-world applications and scalability challenges in high performance computing on Thursday, May 3, at 3:30 p.m., in Room 004 of Kirkbride Lecture Hall. His talk is titled “Opportunities and Challenges in Massive Data-Intensive Computing.”&lt;/p&gt;
&lt;p&gt;Bader serves as the executive director for high performance computing at Georgia Tech, where he is a professor in the School of Computational Science and Engineering, College of Computing. His current research interests include massive-scale data analytics and computational genomics and biology, as well as multicore, manycore and multithread computing for data-intensive applications. Bader also serves a lead scientist for the Defense Advanced Research Project Agency (DARPA) Ubiquitous High Performance Computing program.&lt;/p&gt;
&lt;p&gt;Bader is a fellow of IEEE and the American Association for the Advancement of Science (AAAS). He is the co-author of over 100 articles for conferences and peer-reviewed journals. He is an associate editor for several high impact publications including the Journal of Parallel and Distributed Computing, ACM Journal of Experimental Algorithmics and IEEE DSOnline. He has also served as associate editor for the IEEE Transactions on Parallel and Distributed Systems.&lt;/p&gt;
&lt;p&gt;A National Science Foundation Faculty Early Career Development Award recipient, Bader’s research is supported by NSF, the National Intitutes of Health, DARPA, and the Department of Energy.&lt;/p&gt;
&lt;p&gt;HPCwire recently named Bader among people to watch in 2012 and insideHPC recently called him a &amp;ldquo;rock star&amp;rdquo; of high performance computing.&lt;/p&gt;
&lt;p&gt;The talk is part of the Distinguished Lecture Series sponsored by the Department of Computer and Information Sciences at UD.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Article by Janie Sikes&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Photo by Rob Felt, Georgia Tech&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www1.udel.edu/udaily/2012/feb/bader-computer-lecture-022212.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www1.udel.edu/udaily/2012/feb/bader-computer-lecture-022212.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPCwire Announces the 2012 People to Watch List</title>
      <link>http://localhost:1313/blog/20120124-hpcwire/</link>
      <pubDate>Tue, 24 Jan 2012 23:06:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120124-hpcwire/</guid>
      <description>&lt;p&gt;HPCwire, the most widely recognized and accessed news and information site covering the ecosystem of High Performance Computing (HPC) announced today that it has published the HPCwire 2012 ‘People to Watch’ list. The annual list is comprised of an elite group of community leaders selected from academia, government, business, and industry who’s work is believed will impact and influence the future of High Performance Computing in 2012 and beyond.&lt;/p&gt;
&lt;p&gt;The annual selections are made following a stringent review process, in-depth discussions with the HPCwire editorial and publishing teams, guidance from industry analysts and past recipients, and with input solicited from industry luminaries across the HPC community.&lt;/p&gt;
&lt;p&gt;“This year’s list of stellar individuals is representative of the best and brightest in the HPC community, who have dedicated their lives to sharing their visions and creativity towards developing solutions for improving our quality of life today, and for future generations to come”, said Jeff Hyman, President and Group Publisher of Tabor Communications Inc. “It’s an honor and a privilege for me to acknowledge those who, through their talent and commitment to working with advanced computing technologies, are revolutionizing our world and the universe beyond. Congratulations and best wishes to all who were selected for a productive and outstanding year ahead.”&lt;/p&gt;
&lt;p&gt;The 2012 HPCwire People to Watch list includes the following academic, government, business, and industry thought leaders (in alphabetical order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Full Professor in the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director for High Performance Computing&lt;/li&gt;
&lt;li&gt;Pascal Barbolosi, Vice President, Extreme Computing at Bull Group&lt;/li&gt;
&lt;li&gt;Eric Barton, Chief Technical Officer and Co-founder, Whamcloud, Inc.&lt;/li&gt;
&lt;li&gt;Alex Bouzari, CEO, DataDirect Networks&lt;/li&gt;
&lt;li&gt;David Ferucci, IBM Fellow and the Principal Investigator (PI) for the Watson/Jeopardy! Project&lt;/li&gt;
&lt;li&gt;Steve Reagan, Computational Modeling Manager at L &amp;amp; L Products, Inc.&lt;/li&gt;
&lt;li&gt;Jon Riley, Vice President of Digital Manufacturing at the National Center for Manufacturing Sciences (NCMS)&lt;/li&gt;
&lt;li&gt;Mark Seager, Chief Technology Officer for the High Performance Computing Ecosystem at Intel&lt;/li&gt;
&lt;li&gt;Marc Snir, Director of the Mathematics and Computer Science Division, Argonne National Laboratory co-PI, NCSA Blue Waters Project&lt;/li&gt;
&lt;li&gt;Steve Scott, CTO, Tesla Business Unit, Nvidia *&lt;/li&gt;
&lt;li&gt;Tadashi Watanabe, Project Leader, Next-Generation Supercomputer R&amp;amp;D Center, RIKEN&lt;/li&gt;
&lt;li&gt;John West, Director, Department of Defense, High Performance Computing Modernization Program (&lt;em&gt;Previous Recipient&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Masahiko Yamada, President of Technical Computing Solutions Unit, Fujitsu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To view the entire feature and learn more about the backgrounds, contributions, and the influence that these individuals are making in High Performance Computing, visit &lt;a href=&#34;http://www.hpcwire.com/specialfeatures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/specialfeatures/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;about-hpcwire&#34;&gt;About HPCwire&lt;/h3&gt;
&lt;p&gt;HPCwire is the leading publication for news and information from the high performance computing industry. HPCwire continues to be the portal of choice for business and technology professionals from the academic, government, industrial and vendor communities who are interested in high performance and computationally-intensive computing, including systems, software, tools and applications, middleware, networking and storage For additional information, please visit: &lt;a href=&#34;https://www.hpcwire.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.hpcwire.com&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;about-tabor-communications-inc&#34;&gt;About Tabor Communications Inc.&lt;/h3&gt;
&lt;p&gt;Tabor Communications Inc. is a leading international media, advertising, and communications company that provides solutions, news and information to the High Performance Computing (HPC), data-intensive, cloud and digital manufacturing communities. Publisher of HPCwire, Datanami, HPC in the Cloud and Digital Manufacturing Report, other Tabor Communications companies include Tabor Advertising and Tabor Publications &amp;amp; Events.
Learn more at: &lt;a href=&#34;https://www.taborcommunications.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.taborcommunications.com/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;media-contact&#34;&gt;Media Contact:&lt;/h3&gt;
&lt;p&gt;Caroline Connor&lt;br&gt;
Tabor Communications Inc. &lt;br&gt;
+1 (510) 378-5838&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Elected Chair of SIAM Activity Group on Supercomputing</title>
      <link>http://localhost:1313/blog/20120101-siam/</link>
      <pubDate>Sun, 01 Jan 2012 16:19:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20120101-siam/</guid>
      <description>&lt;h2 id=&#34;the-siag-election-results-are-in&#34;&gt;The SIAG Election results are in!&lt;/h2&gt;
&lt;p&gt;New officers for the following activity groups have been elected. Their term of service will be from January 1, 2012—December 31, 2013.&lt;/p&gt;
&lt;h2 id=&#34;siagsc--supercomputing&#34;&gt;SIAG/SC ‐ Supercomputing&lt;/h2&gt;
&lt;p&gt;Chair: &lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
Vice Chair: Mary Beth Hribar&lt;br&gt;
Secretary: Fatima Abu Salem&lt;br&gt;
Program Director: Ali Pinar&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AAAS Members Elected as Fellows</title>
      <link>http://localhost:1313/blog/20111223-aaas-fellow/</link>
      <pubDate>Fri, 23 Dec 2011 22:18:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111223-aaas-fellow/</guid>
      <description>&lt;p&gt;In November 2011, the &lt;a href=&#34;https://www.aaas.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AAAS&lt;/a&gt; Council elected 539 members as Fellows of AAAS. These individuals will be recognized for their contributions to science and technology at the Fellows Forum to be held on 18 February 2012 during the AAAS Annual Meeting in Vancouver, British Columbia. The new Fellows will receive a certificate and a blue and gold rosette as a symbol of their distinguished accomplishments.&lt;/p&gt;
&lt;p&gt;Presented by section affiliation, they are:&lt;/p&gt;
&lt;h4 id=&#34;section-on-agriculture-food-and-renewable-resources&#34;&gt;Section on Agriculture, Food, and Renewable Resources&lt;/h4&gt;
&lt;p&gt;Martha Ann Belury, Ohio State Univ. • Thomas E. Besser, Washington State Univ. • Daniel R. Bush, Colorado State Univ. • Z. Jeffrey Chen, Univ. of Texas at Austin • Lynda M. Ciuffetti, Oregon State Univ. • Consuelo M. De Moraes, Pennsylvania State Univ. • Gerald E. Edwards, Washington State Univ. • Catherine Feuillet, French National Institute for Agricultural Research • Edward Allen Foegeding, North Carolina State Univ. • Fred Gould, North Carolina State Univ. • Bingru Huang, Rutgers, The State Univ. of New Jersey • Louise E. Jackson, Univ. of California, Davis • Michael R. Ladisch, Purdue Univ. • Rui Hai Liu, Cornell Univ. • Lena Q. Ma, Univ. of Florida • David J. Mackill, Univ. of California, Davis • Gregory D. May, National Center for Genome Resources • Stephen G. Pallardy, Univ. of Missouri-Columbia • Carl A. Pinkert, Auburn Univ. • B.W. Poovaiah, Washington State Univ. • Steven R. Rodermel, Iowa State Univ. • Guy Smagghe, Ghent Univ., Belgium&lt;/p&gt;
&lt;h4 id=&#34;section-on-anthropology&#34;&gt;Section on Anthropology&lt;/h4&gt;
&lt;p&gt;Leslea J. Hlusko, Univ. of California, Berkeley • Peter Neal Peregrine, Lawrence Univ. • Vernon Lee Scarborough, Univ. of Cincinnati • Michael Silverstein, Univ. of Chicago • Dawnie Wolfe Steadman, Binghamton Univ. • Chris Stringer, The Natural History Museum,London • Robert H. Tykot, Univ. of South Florida • Virginia J. Vitzthum, Indiana Univ. • Carol Marie Worthman, Emory Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-astronomy&#34;&gt;Section on Astronomy&lt;/h4&gt;
&lt;p&gt;Lars Bildsten, Univ. of California, Santa Barbara • Megan Donahue, Michigan State Univ. • Debra Meloy Elmegreen, Vassar College • Giuseppina (Pepi) Fabbiano, Smithsonian Astrophysical Observatory • Chryssa Kouveliotou, NASA Marshall Space Flight Center • Chung-Pei Ma, Univ. of California, Berkeley • John C. Mather, NASA Goddard Space Flight Center • Robert David Mathieu, Univ. of Wisconsin-Madison • Sara Seager, Massachusetts Institute of Technology • Kristen Sellgren, Ohio State Univ. • Krzysztof Z. Stanek, Ohio State Univ. • Martin White, Univ. of California, Berkeley • Ernst K. Zinner, Washington Univ. in St. Louis&lt;/p&gt;
&lt;h4 id=&#34;section-on-atmospheric-and-hydrospheric-sciences&#34;&gt;Section on Atmospheric and Hydrospheric Sciences&lt;/h4&gt;
&lt;p&gt;E. Virginia Armbrust, Univ. of Washington • Anthony J. Broccoli, Rutgers, The State Univ. of New Jersey • Antonio Busalacchi, Univ. of Maryland, College Park • J. David Neelin, Univ. of California, Los Angeles • Konrad Steffen, Univ. of Colorado at Boulder • William G. Sunda, NOAA Center for Coastal Fisheries and Habitat Research • Pieter P. Tans, NOAA Earth System Research Laboratory • Robert A. Weller, Woods Hole Oceanographic Institution&lt;/p&gt;
&lt;h4 id=&#34;section-on-biological-sciences&#34;&gt;Section on Biological Sciences&lt;/h4&gt;
&lt;p&gt;Christoph C.H. Adami, Keck Graduate Institute • Jon Ågren, Uppsala Univ., Sweden • Brian Alters, McGill Univ., Canada • Richard M. Amasino, Univ. of Wisconsin-Madison • Jonathan Arnold, Univ. of Georgia, Athens • Motoyuki Ashikari, Nagoya Univ., Japan • Ruma Banerjee, Univ. of Michigan • Brian McRae Barnes, Univ. of Alaska Fairbanks • Carl Bauer, Indiana Univ. • Graeme I. Bell, Univ. of Chicago • George N. Bennett, Rice Univ. • Louis Bernatchez, Université Laval, Canada • David M. Bisaro, Ohio State Univ. • David Boettiger, Univ. of Pennsylvania • Richard G. Brennan, Duke Univ. School of Medicine • Judith Campisi, Buck Institute • P. Bryant Chase, Florida State Univ. • Jiquan Chen, Univ. of Toledo • Xuemei Chen, Univ. of California, Riverside • Scott L. Collins, Univ. of New Mexico • Duane A. Compton, Dartmouth Medical School • Jeffrey Conner, Michigan State Univ. • Barry A. Costa-Pierce, Univ. of Rhode Island • Quentin C.B. Cronk, Univ. of British Columbia, Canada • Bryan Cullen, Duke Univ. • Sandra Joanne Friezner Degen, Univ. of Cincinnati/Cincinnati Children’s Research Foundation • Chuxia Deng, National Institutes of Health • Carmen W. Dessauer, Univ. of Texas Medical School at Houston • David L. Dilcher, Indiana Univ. • Michael Doebeli, Univ. of British Columbia, Canada • Henrik G. Dohlman, Univ. of North Carolina, Chapel Hill • Chen Dong, Univ. of Texas MD Anderson Cancer Center • Xinnian Dong, Duke Univ. • Andrew P. Feinberg, Johns Hopkins Univ. School of Medicine • Patricia L. Foster, Indiana Univ. • Nigel W. Fraser, Univ. of Pennsylvania School of Medicine • Robert J. Full, Univ. of California, Berkeley • Mariano A. Garcia-Blanco, Duke Univ. Medical Center • Susan P. Gilbert, Rensselaer Polytechnic Institute • William E. Goldman, Univ. of North Carolina, Chapel Hill • Byron Goldstein, Los Alamos National Laboratory • Daphne R. Goring, Univ. of Toronto, Canada • Kathleen L. Gould, Vanderbilt Univ. School of Medicine • Bryan T. Grenfell, Princeton Univ. • Jun-Lin Guan, Univ. of Michigan Medical School • Gretchen Hagen, Univ. of Missouri-Columbia • Heidi Elizabeth Hamm, Vanderbilt Univ. Medical Center • Min Han, Univ. of Colorado at Boulder • Dolph Lee Hatfield, National Cancer Institute, National Institutes of Health • Bradford A. Hawkins, Univ. of California, Irvine • Kenneth F. Haynes, Univ. of Kentucky • Sheng Yang He, Michigan State Univ. • Xi He, Children’s Hospital Boston/Harvard Medical School • Eliot Herman, Donald Danforth Plant Science Center • George C. Hill, Vanderbilt Univ. School of Medicine • Gregg A. Howe, Michigan State Univ. • Michael J. Imperiale, Univ. of Michigan • Tadashi Inagami, Vanderbilt Univ. School of Medicine • William R. Jacobs, Jr., Albert Einstein College of Medicine • Bhanu P. Jena, Wayne State Univ. School of Medicine • Sue Jinks-Robertson, Duke Univ. Medical Center • Hideko Kaji, Thomas Jefferson Univ. • Daniel P. Kiehart, Duke Univ. • Thomas D. Kocher, Univ. of Maryland, College Park • Nori Kurata, National Institute of Genetics, Japan • Gary A. Lamberti, Univ. of Notre Dame • Min Li, Johns Hopkins Univ. School of Medicine • Karen R. Lips, Univ. of Maryland, College Park • Jennifer K. Lodge, Washington Univ. School of Medicine in St. Louis • Richard L. Maas, Brigham and Women’s Hospital/Harvard Medical School • Nancy S. Magnuson, Washington State Univ. • Donal T. Manahan, Univ. of Southern California • Edward M. Marcotte, Univ. of Texas at Austin • Kelly Edward Mayo, Northwestern Univ. • W. Richard McCombie, Cold Spring Harbor Laboratory • Sheila McCormick, USDA-ARS/Univ. of California, Berkeley • Michael D. McMullen, Univ. of Missouri-Columbia • Anastasios Melis, Univ. of California, Berkeley • Anthony F. Michaels, Univ. of Southern California • Robert L. Modlin, David Geffen School of Medicine at UCLA • Jason H. Moore, Dartmouth Medical School • James V. Moroney, Louisiana State Univ. • Trudy G. Morrison, Univ. of Massachusetts Medical School • Sean Munro, MRC Laboratory of Molecular Biology, UK • Richard M. Myers, HudsonAlpha Institute for Biotechnology • Prakash S. Nagarkatti, Univ. of South Carolina • Peter L. Nara, Biological Mimetics, Inc./Iowa State Univ. • Neil M. Nathanson, Univ. of Washington • Alexandra C. Newton, Univ. of California, San Diego • Diana E. Northup, Univ. of New Mexico • Douglas L. Oliver, Univ. of Connecticut Health Center • Guillermo Oliver, St. Jude Children’s Research Hospital • George A. O’Toole, Dartmouth Medical School • Fernando Pardo-Manuel de Villena, Univ. of North Carolina, Chapel Hill • Margaret A. Pericak-Vance, Univ. of Miami • Caroline C. Philpott, National Institutes of Health • Kevin W. Plaxco, Univ. of California, Santa Barbara • Jeffrey W. Pollard, Albert Einstein College of Medicine • James W. Posakony, Univ. of California, San Diego • John R. Pringle, Stanford Univ. School of Medicine • Nancy Raab-Traub, Univ. of North Carolina, Chapel Hill • David M. Rand, Brown Univ. • Steven M. Reppert, Univ. of Massachusetts Medical School • Karin D. Rodland, Pacific Northwest National Laboratory • Claudina Rodrigues-Pousada, New Univ. of Lisbon, Portugal • Michael J. Ryan, Univ. of Texas at Austin • David E. Salt, Univ. of Aberdeen, UK • Federico Sánchez, Univ. Nacional Autónoma de México • Richard T. Sayre, Los Alamos National Laboratory • Stephen W. Scherer, Hospital for Sick Children, Canada • Jack C. Schultz, Univ. of Missouri-Columbia • Jeff Sekelsky, Univ. of North Carolina, Chapel Hill • Thomas D. Sharkey, Michigan State Univ. • Amanda A. Simcox, Ohio State Univ. • Patricia Simpson, Univ. of Cambridge, UK • Maureen L. Stanton, Univ. of California, Davis • William T. Starmer, Syracuse Univ. • John D. Storey, Princeton Univ. • F. Robert Tabita, Ohio State Univ. • Andrew T.C. Tsin, Univ. of Texas at San Antonio • Larry N. Vanderhoef, Univ. of California, Davis • Matthew K. Waldor, Brigham and Women’s Hospital • Cheryl Lyn Walker, Univ. of Texas MD Anderson Cancer Center • Angela Wandinger-Ness, Univ. of New Mexico • Gary A. Weisman, Univ. of Missouri-Columbia • Lois S. Weisman, Univ. of Michigan • James B. Whitfield, Univ. of Illinois, Urbana-Champaign • Thomas G. Whitham, Northern Arizona Univ. • Michael C. Whitlock, Univ. of British Columbia, Canada • Gerald S. Wilkinson, Univ. of Maryland, College Park • Joseph B. Williams, Ohio State Univ. • Scott M. Williams, Vanderbilt Univ. • Ned S. Wingreen, Princeton Univ. • Yue Xiong, Univ. of North Carolina, Chapel Hill • Tian Xu, Yale Univ. School of Medicine • Craig M. Young, Univ. of Oregon • Barry R. Zirkin, Johns Hopkins Bloomberg School of Public Health&lt;/p&gt;
&lt;h4 id=&#34;section-on-chemistry&#34;&gt;Section on Chemistry&lt;/h4&gt;
&lt;p&gt;Steven A. Adelman, Purdue Univ. • S. Michael Angel, Univ. of South Carolina • Zlatko Bac˘ic´, New York Univ. • Nathan A. Baker, Pacific Northwest National Laboratory • Alan L. Balch, Univ. of California, Davis • Peter A. Beal, Univ. of California, Davis • Darryl J. Bornhop, Vanderbilt Univ. • Kit Hansell Bowen, Jr., Johns Hopkins Univ. • Laurie J. Butler, Univ. of Chicago • Susan Beda Butts, Council for Chemical Research • Heather A. Carlson, Univ. of Michigan • Carl J. Carrano, San Diego State Univ. • Joseph A. Caruso, Univ. of Cincinnati • Daniel T. Chiu, Univ. of Washington • David E. Clemmer, Indiana Univ. • William J. Cooper, Univ. of California, Irvine • Brian R. Crane, Cornell Univ. • Frederick Dahlquist, Univ. of California, Santa Barbara • John M. Denu, Univ. of Wisconsin-Madison • David M. Dooley, Univ. of Rhode Island • Antonio Facchetti, Northwestern Univ. • James M. Farrar, Univ. of Rochester • Ellen R. Fisher, Colorado State Univ. • Daniel R. Gamelin, Univ. of Washington • Kent S. Gates, Univ. of Missouri-Columbia • Karen I. Goldberg, Univ. of Washington • John C. Gordon, Los Alamos National Laboratory • Arunava Gupta, Univ. of Alabama • Michael M. Haley, Univ. of Oregon • Benjamin S. Hsiao, Stony Brook Univ. • Russell P. Hughes, Dartmouth College • Joseph T. Hupp, Northwestern Univ. • Brent Iverson, Univ. of Texas at Austin • Cynthia J. Jenks, Ames Laboratory • Richard F. Jordan, Univ. of Chicago • Alamgir Karim, Univ. of Akron • Jaqueline L. Kiplinger, Los Alamos National Laboratory • Lukasz Lebioda, Univ. of South Carolina • George W. Luther III, Univ. of Delaware • Anne B. McCoy, Ohio State Univ. • Scott J. Miller, Yale Univ. • Nancy S. Mills, Trinity Univ. • Timothy K. Minton, Montana State Univ. • Karl T. Mueller, Pacific Northwest National Laboratory • Balaji Narasimhan, Iowa State Univ. • Joseph M. O’Connor, Univ. of California, San Diego • Peter J. Ortoleva, Indiana Univ. • Kirk A. Peterson, Washington State Univ. • Piotr Piecuch, Michigan State Univ. • Prasad L. Polavarapu, Vanderbilt Univ. • T. V. RajanBabu, Ohio State Univ. • Bruce H. Robinson, Univ. of Washington • Jeanne M. Robinson, Los Alamos National Laboratory • Robin D. Rogers, Univ. of Alabama • Sandra J. Rosenthal, Vanderbilt Univ. • Michael J. Sailor, Univ. of California, San Diego • Karl A. Scheidt, Northwestern Univ. • Ben Shen, Scripps Research Institute • Mary Jane Shultz, Tufts Univ. • Alan J. Shusterman, Reed College • Matthew S. Sigman, Univ. of Utah • Claudia Turro, Ohio State Univ. • Wilfred A. van der Donk, Univ. of Illinois, Urbana-Champaign • Robert A. Walker, Montana State Univ. • Nils G. Walter, Univ. of Michigan • Yinsheng Wang, Univ. of California, Riverside • Michael D. Ward, New York Univ. • Chrys Wesdemiotis,Univ. of Akron • Henry S. White, Univ. of Utah • M. Christina White, Univ. of Illinois, Urbana-Champaign • Sarah A. Woodson, Johns Hopkins Univ. • X. Nancy Xu, Old Dominion Univ. • Michael J. Zaworotko, Univ. of South Florida • Dongping Zhong, Ohio State Univ. • Ruhong Zhou, IBM Thomas J. Watson Research Center • Dorothy Zolandz, National Academy of Sciences&lt;/p&gt;
&lt;h4 id=&#34;section-on-dentistry&#34;&gt;Section on Dentistry&lt;/h4&gt;
&lt;p&gt;Francesco Chiappelli, UCLA School of Dentistry • Rena N. D’Souza, Texas A&amp;amp;M Health Science Center • Paul H. Krebsbach, Univ. of Michigan School of Dentistry • James E. Melvin, National Institute of Dental and Craniofacial Research, National Institutes of Health • Cun-Yu Wang, UCLA School of Dentistry&lt;/p&gt;
&lt;h4 id=&#34;section-on-education&#34;&gt;Section on Education&lt;/h4&gt;
&lt;p&gt;Fouad Abd-El-Khalick, Univ. of Illinois, Urbana-Champaign • Barbara A. Crawford, Cornell Univ. • Kent J. Crippen, Univ. of Florida • Judith A. Dilts, James Madison Univ. • Joan Ferrini-Mundy, National Science Foundation • Beverly Lindsay, Pennsylvania State Univ. • Karen Kashmanian Oates, Worcester Polytechnic Institute • Jonathan A. Plucker, Indiana Univ. • Christian Dieter Schunn, Univ. of Pittsburgh • Michelle Miller Sulikowski, Vanderbilt Univ. • Molly H. Weinburgh, Texas Christian Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-engineering&#34;&gt;Section on Engineering&lt;/h4&gt;
&lt;p&gt;Ali Adibi, Georgia Institute of Technology • Suresh K. Aggarwal, Univ. of Illinois, Chicago • Muhammad A. Alam, Purdue Univ. • Pedro J.J. Alvarez, Rice Univ. • Panos Antsaklis, Univ. of Notre Dame • Terry Bahill, Univ. of Arizona • Rashid Bashir, Univ. of Illinois, Urbana-Champaign • Wesley R. Burghardt, Northwestern Univ. • Robert J. Butera, Georgia Institute of Technology • C. Barry Carter, Univ. of Connecticut • Sanjeev Chandra, Univ. of Toronto, Canada • Srinivasan Chandrasekar, Purdue Univ. • Ni-Bin Chang, Univ. of Central Florida • Rama Chellappa, Univ. of Maryland, College Park • Vikram L. Dalal, Iowa State Univ. • Pablo G. Debenedetti, Princeton Univ. • Debasish Dutta, Univ. of Illinois, Urbana-Champaign • Suzanne Fortier, Natural Sciences and Engineering Research Council of Canada • Benny Dean Freeman, Univ. of Texas at Austin • Suresh V. Garimella, Purdue Univ. • Andrew Avi Goldenberg, Univ. of Toronto, Canada • Yogi D. Goswami, Univ. of South Florida • Rajiv Gupta, Univ. of California, Riverside • Joseph P. Heremans, Ohio State Univ. • K. Jimmy Hsia, Univ. of Illinois, Urbana-Champaign • Yingbo Hua, Univ. of California, Riverside • Michael M. Khonsari, Louisiana State Univ. • Lee Rybeck Lynd, Dartmouth College • Antonios G. Mikos, Rice Univ. • Larry Akio Nagahara, National Cancer Institute, National Institutes of Health • Chul Park, Univ. of Toronto, Canada • Bhakta B. Rath, Naval Research Laboratory • Lakshmi N. Reddi, Univ. of Central Florida • William B. Russel, Princeton Univ. • Michael Vivian Sefton, Univ. of Toronto, Canada • Michael L. Simpson, Oak Ridge National Laboratory • Tarunraj Singh, Univ. at Buffalo, The State Univ. of New York • Alexander J. Smits, Princeton Univ. • Randall Q. Snurr, Northwestern Univ. • Vijay Srinivasan, National Institute of Standards and Technology • Paul G. Steffes, Georgia Institute of Technology • Michael Tsapatsis, Univ. of Minnesota • Darrell Velegol, Pennsylvania State Univ. • Richard E. Waugh, Univ. of Rochester • Alan E. Willner, Univ. of Southern California • Moe Z. Win, Massachusetts Institute of Technology • Karl Dane Wittrup, Massachusetts Institute of Technology • William W-G. Yeh, Univ. of California, Los Angeles • R. Paul Young, Univ. of Toronto, Canada • Paul K.L. Yu, Univ. of California, San Diego • Hussein M. Zbib, Washington State Univ. • Kemin Zhou, Louisiana State Univ. • Mengchu Zhou, New Jersey Institute of Technology&lt;/p&gt;
&lt;h4 id=&#34;section-on-general-interest-in-science-and-engineering&#34;&gt;Section on General Interest in Science and Engineering&lt;/h4&gt;
&lt;p&gt;Lawrence Bell, The Museum of Science, Boston • Mary Eileen Burke, Academy of Science-St. Louis • Gary G. DeLeo, Lehigh Univ. • David D. Herring, National Oceanic and Atmospheric Administration • James P. O’Brien, Tidewater Community College • Robert F. Phalen, Univ. of California, Irvine • Katherine E. Rowan, George Mason Univ. • Kris M. Wilson, Univ. of Texas at Austin&lt;/p&gt;
&lt;h4 id=&#34;section-on-geology-and-geography&#34;&gt;Section on Geology and Geography&lt;/h4&gt;
&lt;p&gt;John T. Andrews, Univ. of Colorado at Boulder • Huiming Bao, Louisiana State Univ. • Edward J. Brook, Oregon State Univ. • Robert W. Buddemeier, Kansas Geological Survey • Gary R. Byerly, Louisiana State Univ. • Martin B. Goldhaber, U.S. Geological Survey • Daniel A. Griffith, Univ. of Texas at Dallas • Jennifer W. Harden, U.S. Geological Survey • Lloyd D. Keigwin, Jr., Woods Hole Oceanographic Institution • John A. Kelmelis, Pennsylvania State Univ. • Arthur N. Palmer, State Univ. of New York College at Oneonta • Peter A. Rogerson, Univ. at Buffalo, The State Univ. of New York • C.K. Shum, Ohio State Univ. • Lisa Tauxe, Scripps Institution of Oceanography, Univ. of California, San Diego&lt;/p&gt;
&lt;h4 id=&#34;section-on-history-and-philosophy-of-science&#34;&gt;Section on History and Philosophy of Science&lt;/h4&gt;
&lt;p&gt;Steven J. Dick, Retired, National Aeronautics and Space Administration • W. Patrick McCray, Univ. of California, Santa Barbara • Carolyn Merchant, Univ. of California, Berkeley • Helga Nowotny, European Research Council • Rosemary Stevens, Cornell Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-industrial-science-and-technology&#34;&gt;Section on Industrial Science and Technology&lt;/h4&gt;
&lt;p&gt;Quanxi Jia, Los Alamos National Laboratory&lt;/p&gt;
&lt;h4 id=&#34;section-on-information-computing-and-communication&#34;&gt;Section on Information, Computing, and Communication&lt;/h4&gt;
&lt;p&gt;Behnaam Aazhang, Rice Univ. • Martín Abadi, Univ. of California, Santa Cruz /Microsoft Research •  &lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Institute of Technology • Luiz André Barroso, Google, Inc. • Katy Börner, Indiana Univ. • Allan Borodin, Univ. of Toronto, Canada • José A.B. Fortes, Univ. of Florida • James Hendler, Rensselaer Polytechnic Institute • Alan R. Hevner, Univ. of South Florida • Randy H. Katz, Univ. of California, Berkeley • Joseph A. Konstan, Univ. of Minnesota • John E. Laird, Univ. of Michigan • Hector J. Levesque, Univ. of Toronto, Canada • Michael R. Nelson, Georgetown Univ. • Krishna V. Palem, Rice Univ. • Jon M. Peha, Carnegie Mellon Univ. • Martha E. Pollack, Univ. of Michigan • Stuart Russell, Univ. of California, Berkeley • Subhash Suri, Univ. of California, Santa Barbara • Paul F. Uhlir, National Academy of Sciences • Jeffrey Voas, National Institute of Standards and Technology&lt;/p&gt;
&lt;h4 id=&#34;section-on-linguistics-and-language-sciences&#34;&gt;Section on Linguistics and Language Sciences&lt;/h4&gt;
&lt;p&gt;Peter W. Culicover, Ohio State Univ. • John J. Ohala, Univ. of California, Berkeley • Carol Padden, Univ. of California, San Diego&lt;/p&gt;
&lt;h4 id=&#34;section-on-mathematics&#34;&gt;Section on Mathematics&lt;/h4&gt;
&lt;p&gt;Mark S. Alber, Univ. of Notre Dame • Ingrid Daubechies, Duke Univ. • Mark L. Green, Univ. of California, Los Angeles • Claudia Neuhauser, Univ. of Minnesota Rochester • Richard A. Tapia, Rice Univ. • Roger Temam, Indiana Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-medical-sciences&#34;&gt;Section on Medical Sciences&lt;/h4&gt;
&lt;p&gt;Adriano Aguzzi, Univ. Hospital of Zurich • Jayakrishna Ambati, Univ. of Kentucky • Brenda L. Bass, Univ. of Utah • Elaine L. Bearer, Univ. of New Mexico • Thomas L. Benjamin, Harvard Medical School • Nancy J. Brown, Vanderbilt Univ. School of Medicine • Sally A. Camper, Univ. of Michigan Medical School • Christin Carter-Su, Univ. of Michigan Medical School • Ellen Wright Clayton, Vanderbilt Univ. • Carlo M. Croce, Ohio State Univ. • Michael R. DeBaun, Vanderbilt Children’s Hospital • Mark R. Denison, Vanderbilt Univ. Medical Center • Eleftherios P. Diamandis, Univ. of Toronto, Canada • Linda C. Giudice, Univ. of California, San Francisco • Keith W. Kelley, Univ. of Illinois, Urbana-Champaign • Michael M. Lederman, Case Western Reserve Univ. • Beth Levine, Univ. of Texas Southwestern Medical Center • Xiaoxia Li, Cleveland Clinic Foundation • Malcolm J. Low, Univ. of Michigan • Philippa Marrack, National Jewish Health • Ruslan Medzhitov, Yale Univ. School of Medicine • Gordon B. Mills, Univ. of Texas MD Anderson Cancer Center • Josef Penninger, Institute of Molecular Biotechnology, Austria • Stanley Perlman, Univ. of Iowa • Jeffrey E. Pessin, Albert Einstein College of Medicine • Richard George Pestell, Kimmel Cancer Center/Thomas Jefferson Univ. • Paula Pitha-Rowe, Johns Hopkins School of Medicine • Scott A. Rivkees, Yale School of Medicine • Marjorie Robert-Guroff, National Cancer Institute, National Institutes of Health • John J. Rossi, City of Hope • Steven J. Schiff, Pennsylvania State Univ. • Deepak Srivastava, Gladstone Institute of Cardiovascular Disease • James H. Strauss, California Institute of Technology • Robert M. Strieter, Univ. of Virginia School of Medicine • Joseph R. Testa, Fox Chase Cancer Center • Denisa D. Wagner, Harvard Medical School • Mark A. Wainberg, McGill Univ. AIDS Centre, Canada • David B. Weiner, Univ. of Pennsylvania School of Medicine • Jane Y. Wu, Northwestern Univ. Feinberg School of Medicine • Dihua Yu, Univ. of Texas MD Anderson Cancer Center • Susan Zolla-Pazner, New York Univ. School of Medicine/New York Veterans Affairs Medical Center&lt;/p&gt;
&lt;h4 id=&#34;section-on-neuroscience&#34;&gt;Section on Neuroscience&lt;/h4&gt;
&lt;p&gt;Ben A. Barres, Stanford Univ. School of Medicine • Nancy M. Bonini, Univ. of Pennsylvania • Catherine Emily Carr, Univ. of Maryland, College Park • Bruce D. Carter, Vanderbilt Univ. School of Medicine • Barry W. Connors, Brown Univ. • Marie T. Filbin, Hunter College • Stuart Firestein, Columbia Univ. • Michael Frotscher, Univ. of Freiburg, Germany • Anthony A. Grace, Univ. of Pittsburgh • Michael E. Hasselmo, Boston Univ. • Steven E. Hyman, Harvard Univ. • Bruce T. Lamb, Cleveland Clinic Foundation • Diane Lipscombe, Brown Univ. • Stuart A. Lipton, Sanford-Burnham Medical Research Institute • Liqun Luo, Stanford Univ. • Enrico Mugnaini, Northwestern Univ. Feinberg School of Medicine • Sarah L. Pallas, Georgia State Univ. • Gregory J. Quirk, Univ. of Puerto Rico School of Medicine • James B. Ranck, Jr., State Univ. of New York Health Science Center at Brooklyn • Lorna W. Role, Stony Brook Univ. • Roderick A. Suthers, Indiana Univ. • Matthew A. Wilson, Massachusetts Institute of Technology&lt;/p&gt;
&lt;h4 id=&#34;section-on-pharmaceutical-sciences&#34;&gt;Section on Pharmaceutical Sciences&lt;/h4&gt;
&lt;p&gt;Peter J. Houghton, Nationwide Children’s Hospital • Margaret O. James, Univ. of Florida • Donald P. McDonnell, Duke Univ. Medical Center • John C. Reed, Sanford-Burnham Medical Research Institute • Danny D. Shen, Univ. of Washington • Patrick J. Sinko, Rutgers, The State Univ. of New Jersey • Jashvant D. Unadkat, Univ. of Washington • Mary K. Wolpert-DeFilippes, National Cancer Institute, National Institutes of Health • Yun Yen, City of Hope&lt;/p&gt;
&lt;h4 id=&#34;section-on-physics&#34;&gt;Section on Physics&lt;/h4&gt;
&lt;p&gt;Harald Ade, North Carolina State Univ. • Alexander V. Balatsky, Los Alamos National Laboratory • Albert-László Barabási, Northeastern Univ. • Jerzy Bernholc, North Carolina State Univ. • Theodore W. Bowyer, Pacific Northwest National Laboratory • Samuel Leon Braunstein, Univ. of York, UK • R. Sekhar Chivukula, Michigan State Univ. • Margaret Dobrowolska, Univ. of Notre Dame • George William Foster, Research Physicist and Former U.S. Representative • Jacek K. Furdyna, Univ. of Notre Dame • Efim Gluskin, Argonne National Laboratory • Alan J. Heeger, Univ. of California, Santa Barbara • Tin-Lun (Jason) Ho, Ohio State Univ. • Jainendra K. Jain, Pennsylvania State Univ. • Bobby R. Junker, U.S. Office of Naval Research • Shiv N. Khanna, Virginia Commonwealth Univ. • Young-Kee Kim, Fermi National Accelerator Laboratory/Univ. of Chicago • Raymond Laflamme, Univ. of Waterloo, Canada • Daniel Perry Lathrop, Univ. of Maryland, College Park • Ramon E. Lopez, Univ. of Texas, Arlington • Alfred Z. Msezane, Clark Atlanta Univ. • Jeffrey S. Nico, National Institute of Standards and Technology • Jaan Noolandi, Retired, Xerox Research Centre of Canada • Philip A. Pincus, Univ. of California, Santa Barbara • Cedric J. Powell, National Institute of Standards and Technology • Apparao M. Rao, Clemson Univ. • Laura Reina, Florida State Univ. • Gertrude Fleming Rempfer, Portland State Univ. • Lee L. Riedinger, Univ. of Tennessee • Steven Lloyd Rolston, Univ. of Maryland, College Park • Michael Schick, Univ. of Washington • Lu Jeu Sham, Univ. of California, San Diego • Elizabeth Simmons, Michigan State Univ. • Pekka Sinervo, Canadian Institute for Advanced Research • Peter W. Stephens, Stony Brook Univ. • George F. Sterman, Stony Brook Univ. • Robert L. Sugar, Univ. of California, Santa Barbara • Raman Sundrum, Univ. of Maryland, College Park • Mauricio Terrones, Pennsylvania State Univ. • Ram K. Tripathi, NASA Langley Research Center • John D. Weeks, Univ. of Maryland, College Park • William A. Zajc, Columbia Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-psychology&#34;&gt;Section on Psychology&lt;/h4&gt;
&lt;p&gt;Kent Charles Berridge, Univ. of Michigan • Sandra Blakeslee, New York Times • Rosemarie M. Booze, Univ. of South Carolina • Dante Cicchetti, Univ. of Minnesota • Steve W. Cole, David Geffen School of Medicine at UCLA • Mary Anne Fitzpatrick, Univ. of South Carolina • David Cyril Geary, Univ. of Missouri-Columbia • Judith F. Kroll, Pennsylvania State Univ. • Daniel J. Levitin, McGill Univ., Canada • Alan C. Spector, Florida State Univ. • Joseph Edward Steinmetz, Ohio State Univ. • Richard E. Tremblay, Univ. of Montreal, Canada • Zuoxin Wang, Florida State Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-social-economic-and-political-sciences&#34;&gt;Section on Social, Economic, and Political Sciences&lt;/h4&gt;
&lt;p&gt;Myron P. Gutmann, National Science Foundation • John R. Hibbing, Univ. of Nebraska-Lincoln • Sally T. Hillsman, American Sociological Association • Matthew D. McCubbins, Univ. of Southern California • Randolph Roth, Ohio State Univ. • Richard H. Steckel, Ohio State Univ. • Paula Stephan, Georgia State Univ.&lt;/p&gt;
&lt;h4 id=&#34;section-on-societal-impacts-of-science-and-engineering&#34;&gt;Section on Societal Impacts of Science and Engineering&lt;/h4&gt;
&lt;p&gt;Daniel M. Kammen, Univ. of California, Berkeley • Dena Plemmons, Univ. of California, San Diego • Tobin L. Smith, Association of American Universities&lt;/p&gt;
&lt;h4 id=&#34;section-on-statistics&#34;&gt;Section on Statistics&lt;/h4&gt;
&lt;p&gt;George Casella, Univ. of Florida • Dipak K. Dey, Univ. of Connecticut • Robert E. Fay, Westat • Wing Kam Fung, Univ. of Hong Kong • Miguel A. Hernán, Harvard School of Public Health • Joan F. Hilton, Univ. of California, San Francisco • André I. Khuri, Univ. of Florida • Sastry G. Pantula, National Science Foundation • Xiaotong Shen, Univ. of Minnesota • George W. Williams, Amgen, Inc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The AAAS seeks to &amp;ldquo;advance science, engineering, and innovation throughout the world for the benefit of all people.&amp;rdquo; The world&amp;rsquo;s largest multidisciplinary scientific society and a leading publisher of cutting-edge research through its Science family of journals, AAAS has individual members in more than 91 countries around the globe.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pentagon under 24 / 7 DARPA surveillance</title>
      <link>http://localhost:1313/blog/20111223-rt/</link>
      <pubDate>Fri, 23 Dec 2011 11:12:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111223-rt/</guid>
      <description>

















&lt;figure  id=&#34;figure-iraq-fallujah-us-marines-from-the-first-battalion-5th-marines-bravo-company-browse-the-internet-at-camp-mercury-25-april-2004-afp-photo--nicolas-asfouri--afp&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Iraq, Fallujah: US marines from the First Battalion, 5th Marines, Bravo Company, browse the internet at camp Mercury 25 April 2004. (AFP Photo / Nicolas Asfouri) © AFP&#34; srcset=&#34;
               /blog/20111223-rt/first-marines-iraq-fallujah.si_hu_9fd7fa22d810d78b.webp 400w,
               /blog/20111223-rt/first-marines-iraq-fallujah.si_hu_acfbf71e35bc83f2.webp 760w,
               /blog/20111223-rt/first-marines-iraq-fallujah.si_hu_e2b711e5386e056d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111223-rt/first-marines-iraq-fallujah.si_hu_9fd7fa22d810d78b.webp&#34;
               width=&#34;690&#34;
               height=&#34;388&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Iraq, Fallujah: US marines from the First Battalion, 5th Marines, Bravo Company, browse the internet at camp Mercury 25 April 2004. (AFP Photo / Nicolas Asfouri) © AFP
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The Pentagon will soon be prying through the personal correspondence and computer files of US military personnel, thanks to a $9-million program that will put soldiers’ private emails under Uncle Sam’s microscope.
Defense Advanced Research Projects Agency, or DARPA, has awarded the grant to five institutions led by &lt;strong&gt;Georgia Tech&lt;/strong&gt; to help develop a system of spying on solderis’ Internet and computer habits, a multi-million dollar investment that they say will serve as a preemptive measure to make sure “insider threats” can’t materialize in the military.&lt;/p&gt;
&lt;p&gt;The Pentagon is calling the project “Proactive Discovery of Insider Threats Using Graph Analysis and Learning,” or “PRODIGAL,” and it will scour the e-mails, text messages and files transfers of solders’ “for unusual activity,” writes Georgia Tech, using “a suit of algorithms” that will be able to weed out any weirdness within the Department of Defense that could become a security threat.&lt;/p&gt;
&lt;p&gt;A spokesman for DARPA deferred to answer to the Army Times how, exactly, they plan on conducting the surveillance over the correspondence. Wired.com’s Danger Room writes, however, that every keystroke, log-in and file upload initiated over DoD networks will be under strict scrutiny in hopes of breaking up any more Bradley Mannings from making their way into the military.&lt;/p&gt;
&lt;p&gt;Rep. Peter King (Rep-NY) said at a hearing earlier this month that “The Fort Hood attack was not an anomaly.” According to the congressman, the shooting spree carried out by Nidal Hasan in 2009 “was part of al-Qaeda’s two-decade success at infiltrating the US military for terrorism, an effort that is increasing in scope and threat.”&lt;/p&gt;
&lt;p&gt;Given the Senate and House’s recent go-ahead with the National Defense Authorization Act, a legislation that will allow for the government to indefinitely detain and torture American citizens over suspected terrorist ties, a little cyber-sleuthing of soldiers seems like nothing at all.&lt;/p&gt;
&lt;p&gt;For the tens of thousands of defense workers separated from their loved ones by a multitude of miles and battle fields, however, the move comes as one big burden from Big Brother, and a smack in the face sent to the very men and women who are defending a supposed freedom for everyone else in America. Operation Homefront, a program that offers aid to military familes during times of deployment, offer campaigns in which they provide soldiers with laptops so that they can stay in touch with loved ones. Earlier this month, they unloaded several computers on soldiers at Fort Riley thanks to a partnership with CDW Government LLC.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are grateful that through our continued partnership with Operation Homefront, we are able to honor the sacrifice of military families and help alleviate some of the stress they often feel when separated from their deployed family members,” Brigadier General John Howard (Ret.), CDW-G DoD business development manager, said in a statement at the time. “While email can never replace the presence of a parent or spouse at home, these laptops provide a vital connection to home when it is needed the most.”&lt;/p&gt;
&lt;p&gt;“Although we can never take the sacrifice out of a deployment, we hope that the laptops will help improve the quality of life for our military personnel and their families,” added Amy Palmer, chief operating officer for Operation Homefront. “Without the means to afford computers, many soldiers and their families must wait to hear from one another, which can affect morale on and off the battlefield. However, with the help of CDW-G, many families of deployed soldiers can now communicate daily, easing concerns of worried loved ones.”&lt;/p&gt;
&lt;p&gt;While PRODIGAL doesn’t stand to exactly put the Pentagon between the sender of the email and the recipient, it will cause soldiers to censor their thoughts with often the only people they can relate to.&lt;/p&gt;
&lt;p&gt;In the past year, DARPA has announced other plans to pry into military personnel, including the Narrative Networks project to find out who is most susceptible to propaganda, and Power Dreaming, an initiative that will scan brainwave patterns of sleeping soldiers to try to determine what causes what dreams.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rt.com/usa/pentagon-darpa-prodigal-email-537/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.rt.com/usa/pentagon-darpa-prodigal-email-537/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pentagon to monitor military emails for &#34;insider threats&#34;</title>
      <link>http://localhost:1313/blog/20111222-tgdaily/</link>
      <pubDate>Thu, 22 Dec 2011 16:57:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111222-tgdaily/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Trent Nouveau&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Pentagon is kicking of a new initiative to monitor military emails in an attempt to detect insider threats.&lt;/p&gt;
&lt;p&gt;Backed by DARPA, the project seeks to create “a suite of algorithms [capable of] detecting multiple types of insider threats by analyzing massive amounts of data – including email, text messages and file transfers – for unusual activity.”&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20111222-tgdaily/usairforcecyber_hu_d0de446b65d79843.webp 400w,
               /blog/20111222-tgdaily/usairforcecyber_hu_f31023d131c5443d.webp 760w,
               /blog/20111222-tgdaily/usairforcecyber_hu_1e057a455f5e1257.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111222-tgdaily/usairforcecyber_hu_d0de446b65d79843.webp&#34;
               width=&#34;400&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;According to the &lt;a href=&#34;https://www.armytimes.com/news/2011/12/military-darpa-email-surveillance-122111w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Army Times&lt;/a&gt;, military officials hope to identify potential security threats like WikiLeaks suspect Pfc. Bradley Manning who allegedly downloaded thousands of classified documents, or Army major Nidal Hasan who stands accused of killing 13 people at Fort Hood in New Jersey. (Hasan was in contact with Islamic extremists overseas before the shooting).  &lt;/p&gt;
&lt;p&gt;The project, officially dubbed the “Anomaly Detection at Multiple Scales program,” can best be described as “insider threat detection in which malevolent (or possibly inadvertent) actions by a trusted individual are detected against a background of everyday network activity.”&lt;/p&gt;
&lt;p&gt;It remains unclear whether the tracking will be limited to official government computers, or how many troops might be monitored during the development phase which could take up to two years.&lt;/p&gt;
&lt;p&gt;It should be noted that US military intelligence believes insider threats are steadily increasing, with authorities identifying at least five instances of plots or attacks from troops who had become “radicalized.”&lt;/p&gt;
&lt;p&gt;“The Fort Hood attack was not an anomaly,” Rep. Peter King, R-N.Y., recently told a hearing about military insider threats.   &lt;/p&gt;
&lt;p&gt;“It was part of al-Qaeda’s two-decade success at infiltrating the US military for terrorism, an effort that is increasing in scope and threat.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tgdaily.com/security-features/60375-pentagon-to-monitor-military-emails-for-insider-threats/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.tgdaily.com/security-features/60375-pentagon-to-monitor-military-emails-for-insider-threats/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Four Georgia Tech Faculty Named AAAS Fellows</title>
      <link>http://localhost:1313/blog/20111222-gatech-aaas-fellow/</link>
      <pubDate>Thu, 22 Dec 2011 08:03:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111222-gatech-aaas-fellow/</guid>
      <description>&lt;p&gt;The American Association for the Advancement of Science (AAAS) has named four Georgia Tech professors as 2011 Fellows. AAAS is the world’s largest general scientific society, and the election as a Fellow is an honor bestowed upon AAAS members by their peers.&lt;/p&gt;
&lt;p&gt;Three of the new AAAS Fellows at Georgia Tech hail from the College of Engineering and one is on the faculty in the College of Computing. The Fellows were announced today in the journal Science and will be honored at the Fellows Forum, held Feb. 18 at the AAAS Annual Meeting in Vancouver, Canada.&lt;/p&gt;
&lt;p&gt;The new AAAS Fellows at Georgia Tech are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ali Adibi, professor of electrical and computer engineering, who was honored for his “distinguished contributions to the fields of integrated nanophotonics, photonic crystals, and volume holography.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, professor of computational science and engineering in the College of Computing, who earned the distinction for “distinguished contributions to the field of computational science and engineering.”&lt;/li&gt;
&lt;li&gt;Robert Butera, professor of electrical and computer engineering who also holds a joint appointment in the Wallace H. Coulter Department of Biomedical Engineering at Georgia Tech and Emory University, was named Fellow “for advances in computational neuroscience and neurotechnology, promoting engineering through society, editorial, and university leadership, and contributing to STEM policy and educational initiatives.&amp;quot;&lt;/li&gt;
&lt;li&gt;Paul Steffes, professor of electrical and computer engineering, who earned the distinction for “contributions to the understanding of planetary atmospheres through innovative microwave measurements.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AAAS is an international non-profit organization dedicated to advancing science around the world by serving as an educator, leader, spokesperson and professional association. AAAS publishes the journal &lt;em&gt;Science&lt;/em&gt; as well as many scientific newsletters, books and reports, and spearheads programs that raise the bar of understanding for science worldwide. The four Georgia Tech faculty members were among 539 Fellows elected by the AAAS Council in November.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bioengineering.gatech.edu/four-georgia-tech-faculty-named-aaas-fellows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://bioengineering.gatech.edu/four-georgia-tech-faculty-named-aaas-fellows&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pentagon to Develop Computer System to Prevent Another Wikileaks</title>
      <link>http://localhost:1313/blog/20111219-executivebiz/</link>
      <pubDate>Mon, 19 Dec 2011 17:22:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111219-executivebiz/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20111219-executivebiz/Pentagon_hu_aecc1f7d3d84222e.webp 400w,
               /blog/20111219-executivebiz/Pentagon_hu_7d271dc270944112.webp 760w,
               /blog/20111219-executivebiz/Pentagon_hu_7a0fa4711bce06a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111219-executivebiz/Pentagon_hu_aecc1f7d3d84222e.webp&#34;
               width=&#34;168&#34;
               height=&#34;169&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Pentagon is currently working to prevent another WikiLeaks situation within the department. &lt;a href=&#34;http://www.wired.com/dangerroom/2011/12/darpa-email/#more-66805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wired.com&lt;/a&gt; recently reported a group of military funded scientists are developing a sophisticated computer system that can scan and interpret every key stroke, log-in and uploaded files over the Pentagon’s networks.&lt;/p&gt;
&lt;p&gt;The Defense Advanced Research Projects Agency  awarded about $9 million for five institutions’ work for the project. &lt;strong&gt;Georgia Tech&lt;/strong&gt; will be leading the institutions and will kick off an initiative called the “Proactive Discovery of Insider Threats Using Graph Analysis and Learning.”&lt;/p&gt;
&lt;p&gt;The project works to find potentially dangerous insiders before they are able to cause any damage and break the law.&lt;/p&gt;
&lt;p&gt;According to Wired.com, current military analysts investigate five out of thousands of anomalous computer activities a day. PRODIGAL would make sure analysts were looking into the most important ones.&lt;/p&gt;
&lt;p&gt;The program keeps information on users, their search history, behavior as well as scans emails, text-messages, log-ins and web browsing. The computer system would be able to scan 250 million emails, IMs and file transfers daily.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.executivebiz.com/2011/12/pentagon-to-develop-computer-system-to-prevent-another-wikileaks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://blog.executivebiz.com/2011/12/pentagon-to-develop-computer-system-to-prevent-another-wikileaks/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis tool would scan military e-mail for insider threats</title>
      <link>http://localhost:1313/blog/20111215-defensesystems/</link>
      <pubDate>Thu, 15 Dec 2011 22:29:08 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111215-defensesystems/</guid>
      <description>&lt;p&gt;A project funded through the Defense Advanced Research Projects Agency would identify the most serious insider threats to security by scanning all user e-mail messages, text messages, logins, file transfers and Web browsing on military networks, reports Katie Drummond at Wired&amp;rsquo;s &lt;a href=&#34;http://www.wired.com/dangerroom/2011/12/darpa-email/#more-66805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Danger Room blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The two-year Proactive Discovery of Insider Threats Using Graph Analysis and Learning (PRODIGAL) project is being is being conducted by a consortium of five institutions led by &lt;strong&gt;Georgia Tech&lt;/strong&gt;. The project falls under a larger initiative known as Anomaly Detection at Multiple Scales, which applies various technologies to scour through massive datasets.&lt;/p&gt;
&lt;p&gt;By scanning an estimated 250 million e-mail messages, instant messages and file transfers daily, PRODIGAL might lead officials to a WikiLeaker before that person breaks the law. After the technology is ready, it will initially only be tested on government officials and military personnel who’ve agreed to be monitored, DARPA officials told the blog.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://defensesystems.com/Articles/2011/12/15/agg-darpa-prodigal-insider-threats.aspx?a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://defensesystems.com/Articles/2011/12/15/agg-darpa-prodigal-insider-threats.aspx?a&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Warning to Gossipy Grunts: DARPA&#39;s Eyeing Your E-Mails</title>
      <link>http://localhost:1313/blog/20111215-wired/</link>
      <pubDate>Thu, 15 Dec 2011 17:27:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111215-wired/</guid>
      <description>

















&lt;figure  id=&#34;figure-photo-d-sharon-pruittflickr&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Photo: D. Sharon Pruitt/Flickr&#34; srcset=&#34;
               /blog/20111215-wired/5661935796_f19a0c528f_b-660x440_hu_35bd94ab359a6cc1.webp 400w,
               /blog/20111215-wired/5661935796_f19a0c528f_b-660x440_hu_96da274c357c4d8c.webp 760w,
               /blog/20111215-wired/5661935796_f19a0c528f_b-660x440_hu_fd1af8929f3dac1e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111215-wired/5661935796_f19a0c528f_b-660x440_hu_35bd94ab359a6cc1.webp&#34;
               width=&#34;660&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Photo: D. Sharon Pruitt/Flickr
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you don&amp;rsquo;t have anything nice to say, then &lt;em&gt;definitely&lt;/em&gt; don&amp;rsquo;t say it, type it or text it over a military network.&lt;/p&gt;
&lt;p&gt;The Pentagon&amp;rsquo;s intent on weeding out &amp;ldquo;insider threats&amp;rdquo; – troops or other military personnel who might be disgruntled enough to &lt;a href=&#34;https://www.wired.com/threatlevel/2011/03/bradley-manning-more-charge/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Wiki)leak some documents&lt;/a&gt;, or mentally unhinged enough to go on a &lt;a href=&#34;https://www.wired.com/dangerroom/2009/11/breaking-gunmen-kill-seven-in-ft-hood-massacre/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shooting rampage&lt;/a&gt;. Now, military-funded scientists are plotting a computer system that&amp;rsquo;d boast unprecedented abilities to scan and interpret every keystroke, log-in and file upload performed over Pentagon networks.&lt;/p&gt;
&lt;p&gt;Darpa, the military&amp;rsquo;s far-out research arm, recently announced &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a $9 million award&lt;/a&gt; to a consortium of five institutions, led by &lt;strong&gt;Georgia Tech&lt;/strong&gt;, to kick off a two-year project called &amp;ldquo;Proactive Discovery of Insider Threats Using Graph Analysis and Learning,&amp;rdquo; (PRODIGAL). The initiative is one part of a larger Darpa endeavor, ADAMS, that aims to find malevolent insiders before they cause problems. Already, a team at Columbia University is using ADAMS funding to &lt;a href=&#34;https://www.wired.com/dangerroom/2011/11/darpa-trap-wikileaks/#more-62028&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;trick WikiLeaker wannabes&lt;/a&gt; with decoy documents.&lt;/p&gt;
&lt;p&gt;PRODIGAL would take that threat detection up a few notches. Under the Columbia team&amp;rsquo;s plan, decoy documents would give military officials a trail of digital breadcrumbs: If the fakes were released online, analysts might be able to backtrack and figure out when and where they were obtained and how they went public. But PRODIGAL, if it works, could lead officials to a WikiLeaker before that person ever breaks the law.&lt;/p&gt;
&lt;p&gt;Right now, a human analyst in the military has time to find and investigate a mere five anomalous computer activities a day – unusual file transfers, log-in locations or website visits – out of thousands that occur. PRODIGAL would make sure analysts were looking into the most important ones. The program would use a complex combination of algorithms, including those designed to spot anomalies and statistically calibrate their potential threat, and then spit out a ranked list of the unexplained events most in need of examination.&lt;/p&gt;
&lt;p&gt;The program will keep tabs on individual users, checking their activity history against current habits to detect unusual behavior. And it&amp;rsquo;s intended to be incredibly thorough: Researchers plan to create a program that scans e-mails, text messages, log-ins, file transfers and web browsing. All in, the software will be able to scan an estimated 250 million e-mails, IMs and file transfers a day, along with infinite quantities of basic computer activity.&lt;/p&gt;
&lt;p&gt;Sounds like fodder for my next favorite dystopian novel. But Darpa officials are quick to reassure that PRODIGAL will initially only be tested on government officials and military personnel who&amp;rsquo;ve agreed to be monitored. And assuming initial tests go well? Well, maybe don&amp;rsquo;t get too worried. The Pentagon&amp;rsquo;s got bigger plots to foil than your coordinated efforts to steal your captain&amp;rsquo;s underwear.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wired.com/2011/12/darpa-email/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wired.com/2011/12/darpa-email/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC11 Boasts High Georgia Tech Participation in Technical Program</title>
      <link>http://localhost:1313/blog/20111209-gatech-sc11/</link>
      <pubDate>Fri, 09 Dec 2011 08:28:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111209-gatech-sc11/</guid>
      <description>&lt;p&gt;Georgia Tech leveraged its strengths in high performance computing Nov 12-18 at the
SC11 conference in Seattle, with a strong showing in the technical program and through
faculty members building on current research partnerships and collaborations.&lt;/p&gt;
&lt;p&gt;More than 10,000 attendees in industry, academic research and government came to
experience the latest developments in Big Data and high performance computing at SC11,
the international conference for HPC, networking, storage and analysis.&lt;/p&gt;
&lt;p&gt;Georgia Tech faculty and graduate students led or took part in at least 20 activities within
the technical program at the SuperComputing conference, one of the largest industry
tradeshows Georgia Tech attends. More than 40 faculty, students and staff attended the
event.&lt;/p&gt;
&lt;p&gt;“Georgia Tech excels at solving Big Data problems that are emerging in health informatics,
business analytics, social media, and national security,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor of
computing and executive director of high performance computing. “Solving real-world
problems is the hallmark of our School of Computational Science and Engineering, and our
accomplishments in this area received recognition at SC11.”&lt;/p&gt;
&lt;p&gt;In the technical program, the Graph500 announcement, in which Bader unveiled the
ranking of the world&amp;rsquo;s fastest supercomputers, was standing-room-only; and the GT-hosted
Intel machine, mirasol, came in at No. 19 on the list. Aparna Chandramowlishwaran’s
George Michael HPC Fellow presentation engaged a large audience with a high level of
interaction afterwards.&lt;/p&gt;
&lt;p&gt;Georgia Tech’s booth included a continuous presentation on four monitors of more than 60
current research initiatives in which faculty members are involved. The research highlighted
the comprehensive and interdisciplinary approach at Georgia Tech in tackling a broad
range of application areas in massive data and high performance computing.&lt;/p&gt;
&lt;p&gt;For a full roster of Georgia Tech participation in SC11, visit the GT@SC11 website (sc11.gatech.edu).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Brothers, PRODIGAL Sons, and Cybersecurity</title>
      <link>http://localhost:1313/blog/20111207-catoinstitute/</link>
      <pubDate>Wed, 07 Dec 2011 08:30:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111207-catoinstitute/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Julian Sanchez&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I &lt;a href=&#34;https://www.cato.org/blog/cybersecurity-exception-wiretap-laws&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrote on Monday&lt;/a&gt; that a cybersecurity bill overwhelmingly approved by the House Permanent Select Committee on Intelligence risks creating a significantly broader loophole in federal electronic surveillance law than its boosters expect or intend.  Creating both legal leeway and a trusted environment for limited information sharing about cybersecurity threats—such as the idenifying signatures of malware or automated attack patterns—is a good idea. Yet the wording of the proposed statute permits broad collection and disclosure of any information that would be relevant to protecting against “cyber threats,” broadly defined. For now, that mostly means monitoring the behavior of software; in the near future, it could as easily mean monitoring the behavior of people.&lt;/p&gt;
&lt;p&gt;A recent—and somewhat sensationalistic—Fox News article rather breathlessly describes a newly-unveiled security system dubbed PRODIGAL, or Proactive Discovery of Insider Threats Using Graph Analysis and Learning, which “has been built to scan IMs, texts and emails … and can read approximately a quarter billion of them a day.” The article explains:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Every time someone logs on or off, sends an email or text, touches a file or plugs in a USB key, these records are collected within the organization,” &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the Georgia Tech School of Computational Science and Engineering and a principal investigator on the project, told FoxNews.com.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;PRODIGAL scans those records for behavior – emails to unusual recipients, certain words cropping up, files transferred from unexpected servers – that changes over time as an employee “goes rogue.” The system was developed at Georgia Tech in conjunction with the Defense Advanced Research Projects Agency (DARPA), the Army’s secretive research arm that works on everything from flying cars to robotic exoskeletons.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Don’t panic just yet: This is strictly being deployed on the networks of government agencies and contractors that handle sensitive information—places where every employee is well aware that their use of the network is subject to close scrutiny, and with good reason.  There’s not really anything to say in principle against the use of such systems in this context, or for that matter on closed business networks where users are on clear notice that such monitoring occurs.&lt;/p&gt;
&lt;p&gt;It would, by contrast, be a clear and quite outrageous invasion of privacy for such large-scale behavioral monitoring to be conducted on the residential or mobile broadband networks Americans rely on to provide their personal Internet connectivity—a fortiori if the goal is to share the results with the government without a court order.  As I read it, however, House Intel’s cybersecurity bill would at least arguably permit precisely that.&lt;/p&gt;
&lt;p&gt;Under the current language, as long as an Internet provider had a credible good faith belief that it was collecting and sharing behavioral information for one of several broadly defined “cybersecurity purposes”—say, by creating behavioral profiles of potential hackers, disruptive cyberactivists, or “misappropriators” of intellectual property—they’d enjoy full civil and criminal immunity for such actions. That would make any contractual promises to abstain from such monitoring unenforceable—in the highly unlikely event that ordinary users were even able to determine reliably what sort of information was being shared. It would be, to put it as mildly as possible, extraordinarily poor civic hygiene to  enable the construction of this kind of quasi-public/quasi-private monitoring and profiling architecture.&lt;/p&gt;
&lt;p&gt;This is not, I believe, the sort of thing the bill’s own architects aspire to bring about.  But the abstract language employed in pursuit of technological neutrality here avoids the risk of obsolescence only by sacrificing predictability.  Courts have recently begun signalling that they’re belatedly inclined to start insisting on full Fourth Amendment search warrants whenever government seeks digitally stored private contents, closing down statutory loopholes that sometimes gave investigators easier access. And now, just as one backdoor closes, a new backchannel granting access to otherwise private and protected material without any judicial process opens up? It does not take a cynic to predict that there will be a potent and persistent incentive to stretch any such channel as wide as the elastic bonds of the English language will permit.&lt;/p&gt;
&lt;p&gt;The cleanest way to foreclose this is not to paste in a bunch of after-the-fact usage controls, minimization protocols, or special reports to Congress—though those aren’t bad ideas either. It’s to admit that Congress lacks psychic powers, which may entail that statutes regulating protean areas of technology  have to be (or ought to be) swapped for the newer model about as often as iPhones. The specific, narrow categories of sharing everyone thinks are important and unobjectionable from a privacy perspective can be specifically, narrowly authorized now. In a decade, when we’re beaming thoughts directly to each other via quantum-entangled biomechanical brain implants, we can decide what specific statutory language solves the novel security problems of that technology, in a manner consistent with the Fourth Amendment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cato.org/blog/big-brothers-prodigal-sons-cybersecurity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cato.org/blog/big-brothers-prodigal-sons-cybersecurity&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Online Spying</title>
      <link>http://localhost:1313/blog/20111206-cryptome/</link>
      <pubDate>Tue, 06 Dec 2011 22:09:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111206-cryptome/</guid>
      <description>

















&lt;figure  id=&#34;figure-georgia-tech-darpa-adams-leaders&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Georgia Tech DARPA ADAMS leaders&#34; srcset=&#34;
               /blog/20111206-cryptome/pict2_hu_fa7be9e6b13279ad.webp 400w,
               /blog/20111206-cryptome/pict2_hu_ffe53498285b0892.webp 760w,
               /blog/20111206-cryptome/pict2_hu_d35443e36bbacca9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111206-cryptome/pict2_hu_fa7be9e6b13279ad.webp&#34;
               width=&#34;760&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Georgia Tech DARPA ADAMS leaders
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-georgia-tech-darpa-adams-team&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Georgia Tech DARPA ADAMS team&#34; srcset=&#34;
               /blog/20111206-cryptome/pict0_hu_69588e36a6ab72e5.webp 400w,
               /blog/20111206-cryptome/pict0_hu_9e1d6374fcb28de1.webp 760w,
               /blog/20111206-cryptome/pict0_hu_df29848289a24a03.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111206-cryptome/pict0_hu_69588e36a6ab72e5.webp&#34;
               width=&#34;760&#34;
               height=&#34;522&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Georgia Tech DARPA ADAMS team
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-data-collection-environment&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Data collection environment&#34; srcset=&#34;
               /blog/20111206-cryptome/pict1_hu_98517191e43f4bda.webp 400w,
               /blog/20111206-cryptome/pict1_hu_cfcfbb8a1cede9e8.webp 760w,
               /blog/20111206-cryptome/pict1_hu_a9a636ae3a96937d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111206-cryptome/pict1_hu_98517191e43f4bda.webp&#34;
               width=&#34;760&#34;
               height=&#34;509&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Data collection environment
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When a soldier in good mental health becomes homicidal or a government employee abuses access privileges to share classified information, we often wonder why no one saw it coming. When looking through the evidence after the fact, a trail often exists that, had it been noticed, could have possibly provided enough time to intervene and prevent an incident.&lt;/p&gt;
&lt;p&gt;With support from the Defense Advanced Research Projects Agency (DARPA) and the Army Research Office, researchers at the Georgia Institute of Technology are collaborating with scientists from four other organizations to develop new approaches for identifying these &amp;ldquo;insider threats&amp;rdquo; before an incident occurs. The two-year, $9 million project will create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data &amp;ndash; including email, text messages and file transfers &amp;ndash; for unusual activity.&lt;/p&gt;
&lt;p&gt;The project is being led by Science Applications International Corporation (SAIC) and also includes researchers from Oregon State University, the University of Massachusetts and Carnegie Mellon University.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Analysts looking at the electronically recorded activities of employees within government or defense contracting organizations for anomalous behaviors may now have the bandwidth to investigate five anomalies per day out of thousands of possibilities. Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated,&amp;rdquo; said project co-principal investigator &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor with a joint appointment in the Georgia Tech School of Computational Science and Engineering and the Georgia Tech Research Institute (GTRI).&lt;/p&gt;
&lt;p&gt;Under the contract, the researchers will leverage a combination of massively scalable graph-processing algorithms, advanced statistical anomaly detection methods and knowledge-based relational machine learning algorithms to create a prototype Anomaly Detection at Multiple Scales (ADAMS) system. The system could revolutionize the capabilities of counter-intelligence community operators to identify and prioritize potential malicious insider threats against a background of everyday cyber network activity.&lt;/p&gt;
&lt;p&gt;The research team will have access to massive data sets collected from operational environments where individuals have explicitly agreed to be monitored. The information will include electronically recorded activities, such as computer logins, emails, instant messages and file transfers. The ADAMS system will be capable of pulling these terabytes of data together and using novel algorithms to quickly analyze the information to discover anomalies.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We need to bring together high-performance computing, algorithms and systems on an unprecedented scale because we&amp;rsquo;re collecting a massive amount of information in real time for a long period of time,&amp;rdquo; explained Bader. &amp;ldquo;We are further challenged because we are capturing the information at different rates &amp;ndash; keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition to Bader, other Georgia Tech researchers supporting key components of this program include School of Interactive Computing professor Irfan Essa, School of Computational Science and Engineering associate professor Edmond Chow, GTRI principal research engineers Lora Weiss and Fred Wright, GTRI senior research scientist Richard Boyd, and GTRI research scientists Joshua L. Davis and Erica Briscoe.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We look forward to working with DARPA and our academic partners to develop a prototype ADAMS system that can detect anomalies in massive data sets that can translate to significant, often critical, actionable insider threat information across a wide variety of application domains,&amp;rdquo; said John Fratamico, SAIC senior vice president and business unit general manager.&lt;/p&gt;
&lt;p&gt;Research News &amp;amp; Publications Office&lt;br&gt;
Georgia Institute of Technology&lt;br&gt;
75 Fifth Street, N.W., Suite 314&lt;br&gt;
Atlanta, Georgia 30308 USA&lt;/p&gt;
&lt;p&gt;Media Relations Contacts: Abby Robinson (abby[at]innovate.gatech.edu; 404-385-3364) or John Toon (jtoon[at]gatech.edu; 404-894-6986)&lt;/p&gt;
&lt;p&gt;Writer: Abby Robinson&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Proactive_Discovery_of_Insider_Threats_Using_Graph_Analysis_and_Learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRODIGAL&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://cryptome.org/isp-spy/ga-tech-spy/ga-tech-spy.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://cryptome.org/isp-spy/ga-tech-spy/ga-tech-spy.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sifting through petabytes: PRODIGAL monitoring for lone wolf insider threats</title>
      <link>http://localhost:1313/blog/20111206-computerworld/</link>
      <pubDate>Tue, 06 Dec 2011 21:47:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111206-computerworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Darlene Storm&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Homeland Security Director Janet Napolitano said the &amp;ldquo;risk of &amp;rsquo;lone wolf&amp;rsquo; attackers, with no ties to known extremist networks or grand conspiracies, is on the rise as the global terrorist threat has shifted,&amp;rdquo; &lt;a href=&#34;http://www.cbsnews.com/8301-201_162-57336080/napolitano-lone-wolf-terror-threat-growing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reported CBSNews&lt;/a&gt;. An alleged example of such a lone wolf terror suspect is U.S. citizen Jose Pimentel, who learned &amp;ldquo;bomb-making on the Internet and considered changing his name to Osama out of loyalty to Osama bin Laden.&amp;rdquo; He was &lt;a href=&#34;http://articles.latimes.com/2011/nov/20/nation/la-na-new-york-terror-20111121&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arrested&lt;/a&gt; on charges of &amp;ldquo;plotting to blow up post offices and police cars and to kill U.S. troops.&amp;rdquo; But the &lt;a href=&#34;http://www.csmonitor.com/USA/Justice/2011/1122/Can-lone-wolf-terror-suspect-claim-entrapment-It-will-be-hard-to-prove&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CSMonitor reported&lt;/a&gt; the FBI decided Pimentel was not a credible threat. It&amp;rsquo;s unlikely Pimentel will be able to claim &amp;ldquo;entrapment&amp;rdquo; since he &amp;ldquo;left muddy footprints on the Internet&amp;rdquo; which proves &amp;ldquo;his intent was to cause harm.&amp;rdquo; The grand jury decision against Pimentel &lt;a href=&#34;http://blogs.wsj.com/metropolis/2011/12/05/pimentels-terror-case-delayed-again/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;was delayed until January&lt;/a&gt;, as others described &amp;ldquo;&lt;a href=&#34;http://www.foreignpolicy.com/articles/2011/11/30/the_idiot_jihadist_next_door&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the Idiot Jihadist Next Door&lt;/a&gt;&amp;rdquo; as just another &amp;ldquo;homegrown U.S. terrorist wannabe.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But lone wolf insider threats certainly do exist and, after they&amp;rsquo;ve gone bad, people wonder why no one saw the lone wolf employee problem coming. It can allegedly take years for an individual to become radicalized and that person may not even realize it&amp;rsquo;s happening. With support from DARPA, &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia Tech announced&lt;/a&gt; that it will help find lone wolf insider threats by developing a system capable of sifting through mindbogglingly massive datasets, terabytes and &lt;a href=&#34;http://en.wikipedia.org/wiki/Petabyte&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;petabytes&lt;/a&gt; (1,000 terabytes). The &amp;ldquo;two-year, $9 million project will create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data &amp;ndash; including email, text messages and file transfers &amp;ndash; for unusual activity.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;DARPA&amp;rsquo;s Anomaly Detection at Multiple Scales (&lt;a href=&#34;http://www.darpa.mil/Our_Work/I2O/Programs/Anomaly_Detection_at_Multiple_Scales_%28ADAMS%29.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ADAMS&lt;/a&gt;) system is being designed to run new large-scale, complex algorithms &amp;ldquo;using graph analysis and machine learning approaches to try to explain unanswered, unexplainable events.&amp;rdquo; It will find nontraditional pattern recognition clues and behavioral changes over long periods of time by hovering up petabytes of recorded logs from simple actions like sending email, accessing file, logging in, plugging in a USB, and other such records. Project co-principal investigator &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor with a joint appointment in the Georgia Tech School of Computational Science and Engineering and the Georgia Tech Research Institute (GTRI), &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;added&lt;/a&gt;, &amp;ldquo;We are further challenged because we are capturing the information at different rates &amp;ndash; keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;While most people have sent a disgruntled email to their boss, and such actions may get logged, these emails are not what leads to the serious insider threats that need to be detected. As it stands now, analysts are drowning in data and it is humanly impossible to investigate the &amp;ldquo;tens of thousands&amp;rdquo; of daily logged anomalous events. &lt;a href=&#34;http://www.foxnews.com/scitech/2011/12/03/could-us-government-start-reading-your-emails/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PRODIGAL&lt;/a&gt; (Proactive Discovery of Insider Threats Using Graph Analysis and Learning), part of ADAMS, will go through about a quarter billion IMs, texts, emails, and other daily digital records to identify the five most serious threats per day so that analysts may have time to scrutinize them.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;http://www.youtube.com/watch?v=oynCgx8XEIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video interview&lt;/a&gt; about using Big Data Analytics to find personnel that might be on the verge of &amp;lsquo;Breaking Bad,&amp;rsquo; Bader gave simplistic examples of what might be analyzed over long periods of time to find the employees who &amp;ldquo;may not realize they are going down the slippery slope.&amp;rdquo; He said, &amp;ldquo;Maybe someone starts shifting their workday by five minutes a day, until they are working at night instead of the day. Maybe someone changes what they eat in the cafeteria. Maybe they come in unexpectedly or unexplained at two in the morning. Those are the sorts of patterns we may start to look for, start to understand.&amp;rdquo; If such anomalous behaviors can be explained &amp;ldquo;then it lowers the profile,&amp;rdquo; yet might help &amp;ldquo;stitch together&amp;rdquo; info to spot lone wolf type of problems.&lt;/p&gt;
&lt;p&gt;While traditional security looks at intruders from the outside, PRODIGAL is looking for threats coming from the inside. When asked if this is a bit like Big Brother spying on Big Brother, Bader said no. In fact, he said, &amp;ldquo;No spying takes place.&amp;rdquo; This will be used only where people have explicitly agreed to be monitored like &amp;ldquo;defense contractors, government agencies, and military on information networks.&amp;rdquo; Everyone who is inside with a security clearance &amp;ldquo;has the keys to the castle and they know the monitoring is taking place.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;There may eventually be commercial applications for such PRODIGAL predictive analytics like when looking for insider trading, but this project &amp;ldquo;only works with big datasets where everyone has agreed to be monitored&amp;rdquo; and it&amp;rsquo;s looking very hard at the lone wolf problem. However &amp;ldquo;the very existence of such a project is sure to unnerve citizens,&amp;rdquo; &lt;a href=&#34;http://www.foxnews.com/scitech/2011/12/03/could-us-government-start-reading-your-emails/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FoxNews stated&lt;/a&gt; before claiming citizens will ask, &amp;ldquo;Is the government reading my emails? Are they already monitoring me?&amp;hellip; PRODIGAL&amp;rsquo;s ability to scan reams of data is clearly the next step in tracking unusual activity, and it&amp;rsquo;s guaranteed to raise a red flag.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Anthony Howard, a &amp;ldquo;security expert who has consulted for the Department of Homeland Security,&amp;rdquo; told FoxNews, &amp;ldquo;Some people say it&amp;rsquo;s one step further toward a police state.&amp;rdquo; He added, &amp;ldquo;Since people tend to be imperfect, the data captured can easily be mishandled. Where does it end?&amp;rdquo; Yet &amp;ldquo;Bader equated the PRODIGAL system to &lt;a href=&#34;http://www.raytheon.com/capabilities/products/cybersecurity/insiderthreat/products/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raytheon SureView&lt;/a&gt;, an internal scanning system that looks for suspicious activity and alerts federal agencies about possible threats. Another system is the Einstein project, which was developed after 9 / 11 and scans government employees for key words and links suspicious activity to National Security Agency databases.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;One woman told &lt;a href=&#34;http://www.foxnews.com/scitech/2011/12/03/could-us-government-start-reading-your-emails/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FoxNews&lt;/a&gt; that she&amp;rsquo;s &amp;ldquo;convinced the federal government is reading her emails. But she&amp;rsquo;s all right with that. I assume it&amp;rsquo;s part of the Patriot Act and I really don&amp;rsquo;t mind. I figure I&amp;rsquo;m probably boring them to death.&amp;rdquo; After seeing that article, &lt;a href=&#34;http://lmliberty.tumblr.com/post/13776001817/big-brother-is-reading-your-email-without-a-warrant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lmliberty added&lt;/a&gt; to this by saying &amp;ldquo;Big Brother is reading your email without a warrant.&amp;rdquo; Furthermore, people like that woman who don&amp;rsquo;t &amp;ldquo;care about the Constitution, particularly the 4th Amendment nor the rule of law,&amp;rdquo; are &amp;ldquo;the problem&amp;rdquo; and &amp;ldquo;poster children for what&amp;rsquo;s wrong with America today.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;While I didn&amp;rsquo;t &amp;ldquo;hear&amp;rdquo; that Big Bro is using &lt;em&gt;this&lt;/em&gt; system to scarf up and read your email without a warrant, here&amp;rsquo;s the &lt;a href=&#34;http://www.youtube.com/watch?v=oynCgx8XEIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interview: DARPA&amp;rsquo;s ADAMS Project Taps Big Data to Find the Breaking Bad&lt;/a&gt; video so can judge for yourself if you believe it&amp;rsquo;s a threat to American citizens.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Darlene Storm (not her real name) is a freelance writer with a background in information technology and information security.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2471635/sifting-through-petabytes--prodigal-monitoring-for-lone-wolf-insider-threats.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2471635/sifting-through-petabytes--prodigal-monitoring-for-lone-wolf-insider-threats.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>System would monitor feds for signs they&#39;re &#39;breaking bad&#39;</title>
      <link>http://localhost:1313/blog/20111206-gcn/</link>
      <pubDate>Tue, 06 Dec 2011 11:04:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111206-gcn/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Kevin McCaney&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Researchers backed by the Defense Advanced Research Projects Agency are developing a system than could scan up to 250 million text messages, e-mail messages and file transfers a day in search of anomalies that could help identify insider threats or employees who might be about to “break bad.”&lt;/p&gt;
&lt;p&gt;The system, dubbed PRODIGAL, for Proactive Discovery of Insider Threats Using Graph Analysis and Learning, will combine graph processing, anomaly detection and relational machine learning on a massive scale to create a prototype Anomaly Detection at Multiple Scales (ADAMS) system, according to a release from the &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia Institute of Technology&lt;/a&gt;, which is working with four other organizations on the project.&lt;/p&gt;
&lt;p&gt;PRODIGAL, which would be used initially to monitor the communications in civilian government and military organizations where employees have agreed to be monitored, is intended to identify “rogue” individuals — such as a potential mass-attack gunman, terrorist or spy — before they act, Georgia Tech said.&lt;/p&gt;
&lt;p&gt;Analysts now have the capacity to investigate about “five anomalies per day out of thousands of possibilities,” said Georgia Tech professor &lt;strong&gt;David Bader&lt;/strong&gt;, co-principal investigator on the project. “Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated.”&lt;/p&gt;
&lt;p&gt;DARPA and the Army Research Office are supporting the two-year, $9 million project. Science Applications International Corp. is leading the project, which also includes researchers from Oregon State University, the University of Massachusetts and Carnegie Mellon University.&lt;/p&gt;
&lt;p&gt;The idea of a system that scans a quarter-billion e-mails and terabytes of information has already touched off concerns that the government will be monitoring everyone’s e-mails, but Bader told &lt;a href=&#34;http://www.foxnews.com/scitech/2011/12/03/could-us-government-start-reading-your-emails/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fox News&lt;/a&gt; that the scans work only on internal systems with the users’ consent, not across the Internet.&lt;/p&gt;
&lt;p&gt;In a video interview at the SC11 high-performance computing conference in Seattle in November, Bader said the system would scan the communications of people with security clearances for signs that they might be “breaking bad.”&lt;/p&gt;
&lt;p&gt;For example, he referred to the &lt;a href=&#34;http://en.wikipedia.org/wiki/Fort_Hood_shooting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fort Hood gunman&lt;/a&gt;, who killed 13 people and wounded 29 others in 2009 and was later linked to al-Qaida, and &lt;a href=&#34;http://en.wikipedia.org/wiki/Bradley_Manning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bradley Manning&lt;/a&gt;, the U.S. soldier accused of giving confidential information to WikiLeaks. In those cases, there were clues that went unheeded. The ADAMS project was to create a system that can put those clues together “before something happens,” Bader said.&lt;/p&gt;
&lt;p&gt;Bader said the system would be used only on sensitive networks whose users are aware that communications are being monitored and have agreed to it as part of their security clearance.&lt;/p&gt;
&lt;p&gt;When completed, ADAMS could represent a breakthrough in “the capabilities of counter-intelligence community operators to identify and prioritize potential malicious insider threats against a background of everyday cyber network activity,” according to Georgia Tech’s announcement.&lt;/p&gt;
&lt;p&gt;It will analyze massive datasets gathered from activities such as network logins, e-mails, instant messages and file transfers looking for patterns that indicate the potential for trouble.&lt;/p&gt;
&lt;p&gt;Because of its scope, the project represents a big-data challenge for the researchers.&lt;/p&gt;
&lt;p&gt;“We need to bring together high-performance computing, algorithms and systems on an unprecedented scale because we&amp;rsquo;re collecting a massive amount of information in real time for a long period of time,” Bader said. “We are further challenged because we are capturing the information at different rates — keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This story has been updated to correct references to the Fort Hood shooting.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About the Author&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Kevin McCaney is a former editor of Defense Systems and GCN.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gcn.com/articles/2011/12/06/darpa-prodigal-email-monitoring-insider-threats.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gcn.com/articles/2011/12/06/darpa-prodigal-email-monitoring-insider-threats.aspx&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DARPA Scanner To Search E-mail For &#34;Insider Threats&#34;</title>
      <link>http://localhost:1313/blog/20111203-thirdage/</link>
      <pubDate>Sat, 03 Dec 2011 11:28:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111203-thirdage/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Claire Shefchik&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20111203-thirdage/email_hu_90589549aabb00c3.webp 400w,
               /blog/20111203-thirdage/email_hu_723b10b93cfe1c33.webp 760w,
               /blog/20111203-thirdage/email_hu_cf6d1b2a17551db1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111203-thirdage/email_hu_90589549aabb00c3.webp&#34;
               width=&#34;220&#34;
               height=&#34;165&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;A new project from DARPA (Defense Advanced Research Projects Agency) and George Institute of Technology will begin scanning e-mails, text messages and file transfers to pick up on what it calls &amp;ldquo;insider threats,&amp;rdquo; the university announced earlier this month.&lt;/p&gt;
&lt;p&gt;Analysts &amp;ldquo;may now have the bandwidth to investigate five anomalies per day out of thousands of possibilities. Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated,&amp;rdquo; project investigator &lt;strong&gt;David A. Bader&lt;/strong&gt; of Georgia Tech School of Computational Science and Engineering and the Georgia Tech Research Institute (GTRI) said in a news release.&lt;/p&gt;
&lt;p&gt;The two-year, $9 million project, led by Science Applications International Corporation (SAIC) in cooperation Oregon State University, the University of Massachusetts and Carnegie Mellon University, will initially use algorithms to detect possible threats from government employees and military personnel, such as sharing classified information or violent tendencies, according to the university.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Every time someone logs on or off, sends an email or text, touches a file or plugs in a USB key, these records are collected within the organization,&amp;rdquo; Bader told Fox News&amp;ndash;about a quarter-billion per day.&lt;/p&gt;
&lt;p&gt;Known as PRODIGAL, the system scans for e-mails to unusual recipients, specific words, and odd file transfers that change over time, though at first only for military volunteers and federal officials. One homeland security expert told Fox it sounds &amp;ldquo;one step further to a police state.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader is quick to reassure the public that PRODIGAL couldn&amp;rsquo;t monitor everyone, since it only works on internal systems, not the entire Internet, and can be useful in preventing  catastrophes, such as a soldier suddenly turning homicidal.&lt;/p&gt;
&lt;p&gt;John Fratamico, SAIC senior vice president and business unit general manager said PRODIGAL &amp;ldquo;can translate to significant, often critical, actionable insider threat information across a wide variety of application domains.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20111207074347/http://www.thirdage.com/news/darpa-scanner-to-search-e-mail-for-insider-threats_12-03-2011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.thirdage.com/news/darpa-scanner-to-search-e-mail-for-insider-threats_12-03-2011&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Could the U.S. Government Start Reading Your Emails?</title>
      <link>http://localhost:1313/blog/20111203-foxnews/</link>
      <pubDate>Sat, 03 Dec 2011 07:34:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111203-foxnews/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Brandon, Fox News&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-nov-10-2011-the-data-collection-environment-at-georgia-tech-where-researchers-use-a-combination-of-massively-scalable-graph-processing-algorithms-and-statistical-analysis-to-scan-through-emails-text-messages-and-ims-for-anomalies-rick-robinson--georgia-tech&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Nov. 10, 2011: The &amp;#34;data collection environment&amp;#34; at Georgia Tech, where researchers use a combination of massively scalable graph-processing algorithms and statistical analysis to scan through emails, text messages and IMs for &amp;#34;anomalies.&amp;#34; (Rick Robinson / Georgia Tech)&#34; srcset=&#34;
               /blog/20111203-foxnews/Georgia-Tech-Email-study_hu_f14d27fbf9582257.webp 400w,
               /blog/20111203-foxnews/Georgia-Tech-Email-study_hu_c5de425e8a774387.webp 760w,
               /blog/20111203-foxnews/Georgia-Tech-Email-study_hu_551b2a1bbe5c1f04.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111203-foxnews/Georgia-Tech-Email-study_hu_f14d27fbf9582257.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Nov. 10, 2011: The &amp;ldquo;data collection environment&amp;rdquo; at Georgia Tech, where researchers use a combination of massively scalable graph-processing algorithms and statistical analysis to scan through emails, text messages and IMs for &amp;ldquo;anomalies.&amp;rdquo; (Rick Robinson / Georgia Tech)
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Cherie Anderson runs a travel company in southern California, and she’s convinced the federal government is reading her emails. But she’s all right with that.&lt;/p&gt;
&lt;p&gt;“I assume it&amp;rsquo;s part of the Patriot Act and I really don&amp;rsquo;t mind,” she says. “I figure I&amp;rsquo;m probably boring them to death.”&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s likely Anderson is not alone in her concerns that the government may be monitoring what Americans say, write, and read. And now there may be even more to worry about: &lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a newly revealed security research project&lt;/a&gt; called PRODIGAL &amp;ndash; the Proactive Discovery of Insider Threats Using Graph Analysis and Learning &amp;ndash; which has been built to scan IMs, texts and emails . . . and can read approximately a quarter billion of them a day.&lt;/p&gt;
&lt;p&gt;“Every time someone logs on or off, sends an email or text, touches a file or plugs in a USB key, these records are collected within the organization,” &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at the Georgia Tech School of Computational Science and Engineering and a principal investigator on the project, told FoxNews.com.&lt;/p&gt;
&lt;p&gt;PRODIGAL scans those records for behavior &amp;ndash; emails to unusual recipients, certain words cropping up, files transferred from unexpected servers &amp;ndash; that changes over time as an employee &amp;ldquo;goes rogue.&amp;rdquo; The system was developed at Georgia Tech in conjunction with the Defense Advanced Research Projects Agency (DARPA), the Army&amp;rsquo;s secretive research arm that works on everything from flying cars to robotic exoskeletons.&lt;/p&gt;
&lt;p&gt;Initially, PRODIGAL will scan only the communications of military volunteers and people who work in federal agencies. But the very existence of such a project is sure to unnerve citizens like Anderson. Is the government reading my emails? Are they already monitoring me?&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Some people say it&amp;rsquo;s one step further toward a police state,&amp;rdquo; said Anthony Howard, a book author and security expert who has consulted for the Department of Homeland Security.&lt;/p&gt;
&lt;p&gt;But Bader and other experts are quick to dismiss the idea that PRODIGAL could be used to monitor everyone in America. The scans work only on internal systems, they say &amp;ndash; not across the entire Internet. And the experts say such a project is long overdue: by monitoring for &amp;ldquo;anomalies&amp;rdquo; and predicting extreme behavior, catastrophes can be prevented, such as a soldier in good mental health becoming homicidal or a government employee sharing key classified information.&lt;/p&gt;
&lt;p&gt;“Today, an analyst may receive tens of thousands of &amp;lsquo;anomalies&amp;rsquo; per day, where an anomaly is an unexplained event,” Bader said.&lt;/p&gt;
&lt;p&gt;The new system is designed to aid analysts in processing those anomalies. And it&amp;rsquo;s not alone.&lt;/p&gt;
&lt;p&gt;Bader equated the PRODIGAL system to &lt;a href=&#34;http://www.raytheon.com/capabilities/products/cybersecurity/insiderthreat/products/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raytheon SureView&lt;/a&gt;, an internal scanning system that looks for suspicious activity and alerts federal agencies about possible threats. Another system is the Einstein project, which was developed after 9 / 11 and scans government employees for key words and links suspicious activity to National Security Agency databases.&lt;/p&gt;
&lt;p&gt;But PRODIGAL scans vastly more data than those systems: as much as a terabyte or more per day, what Georgia Tech described as &amp;ldquo;massive data sets.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;PRODIGAL is part of an existing DARPA security project called Anomaly Detection at Multiple Scales (ADAMS), which was announced earlier this year. Details about how ADAMS works are not widely known; Georgia Tech&amp;rsquo;s recent announcement is one of the first reports to explain how these detection engines work.&lt;/p&gt;
&lt;p&gt;According to Bader, PRODIGAL uses complex &amp;ldquo;graph-processing&amp;rdquo; algorithms to analyze threats and piece together a jigsaw puzzle of communications. The system then ranks the unusual activity before feeding the most suspicious threats to agents.&lt;/p&gt;
&lt;p&gt;Cyber-security expert Joseph Steinberg, CEO of &lt;a href=&#34;http://www.greenarmor.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Green Armor Solutions&lt;/a&gt;, said ADAMS is unique in that it scans through a massive stream of data. He says the new project, which will take about two years to develop and will cost $9 million, will be more effective at analyzing threats and determining if they are valid.&lt;/p&gt;
&lt;p&gt;But the issue is not the scanning technology itself; it’s how the information is interpreted &amp;ndash; and whether it ultimately helps at all, Howard told FoxNews.com.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Since there is no real data publicly available to substantiate that any of this technology is preventing terrorist attacks or strengthening our borders from within, [we can&amp;rsquo;t] really say definitively that this technology is doing any good,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;The challenge, he said, is that criminals and terrorists often use multiple channels of communication, some encrypted &amp;ndash; and know how to avoid existing detection systems.&lt;/p&gt;
&lt;p&gt;Nevertheless, PRODIGAL’s ability to scan reams of data is clearly the next step in tracking unusual activity, and it’s guaranteed to raise a red flag for Anderson and others.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Since people tend to be imperfect, the data captured can easily be mishandled. Where does it end?&amp;rdquo; Howard said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.foxnews.com/tech/could-the-u-s-government-start-reading-your-emails&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.foxnews.com/tech/could-the-u-s-government-start-reading-your-emails&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nbcnews.com/id/45535122/ns/technology_and_science-science/t/could-us-government-start-reading-your-emails/#.XV0sXehKjIU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.nbcnews.com/id/45535122/ns/technology_and_science-science/t/could-us-government-start-reading-your-emails/#.XV0sXehKjIU&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPCwire People to Watch 2012</title>
      <link>http://localhost:1313/blog/20111201-hpcwire-people/</link>
      <pubDate>Thu, 01 Dec 2011 08:09:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111201-hpcwire-people/</guid>
      <description>&lt;h2 id=&#34;david-bader-full-professor-in-the-school-of-computational-science-and-engineering-college-of-computing-at-georgia-institute-of-technology-and-executive-director-for-high-performance-computing&#34;&gt;David Bader, Full Professor in the School of Computational Science and Engineering, College of Computing, at Georgia Institute of Technology, and Executive Director for High Performance Computing&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader&lt;/strong&gt; is a lead scientist in the DARPA Ubiquitous High Performance Computing (UHPC) program. He received his Ph.D. in 1996 from The University of Maryland, and his research is supported through highly competitive research awards, primarily from NSF, NIH, DARPA, and DOE.&lt;/p&gt;
&lt;p&gt;Bader is also one of the original co-founders of the Graph 500, who, since its first list was unveiled at SC ’10, is establishing itself as the preeminent benchmark for data-intensive applications, which are becoming extremely important for business analytics, finance, and any other field where vast data sets need to be evaluated. This is the first serious approach to complement the TOP500 with data intensive applications, and we are looking forward to watching it continue to grow and evolve over the upcoming year.&lt;/p&gt;
&lt;p&gt;He serves on the Research Advisory Council for Internet2, the Steering Committees of the IPDPS and HiPC conferences, the General Chair of IPDPS 2010 and Chair of SIAM PP12. He is also an associate editor for several high impact publications including the Journal of Parallel and Distributed Computing (JPDC), ACM Journal of Experimental Algorithmics (JEA), IEEE DSOnline, Parallel Computing, and Journal of Computational Science, and has been an associate editor for the IEEE Transactions on Parallel and Distributed Systems (TPDS).&lt;/p&gt;
&lt;p&gt;Bader’s interests center upon the intersection of high-performance computing and real-world applications, including computational biology, genomics and massive-scale data analytics. He has co-chaired a series of meetings, the IEEE International Workshop on High-Performance Computational Biology (HiCOMB), co-organized the NSF Workshop on Petascale Computing in the Biological Sciences, written several book chapters, and co-edited special issues of the Journal of Parallel and Distributed Computing (JPDC) and IEEE TPDS on high-performance computational biology.&lt;/p&gt;
&lt;p&gt;He is also a leading expert on multicore, manycore, and multithreaded computing for data-intensive applications, such as those in massive-scale graph analytics. As if that was not impressive enough, Bader has also co-authored over 100 articles in peer-reviewed journals and conferences, and his main areas of research are in parallel algorithms, combinatorial optimization, massive-scale social networks, and computational biology and genomics.&lt;/p&gt;
&lt;p&gt;Prof. Bader is a Fellow of the IEEE and AAAS, a National Science Foundation CAREER Award recipient, and has received numerous industrial awards from IBM, Cray, NVIDIA, Intel, Oracle/Sun Microsystems, and Microsoft Research. He served as a member of the IBM PERCS team for the DARPA High Productivity Computing Systems program, was a distinguished speaker in the IEEE Computer Society Distinguished Visitors Program, and has also served as Director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interview: DARPA&#39;s ADAMS Project Taps Big Data to Find the Breaking Bad</title>
      <link>http://localhost:1313/blog/20111129-sc11/</link>
      <pubDate>Tue, 29 Nov 2011 13:42:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111129-sc11/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Rich Brueckner, insideHPC&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In this &lt;a href=&#34;https://www.youtube.com/watch?v=oynCgx8XEIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;, Professor &lt;strong&gt;David Bader&lt;/strong&gt; from Georgia Tech discusses his participation in the DARPA ADAMS project. The Anomaly Detection at Multiple Scales (ADAMS) program uses Big Data Analytics to look for cleared personnel that might be on the verge of &amp;ldquo;Breaking Bad&amp;rdquo; and becoming internal security threats.&lt;/p&gt;
&lt;p&gt;Recorded at SC11. Learn more &lt;a href=&#34;https://web.archive.org/web/20111217120847/http://www.darpa.mil/Our_Work/I2O/Programs/Anomaly_Detection_at_Multiple_Scales_%28ADAMS%29.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Data To Pinpoint Rogue Insiders</title>
      <link>http://localhost:1313/blog/20111129-darkreading/</link>
      <pubDate>Tue, 29 Nov 2011 07:21:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111129-darkreading/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Robert Lemos, Contributing Editor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The hunt for technology to identify malicious insiders took off in 2011 with the research arm of the Pentagon, the Defense Advanced Research Projects Agency (DARPA), offering up millions of dollars in grants to fund research.&lt;/p&gt;
&lt;p&gt;Earlier this month, for example, the Georgia Institute of Technology announced that DARPA had funded a collective effort by the school and four other organizations to create a suite of algorithms that turn disparate data feeds into real-time alerts of anomalous activity. The project, funded to the tune of $9 million over two years, will detect multiple types of insider threats and is funded &lt;a href=&#34;http://www.darkreading.com/security/news/227701306/darpa-project-to-tackle-inside-security-threats.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;under DARPA&amp;rsquo;s Anomaly Detection at Multiple Scales (ADAMS) project&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are going after the hardest insider threats &amp;ndash; an organization where everyone is trusted and perhaps cleared,&amp;rdquo; says &lt;strong&gt;David Bader&lt;/strong&gt;, a professor at Georgia Tech&amp;rsquo;s College of Computing and co-principal investigator on the project. &amp;ldquo;We are looking at an area where people might, over years, head down the slippery slope.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The team is led by Science Applications International Corp. (SAIC) and &amp;ndash; in addition to Georgia Tech &amp;ndash; includes researchers from Oregon University, the University of Massachusetts, and Carnegie Mellon University.&lt;/p&gt;
&lt;p&gt;DARPA has yet to announce grants for its second project, the Cyber Insider Threat (CINDER) program.&lt;/p&gt;
&lt;p&gt;Analyzing big data for business intelligence has become a key tool for companies to compete. Now universities and security firms are modifying the techniques to analyze data from multiple sources and identify anomalous behavior of individuals. SAIC, Georgia Tech, and the rest of its team will use a variety of big-data techniques and machine learning to create a prototype system. The technology will go beyond typical network anomaly detection and include non-network data.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We need specialized technology to do this, but whether or not we need government deployed software versus COTS is an open question,&amp;rdquo; says Eddie Schwartz, chief security officer for security giant RSA. Among the other groups that won a grant for DARPA&amp;rsquo;s ADAMS project is Raytheon, for a commercial system that is already used. The system, SureView, monitors and captures end-user activity that is anomalous and could be malicious.&lt;/p&gt;
&lt;p&gt;Such specialized systems &lt;a href=&#34;http://www.darkreading.com/insider-threat/167801100/security/news/227900189/pentagon-s-insider-threat-push-offers-lessons-for-enterprises.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;are necessary to detect insider threats&lt;/a&gt;, Schwartz says. In the past, counterintelligence techniques called for identifying anomalies in the behavior of individuals in sensitive positions.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If you think classically, how would you find indicators in people&amp;rsquo;s activities? Large deposits in their bank accounts, changes in the way they drive to work,&amp;rdquo; he says. &amp;ldquo;Those types of human intelligence observations that we saw classically during the Cold War, we are just extending to the dark side of cyberspace.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A key benefit of anomaly detection is that previously unknown threats can be detected. But a drawback is that the systems typically create a large number of alerts, many of them false, says Malek Bin Salem, a cybersecurity research scientist at Accenture Technology Labs. Columbia University has created a system that seeds directories with decoy documents that appear interesting but will alert the owners if opened or copied. Salem, a former Columbia researcher that worked on the project, found that 20 decoy files can typically catch an intruder on a personal file system containing 100,000 documents.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The advantage of any honeypot technology [like the Columbia system] is that the signal is going to be stronger &amp;ndash; if you see an alert, it is very likely going to be a real attack,&amp;rdquo; she says.&lt;/p&gt;
&lt;p&gt;A startup company, Allure Security Technology, has licensed the technology from Columbia and is also funded under the ADAMS program.&lt;/p&gt;
&lt;p&gt;With such systems, however, comes the danger that an employee who changes his behavior for benign reasons or that inadvertently accesses a decoy file could find himself under suspicion. The Pentagon is most interested in detecting malicious insiders before they commit their ultimate rogue act, suggesting the precrime predictions of the movie &lt;em&gt;Minority Report&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Yet Georgia Tech&amp;rsquo;s Bader says the goal is not prediction, but accurate documenting an insider&amp;rsquo;s behavior.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are not looking for pre-crime,&amp;rdquo; he says. &amp;ldquo;We are looking for a chain of evidence. This is a new type of security that we will see in the future.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://w1.darkreading.com/vulnerabilities---threats/analyzing-data-to-pinpoint-rogue-insiders/d/d-id/1136738&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://w1.darkreading.com/vulnerabilities---threats/analyzing-data-to-pinpoint-rogue-insiders/d/d-id/1136738&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Announcing Our Newest Rock Star of HPC: David Bader</title>
      <link>http://localhost:1313/blog/20111128-rockstarhpc/</link>
      <pubDate>Mon, 28 Nov 2011 07:19:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111128-rockstarhpc/</guid>
      <description>&lt;p&gt;At insideHPC, we are pleased to announce that &lt;strong&gt;David Bader&lt;/strong&gt; is our latest Rock Star of HPC.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;&lt;em&gt;While HPC tends to focus on compute-intensive problems, Big Data challenges require novel architectures for data-intensive computing. My group has been the first to parallelize and implement large-scale graph theoretic algorithms, which are quite a challenge because of the irregular memory accesses, little computation to overlap with these memory references, and fine grain synchronization. In the past several years, our research has enabled social scientists to analyze some of the largest social networks, detecting communities, finding the proverbial “needle in the haystack”, and “connecting the dots” by identifying central actors hidden in these networks. As you know, data and social media are now torrential streams of information that may provide valuable information to make decisions related to business intelligence, market analysis, and social trends.&lt;/em&gt;&amp;rdquo; - &lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;RockStarDavidBader.pdf&#34;&gt;Read the Full Story&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2011/11/announcing-our-newest-rock-star-of-hpc-david-bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2011/11/announcing-our-newest-rock-star-of-hpc-david-bader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Threats Before They Happen</title>
      <link>http://localhost:1313/blog/20111125-industrialsafety/</link>
      <pubDate>Fri, 25 Nov 2011 22:37:05 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111125-industrialsafety/</guid>
      <description>&lt;p&gt;“We never saw it coming.” Those words often fall from the lips of victims or witnesses to incidents when a person takes the law into their own hands.&lt;/p&gt;
&lt;p&gt;However, when doing the forensics in the case, a trail often exists that if anyone noticed, it could have possibly provided enough time to intervene and prevent an incident.&lt;/p&gt;
&lt;p&gt;Potentially solving those mysteries could be a bit closer because researchers at the Georgia Institute of Technology are collaborating with scientists from four other organizations to develop new approaches for identifying these “insider threats” before an incident occurs.&lt;/p&gt;
&lt;p&gt;The two-year, $9 million project will create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data — including email, text messages and file transfers — for unusual activity. The Defense Advanced Research Projects Agency (DARPA) and the Army Research Office are also helping in this effort.&lt;/p&gt;
&lt;p&gt;Science Applications International Corporation (SAIC) is leading the project and also includes researchers from Oregon State University, the University of Massachusetts and Carnegie Mellon University.&lt;/p&gt;
&lt;p&gt;“Analysts looking at the electronically recorded activities of employees within government or defense contracting organizations for anomalous behaviors may now have the bandwidth to investigate five anomalies per day out of thousands of possibilities,” said project co-principal investigator &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor with a joint appointment in the Georgia Tech School of Computational Science and Engineering and the Georgia Tech Research Institute (GTRI). “Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated.”&lt;/p&gt;
&lt;p&gt;Under the contract, the researchers will leverage a combination of massively scalable graph-processing algorithms, advanced statistical anomaly detection methods and knowledge-based relational machine learning algorithms to create a prototype Anomaly Detection at Multiple Scales (ADAMS) system. The system could revolutionize the capabilities of counter-intelligence community operators to identify and prioritize potential malicious insider threats against a background of everyday cyber network activity.&lt;/p&gt;
&lt;p&gt;The research team will have access to massive data sets collected from operational environments where individuals have explicitly agreed to undergo monitoring. The information will include electronically recorded activities, such as computer logins, emails, instant messages and file transfers. The ADAMS system will be capable of pulling these terabytes of data together and using novel algorithms to quickly analyze the information to discover anomalies.&lt;/p&gt;
&lt;p&gt;“We need to bring together high-performance computing, algorithms and systems on an unprecedented scale because we’re collecting a massive amount of information in real time for a long period of time,” Bader said. “We are further challenged because we are capturing the information at different rates — keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://isssource.com/detecting-threats-before-they-happen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://isssource.com/detecting-threats-before-they-happen/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Helps to Develop System That Will Detect Insider Threats From Massive Data Sets</title>
      <link>http://localhost:1313/blog/20111116-insiderthreat/</link>
      <pubDate>Wed, 16 Nov 2011 07:26:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111116-insiderthreat/</guid>
      <description>&lt;p&gt;Researchers at the U.S. Defense Advanced Research Projects Agency (DARPA), the Army Research Office, and Georgia Tech are developing new approaches for identifying insider threats before a data breach occurs.&lt;/p&gt;
&lt;p&gt;The researchers are developing a suite of algorithms that can detect different types of insider threats by analyzing massive amounts of data for unusual activity. &amp;ldquo;Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated,&amp;rdquo; says Georgia Tech professor &lt;strong&gt;David A. Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The researchers also are developing a prototype Anomaly Detection at Multiple Scales (ADAMS) system, which they say could revolutionize the capabilities of counterintelligence professions by prioritizing potential malicious insider threats against a background of normal network activity.&lt;/p&gt;
&lt;p&gt;The ADAMS system will analyze terabytes of data using new algorithms to quickly find anomalies. &amp;ldquo;We need to bring together high-performance computing, algorithms, and systems on an unprecedented scale because we&amp;rsquo;re collecting a massive amount of information in real time for a long period of time,&amp;rdquo; Bader says.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gatech.edu/newsroom/release.html?nid=72599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Georgia Tech News&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cacm.acm.org/news/141805-georgia-tech-helps-to-develop-system-that-will-detect-insider-threats-from-massive-data-sets/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cacm.acm.org/news/141805-georgia-tech-helps-to-develop-system-that-will-detect-insider-threats-from-massive-data-sets/fulltext&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader interviewed by NPR on DARPA ADAMS project</title>
      <link>http://localhost:1313/blog/20111111-wabe/</link>
      <pubDate>Fri, 11 Nov 2011 16:13:33 -0500</pubDate>
      <guid>http://localhost:1313/blog/20111111-wabe/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20111111-wabe/20111111-WABE_hu_275bb39f0e08fdbc.webp 400w,
               /blog/20111111-wabe/20111111-WABE_hu_b7f10366714755dd.webp 760w,
               /blog/20111111-wabe/20111111-WABE_hu_ab22a1a46965df6d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111111-wabe/20111111-WABE_hu_275bb39f0e08fdbc.webp&#34;
               width=&#34;669&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Georgia Tech Helps to Develop System That Will Detect Insider Threats from Massive Data Sets</title>
      <link>http://localhost:1313/blog/20111110-darpa-adams/</link>
      <pubDate>Thu, 10 Nov 2011 07:37:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111110-darpa-adams/</guid>
      <description>

















&lt;figure  id=&#34;figure-researchers-from-georgia-tech-are-helping-to-create-a-suite-of-algorithms-that-can-detect-multiple-types-of-insider-threats-by-analyzing-massive-amounts-of-data-for-unusual-activity-the-georgia-tech-research-team-includes-left-right-erica-briscoe-andy-register-david-a-bader-richard-boyd-anita-zakrzewska-oded-green-lora-weiss-edmond-chow-and-oguz-kaya-credit-gary-meek&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Researchers from Georgia Tech are helping to create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data for unusual activity. The Georgia Tech research team includes (left-right) Erica Briscoe, Andy Register, David A. Bader, Richard Boyd, Anita Zakrzewska, Oded Green, Lora Weiss, Edmond Chow and Oguz Kaya. *(Credit: Gary Meek)*&#34; srcset=&#34;
               /blog/20111110-darpa-adams/37929_web_hu_db039968bae139b5.webp 400w,
               /blog/20111110-darpa-adams/37929_web_hu_cbb400041e475f13.webp 760w,
               /blog/20111110-darpa-adams/37929_web_hu_2f0adb31a8744e20.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111110-darpa-adams/37929_web_hu_db039968bae139b5.webp&#34;
               width=&#34;400&#34;
               height=&#34;274&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Researchers from Georgia Tech are helping to create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data for unusual activity. The Georgia Tech research team includes (left-right) Erica Briscoe, Andy Register, David A. Bader, Richard Boyd, Anita Zakrzewska, Oded Green, Lora Weiss, Edmond Chow and Oguz Kaya. &lt;em&gt;(Credit: Gary Meek)&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;When a soldier in good mental health becomes homicidal or a government employee abuses access privileges to share classified information, we often wonder why no one saw it coming. When looking through the evidence after the fact, a trail often exists that, had it been noticed, could have possibly provided enough time to intervene and prevent an incident.&lt;/p&gt;
&lt;p&gt;With support from the Defense Advanced Research Projects Agency (DARPA) and the Army Research Office, researchers at the Georgia Institute of Technology are collaborating with scientists from four other organizations to develop new approaches for identifying these &amp;ldquo;insider threats&amp;rdquo; before an incident occurs. The two-year, $9 million project will create a suite of algorithms that can detect multiple types of insider threats by analyzing massive amounts of data &amp;ndash; including email, text messages and file transfers &amp;ndash; for unusual activity.&lt;/p&gt;
&lt;p&gt;The project is being led by Science Applications International Corporation (SAIC) and also includes researchers from Oregon State University, the University of Massachusetts and Carnegie Mellon University.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Analysts looking at the electronically recorded activities of employees within government or defense contracting organizations for anomalous behaviors may now have the bandwidth to investigate five anomalies per day out of thousands of possibilities. Our goal is to develop a system that will provide analysts for the first time a very short, ranked list of unexplained events that should be further investigated,&amp;rdquo; said project co-principal investigator &lt;em&gt;David A. Bader&lt;/em&gt;, a professor with a joint appointment in the Georgia Tech School of Computational Science and Engineering and the Georgia Tech Research Institute (GTRI).&lt;/p&gt;
&lt;p&gt;Under the contract, the researchers will leverage a combination of massively scalable graph-processing algorithms, advanced statistical anomaly detection methods and knowledge-based relational machine learning algorithms to create a prototype &lt;a href=&#34;https://www.darpa.mil/program/anomaly-detection-at-multiple-scales&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anomaly Detection at Multiple Scales (ADAMS)&lt;/a&gt; system. The system could revolutionize the capabilities of counter-intelligence community operators to identify and prioritize potential malicious insider threats against a background of everyday cyber network activity.&lt;/p&gt;
&lt;p&gt;The research team will have access to massive data sets collected from operational environments where individuals have explicitly agreed to be monitored. The information will include electronically recorded activities, such as computer logins, emails, instant messages and file transfers. The ADAMS system will be capable of pulling these terabytes of data together and using novel algorithms to quickly analyze the information to discover anomalies.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We need to bring together high-performance computing, algorithms and systems on an unprecedented scale because we&amp;rsquo;re collecting a massive amount of information in real time for a long period of time,&amp;rdquo; explained Bader. &amp;ldquo;We are further challenged because we are capturing the information at different rates &amp;ndash; keystroke information is collected at very rapid rates and other information, such as file transfers, is collected at slower rates.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition to Bader, other Georgia Tech researchers supporting key components of this program include School of Interactive Computing professor Irfan Essa, School of Computational Science and Engineering associate professor Edmond Chow, GTRI principal research engineers Lora Weiss and Fred Wright, GTRI senior research scientist Richard Boyd, and GTRI research scientists Joshua L. Davis and Erica Briscoe.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We look forward to working with DARPA and our academic partners to develop a prototype ADAMS system that can detect anomalies in massive data sets that can translate to significant, often critical, actionable insider threat information across a wide variety of application domains,&amp;rdquo; said John Fratamico, SAIC senior vice president and business unit general manager.&lt;/p&gt;
&lt;p&gt;Research News &amp;amp; Publications Office&lt;br&gt;
Georgia Institute of Technology&lt;br&gt;
75 Fifth Street, N.W., Suite 314&lt;br&gt;
Atlanta, Georgia 30308 USA&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Media Relations Contacts: Abby Robinson (&lt;a href=&#34;mailto:abby@innovate.gatech.edu&#34;&gt;abby@innovate.gatech.edu&lt;/a&gt;; 404-385-3364) or John Toon (&lt;a href=&#34;mailto:jtoon@gatech.edu&#34;&gt;jtoon@gatech.edu&lt;/a&gt;; 404-894-6986)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Writer: Abby Robinson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;20111110-GeorgiaTech-ResearchNews.pdf&#34;&gt;Archive copy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2011/11/10/georgia-tech-helps-develop-system-will-detect-insider-threats-massive-data-sets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2011/11/10/georgia-tech-helps-develop-system-will-detect-insider-threats-massive-data-sets&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Elected AAAS Fellow</title>
      <link>http://localhost:1313/blog/20111104-aaas/</link>
      <pubDate>Fri, 04 Nov 2011 21:01:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111104-aaas/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20111104-aaas/AAAS-Fellow_hu_305e924ea4a85c8a.webp 400w,
               /blog/20111104-aaas/AAAS-Fellow_hu_5c4d589f1e2c0dfe.webp 760w,
               /blog/20111104-aaas/AAAS-Fellow_hu_b8f50bbf5e6e1bcb.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20111104-aaas/AAAS-Fellow_hu_305e924ea4a85c8a.webp&#34;
               width=&#34;588&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;This is to certify that &lt;strong&gt;David A. Bader&lt;/strong&gt; was elected a Fellow of the American Association for the Advancement of Science this fourth day of November 2011 in testimony whereof the President and the Chief Executive Officer have hereunto set their hands and the seal of the Association.&lt;/p&gt;
&lt;p&gt;Susam Amara, President&lt;br&gt;
Sudip Parikh, Chief Executive Offiver&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CODONiS Teaming with ISB to Meet Big Challenge: Managing Terabytes of Personal Biomedical Data</title>
      <link>http://localhost:1313/blog/20111103-codonis/</link>
      <pubDate>Thu, 03 Nov 2011 07:26:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20111103-codonis/</guid>
      <description>&lt;p&gt;Seattle-based &lt;a href=&#34;http://codonis.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CODONiS&lt;/a&gt;, a provider of advanced computing platforms for life sciences and healthcare, has teamed up with scientists from the world-renowned Institute for Systems Biology, a nonprofit research organization in Seattle, to advance biomedical computing for future personalized healthcare. The results from this ground-breaking collaboration will be discussed at the “Personalized Healthcare Challenges for High Performance Computing” panel discussion being held at the SC11 Conference in Seattle on November 15, 2011.&lt;/p&gt;
&lt;p&gt;Dr. David Galas of the &lt;a href=&#34;http://systemsbiology.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute for Systems Biology&lt;/a&gt;, will lead the session highlighting current efforts and opportunities in data-intensive, next-generation medicine. One of the most complex issues in high-performance computing and personalized healthcare is the challenge of aggregating and analyzing massive amounts of data generated from numerous and diverse sources.&lt;/p&gt;
&lt;p&gt;“The impact of high performance computing on biomedical data analysis and the consequent understanding of health and disease is growing explosively,” Galas said. “The future impact on healthcare will be immense.”&lt;/p&gt;
&lt;p&gt;This 90-minute session will offer a number of perspectives on current and future clinical applications of life sciences computing, with audience participation encouraged. In addition to Dr. Galas, other panelists will include: James Harding, president of CODONiS; Dr. Eric Stahlberg, of the National Cancer Institute; &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, of the Georgia Institute of Technology; and Dr. Seunghwa Kang, of Pacific Northwest National Laboratories.&lt;/p&gt;
&lt;p&gt;Session Details:
“&lt;em&gt;Personalized Healthcare Challenges for High Performance Computing&lt;/em&gt;” &lt;br&gt;
SC11 International Conference for High Performance Computing, Networking, Storage and Analysis &lt;br&gt;
November 15, 2011 &lt;br&gt;
5:30 p.m. &lt;br&gt;
Room TCC 204 &lt;br&gt;
Washington State Convention Center &lt;br&gt;
&lt;a href=&#34;http://sc11.supercomputing.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sc11.supercomputing.org&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;contacts&#34;&gt;Contacts&lt;/h3&gt;
&lt;p&gt;CODONiS, Inc &lt;br&gt;
Director of Computing Services &lt;br&gt;
David Pellerin, 206-281-8700 &lt;br&gt;
&lt;a href=&#34;mailto:dpellerin@codonis.com&#34;&gt;dpellerin@codonis.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.businesswire.com/news/home/20111103006921/en/CODONiS-Teaming-ISB-Meet-Big-Challenge-Managing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.businesswire.com/news/home/20111103006921/en/CODONiS-Teaming-ISB-Meet-Big-Challenge-Managing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computing innovations to imitate, not replace, human brain</title>
      <link>http://localhost:1313/blog/20110907-humanbrain/</link>
      <pubDate>Wed, 07 Sep 2011 07:37:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110907-humanbrain/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Ellyne Phneah&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Innovations around computing are increasingly designed to imitate the human brain but the day artificial intelligence becomes &amp;ldquo;smarter&amp;rdquo; than human intelligence is a &amp;ldquo;long time&amp;rdquo; away, said experts.&lt;/p&gt;
&lt;p&gt;Leong Tze Yun, associate professor at National University of Singapore&amp;rsquo;s (NUS) School of Computing, said alternate computational models have been &amp;ldquo;actively pursued&amp;rdquo; in recent years. Emulating the human brain&amp;ndash;which she described as the &amp;ldquo;ultimate and best computer&amp;rdquo;&amp;ndash;is one of the main directions researchers have been moving toward.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Some of the main functions that &amp;lsquo;brain chips&amp;rsquo; and other brain-like computing models aim to emulate include parallel processing, integration of processing and memory units and adaptive configuration of emerging requirements and functions,&amp;rdquo; the academic elaborated in her e-mail.&lt;/p&gt;
&lt;p&gt;This is because current computational models with separate memory and processor units for crunching information has limitations in areas of computing capacity, efficiency and the range of tasks that can be solved, Leong added.&lt;/p&gt;
&lt;p&gt;Her comments come after an August report by CBS News revealed that Big Blue has built two prototype chips that process data similar to how the human brain digests information in that they are able to adapt to information that it was not programmed for.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, professor high performance computing executive director at the Georgia Institute of Technology&amp;rsquo;s College of Computing in the United States, chimed in, saying that by using multicore Intel processors and massively parallel graphic processors by Nvidia, a &amp;ldquo;thinking&amp;rdquo; computer becomes &amp;ldquo;closer to reality&amp;rdquo;. This way of harnessing the power of multiple processors has helped solve some of the most challenging problems in science and engineering, he pointed out in an e-mail.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The ability to compute thousands of trillions of operations per second or sift through billions of pieces of information per second allows them to solve problems once thought intractable,&amp;rdquo; said Bader, pointing to how IBM&amp;rsquo;s Watson supercomputer defeated human contestants during the U.S. game show Jeopardy earlier this year as an example.&lt;/p&gt;
&lt;h4 id=&#34;correctly-harnessing-compute-power&#34;&gt;Correctly harnessing compute power&lt;/h4&gt;
&lt;p&gt;IBM Singapore&amp;rsquo;s CTO Foong Sew Bun clarified that its Watson project was not to build a human brain as it is &amp;ldquo;much too complex and marvelous to ever replicate&amp;rdquo; in silicon form.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are simply drawing inspiration from the brain&amp;rsquo;s ability for massively parallel processing to build a more efficient computer,&amp;rdquo; he stated.&lt;/p&gt;
&lt;p&gt;Leong, too, agreed that it is unlikely these supercomputers will ever replace the human brain while Leong was slightly more hopeful, saying that it would be a &amp;ldquo;long, long time&amp;rdquo; before a computer will behave like those seen in the movie &amp;ldquo;Terminator&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;She elaborated that computers will not be &amp;ldquo;smarter&amp;rdquo; than the human brain if the criterion is defined in the broad, human sense of being able to adapt, learn and improve themselves, the people and things around and for the &amp;ldquo;goodness of mankind&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;That said, if it is defined as &amp;ldquo;the narrow sense of task efficiency and effectiveness&amp;rdquo;, there are already processes that computers or other technologies can do better than human beings, Leong qualified.&lt;/p&gt;
&lt;p&gt;The NUS professor also predicted that there will come a time when computers are &amp;ldquo;assimilated into our daily lives intelligently and seamlessly&amp;rdquo;. In such a reality, man and machine will co-exist, interact, complement, support and improve each other in various physical, social and economic activities, she pointed out.&lt;/p&gt;
&lt;p&gt;In terms of optimizing the use of computing technologies to better our lives, Leong said companies should set right objectives and values in the development and use of these innovations.&lt;/p&gt;
&lt;p&gt;She also stressed that people do not &amp;ldquo;stop thinking&amp;rdquo; when working with smart computers. &amp;ldquo;As developers and users, we decide the types of &amp;lsquo;smart computers&amp;rsquo; we want to build and cannot always rely entirely on the results and suggestions produced by computers,&amp;rdquo; the professor cautioned.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zdnet.com/article/computing-innovations-to-imitate-not-replace-human-brain/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.zdnet.com/article/computing-innovations-to-imitate-not-replace-human-brain/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSI to Host Two-Day Workshop on Data-Intensive Computing, Graphs, and Combinatorics in Bio-Informatics, Finance, Linguistics, and National Security</title>
      <link>http://localhost:1313/blog/20110720-cuny/</link>
      <pubDate>Wed, 20 Jul 2011 19:18:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110720-cuny/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Carlo Alaimo&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;

















&lt;figure  id=&#34;figure-an-artists-rendering-of-a-proposed-interdisciplinary-high-performance-computation-center-part-of-the-csi-master-plan&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An artist’s rendering of a proposed Interdisciplinary High Performance Computation Center, part of the CSI Master Plan.&#34; srcset=&#34;
               /blog/20110720-cuny/hpcc_rendering_072011-300x225_hu_6c17f59ce6bd3d0f.webp 400w,
               /blog/20110720-cuny/hpcc_rendering_072011-300x225_hu_4bace545a8a9fad.webp 760w,
               /blog/20110720-cuny/hpcc_rendering_072011-300x225_hu_d30702a51660b525.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20110720-cuny/hpcc_rendering_072011-300x225_hu_6c17f59ce6bd3d0f.webp&#34;
               width=&#34;300&#34;
               height=&#34;225&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An artist’s rendering of a proposed Interdisciplinary High Performance Computation Center, part of the CSI Master Plan.
    &lt;/figcaption&gt;&lt;/figure&gt;

CSI will host a two-day workshop on Data-Intensive Computing on July 26-27, 2011 from 8:15am to 4:45pm in the Lecture Hall of the Center for the Arts.&lt;/p&gt;
&lt;p&gt;The event is designed to cover topics in data-intensive computing including graph theoric and combinatoric approaches in bio-informatics, financial data analytics, linguistics, and national security. The two-day workshop will focus on new techniques and ways of implementing large-scale data analysis on high-performance computing systems.&lt;/p&gt;
&lt;p&gt;There is an attendance fee of $85 per person ($50 for students) and breakfast, lunch, and refreshments will be provided each day. Advanced registration is required.&lt;/p&gt;
&lt;p&gt;Noted representatives from academe, government research labs, and industry will participate in the workshop, including the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, Georgia Institute of Technology, will present on, “Opportunities and Challenges in Massive Data-Intensive Computing.”  Bader will discuss the opportunities and challenges in massive data-intensive computing for applications in computational biology, genomics, and security. Bader is a lead scientist in the DARPA Ubiquitous High Performance Computing (UHPC) program and is a leading expert on multicore, many core, and multithreaded computing for data-intensive applications such as those in massive-scale graph analytics.&lt;/p&gt;
&lt;p&gt;Other notable speakers from academe are Drs. Alok Choudhary, John G. Searle Professor and Chair of the Electrical Engineering and Computer Science, Kellogg School of Management and Director for the Center for Ultra-Scale Computing and Information Security at Northwestern University; Bud Mishra, Courant Institute of Mathematical Science; Alex Pothen, Professor of Computer Science and Director of the Institute for Combinatorial Scientific Computing and Petascale Simulations at Purdue University; and, Jiahong Yuan, University of Pennsylvania.&lt;/p&gt;
&lt;p&gt;John Avery, SunGard Global Services, will discuss, “Data in Financial Services—Use It or Lose It!” He will address the “BigData” movement and how time, cost, and information asymmetry are critical competitive advantages in financial services. Avery, who has been quoted in numerous industry publications including Wall Street &amp;amp; Technology and Hedge Funds Review, has been building, managing, integrating, and testing software applications on Wall Street for the past 13 years.&lt;/p&gt;
&lt;p&gt;Edward A. Epstein, IBM T.J. Watson Research Labs, was part of the Watson/Deep QA project, the computer that won the game show, Jeopardy! He was responsible for scaling out Watson’s computation over thousands of compute cores in order to achieve the speed needed to make the computer competitive for the live game.&lt;/p&gt;
&lt;p&gt;Dr. Reinhardt of Microsoft will be heading the workshop, “Enabling Non-Graph-Expert Use of Very-Large-Scale Graph Analysis.” This talk will describe an interface for graph analysis known as the Knowledge Discovery Toolbox (KDT).  Dr. Reinhardt will explain KDT’s content and demonstrate its use to solve sample graph-analytic workflows and sketch future directions.&lt;/p&gt;
&lt;p&gt;These and others will take part in this workshop to address the challenges that confront researchers in how to discern meaningful information and relationships from the plethora of data now at the disposal of researchers.&lt;/p&gt;
&lt;p&gt;For more information, send email to:  &lt;a href=&#34;mailto:hpcworkshops@csi.cuny.edu&#34;&gt;hpcworkshops@csi.cuny.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20130603012705/http://csitoday.com/2011/07/csi-to-host-two-day-workshop-on-data-intensive-computing-graphs-and-combinatorics-in-bio-informatics-finance-linguistics-and-national-security/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://csitoday.com/2011/07/csi-to-host-two-day-workshop-on-data-intensive-computing-graphs-and-combinatorics-in-bio-informatics-finance-linguistics-and-national-security/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Department Status for CSE at Georgia Tech</title>
      <link>http://localhost:1313/blog/20110621-siam-cse/</link>
      <pubDate>Tue, 21 Jun 2011 11:20:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110621-siam-cse/</guid>
      <description>&lt;p&gt;The Georgia Institute of Technology inaugurated its School of Computational Science and Engineering at a convocation in February. The result of an initiative that began in 2005, the new school is located within the College of Computing, where it joins the School of Computer Science and the School of Interactive Computing. Georgia Tech is one of the rare institutions to have a separate department of CSE with its own faculty.&lt;/p&gt;
&lt;p&gt;By creating the School of Computational Science and Engineering, says founding chair Richard Fujimoto, Georgia Tech has taken the position that CSE is a discipline in its own right, one that deserves an academic home for its faculty and students. &amp;ldquo;CSE derives much of its richness from collaborations with disciplines such as mathematics, science, and engineering,&amp;rdquo; he continues. &amp;ldquo;This is a key principle underlying the formation of the School of CSE.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Founding members, in addition to Fujimoto, are Haesun Park and &lt;strong&gt;David Bader&lt;/strong&gt;, and seven new members have joined the school since 2005. Through several joint and adjunct appointments, the school has established links with other disciplines and, in some cases, other institutions, including Oak Ridge National Laboratory.&lt;/p&gt;
&lt;p&gt;Defining CSE as &amp;ldquo;a discipline devoted to the systematic study, creation and application of computer-based models to understand, analyze, and/or design natural and engineered systems,&amp;rdquo; Georgia Tech elected to focus on the following subfields: high-performance computing, computational data analytics, modeling and simulation, numerical computing, and computational algorithms. These areas form the core of the CSE graduate program curriculum, which currently enrolls approximately 60 students; they are also central to an undergraduate minor program currently under development. Both the undergraduate and the graduate programs emphasize the development of expertise in a science or engineering domain in conjunction with knowledge of computation. A noteworthy aspect of Georgia Tech&amp;rsquo;s new school is its bringing together of modeling and simulation with data analytics, sometimes called the fourth paradigm of science, in one department.&lt;/p&gt;
&lt;p&gt;The February convocation, held on the Georgia Tech campus in Atlanta, included a panel discussion of future directions for the discipline of CSE and a keynote address, &amp;ldquo;The Exascale: Why and How,&amp;rdquo; by David Keyes (King Abdullah University of Science and Technology). &amp;ldquo;In many fields the cost of experiments has risen past the ability of any one nation to perform, while the cost of simulation continues to plummet,&amp;rdquo; Keyes remarked. &amp;ldquo;These trends favor an increasing number of computational scientists to work among seven million publishing scientific researchers in the world today. Georgia Tech continues to take leadership steps in this campaign.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Haesun Park, a member of the panel, discussed the need to establish a body of knowledge to help curriculum development efforts. &amp;ldquo;Deriving knowledge and insight from massive, complex data,&amp;rdquo; she said, &amp;ldquo;has come to the forefront as a key technological challenge that must be addressed in order to enable discovery and innovation in many science and engineering fields.&amp;rdquo; Discussing challenges in high-performance computing, Jeffrey Vetter, who holds a joint appointment at Georgia Tech&amp;rsquo;s School of CSE and Oak Ridge National Laboratory, identified two of the most important: designing new architectures that can improve energy efficiency by two orders of magnitude, and designing innovative programming systems that allow users to build applications that use billions of threads, and that are efficient and reliable. Jeffrey Skolnick, a professor of biology at Georgia Tech, considered multi-scale computational methods and critical challenges in the modeling of cells on a molecular level. Such simulations are of interest not only because they can provide fundamental understanding, he said; they could also be used to model the transformation of normal cells to cancerous ones. Georgia Tech chemistry professor David Sherrill highlighted such challenges as scalability and fault tolerance, noting that 50% of the quantum chemistry codes he executes fail on large supercomputers today because of node failures. He cited the need for extensive domain knowledge in order to determine acceptable approximations for exascale codes in quantum chemistry. Both Sherrill and Skolnick hold joint appointments with the new school.&lt;/p&gt;
&lt;p&gt;Additional information can be found at &lt;a href=&#34;http://www.cse.gatech.edu/about&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cse.gatech.edu/about&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://archive.siam.org/news/news.php?id=1891&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://archive.siam.org/news/news.php?id=1891&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computational Nanogami: RNA Sequence Search Stretches Across Georgia Tech Boundaries</title>
      <link>http://localhost:1313/blog/20110606-rna/</link>
      <pubDate>Mon, 06 Jun 2011 15:48:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110606-rna/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20110606-rna/image_hu_c150c3dcdccd29da.webp 400w,
               /blog/20110606-rna/image_hu_f465257843e55dac.webp 760w,
               /blog/20110606-rna/image_hu_8e38d077a6173503.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20110606-rna/image_hu_c150c3dcdccd29da.webp&#34;
               width=&#34;760&#34;
               height=&#34;313&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Back in 2009, Josh Anderson didn’t know much about biology. But he knew that a summer undergraduate research assistantship working on something called “RNA folding” had to be better than the job his mother had lined up for him. Prashant Gaurav recalls that in 2009 he was thinking about applying to graduate school in computer science—not about the base pairings of a nasty RNA virus like Hepatitis C.&lt;/p&gt;
&lt;p&gt;Yet in 2011 both Anderson (double-major in discrete mathematics and computer science) and Gaurav (M.S. student in computational science and engineering) are hard at work on problems in computational molecular biology as part of an interdisciplinary research team led by Christine Heitsch, associate professor in the School of Mathematics, with professors Steve Harvey from Biology and &lt;strong&gt;David Bader&lt;/strong&gt; from CSE. The computational side of this math/bio/CSE research collaboration is rounded out by Emily Rogers, a CSE Ph.D. student, and CS major Andrew Ash. (Ash and Anderson both graduated in May 2011.) Together with other students and postdocs from math and biology, the team is trying to reverse engineer in silico some of the mysteries of “molecular origami,” as Heitsch describes RNA folding.&lt;/p&gt;
&lt;p&gt;“This is one of Nature’s versions of origami,” explains Heitsch. “Instead of a flat piece of paper which is folded along creases to form a complicated 3D shape, an RNA sequence ‘folds’ along complementary Watson-Crick pairings between nucleotides in different parts of the sequence. Think of the base pairing of an RNA sequence like a string made up of bits of Velcro that can and do stick together.” Those Watson-Crick base pairings are also what make up the structure of RNA’s more famous relative, the double-stranded DNA helix.&lt;/p&gt;
&lt;p&gt;Knowing the structure of DNA is fundamental to understanding how genetic information is stored and passed on. But the genomes of many, if not most, viruses are actually RNA, not DNA, and there is experimental evidence suggesting that the folded structure an RNA viral genome affects the functioning of the virus. So advancing understanding of the base pairings in RNA viruses should help with treating the diseases they cause.&lt;/p&gt;
&lt;p&gt;Since RNA base pairings are both strong and stable, predicting those pairings can be approached as a thermodynamic optimization problem. This and related approaches work reasonably well for short sequences, but definitely do not scale up to lengthy RNA viral genomes. Hence, the ongoing scientific challenge is to develop quantitative methods to find the proverbial needle (that is, native base pairings) in the very large haystack of possible structures for an RNA virus like Hepatitis C.
Enter Anderson, Ash, Gaurav and Rogers, who are helping to design new computational approaches to understanding where base pairs can and do occur, and how they influence RNA structure.&lt;/p&gt;
&lt;p&gt;“We’re essentially building complicated machines that people are running right on top of us,” says Anderson, who started on the project in 2009 as a summer Research Experience for Undergraduates (REU) student. “I’ve been a bit of a jack of all trades: I’ve done code optimization, algorithm design. What I brought into the project was a lack of fear of coding.”&lt;/p&gt;
&lt;p&gt;“I didn’t have any biology background,” echoes Gaurav, “but I’ve been able to achieve some significant speed improvements by bringing more efficiency to the program’s parallelism. We leveraged multicore computing techniques that enable the biologists to analyze and process large RNA sequences—like the HIV virus—much faster.”&lt;/p&gt;
&lt;h2 id=&#34;what-a-lovely-mess&#34;&gt;What a lovely mess&lt;/h2&gt;
&lt;p&gt;This research team traces its origins to a conference that Heitsch attended shortly before starting her faculty position at Georgia Tech. As it happened, a lunch conversation about potential computational collaborators pointed her to Bader as a computer scientist with interests in biological applications. Another serendipitous connection was made shortly after Heitsch arrived on campus and met Harvey, a biophysicist whose research interests include 3D structures of RNA viral genomes.&lt;/p&gt;
&lt;p&gt;“Biology is inherently complicated,” Heitsch says. “Mathematical theories are very powerful tools for gaining new insights into why the current folding methods don’t provide sufficiently accurate results. But translating these theoretical insights into practical applications requires dealing with all the biological complexity that you can. For me, that means collaborating with a computational scientist who is excited by the algorithmic challenges this presents.”&lt;/p&gt;
&lt;p&gt;Bader says he’s thrilled to participate in a research project integrating mathematics and computer science to tackle a fundamental problem in biology. Too often in computer science, he says, researchers fall into the groove of only doing research that promises tidy outcomes. It’s part of the computer scientist’s neural wiring, he says, to seek out problems that tease with objectively “correct” solutions.&lt;/p&gt;
&lt;p&gt;“While we are focusing on computing innovations to solve real-world problems like those in biology and physics, it’s a relatively new phenomenon among computer scientists,” Bader says. “It’s just not a traditional mindset among computing experts, but it’s in our genes at Georgia Tech.”&lt;/p&gt;
&lt;p&gt;“In too many places, mathematics is completely separate from computer science, and both are separate from biology,” Heitsch says. “One of the things that’s so great about Georgia Tech is the overlap between disciplines, so projects like this can happen.”&lt;/p&gt;
&lt;h2 id=&#34;using-cs-and-math-to-solve-biological-problems&#34;&gt;Using CS and math to solve biological problems&lt;/h2&gt;
&lt;p&gt;The junior members of this interdisciplinary team each contribute a unique mix of mathematical, computational and biological expertise. With undergraduate degrees in computer science and in biology from the University of California at Berkeley, and a master’s in bioinformatics from Georgia Tech, Rogers is a great fit for this project—and the CSE Ph.D. program. “I really like this area, where computation and biology meet,” says Rogers. “I enjoy my CS classes but I’m not so interested in pure CS research; I like to use CS tools to solve biological problems.”&lt;/p&gt;
&lt;p&gt;“It’s a very creative project,” says Zsuzsanna Sukosd, a visiting graduate student on leave from her nanoscience Ph.D. program at Denmark’s Aarhus University. “When people do bioinformatics work, they’re often running programs that other people wrote. In this project we’re working together to write the program ourselves.”&lt;/p&gt;
&lt;p&gt;Postdoctoral researcher Shel Swenson acts as the project “lynchpin,” Heitsch says, with a background that positions her at the center of the three disciplines involved: Swenson earned her Ph.D. in mathematics from the University of Texas at Austin, advised by computer scientist Tandy Warnow, one of the world’s leading computational phylogeneticists.&lt;/p&gt;
&lt;p&gt;Swenson also has a successful track record of incorporating students into research projects, including three summers of leading REU programs at Austin. “The best part about mentoring undergraduates is when they start to actively question information they’re given, instead of just passively absorbing it,” Swenson says. This summer, she and another mathematics postdoctoral researcher, Svetlana Poznanovik, are introducing new REU students to the challenges of RNA folding.&lt;/p&gt;
&lt;p&gt;For the undergraduates, the project has been a full-on immersion into the world of academic research. Mathematics undergrad Dragos Ilas, who was mentored by Poznanovik and also graduated this spring, says that he “could see the energy with which people worked on their projects, and the sense of productivity and achievement becomes contagious after a while.”&lt;/p&gt;
&lt;p&gt;“It’s hard but rewarding,” Anderson says. “Half the time it feels like you are banging your head against the wall, but every now and then you get something to work, and it’s such a rush.”&lt;/p&gt;
&lt;p&gt;“I feel like our contributions were highly valued, and we were able to work on critical portions of the project,” says Ash. “Computing accelerates the pace of all other disciplines, and it’s in demand everywhere.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;College of Computing, Georgia Institute of Technology&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cc.gatech.edu/computational-nanogami-rna-sequence-search-stretches-across-georgia-tech-boundaries&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cc.gatech.edu/computational-nanogami-rna-sequence-search-stretches-across-georgia-tech-boundaries&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Genomics on the Petascale</title>
      <link>http://localhost:1313/blog/20110523-genomicspetascale/</link>
      <pubDate>Mon, 23 May 2011 08:10:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110523-genomicspetascale/</guid>
      <description>&lt;p&gt;As a new era centered on human health dawns around the world, the life sciences — accelerated by the tremendous bloom of genomics — are poised to open new horizons. And, in this relatively new, interdisciplinary branch of biology called genomics, computing plays a critical role, particularly in such areas as genome assembly, analysis and interpretation.&lt;/p&gt;
&lt;p&gt;Genome sequences are available for many organisms, but making biological sense of the genomic data requires high performance computing methods and an evolutionary perspective, whether one is trying to understand how genes of new functions arise, why genes are organized as they are in chromosomes, or why these arrangements are subject to change.&lt;/p&gt;
&lt;p&gt;In the last five years, next-generation sequencing technologies, pioneered by 454 Life Sciences, Solexa/Illumina, SOLiD/ABI and other industry beacons, have produced an acceleration of sequencing speed three orders faster than the best Sanger machines renowned for their use in the Human Genome Project. Today, with the genomes of more than 5,000 biological species sequenced or well in progress, the complexity of algorithms applied for genome decoding, gene identification, comparison and inference of biological function and evolution grows fast.&lt;/p&gt;
&lt;p&gt;In any normally equipped bioinformatics lab, one will find computers whose running times of bioinformatics applications are simply too slow for effective workflow. Hence, genomics becomes a research area where high performance computing applications are perhaps in the highest demand.&lt;/p&gt;
&lt;h3 id=&#34;decoding-fragaria-vesca&#34;&gt;Decoding Fragaria vesca&lt;/h3&gt;
&lt;p&gt;Consider as one example the success of an international consortium working on the genome of the woodland strawberry, an endeavor that depended heavily on the accuracy and speed of two new gene-prediction algorithms developed in the lab of Mark Borodovsky at Georgia Tech, one of the authors of this article.&lt;/p&gt;
&lt;p&gt;The first algorithm, GeneMark.hmm-ES, takes as input an anonymous genomic sequence, then works in iterations to converge on algorithm parameters that deliver probabilistic (Hidden Markov) models of genomic regions that carry and do not carry the genetic code. Afterward, the thus-defined models are used in Viterbi algorithms that parse the genomic sequence into coding and non-coding regions. The second algorithm, GeneMark-ES+,“pre-processes” the transcriptome sequence data that provide additional evidence for protein coding genes and integrates this evidence as a restriction into the Viterbi optimal parse.&lt;/p&gt;
&lt;p&gt;Working on a strawberry genome of some 240 million nucleotides, and 11 million long sequences from the expressed transcripts, it would take two weeks for each full run of these algorithms on conventional computers. And, to repeat these runs on several genome assemblies and transcriptome versions would add another four or five months to the project.&lt;/p&gt;
&lt;p&gt;Fortunately, the Borodovsky lab had access to a teraflop cluster sponsored by NIH with the ability to perform parallel computations on 300 processors. This computer reduced the time of a full run of the two algorithms to just four hours, and the results of all necessary runs were obtained within a total of 48 hours. The algorithms confidently — and quickly! — identified 34,809 strawberry genes encoding strawberry proteins.&lt;/p&gt;
&lt;p&gt;A paper on the strawberry genome was published in the journal Nature Genetics in December 2010. From a genetic standpoint, the woodland strawberry, formally known as Fragaria vesca, is similar to the cultivated strawberry, but less complex, making it easier to study, breed and improve.&lt;/p&gt;
&lt;h3 id=&#34;supercomputing-toward-phylogenies&#34;&gt;Supercomputing toward phylogenies&lt;/h3&gt;
&lt;p&gt;Still, decoding of larger and more complex genomes, such as the 20 GB genome of Norway spruce (otherwise known as a Christmas tree), is posing even larger challenges. Borodovsky and his fellow Georgia Tech researchers are hoping to address these open problems by developing new algorithms that will delineate large swaths of surely non-coding regions, prior to pinpointing exact locations of protein-coding genes.&lt;/p&gt;
&lt;p&gt;Given the ready availability of sequenced genomes, a natural question arises: How are these sequences related in evolutionary terms? And how can we use those similarities to reconstruct their phylogenetic history? A phylogeny is an evolutionary tree reconstructed from its leaves (each of which represents a different species) by comparing DNA sequences or gene data with a plausible model of evolution. Because phylogenies are crucial to answering many fundamental open questions in biomolecular evolution, biologists have a strong interest in algorithms that enable resolution of such ancient relationships.&lt;/p&gt;
&lt;p&gt;A considerable body of applied research depends on these algorithms as well. Pharmaceutical companies use phylogenetic analysis in drug discovery — for instance, in discovering biochemical pathways unique to target organisms. Health organizations study the phylogenies of such organisms as HIV to understand their epidemiologies and to aid in predicting the course of disease over time within an individual. Government laboratories work to develop improved strains of basic foodstuffs, such as rice, wheat and potatoes, using an understanding of the phylogenetic distribution of variation in wild populations. Finally, the reconstruction of large phylogenies could yield fundamental new insights into the process of evolution itself.&lt;/p&gt;
&lt;p&gt;Technological advances in high-throughput DNA sequencing have opened up the possibility of determining how living things are related by analyzing the ways in which their genes have been rearranged on chromosomes. However, inferring such evolutionary relationships from rearrangement events is computationally intensive even on the most advanced computing systems available today.&lt;/p&gt;
&lt;h3 id=&#34;grappa-ling-with-genomes&#34;&gt;GRAPPA-ling with genomes&lt;/h3&gt;
&lt;p&gt;Georgia Tech computational scientist &lt;strong&gt;David A. Bader&lt;/strong&gt;, one of the authors of this article, leads a research group that has developed the software package GRAPPA for reconstructing evolutionary histories using gene-order data. GRAPPA was first implemented to use breakpoint distance between genomes. Bader’s research took a new approach: using the inversion distance between genomes, which is a more biologically accurate measure. His team designed new techniques for reconstructing large-scale phylogenies with hundreds to thousands of taxa. For example, on a dataset of a dozen bellflower genomes, the latest version of GRAPPA determined the flowers’ evolutionary relatedness a billion times faster than the original implementation, which did not utilize parallel processing or optimization.&lt;/p&gt;
&lt;p&gt;Research recently funded by the American Recovery and Reinvestment Act of 2009 aims to develop computational tools that will utilize next-generation petascale computers to understand genomic evolution. The four-year $1 million project, supported by the National Science Foundation’s PetaApps program, was awarded to a team of universities that includes Georgia Tech, University of South Carolina (USC) and Pennsylvania State University.&lt;/p&gt;
&lt;p&gt;Even on today’s fastest parallel computers, it could take centuries to analyze genome rearrangements for large, complex organisms. That is why the research team — led by Bader at Georgia Tech and also including Jijun Tang, an associate professor of computer science and engineering at USC; and Stephen Schaeffer, an associate professor of biology at Penn State — is focusing on future generations of petascale machines, which will be able to process more than a thousand trillion (or 1015) calculations per second. Today, most personal computers can only process a few hundred thousand calculations per second.&lt;/p&gt;
&lt;p&gt;The researchers are developing a new high performance software package called COGNAC (Comparing Orders of Genes using Novel Algorithms and high-performance Computers) and will test the performance of their new algorithms by analyzing a collection of fruit fly (Drosophilia) genomes. The analysis of genome rearrangements in Drosophila will provide a relatively simple system to understand the mechanisms that underlie gene order diversity, which can later be extended to more complex mammalian genomes, such as primates.&lt;/p&gt;
&lt;p&gt;These new algorithms, the researchers believe, will make genome rearrangement analysis more reliable and efficient, while potentially revealing new evolutionary patterns. Armed with this understanding, scientists in fields from drug discovery to food production and dozens of others will be better equipped to address many of the grand challenges facing humanity today.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Strawberry genome paper: &lt;a href=&#34;http://opal.biology.gatech.edu/GeneMark/PAPERS/Strawberry_NG.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://opal.biology.gatech.edu/GeneMark/PAPERS/Strawberry_NG.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GeneMark-ES: &lt;a href=&#34;http://nar.oxfordjournals.org/content/33/20/6494.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://nar.oxfordjournals.org/content/33/20/6494.full&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GRAPPA: “Industrial Applications of High-Performance Computing for Phylogeny Reconstruction,” D.A. Bader, B. M.E. Moret, and L. Vawter, SPIE ITCom: Commercial Applications for High-Performance Computing (SPIE ITCom2001), Denver, CO, SPIE Vol. 4528, pp. 159-168, August 21-22, 2001.&lt;/li&gt;
&lt;li&gt;COGNAC: “Rec-DCM-Eigen: Reconstructing a Less Parsimoniousbut More Accurate Tree in Shorter Time,” Seunghwa Kang, Jijun Tang, Stephen W. Schaeffer, and David A. Bader. Technical Report, Georgia Institute of Technology, February 2011.&lt;/li&gt;
&lt;li&gt;COGNAC: “On the Design of Architecture-Aware Algorithms for Emerging Applications,” Seunghwa Kang, Ph.D. Dissertation, School of Electrical and Computer Engineering, Georgia Institute of Technology, January 2011.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Mark Borodovsky is Regents’ Professor in the Wallace H. Coulter Department of Biomedical Engineering and the School of Computational Science &amp;amp; Engineering, as well as Director of the Center of Bioinformatics and Computational Genomics at the Georgia Institute of Technology. David A. Bader is Professor in the School of Computational Science &amp;amp; Engineering and Executive Director of High Performance Computing at the Georgia Institute of Technology. They may be reached at &lt;a href=&#34;mailto:editor@ScientificComputing.com&#34;&gt;editor@ScientificComputing.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;by Mark Borodovsky and &lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.rdmag.com/article/2011/05/genomics-petascale&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.rdmag.com/article/2011/05/genomics-petascale&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader recognized as 25th Year Planning Chair of IPDPS</title>
      <link>http://localhost:1313/blog/20110516-ipdps/</link>
      <pubDate>Mon, 16 May 2011 16:58:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110516-ipdps/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20110516-ipdps/award_hu_db2124d1df866d1d.webp 400w,
               /blog/20110516-ipdps/award_hu_6dc0ce8f585b693b.webp 760w,
               /blog/20110516-ipdps/award_hu_25c2381fa7fe0b49.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20110516-ipdps/award_hu_db2124d1df866d1d.webp&#34;
               width=&#34;650&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;25th IEEE International Parallel &amp;amp; Distributed Processing Symposium&lt;/p&gt;
&lt;p&gt;May 16-20, 2011&lt;/p&gt;
&lt;h2 id=&#34;david-bader&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&#34;25th-year-planning-chair&#34;&gt;25th Year Planning Chair&lt;/h3&gt;
&lt;p&gt;With Grateful Appreication for Efforts on Behalf of IPDPS 2011&lt;/p&gt;
&lt;p&gt;Sponsored by IEEE Computer Society Technical Committee on Parallel Processing&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives IEEE Golden Core Member Award</title>
      <link>http://localhost:1313/blog/20110204-gatech/</link>
      <pubDate>Fri, 04 Feb 2011 19:30:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110204-gatech/</guid>
      <description>&lt;p&gt;&lt;em&gt;Atlanta, GA | Posted: February 4, 2011&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Professor &lt;strong&gt;David Bader&lt;/strong&gt; (CompSci &amp;amp; Eng) received the IEEE Computer Society’s Golden Core Member award during the IEEE Computer Society’s Executive Board Meeting in Long Beach, Calif., Feb. 2.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://www.computer.org/portal/web/awards/goldencore&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Golden Core Member award&lt;/a&gt; was established in 1996 and recognizes individuals for long-standing service to the society. Each year the awards committee selects recipients from a pool of qualified candidates and permanently includes their names in the Golden Core Member master list.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cc.gatech.edu/~bader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bader&lt;/a&gt;, who serves as executive director of high performance computing in the School of Computational Science &amp;amp; Engineering, received the award for his extensive service to the IEEE Computer Society.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://hg.gatech.edu/node/64095&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hg.gatech.edu/node/64095&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Computer Society Golden Core Award</title>
      <link>http://localhost:1313/blog/20110202-ieee-cs-golden-core/</link>
      <pubDate>Wed, 02 Feb 2011 12:06:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20110202-ieee-cs-golden-core/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20110202-ieee-cs-golden-core/award_hu_c37330c7e8262faa.webp 400w,
               /blog/20110202-ieee-cs-golden-core/award_hu_c15a79ef8403df5d.webp 760w,
               /blog/20110202-ieee-cs-golden-core/award_hu_af02e43583a8e8da.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20110202-ieee-cs-golden-core/award_hu_c37330c7e8262faa.webp&#34;
               width=&#34;581&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;In 2010, &lt;strong&gt;David A. Bader&lt;/strong&gt; received the &lt;a href=&#34;https://www.computer.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Computer Society&lt;/a&gt; &lt;a href=&#34;https://www.computer.org/volunteering/awards/golden-core&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Golden Core Award&lt;/a&gt;. A plaque is awarded for long-standing member or staff service to the society. This program was initiated in 1996 with a charter membership of 450. Each year the Awards Committee will select additional recipients from a continuing pool of qualified candidates and permanently include their names in the Golden Core Member master list.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computer.org/volunteering/awards/golden-core&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computer.org/volunteering/awards/golden-core&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tougher rating system evaluates nine supercomputer capabilities</title>
      <link>http://localhost:1313/blog/20101118-graph500/</link>
      <pubDate>Thu, 18 Nov 2010 21:53:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101118-graph500/</guid>
      <description>&lt;p&gt;Nine supercomputers have been tested, validated and ranked by the new &amp;ldquo;Graph500&amp;rdquo; challenge, first introduced this week by an international team led by Sandia National Laboratories. The list of submitters and the order of their finish was released Nov. 17 at the supercomputing conference SC10 meeting in New Orleans.&lt;/p&gt;
&lt;p&gt;The machines were tested for their ability to solve complex problems involving random-appearing graphs, rather than for their speed in solving a basic numerical problem, today&amp;rsquo;s popular method for ranking top systems.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Some, whose supercomputers placed very highly on simpler tests like the Linpack, also tested them on the Graph500, but decided not to submit results because their machines would shine much less brightly,&amp;rdquo; said Sandia computer scientist Richard Murphy, a lead researcher in creating and maintaining the test.&lt;/p&gt;
&lt;p&gt;Murphy developed the Graph500 Challenge with researchers at the &lt;strong&gt;Georgia Institute of Technology&lt;/strong&gt;, University of Illinois at Urbana-Champaign, and Indiana University, among others.&lt;/p&gt;
&lt;p&gt;Complex problems involving huge numbers of related data points are found in the medical world where large numbers of medical entries must be correlated, in the analysis of social networks with their huge numbers of electronically related participants, or in international security where huge numbers of containers on ships roaming the world and their ports of call must be tracked.&lt;/p&gt;
&lt;p&gt;Such problems are solved by creating large, complex graphs with vertices that represent the data points &amp;ndash; say, people on Facebook &amp;ndash; and edges that represent relations between the data points &amp;ndash; say, friends on Facebook. These problems stress the ability of computing systems to store and communicate large amounts of data in irregular, fast-changing communication patterns, rather than the ability to perform many arithmetic operations. The Graph500 benchmarks are indicative of the ability of supercomputers to handle such complex problems.&lt;/p&gt;
&lt;p&gt;The Graph500 benchmarks present problems in different input sizes. These are described as huge, large, medium, small, mini and toy. No machine proved capable of handling problems in the huge or large categories.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I consider that a success,&amp;rdquo; Murphy said. &amp;ldquo;We posed a really hard challenge and I think people are going to have to work to do &amp;rsquo;large&amp;rsquo; or &amp;lsquo;huge&amp;rsquo; problems in the available time.&amp;rdquo; More memory, he said, might help.&lt;/p&gt;
&lt;p&gt;The abbreviations &amp;ldquo;GE/s&amp;rdquo; and &amp;ldquo;ME/s&amp;rdquo; represented in the table below describe each machine&amp;rsquo;s capabilities in giga-edges per second and mega-edges per second &amp;ndash; a billion and million edges traversed in a second, respectively.&lt;/p&gt;
&lt;p&gt;Competitors were ranked first by the size of the problem attempted and then by edges per second.&lt;/p&gt;
&lt;p&gt;The rankings were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rank #1 - Intrepid, Argonne National Laboratory - 6.6 GE/s on scale 36 (Medium)&lt;/li&gt;
&lt;li&gt;Rank #2 - Franklin, National Energy Research Scientific Computing Center - 5.22 GE/s on Scale 32 (Small)&lt;/li&gt;
&lt;li&gt;Rank #3 - cougarxmt, Pacific Northwest National Laboratory - 1.22 GE/s on Scale 29 (Mini)&lt;/li&gt;
&lt;li&gt;Rank #4 - graphstorm, Sandia National Laboratories&amp;rsquo; - 1.17 GE/s on Scale 29 (Mini)&lt;/li&gt;
&lt;li&gt;Rank #5 - Endeavor, Intel Corporation, 533 ME/s on Scale 29 (Mini)&lt;/li&gt;
&lt;li&gt;Rank #6 - Erdos, Oak Ridge National Laboratory - 50.5 ME/s on Scale 29 (Mini)&lt;/li&gt;
&lt;li&gt;Rank #7 - Red Sky, Sandia National Laboratories - 477.5 ME/s on Scale 28 (Toy++)&lt;/li&gt;
&lt;li&gt;Rank #8 - Jaguar, Oak Ridge National Laboratory - 800 ME/s on Scale 27 (Toy+)&lt;/li&gt;
&lt;li&gt;Rank #9 - Endeavor, Intel Corporation - 615.8 ME/s on Scale 26 (Toy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more detailed description of the Graph500 benchmark and additional results are available at graph500.org. Any organization may participate in the ratings. The next Graph500 Challenge list is expected to be released at the International Supercomputing Conference 2011 next summer, and then at SC 2011 again in the fall.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sandia National Laboratories is a multiprogram laboratory operated and managed by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy&amp;rsquo;s National Nuclear Security Administration. With main facilities in Albuquerque, N.M., and Livermore, Calif., Sandia has major R&amp;amp;D responsibilities in national security, energy and environmental technologies, and economic competitiveness.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://eurekalert.org/pub_releases/2010-11/dnl-trs111810.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://eurekalert.org/pub_releases/2010-11/dnl-trs111810.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Keeps Sights Set On Exascale at SC10</title>
      <link>http://localhost:1313/blog/20101111-sc10/</link>
      <pubDate>Thu, 11 Nov 2010 06:47:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101111-sc10/</guid>
      <description>&lt;p&gt;The road to exascale computing is a long one, but the Georgia Institute of Technology, a leader in high-performance computing (HPC) research and education, continues to win new awards and attract new talent to drive technology innovation. From algorithms to architectures and applications, Georgia Tech&amp;rsquo;s researchers are collaborating with top companies, national labs and defense organizations to solve the complex challenges of tomorrow&amp;rsquo;s supercomputing systems. Ongoing projects and new research initiatives spanning several Georgia Tech disciplines directly addressing core HPC issues such as sustainability, reliability and massive data computation will be on display November 13-19, 2010 at SC10 in New Orleans, LA.&lt;/p&gt;
&lt;p&gt;Led by Jeffrey Vetter, joint professor of computational science and engineering at Georgia Tech and Oak Ridge National Laboratory (ORNL), Keeneland is a project funded by the U.S. National Science Foundation (NSF) to deploy a high-performance heterogeneous computing system consisting of HP servers integrated with Nvidia Tesla GPUs. Entering its second-year, the project will deploy its initial delivery system—the first of two experimental systems—this month. During the initial performance runs, the Keeneland system was clocked at running 64 teraflops per second, placing it well within the top 100 systems in the world on the most recent TOP500 list of supercomputers, published June 2010. Given the system&amp;rsquo;s excellent energy efficiency of approximately 650 megaflops per second per watt on the TOP500 Linpack, the team is hoping to secure a strong position on the Green500 list of the most energy efficient supercomputers in the world. Keeneland is supported by a $12 million grant from NSF&amp;rsquo;s Track 2D program, a five-year activity designed to fund the deployment and operation of two innovative computing systems, with an overarching goal of preparing the open computational science community for emerging architectures that have high performance and are energy efficiency.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Heterogeneous computing will play an important role in the future of high performance computing due to the new challenges of extreme parallelism and energy efficiency,&amp;rdquo; Vetter says. &amp;ldquo;The Keeneland partnership is providing hardware and software resources, training, and expertise to the computational science community at a critical time in this transition to new computing architectures.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A Georgia Tech team led by George Biros is a Gordon Bell Prize finalist at SC10 for their work demonstrating the simulation of blood flow using heterogeneous architectures and programming models at the petascale using CPU and hybrid CPU-GPU platforms, including the new Nvidia Fermi architecture and 200,000 cores of ORNL&amp;rsquo;s Jaguar system.&lt;/p&gt;
&lt;p&gt;Reliable and sustainable computing are core aspects of DARPA&amp;rsquo;s recently announced Ubiquitous High Performance Computing (UHPC) program, a $100 million initiative to build future systems that dramatically reduce power consumption while delivering a thousand-fold increase in processing capabilities. Georgia Tech researchers are supporting several components of the Nvidia-led UHPC team, ECHELON, while the Georgia Tech Research Institute (GTRI) will lead another group, CHASM, that will develop applications, benchmarking and metrics to drive UHPC system design considerations and support performance analysis of the developing system designs.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The key to solving the energy requirement roadblock to future systems is massive parallelism, which requires an entirely new way of thinking about today&amp;rsquo;s algorithms and architectures,&amp;rdquo; says Dan Campbell, senior researcher at GTRI and a co-principal investigator of CHASM.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;UHPC provides an opportunity for anticipated application challenges to influence the high-end system designs, in ways that are outside the traditional planning of industrial roadmaps in high performance computing,&amp;rdquo; says &lt;strong&gt;David Bader&lt;/strong&gt;, professor of Computational Science &amp;amp; Engineering at Georgia Tech, and Applications Lead for ECHELON.&lt;/p&gt;
&lt;p&gt;Georgia Tech was also named an Nvidia CUDA Center of Excellence in August 2010, further empowering the Institute to conduct game changing research and increase the computing power available to scientists and engineers through massively parallel computing.&lt;/p&gt;
&lt;p&gt;While computing systems one thousand times faster than current petascale levels is still 10 years away, massive amounts of data are currently being generated every day in health care, computational biology, homeland security, commerce, social media and many other industries. Georgia Tech is attacking the massive data analytics challenge. The Georgia Tech-led Foundations on Data Analysis and Visual Analytics (FODAVA) research initiative is in its third year, developing state-of-the-art approaches for analyzing massive and complex data sets. In September 2010, Edmond Chow joined the Georgia Tech School of Computational Science and Engineering as an associate professor to continue his work applying numerical and discrete algorithms to simulated physical and scientific systems such as microbiology and quantum chemistry as part of Georgia Tech&amp;rsquo;s new Institute for Data and High Performance Computing.&lt;/p&gt;
&lt;p&gt;Georgia Tech is making the investments in personnel and infrastructure required to be positioned competitively alongside the nation&amp;rsquo;s top HPC institutions. The Institute will continue to support research and educational initiatives that push the boundaries of technological capabilities and broaden the reach of computing innovation.&lt;/p&gt;
&lt;p&gt;Georgia Tech representatives will be at Booth 1561 at the SC10 show in New Orleans, LA November 13-19, 2010.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cacm.acm.org/news/101538-georgia-tech-keeps-sights-set-on-exascale-at-sc10/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cacm.acm.org/news/101538-georgia-tech-keeps-sights-set-on-exascale-at-sc10/fulltext&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/pub_releases/2010-11/giot-gtk111010.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eurekalert.org/pub_releases/2010-11/giot-gtk111010.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Keeps Sights Set On Exascale at SC10</title>
      <link>http://localhost:1313/blog/20101110-hpcwire/</link>
      <pubDate>Wed, 10 Nov 2010 06:47:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101110-hpcwire/</guid>
      <description>&lt;p&gt;The road to exascale computing is a long one, but the Georgia Institute of Technology, a leader in high-performance computing (HPC) research and education, continues to win new awards and attract new talent to drive technology innovation. From algorithms to architectures and applications, Georgia Tech&amp;rsquo;s researchers are collaborating with top companies, national labs and defense organizations to solve the complex challenges of tomorrow&amp;rsquo;s supercomputing systems. Ongoing projects and new research initiatives spanning several Georgia Tech disciplines directly addressing core HPC issues such as sustainability, reliability and massive data computation will be on display November 13-19, 2010 at SC10 in New Orleans, LA.&lt;/p&gt;
&lt;p&gt;Led by Jeffrey Vetter, joint professor of computational science and engineering at Georgia Tech and Oak Ridge National Laboratory (ORNL), Keeneland is a project funded by the U.S. National Science Foundation (NSF) to deploy a high-performance heterogeneous computing system consisting of HP servers integrated with Nvidia Tesla GPUs. Entering its second-year, the project will deploy its initial delivery system—the first of two experimental systems—this month. During the initial performance runs, the Keeneland system was clocked at running 64 teraflops per second, placing it well within the top 100 systems in the world on the most recent TOP500 list of supercomputers, published June 2010. Given the system&amp;rsquo;s excellent energy efficiency of approximately 650 megaflops per second per watt on the TOP500 Linpack, the team is hoping to secure a strong position on the Green500 list of the most energy efficient supercomputers in the world. Keeneland is supported by a $12 million grant from NSF&amp;rsquo;s Track 2D program, a five-year activity designed to fund the deployment and operation of two innovative computing systems, with an overarching goal of preparing the open computational science community for emerging architectures that have high performance and are energy efficiency.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Heterogeneous computing will play an important role in the future of high performance computing due to the new challenges of extreme parallelism and energy efficiency,&amp;rdquo; Vetter says. &amp;ldquo;The Keeneland partnership is providing hardware and software resources, training, and expertise to the computational science community at a critical time in this transition to new computing architectures.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A Georgia Tech team led by George Biros is a Gordon Bell Prize finalist at SC10 for their work demonstrating the simulation of blood flow using heterogeneous architectures and programming models at the petascale using CPU and hybrid CPU-GPU platforms, including the new Nvidia Fermi architecture and 200,000 cores of ORNL&amp;rsquo;s Jaguar system.&lt;/p&gt;
&lt;p&gt;Reliable and sustainable computing are core aspects of DARPA&amp;rsquo;s recently announced Ubiquitous High Performance Computing (UHPC) program, a $100 million initiative to build future systems that dramatically reduce power consumption while delivering a thousand-fold increase in processing capabilities. Georgia Tech researchers are supporting several components of the Nvidia-led UHPC team, ECHELON, while the Georgia Tech Research Institute (GTRI) will lead another group, CHASM, that will develop applications, benchmarking and metrics to drive UHPC system design considerations and support performance analysis of the developing system designs.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The key to solving the energy requirement roadblock to future systems is massive parallelism, which requires an entirely new way of thinking about today&amp;rsquo;s algorithms and architectures,&amp;rdquo; says Dan Campbell, senior researcher at GTRI and a co-principal investigator of CHASM.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;UHPC provides an opportunity for anticipated application challenges to influence the high-end system designs, in ways that are outside the traditional planning of industrial roadmaps in high performance computing,&amp;rdquo; says &lt;strong&gt;David Bader&lt;/strong&gt;, professor of Computational Science &amp;amp; Engineering at Georgia Tech, and Applications Lead for ECHELON.&lt;/p&gt;
&lt;p&gt;Georgia Tech was also named an Nvidia CUDA Center of Excellence in August 2010, further empowering the Institute to conduct game changing research and increase the computing power available to scientists and engineers through massively parallel computing.&lt;/p&gt;
&lt;p&gt;While computing systems one thousand times faster than current petascale levels is still 10 years away, massive amounts of data are currently being generated every day in health care, computational biology, homeland security, commerce, social media and many other industries. Georgia Tech is attacking the massive data analytics challenge. The Georgia Tech-led Foundations on Data Analysis and Visual Analytics (FODAVA) research initiative is in its third year, developing state-of-the-art approaches for analyzing massive and complex data sets. In September 2010, Edmond Chow joined the Georgia Tech School of Computational Science and Engineering as an associate professor to continue his work applying numerical and discrete algorithms to simulated physical and scientific systems such as microbiology and quantum chemistry as part of Georgia Tech&amp;rsquo;s new Institute for Data and High Performance Computing.&lt;/p&gt;
&lt;p&gt;Georgia Tech is making the investments in personnel and infrastructure required to be positioned competitively alongside the nation&amp;rsquo;s top HPC institutions. The Institute will continue to support research and educational initiatives that push the boundaries of technological capabilities and broaden the reach of computing innovation.&lt;/p&gt;
&lt;p&gt;Georgia Tech representatives will be at Booth 1561 at the SC10 show in New Orleans, LA November 13-19, 2010.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech engaged in $100 million next-generation high-performance computing initiative</title>
      <link>http://localhost:1313/blog/20101108-darpa-uhpc/</link>
      <pubDate>Mon, 08 Nov 2010 23:06:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101108-darpa-uhpc/</guid>
      <description>

















&lt;figure  id=&#34;figure-researchers-mark-richards-david-bader-and-dan-campbell-left-to-right-pose-in-the-advanced-computing-technology-lab-operated-by-the-georgia-tech-research-institute-image-credit-gary-meek&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Researchers Mark Richards, David Bader and Dan Campbell (left to right) pose in the Advanced Computing Technology Lab operated by the Georgia Tech Research Institute. *Image Credit: Gary Meek*&#34; srcset=&#34;
               /blog/20101108-darpa-uhpc/image_hu_30437e206ce57bfc.webp 400w,
               /blog/20101108-darpa-uhpc/image_hu_68eee33e70a35e5e.webp 760w,
               /blog/20101108-darpa-uhpc/image_hu_17a31ea416903be1.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20101108-darpa-uhpc/image_hu_30437e206ce57bfc.webp&#34;
               width=&#34;400&#34;
               height=&#34;287&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Researchers Mark Richards, David Bader and Dan Campbell (left to right) pose in the Advanced Computing Technology Lab operated by the Georgia Tech Research Institute. &lt;em&gt;Image Credit: Gary Meek&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Imagine that one of the world&amp;rsquo;s most powerful high performance computers could be packed into a single rack just 24 inches wide and powered by a fraction of the electricity consumed by comparable current machines. That would allow an unprecedented amount of computing power to be installed on aircraft, carried onto the battlefield for commanders - and made available to researchers everywhere.&lt;/p&gt;
&lt;p&gt;Putting this computing power into a small and energy-efficient package, and making it reliable and easier to program, are among the goals of the new DARPA Ubiquitous High Performance Computing (UHPC) initiative. Georgia Tech researchers from three different units are supporting key components of this $100 million challenge, which will require development of revolutionary approaches not bound by existing computing paradigms.&lt;/p&gt;
&lt;p&gt;If UHPC meets its ambitious eight-year goals, the new approaches and technologies it develops could redefine the way that computing systems are envisioned, designed and used.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The opportunity we have is to go far beyond the current product roadmaps,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, a professor in Georgia Tech&amp;rsquo;s School of Computational Science and Engineering. &amp;ldquo;We really have the opportunity to change the industry and to design our applications with new computing architectures. For the first time in the history of computing, we will be able to work with a clean slate.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To attain the program&amp;rsquo;s ambitious goals, DARPA funded four groups - led by NVIDIA Corp., Intel Corp., the Massachusetts Institute of Technology and Sandia National Laboratories - to develop UHPC prototypes. A fifth group, led by the Georgia Tech Research Institute (GTRI), will develop applications, benchmarking and metrics that will be used to drive UHPC system design considerations and support performance analysis of the developing system designs.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our team is developing a set of five difficult problems of a size and scope that the machines they are talking about should be able to accomplish,&amp;rdquo; said Dan Campbell, a GTRI principal research engineer who is co-principal investigator of the benchmarking initiative. &amp;ldquo;Our challenge is picking the right problems and specifying them at the right level of abstraction to allow innovation and properly represent what the DoD will need in 2018.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The five problems highlight the unique computing needs of the U.S. military:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analysis of the vast streams of data originating with widespread sensor systems, unmanned aerial vehicles and new generations of radar systems. The data will be analyzed for nuggets of useful information in ways that are not possible today.&lt;/li&gt;
&lt;li&gt;A dynamic graph challenge, in which many entities interact to create a problem of &amp;ldquo;connecting the dots.&amp;rdquo; That could mean analyzing relationships in social media to find possible adversaries, or understanding network traffic for cyber-security challenges.&lt;/li&gt;
&lt;li&gt;The decision tree, comparable to a chess game in which many possible interconnected options, each with complex implications, must be analyzed quickly. This could help field commanders or corporate CEOs make better decisions.&lt;/li&gt;
&lt;li&gt;Materials shock and hydrodynamics issues, challenges important to improving future generations of materials.&lt;/li&gt;
&lt;li&gt;Molecular dynamics simulations, which use high-performance computers to understand interactions between very large systems, such as protein folding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;We need to be able to take in a lot more data and understand it a lot more thoroughly than we can now,&amp;rdquo; said Mark Richards, a principal research engineer in the Georgia Tech School of Electrical and Computer Engineering and co-principal investigator of the benchmarking team. &amp;ldquo;That might allow us to find adversaries we can&amp;rsquo;t find now because we&amp;rsquo;re unable to tease that information out of the data flow.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;While the benefits of making such computing power widely available are obvious, how these machines will be designed, built and reliably operated is not.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Meeting these very ambitious program goals will pose significant technical challenges,&amp;rdquo; said Bader, who leads application development on the NVIDIA team and is part of the benchmarking group. &amp;ldquo;The technology roadmaps in such areas as interconnection networks, microprocessor design and technology fabrication will be pushed to their limits.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Meeting power limitations of just 57 kilowatts per rack - the amount of electricity produced by a portable military generator - may be the toughest among them. The fastest computer currently in operation requires seven megawatts of power.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Reducing the power consumption means less energy per computation,&amp;rdquo; noted Richards. &amp;ldquo;But as we lower the device voltage, we get closer to the physical noise. That will allow more errors due to the physics of the devices, and all kinds of things will have to be done to address that.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;And the entire machine will have to fit into a 24-inch wide, 78-inch high and 40-inch deep cabinet.&lt;/p&gt;
&lt;p&gt;But the physical implementation of the machines is just one part of the challenge, Bader noted. How people will work with them poses a perhaps more difficult challenge because it will require thinking about computers in a new way.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Over the past 20 or 30 years, we&amp;rsquo;ve taken a single computing design and kept tweaking it through advances like miniaturizing parts,&amp;rdquo; he said. &amp;ldquo;But we really haven&amp;rsquo;t changed the global nature of how the machine works. To meet DARPA&amp;rsquo;s power efficiency goals, we really will need to change the way we program the machine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;That also affects the humans who interact with these highly-parallel machines, which could have as many as a half-million separate threads operating at the same time. DARPA&amp;rsquo;s initial goal is to build machines capable of petaflop speed - a trillion operations per second - which could lead into the next generation of exascale computers a thousand times more capable.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We will need to find new ways of thinking about computers that will make it feasible for humans to comprehend what is going on inside,&amp;rdquo; Campbell said. &amp;ldquo;It&amp;rsquo;s a huge programming challenge.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To encourage collaboration in solving these complex problems, DARPA has embraced the idea of open innovation. It expects the organizations to work together on common critical topics, creating a collaborative environment to address the system challenges. New technology generated by the program - believed to be today&amp;rsquo;s largest DoD computing research initiative - is likely to move quickly into industry.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;There is certainly an expectation among the companies that what they are doing in this project is going to change how we do mainstream computing,&amp;rdquo; Bader said. &amp;ldquo;The technology transfer implications are certainly obvious.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://news.gatech.edu/2010/11/08/georgia-tech-engaged-100-million-next-generation-computing-initiative&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://news.gatech.edu/2010/11/08/georgia-tech-engaged-100-million-next-generation-computing-initiative&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Engaged in $100 Million DARPA Program to Develop Next Generation of High Performance Computers</title>
      <link>http://localhost:1313/blog/20101108-hpcwire/</link>
      <pubDate>Mon, 08 Nov 2010 22:30:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101108-hpcwire/</guid>
      <description>&lt;p&gt;Imagine that one of the world&amp;rsquo;s most powerful high performance computers could
be packed into a single rack just 24 inches wide and powered by a fraction of the electricity
consumed by comparable current machines. That would allow an unprecedented amount of
computing power to be installed on aircraft, carried onto the battlefield for commanders &amp;ndash;
and made available to researchers everywhere.&lt;/p&gt;
&lt;p&gt;Putting this computing power into a small and energy-efficient package, and making it
reliable and easier to program, are among the goals of the new DARPA Ubiquitous High
Performance Computing (UHPC) initiative. Georgia Tech researchers from three different
units are supporting key components of this $100 million challenge, which will require
development of revolutionary approaches not bound by existing computing paradigms.&lt;/p&gt;
&lt;p&gt;If UHPC meets its ambitious eight-year goals, the new approaches and technologies it
develops could redefine the way that computing systems are envisioned, designed and used.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The opportunity we have is to go far beyond the current product roadmaps,&amp;rdquo; said &lt;strong&gt;David
Bader&lt;/strong&gt;, a professor in Georgia Tech&amp;rsquo;s School of Computational Science and Engineering. &amp;ldquo;We
really have the opportunity to change the industry and to design our applications with new
computing architectures. For the first time in the history of computing, we will be able to
work with a clean slate.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To attain the program&amp;rsquo;s ambitious goals, DARPA funded four groups &amp;ndash; led by NVIDIA Corp.,
Intel Corp., the Massachusetts Institute of Technology and Sandia National Laboratories &amp;ndash;
to develop UHPC prototypes. A fifth group, led by the Georgia Tech Research Institute
(GTRI), will develop applications, benchmarking and metrics that will be used to drive
UHPC system design considerations and support performance analysis of the developing
system designs.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our team is developing a set of five difficult problems of a size and scope that the machines
they are talking about should be able to accomplish,&amp;rdquo; said Dan Campbell, a GTRI principal
research engineer who is co-principal investigator of the benchmarking initiative. &amp;ldquo;Our
challenge is picking the right problems and specifying them at the right level of abstraction
to allow innovation and properly represent what the DoD will need in 2018.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The five problems highlight the unique computing needs of the U.S. military:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Analysis of the vast streams of data originating with widespread sensor systems, unmanned aerial vehicles and new generations of radar systems. The data will be analyzed for nuggets of useful information in ways that are not possible today.&lt;/li&gt;
&lt;li&gt;A dynamic graph challenge, in which many entities interact to create a problem of &amp;ldquo;connecting the dots.&amp;rdquo; That could mean analyzing relationships in social media to find possible adversaries, or understanding network traffic for cyber-security challenges.&lt;/li&gt;
&lt;li&gt;The decision tree, comparable to a chess game in which many possible interconnected options, each with complex implications, must be analyzed quickly. This could help field commanders or corporate CEOs make better decisions.&lt;/li&gt;
&lt;li&gt;Materials shock and hydrodynamics issues, challenges important to improving future generations of materials.&lt;/li&gt;
&lt;li&gt;Molecular dynamics simulations, which use high-performance computers to understand interactions between very large systems, such as protein folding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;ldquo;We need to be able to take in a lot more data and understand it a lot more thoroughly than
we can now,&amp;rdquo; said Mark Richards, a principal research engineer in the Georgia Tech School
of Electrical and Computer Engineering and co-principal investigator of the benchmarking
team. &amp;ldquo;That might allow us to find adversaries we can&amp;rsquo;t find now because we&amp;rsquo;re unable to
tease that information out of the data flow.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;While the benefits of making such computing power widely available are obvious, how these
machines will be designed, built and reliably operated is not.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Meeting these very ambitious program goals will pose significant technical challenges,&amp;rdquo; said
Bader, who leads application development on the NVIDIA team and is part of the
benchmarking group. &amp;ldquo;The technology roadmaps in such areas as interconnection networks,
microprocessor design and technology fabrication will be pushed to their limits.&amp;rdquo;
Meeting power limitations of just 57 kilowatts per rack &amp;ndash; the amount of electricity produced
by a portable military generator – may be the toughest among them. The fastest computer
currently in operation requires seven megawatts of power.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Reducing the power consumption means less energy per computation,&amp;rdquo; noted Richards.
&amp;ldquo;But as we lower the device voltage, we get closer to the physical noise. That will allow more
errors due to the physics of the devices, and all kinds of things will have to be done to
address that.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;And the entire machine will have to fit into a 24-inch wide, 78-inch high and 40-inch deep
cabinet.&lt;/p&gt;
&lt;p&gt;But the physical implementation of the machines is just one part of the challenge, Bader
noted. How people will work with them poses a perhaps more difficult challenge because it
will require thinking about computers in a new way.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Over the past 20 or 30 years, we&amp;rsquo;ve taken a single computing design and kept tweaking it
through advances like miniaturizing parts,&amp;rdquo; he said. &amp;ldquo;But we really haven&amp;rsquo;t changed the
global nature of how the machine works. To meet DARPA&amp;rsquo;s power efficiency goals, we really
will need to change the way we program the machine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;That also affects the humans who interact with these highly-parallel machines, which could
have as many as a half-million separate threads operating at the same time. DARPA&amp;rsquo;s initial
goal is to build machines capable of petaflop speed &amp;ndash; a trillion operations per second &amp;ndash;
which could lead into the next generation of exascale computers a thousand times more
capable.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We will need to find new ways of thinking about computers that will make it feasible for
humans to comprehend what is going on inside,&amp;rdquo; Campbell said. &amp;ldquo;It&amp;rsquo;s a huge programming
challenge.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;To encourage collaboration in solving these complex problems, DARPA has embraced the
idea of open innovation. It expects the organizations to work together on common critical
topics, creating a collaborative environment to address the system challenges. New
technology generated by the program &amp;ndash; believed to be today&amp;rsquo;s largest DoD computing
research initiative &amp;ndash; is likely to move quickly into industry.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;There is certainly an expectation among the companies that what they are doing in this
project is going to change how we do mainstream computing,&amp;rdquo; Bader said. &amp;ldquo;The technology
transfer implications are certainly obvious,&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputing Meets Social Media</title>
      <link>http://localhost:1313/blog/20101021-hpcwire/</link>
      <pubDate>Thu, 21 Oct 2010 20:23:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101021-hpcwire/</guid>
      <description>&lt;p&gt;In supercomputing these days, it&amp;rsquo;s usually the big science applications (astrophysics, climate simulations, earthquake predictions and so on) that seem to garner the most attention. But a new area is quickly emerging onto the HPC scene under the general category of informatics or data-intensive computing. To be sure, informatics is not new at all, but its significance to the HPC realm is growing, mainly due to emerging application areas like cybersecurity, bioinformatics, and social networking.&lt;/p&gt;
&lt;p&gt;The rise of social media, in particular, is injecting enormous amounts of data into the global information stream. Making sense of it with conventional computers and software is nearly impossible. With that in mind, a story in MIT Technology Review about using a supercomputer to analyze Twitter data caught my attention. In this case, the supercomputer was a Cray XMT machine operated by the DOE at Pacific Northwest National Lab (PNNL) as part of their CASS-MT infrastructure.&lt;/p&gt;
&lt;p&gt;The application software used to drive this analysis was GraphCT, developed by researchers at &lt;strong&gt;Georgia Tech&lt;/strong&gt; in collaboration with the PNNL folks. GraphCT is short for Graph Characterization Toolkit, and is designed to analyze really massive graph structures, like for example, the type of data that makes up social networks such as Twitter.&lt;/p&gt;
&lt;p&gt;For those of you who have been hiding under a rock for the last few years, Twitter is a social media site for exchanging 140-character microblogs, aka tweets. As of April 2010, there were over 105 million registered users, generating an average of 55 million tweets a day. The purpose of Twitter is, of course&amp;hellip; well, nobody knows for sure. But it does represent an amazing snapshot of what is capturing the attention of Web-connected humans on any given day. If only one could make sense of it.&lt;/p&gt;
&lt;p&gt;Counting tweets or even searching them is a pretty simple task for a computer, but sifting out the Twitter leaders from the followers and figuring out the access patterns is a lot trickier. That&amp;rsquo;s where GraphCT and Cray supercomputing comes in.&lt;/p&gt;
&lt;p&gt;GraphCT is able to map the Twitter network data to a graph, and make use of certain metrics to assign importance to the user interactions. It measures something called &amp;ldquo;betweenness centrality,&amp;rdquo; to rank the significance of tweeters.&lt;/p&gt;
&lt;p&gt;Because of the size of the Twitter data and the highly multithreaded nature of the GraphCT software, the researchers couldn&amp;rsquo;t rely on the vanilla Web servers that make up the Internet itself, or even conventional HPC computing gear. Fine-grained parallelism plus sparse memory access patterns necessitated a large-scale, global address space machine, built to tolerate high memory latency.&lt;/p&gt;
&lt;p&gt;The Cray XMT, a proprietary SMP-type supercomputer is such a machine, and is in fact specifically designed for this application profile. I suspect the reason you don&amp;rsquo;t hear more about the XMT is because most of them are probably deployed at those top secret three-letter government agencies, where data mining and analysis are job one.&lt;/p&gt;
&lt;p&gt;The XMT at PNNL is a 128-processor system with 1 terabyte of memory. The distinguishing characteristic of this architecture is that each custom &amp;ldquo;Threadstorm&amp;rdquo; processor is capable of managing up to 128 threads simultaneously. Tolerance for high memory latencies is supported by efficient management of thread context at the hardware level.&lt;/p&gt;
&lt;p&gt;The system&amp;rsquo;s 1 TB of global RAM is enough to hold more than 4 billion vertices and 34 billion edges of a graph. To put that in perspective, one of the Twitter datasets from September 2009 was encapsulated in 735 thousand vertices and 1 million edges, requiring only about 30 MB of memory. Applying the GraphCT analysis, the data required less than 10 seconds to process. The researchers estimated that a much larger Twitter dataset of 61.6 million vertices and 1.47 billion edges would require only 105 minutes.&lt;/p&gt;
&lt;p&gt;When the Georgia Tech and PNNL researchers ran the numbers, they found that relatively few Twitter accounts were responsible for a disproportionate amount of the traffic, at least on the particular datasets they analyzed. The largest dataset was made up of all public tweets from September 20th to 25th in 2009, containing the hashtag #atlflood (to capture tweets about the Atlanta flood event). In this case, at least, the most influential tweets originated with a few major media and government outlets.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re likely to be hearing more about the graph applications in HPC in the near future. Data sets and data streams are outpacing the capabilities of conventional computers, and demand for digesting all these random bytes is building rapidly. Since the optimal architectures for this scale of data-intensive processing is apt to be quite different than that of conventional HPC platforms (which tend to be optimized for compute-intensive science codes), this could spur a lot more diversity in supercomputer designs.&lt;/p&gt;
&lt;p&gt;To that end, a new group called the Graph 500 has developed a benchmark aimed at this category of applications, and intends to maintain a list of the top 500 most performant graph-capable systems. The first Graph 500 list is scheduled to be released at the upcoming Supercomputing Conference (SC10) in New Orleans next month.&lt;/p&gt;
&lt;p&gt;In the meantime, if you&amp;rsquo;re interested in giving GraphCT a whirl, a pre-1.0 release of the software can be downloaded for free from the Georgia Tech website. You&amp;rsquo;ll just need a spare Cray XMT or POSIX-compliant machine to run it on.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20101027232415/http://www.hpcwire.com/blogs/Supercomputing-Meets-Social-Media-105493293.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/blogs/Supercomputing-Meets-Social-Media-105493293.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputer Digests Twitter in Real-Time</title>
      <link>http://localhost:1313/blog/20101020-mit-technology-review/</link>
      <pubDate>Wed, 20 Oct 2010 20:28:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20101020-mit-technology-review/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Christopher Mims&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Determining the most influential users of Twitter is probably not what the creators of the Cray XMT supercomputer had in mind when they designed their machine. But when you’re packing this much computational heat, you go where the hard problems are. Twitter, Facebook and the rest of the social Web have become the modern-day equivalent of the water cooler, albeit with an automatic transcriptionist present. And processing all the data that conversation generates turns out to be a very hard problem.&lt;/p&gt;


















&lt;figure  id=&#34;figure-cray-xmt-supercomputer-courtesy-cray&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cray XMT supercomputer courtesy Cray&#34; srcset=&#34;
               /blog/20101020-mit-technology-review/cray_xmt_hu_cdd9696061168328.webp 400w,
               /blog/20101020-mit-technology-review/cray_xmt_hu_385c39b0ab4f997d.webp 760w,
               /blog/20101020-mit-technology-review/cray_xmt_hu_62d0816c2a858eac.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20101020-mit-technology-review/cray_xmt_hu_cdd9696061168328.webp&#34;
               width=&#34;300&#34;
               height=&#34;297&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cray XMT supercomputer courtesy Cray
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For example, as of February 2010, Facebook included 400 million active users with an average of 120 “friend” connections each, all of whom collectively shared 5 billion pieces of information in a single month.&lt;/p&gt;
&lt;p&gt;Figuring out who the “influencers” are in such a massive social networks requires creating a gigantic social graph, where each user is a vertex and the connections between them are lines. Ranking users within such a graph requires a determination of their “centrality”. That is, how many other people are connected to them, and how many people are connected to them, and so on, until you get to the trunk of the tree structure underlying connectedness on a service like Twitter.&lt;/p&gt;
&lt;p&gt;It turns out this is not the sort of problem that is readily handled even by the usual go-to workstations of the scientific supercomputing world – the GPGPU-powered supercomputers that leverage the graphics chips usually used to render lush 3D environments in videogames. These GPGPU workstations simply don’t allow enough control over how many processes are running in parallel to efficiently churn through social graphs as big as the one represented by Twitter or Facebook.&lt;/p&gt;
&lt;p&gt;That’s why David Ediger of Georgia Tech, &lt;a href=&#34;https://web.archive.org/web/20101025140234/http://www.cc.gatech.edu/~bader/papers/MassiveTwitter.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;with the help of a long list of collaborators&lt;/a&gt;, turned to the 128-CPU Cray XMT housed at the Pacific Northwest National Laboratory. The XMT is a favorite of supercomputing hot-rodders and uber-geeks who appreciate its fine-grained massively multithreaded tunability. This machine is usually pressed into service for solving problems like “Hierarchical Bayesian Modeling for Text Analysis” or &lt;a href=&#34;http://www.cray.com/Products/XMT/Product/Resources.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;analyzing the stability of America’s power grid&lt;/a&gt;, but Ediger had it cogitating on every stray thought from a single day’s worth of the &lt;a href=&#34;http://mashable.com/2010/03/01/twitter-opens-up-the-firehose-to-startups/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter firehose&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Cray made short work of Twitter, disposing of an entire day’s worth of connections in under an hour. The results will surprise no one – on Twitter, a tiny fraction of sources are retweeted widely, mostly government and media, while the rest of the service is either people talking in small groups or literally talking to themselves.&lt;/p&gt;
&lt;p&gt;The point, though, is that throwing a finely-tuned Cray running Ediger’s custom software – GraphCT – at Twitter allowed the researchers to digest the service in something like real time. Which is exactly the sort of capability that intelligence agencies, marketers and perhaps even Twitter itself might want to have.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.technologyreview.com/s/421274/supercomputer-digests-twitter-in-real-time/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.technologyreview.com/s/421274/supercomputer-digests-twitter-in-real-time/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NVIDIA Names GATech CUDA Center of Excellence</title>
      <link>http://localhost:1313/blog/20100823-insidehpc/</link>
      <pubDate>Mon, 23 Aug 2010 20:16:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100823-insidehpc/</guid>
      <description>&lt;p&gt;NVIDIA announced today that they have officially named Georgia Institute of Technology a CUDA Center of Excellence.  Jeff Vetter, joint professor of the Georgia Tech College of Computing and Group Leader at Oak Ridge National Laboratory will serve as principal investigator for the center.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Georgia Tech has a long history of education and research that depends heavily on the parallel processing capabilities that NVIDIA has introduced with its CUDA architecture,” Vetter said. “This award allows us to focus, what is now a large amount of activity across 25 different research groups, under a single center, which will significantly amplify our research capabilities.”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;NVIDIA and Georgia Tech are already collaborating on several projects.  The National Science Foundation Track 2D Keeneland Project will initially deploy a significant system of NVIDIA(R) Tesla(TM) processors this year, with a larger, petaflop-class system to be in place by 2012. Georgia Tech and Oak Ridge are also collaborating with NVIDIA in the recently announced DARPA Ubiquitous High Performance Computing program, with the goal of designing an energy efficient “petaflop in a cabinet&amp;rdquo; prototype system in 2018.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;By cross-pollinating ideas and skills, sharing software and hardware facilities, and streamlining interactions with priority access to NVIDIA staff and capabilities, this status will add considerable strength to our research and educational programs,” Vetter added.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For more info on the partnership, read their full release &lt;a href=&#34;http://www.marketwatch.com/story/nvidia-names-georgia-institute-of-technology-a-cuda-center-of-excellence-2010-08-23?reflink=MW_news_stmp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://insidehpc.com/2010/08/nvidia-names-gatech-cuda-center-of-excellence/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://insidehpc.com/2010/08/nvidia-names-gatech-cuda-center-of-excellence/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NVIDIA Expands CUDA Developer Ecosystem With New CUDA Research and Teaching Centers in the U.S., Canada and Europe</title>
      <link>http://localhost:1313/blog/20100823-nvidia/</link>
      <pubDate>Mon, 23 Aug 2010 20:06:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100823-nvidia/</guid>
      <description>&lt;p&gt;NVIDIA today announced the addition of new research and educational centers dedicated to leveraging the immense processing power of graphics processing units (GPUs) to address today&amp;rsquo;s most challenging computing issues.&lt;/p&gt;
&lt;p&gt;CUDA Research Centers are recognized institutions that embrace and utilize GPU computing across multiple research fields. CUDA Teaching Centers are institutions that have integrated GPU computing techniques into their mainstream computer programming curriculum. The new centers are:&lt;/p&gt;
&lt;p&gt;CUDA Research Centers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Barcelona Supercomputing Center, UPC (Spain)&lt;/li&gt;
&lt;li&gt;Clemson University&lt;/li&gt;
&lt;li&gt;HP Labs&lt;/li&gt;
&lt;li&gt;Massachusetts General Hospital - Northeastern University&lt;/li&gt;
&lt;li&gt;Swinburne University of Technology (Australia)&lt;/li&gt;
&lt;li&gt;University of California at Los Angeles - UCLA&lt;/li&gt;
&lt;li&gt;University of Warsaw (Poland)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CUDA Teaching Centers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;American University of Beirut (Lebanon)&lt;/li&gt;
&lt;li&gt;Florida A&amp;amp;M University&lt;/li&gt;
&lt;li&gt;Hood College&lt;/li&gt;
&lt;li&gt;McMaster University (Canada)&lt;/li&gt;
&lt;li&gt;University of California at Los Angeles - UCLA&lt;/li&gt;
&lt;li&gt;University of Minnesota&lt;/li&gt;
&lt;li&gt;University of North Carolina at Charlotte&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Launched in June 2010, the CUDA Research Center program fosters collaboration with research groups at universities and research institutes that are expanding the frontier of massively parallel computing. Among the benefits are exclusive events with key researchers and academics, a designated NVIDIA® technical liaison and access to specialized online and in-person training sessions.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;HP Labs conducts high-impact scientific research to address the most important challenges and opportunities facing our customers and society in the next decade,&amp;rdquo; said Dr. Ren Wu, senior scientist at HP. &amp;ldquo;The CUDA architecture represents the next evolution of high-performance computing, and HP Labs has been working with NVIDIA for some time to ensure that HP&amp;rsquo;s professional computing products leverage the latest GPU computing technologies and practices. HP Labs is proud to be recognized by NVIDIA for our innovative work in massively parallel computing and the value we are providing to customers.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The CUDA Teaching Center program, also launched in June 2010, is the first program of its kind to be developed and offered to universities and colleges by a hardware vendor. The program has many benefits, including the donation of teaching kits consisting of textbooks, software licenses and CUDA™ architecture-enabled GPUs for teaching lab computers, as well as academic discounts for additional hardware if required.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The addition of these new educational programs underscores the tremendous interest in harnessing the power of GPUs to solve a today&amp;rsquo;s most pressing computing challenges,&amp;rdquo; said Sanford Russell, general manager of CUDA &amp;amp; GPU Computing at NVIDIA. &amp;ldquo;There are more than 350 universities worldwide teaching the CUDA programming model within their curriculum, and more than 100,000 programmers actively developing applications that use the GPU. With the addition of these new programs, we expect to see even broader interest and adoption of GPU computing practices across a wide variety of industries worldwide.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Existing CUDA Research Centers include: John Hopkins University, Nanyang University (Singapore), Technical University of Ostrava (Czech Republic), CSIRO (Australia), ICHEC (Ireland) and SINTEF (Norway). Existing CUDA Teaching Centers include State University of New York, Potsdam (U.S.), California Polytechnic State University, San Luis Obispo, ITESM (Mexico), Czech Technical University (Czech Republic), and Qingdao University (China).&lt;/p&gt;
&lt;p&gt;These programs augment the CUDA Center of Excellence program, the elite network of 11 institutes focused on advancing parallel computing on the GPU. They are: Cambridge University, Chinese Academy of Sciences, &lt;strong&gt;Georgia Institute of Technology&lt;/strong&gt;, Harvard University, University of Maryland, National Taiwan University, Tokyo Tech, Tsinghua University, University of Illinois at Urbana-Champaign, University of Tennessee, and University of Utah.&lt;/p&gt;
&lt;p&gt;For more information on NVIDIA research activities and these programs, please visit the &lt;a href=&#34;http://research.nvidia.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVResearch site&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;tags--keywords&#34;&gt;Tags / Keywords:&lt;/h3&gt;
&lt;p&gt;NVIDIA, GTC, GPU, supercomputing, parallel computing, CUDA, GPGPU, high performance computing, OpenCL, DirectCompute, GPU Computing, GPU Compute, visual computing, developers, bioscience, oil &amp;amp; gas, medical, finance&lt;/p&gt;
&lt;h3 id=&#34;about-nvidia&#34;&gt;About NVIDIA&lt;/h3&gt;
&lt;p&gt;NVIDIA (NASDAQ: NVDA) awakened the world to the power of computer graphics when it invented the GPU in 1999. Since then, it has consistently set new standards in visual computing with breathtaking, interactive graphics available on devices ranging from tablets and portable media players to notebooks and workstations. NVIDIA&amp;rsquo;s expertise in programmable GPUs has led to breakthroughs in parallel processing which make supercomputing inexpensive and widely accessible. The company holds more than 1,100 U.S. patents, including ones covering designs which are fundamental to modern computing. For more information, see &lt;a href=&#34;https://www.nvidia.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nvidia.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Certain statements in this press release including, but not limited to, statements as to: the benefits of CUDA Research Center and CUDA Teaching Center; adoption of GPU computing; expertise in visual computing and parallel processing; and the impact of the company&amp;rsquo;s patents on modern computing are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of faster or more efficient technology; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the reports NVIDIA files with the Securities and Exchange Commission, or SEC, including its Form 10-Q for the fiscal period ended August 1, 2010. Copies of reports filed with the SEC are posted on the company&amp;rsquo;s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.&lt;/p&gt;
&lt;p&gt;Copyright © 2010. All rights reserved. NVIDIA, the NVIDIA logo, and CUDA, are trademarks or registered trademarks of NVIDIA Corporation in the United States and other countries around the world. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability, and specifications are subject to change without notice&lt;/p&gt;
&lt;h3 id=&#34;media-contacts&#34;&gt;Media Contacts&lt;/h3&gt;
&lt;p&gt;Kristin Bryson&lt;br&gt;
+1-203-241-9190&lt;br&gt;
&lt;a href=&#34;mailto:kbryson@nvidia.com&#34;&gt;kbryson@nvidia.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nvidianews.nvidia.com/news/nvidia-expands-cuda-developer-ecosystem-with-new-cuda-research-and-teaching-centers-in-the-u-s-canada-and-europe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nvidianews.nvidia.com/news/nvidia-expands-cuda-developer-ecosystem-with-new-cuda-research-and-teaching-centers-in-the-u-s-canada-and-europe&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NVIDIA Names Georgia Institute of Technology a CUDA Center of Excellence</title>
      <link>http://localhost:1313/blog/20100823-hpcwire/</link>
      <pubDate>Mon, 23 Aug 2010 19:50:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100823-hpcwire/</guid>
      <description>&lt;p&gt;NVIDIA today recognized Georgia Institute of Technology
(Georgia Tech) as a CUDA Center of Excellence. One of the world&amp;rsquo;s premier engineering and
science universities, Georgia Tech is engaged in a wide number of research, development and
educational activities which leverage GPU Computing.&lt;/p&gt;
&lt;p&gt;Jeffrey Vetter, joint professor of the Georgia Tech College of Computing and Group Leader at
Oak Ridge National Laboratory, will serve as principal investigator of the CUDA Center of
Excellence.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Georgia Tech has a long history of education and research that depends heavily on the
parallel processing capabilities that NVIDIA has introduced with its CUDA architecture,&amp;rdquo;
Vetter said. &amp;ldquo;This award allows us to focus, what is now a large amount of activity across 25
different research groups, under a single center, which will significantly amplify our research
capabilities.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Georgia Tech&amp;rsquo;s non-profit research arm, Georgia Tech Research Institute, is also leveraging
the capabilities of the GPU in its work with industry and government groups such as the U.S.
Defense Department.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;By cross-pollinating ideas and skills, sharing software and hardware facilities, and
streamlining interactions with priority access to NVIDIA staff and capabilities, this status
will add considerable strength to our research and educational programs,&amp;rdquo; Vetter added.&lt;/p&gt;
&lt;p&gt;NVIDIA and Georgia Tech are already collaborating on a number of projects that will help
shape the national science infrastructure. The National Science Foundation Track 2D
Keeneland Project will initially deploy a significant system of NVIDIA Tesla processors
this year, with a larger, petaflop-class system to be in place by 2012. Georgia Tech and Oak
Ridge are also collaborating with NVIDIA in the recently announced DARPA Ubiquitous
High Performance Computing program, with the goal of designing an energy efficient
&amp;ldquo;petaflop in a cabinet¿ prototype system in 2018.&lt;/p&gt;
&lt;p&gt;One example of the work the University is doing in the field of software tools is &amp;ldquo;Ocelot&amp;rdquo;, a
compiler that allows CUDA code to run seamlessly on multicore CPUs. The compiler will be
available and distributed through the CCOE and will help to catalyze research on top of this
open source infrastructure.&lt;/p&gt;
&lt;p&gt;Georgia Tech joins a select group of 10 other universities and research organizations in the
US and abroad, including Harvard University, Cambridge University and the Chinese
Academy of Sciences, that are designated as a CUDA Center of Excellence. More than 350
universities worldwide teach the CUDA programming model within their curriculum.&lt;/p&gt;
&lt;p&gt;CUDA is NVIDIA&amp;rsquo;s computing architecture that enables its GPUs to be programmed using
industry standard programming languages and APIs, opening up their massive parallel
processing power to a broad range of applications beyond graphics.&lt;/p&gt;
&lt;p&gt;More information on the Georgia Tech CUDA Center of Excellence can be found here, or
visit the NVIDIA CUDA Center of Excellence program page for more information on the
program.&lt;/p&gt;
&lt;h3 id=&#34;about-nvidia&#34;&gt;About NVIDIA&lt;/h3&gt;
&lt;p&gt;NVIDIA (NASDAQ: NVDA) awakened the world to the power of computer graphics when it
invented the graphics processing unit (GPU) in 1999. Since then, it has consistently set new
standards in visual computing with breathtaking, interactive graphics available on devices
ranging from portable media players to notebooks to workstations. NVIDIA&amp;rsquo;s expertise in
programmable GPUs has led to breakthroughs in parallel processing which make
supercomputing inexpensive and widely accessible. Fortune magazine has ranked NVIDIA
#1 in innovation in the semiconductor industry for two years in a row. For more information,
see &lt;a href=&#34;https://www.nvidia.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nvidia.com&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Burrowing into the bran tub</title>
      <link>http://localhost:1313/blog/20100812-scw/</link>
      <pubDate>Thu, 12 Aug 2010 15:58:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100812-scw/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;20100812-SCW.pdf&#34;&gt;View the article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://ftp.scientific-computing.com/feature/burrowing-bran-tub&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://ftp.scientific-computing.com/feature/burrowing-bran-tub&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DARPA Sets Ubiquitous HPC Program in Motion</title>
      <link>http://localhost:1313/blog/20100810-hpcwire/</link>
      <pubDate>Tue, 10 Aug 2010 11:30:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100810-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Michael Feldman&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The US Defense Advanced Research Projects Agency (DARPA) has selected four “performers” to develop prototype systems for its Ubiquitous High Performance Computing (UHPC) program. According to a &lt;a href=&#34;http://www.darpa.mil/news/2010/UHPCNewsRelease.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;press release&lt;/a&gt; issued on August 6, the organizations include Intel, NVIDIA, MIT, and Sandia National Laboratory. &lt;strong&gt;Georgia Tech&lt;/strong&gt; was also tapped to head up an evaluation team for the systems under development. The first UHPC prototype systems are slated to be completed in 2018.&lt;/p&gt;
&lt;p&gt;The overarching goal of UHPC is, in the words of the program’s mission, “to reinvent computing.” Of course, nearly every DARPA computing program purports to do that, to one extent or another. So this is just business as usual for the DoD’s premier research agency.&lt;/p&gt;
&lt;p&gt;DARPA has dubbed the intended architecture “ExtremeScale” supercomputing, and one of the principle goals of the program is to develop dense, energy-efficient petascale and exascale architectures. Specifically, UHPC systems will need to deliver a petaflop of High Performance Linpack (HPL) in a single cabinet and achieve an energy efficiency of at least 50 gigaflops/watt. By comparison, the most efficient supercomputers today, according to the Green500 list, are based on IBM’s Cell processor technology, and offer well under 1 gigaflop/watt. So these ExtremeScale machines will need to be on the order of 100 times more efficient than today’s supercomputers.&lt;/p&gt;
&lt;p&gt;Oh, and the machines have to be easy to program. That is, the system design and accompanying software should make it possible for application developers to be freed from managing low-level hardware features or explicitly programming for data locality and concurrency. In fact, the programmer should even be able to implement parallelism without using MPI, or any other communication-based mechanism. In addition, the operating system for these machines has to be “self-aware,” such that it can dynamically manage performance, dependability and system resources.&lt;/p&gt;
&lt;p&gt;Since this is under DARPA, the applications UHPC is concerned with are all DoD-type HPC workloads. These include processing massive amounts of streaming sensor data, handling large graph-based, informatics problems, and solving decision class problems. Two other types of target problems, drawn from other DoD applications, are to be selected later. Obviously, such systems will also be applicable to many HPC-type applications in research and industry.&lt;/p&gt;
&lt;p&gt;The DARPA announcement last Friday (August 6) was rather brief, only specifying the five organizations that have been selected. As of this writing, only NVIDIA has come out with a public announcement acknowledging the DARPA win. It is unclear whether additional awardees will be named at a later date.&lt;/p&gt;
&lt;p&gt;For its part, NVIDIA is teaming with Cray, Oak Ridge National Laboratory and six unnamed universities to design its ExtremeScale prototype. The NVIDIA-Cray-ORNL troika is a natural for this project. When the Fermi GPU was unveiled last September, Oak Ridge announced plans for a multi-petaflop supercomputer based on NVIDIA’s latest GPU. Although not explicitly stated, the ORNL machine will likely be a Cray system, given the DOE lab’s propensity to buy supercomputing gear from them. Of course, when the first ExtremeScale prototype system is produced, NVIDIA is presumably going to be a few generations beyond its Fermi-class GPUs, and by 2018 may even be offering some sort of integrated CPU-GPU processor, a la AMD’s Fusion architecture. NVIDIA’s CUDA software development environment will also have gone through several iterations of enhancements by then.&lt;/p&gt;
&lt;p&gt;As the only other commercial firm awarded under UHPC, Intel has a few options to exercise as far as processor architecture is concerned. But with the recent revival of Larrabee, now known as the Many Integrated Core (MIC) processor, Intel will almost certainly be looking to this architecture to get the level of energy efficiency that the UHPC program demands. The chipmaker’s expertise with parallel programming development tools (Ct language, Threading Building Blocks, Parallel Studio, etc.) is also likely to help address the ease-of-programming aspect for these systems.&lt;/p&gt;
&lt;p&gt;It’s noteworthy that the two companies awarded UHPC money were chip vendors with a good parallel software base. Contrast this with AMD (assuming they bid for the UHPC work), which has a decent performance/watt story with its CPU-GPU Fusion architecture, but no well-defined software infrastructure to back it up as yet. It’s even more interesting that NVIDIA is now garnering this kind of respect from the HPC research community. As recently as five years ago, no one would have though the GPU maker would be at the cutting-edge of supercomputing.&lt;/p&gt;
&lt;p&gt;As one of the ExtremeScale system developers, NVIDIA will receive $25 million spread over the next four years (two 24-month research phases). That effort will produce a preliminary design. The two follow-on phases, when the actual prototypes are developed, will be initiated under a separate solicitation and may be awarded to different teams than were selected for the first two phases.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2010/08/10/darpa_sets_ubiquitous_hpc_program_in_motion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2010/08/10/darpa_sets_ubiquitous_hpc_program_in_motion/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DARPA Developing ExtremeScale Supercomputer System</title>
      <link>http://localhost:1313/blog/20100806-darpa-uhpc/</link>
      <pubDate>Fri, 06 Aug 2010 09:24:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100806-darpa-uhpc/</guid>
      <description>&lt;p&gt;Advanced computing is the backbone of the Department of Defense and of critical strategic importance to our nation’s defense. All DoD sensors, platforms and missions depend heavily on computer systems. To meet the escalating demands for greater processing performance, it is imperative that future computer system designs be developed to support new generations of advanced DoD systems and enable new computing application code. Targeting this crucial need, the Defense Advanced Research Projects Agency (DARPA) has initiated the Ubiquitous High Performance Computing (UHPC) program to create an innovative, revolutionary new generation of computing systems that overcomes the limitations of current evolutionary approach.&lt;/p&gt;
&lt;p&gt;Computing performance increases have been driven by Moore’s Law (doubling the transistors that can be placed on an integrated circuit every two years). The ability to achieve projected performance gains is limited by significant power consumption, architectural and programming complexity issues. To exploit available technological advances fully, highly programmable high performance computers must be developed that require dramatically less energy per computation. The goal of DARPA’s UHPC program is to re-invent computing. It plans to develop radically new computer architectures and programming models that are 100 to 1,000 times more energy efficient, with higher performance, and that are easier to program than current systems.&lt;/p&gt;
&lt;p&gt;The UHPC program directly addresses major priorities expressed by the President’s “Strategy for American Innovation”. These priorities include “exascale” supercomputing Century “Grand Challenge”, energy-efficient computing and worker productivity. The resulting UHPC capabilities will provide at least 50-times greater energy, computing and productivity efficiency, which will slash the time needed to design and develop complex computing applications.&lt;/p&gt;
&lt;p&gt;President Obama’s “Strategy for American Innovation” notes DARPA’s successes and innovations that have driven America’s technology advances. DARPA historically has pursued innovative computing development and enabled significant advances. UHPC will pursue non-traditional, innovative developments in an open collaborative research environment. This approach and the resulting technical advances are critical to re-inventing computing.&lt;/p&gt;
&lt;p&gt;Prototype UHPC systems are expected to be complete by 2018. The four performers selected to develop UHPC prototype systems are Intel Corporation, Hillsboro, Ore., NVIDIA Corporation, Santa Clara, Calif., Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory, Boston and Sandia National Laboratory, Albuquerque, N.M. &lt;strong&gt;Georgia Institute of Technology&lt;/strong&gt;, Atlanta, was selected to lead an Applications, Benchmarks and Metrics team for evaluating the UHPC systems under development.&lt;/p&gt;
&lt;p&gt;Defense Advanced Research Projects Agency&lt;br&gt;
3701 North Fairfax Drive&lt;br&gt;
Arlington, VA 22203-1714&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Media with questions, please contact Eric Mazzacone, (703) 526-4758, or &lt;a href=&#34;mailto:eric.mazzcone@darpa.mil&#34;&gt;eric.mazzcone@darpa.mil&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High Performance Computational Life Sciences at ISC&#39;10</title>
      <link>http://localhost:1313/blog/20100531-isc10/</link>
      <pubDate>Mon, 31 May 2010 22:05:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100531-isc10/</guid>
      <description>&lt;p&gt;&amp;lsquo;Life Sciences&amp;rsquo; is becoming a strategic discipline at the frontier between
Molecular Biology and Computer Science, impacting medicine,
biotechnology, as well as society. &amp;lsquo;Life Sciences&amp;rsquo;, the leading edge
research is one of the most challenging supercomputer application for
the future. At ISC’10 we will organize a special session about &amp;lsquo;Life
Sciences&amp;rsquo;, to fulfill the growing interest from the public and the
scientific world.&lt;/p&gt;
&lt;p&gt;&amp;lsquo;Computational Life Sciences&amp;rsquo; is fast emerging as an important
discipline for academic research and industrial application. The large
size of biological data sets, inherent complexity of biological problems
and the ability to deal with error-prone data all result in large run-time
and memory requirements. This session will provide a forum for
discussion of latest research (from world-class speakers and reputed
researchers in the field) in developing high-performance computing
solutions to problems arising from molecular biology.&lt;/p&gt;
&lt;p&gt;&amp;lsquo;Life Sciences&amp;rsquo; span a whole set of research topics to better understand
how the components of complex living systems interact and give rise to
life and how their malfunction causes disease. The use of HPC is clearly
a must and future progress and development in this field is only
possible if supercomputers are heavily involved.&lt;/p&gt;
&lt;p&gt;Chair: &lt;strong&gt;David A. Bader&lt;/strong&gt;, Professor, Georgia Institute of Technology &amp;amp;
Matthias Rarey, Professor, University of Hamburg&lt;/p&gt;
&lt;p&gt;Come and join our &amp;lsquo;Life Sciences&amp;rsquo; session and all the other sessions at
ISC’10.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Computer Society Meritorious Service Award</title>
      <link>http://localhost:1313/blog/20100421-ieee-cs-meritorious/</link>
      <pubDate>Wed, 21 Apr 2010 12:00:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100421-ieee-cs-meritorious/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20100421-ieee-cs-meritorious/certificate_hu_de3021e166f5dbd9.webp 400w,
               /blog/20100421-ieee-cs-meritorious/certificate_hu_a34caf86f5ec592c.webp 760w,
               /blog/20100421-ieee-cs-meritorious/certificate_hu_369018d21436ac0a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20100421-ieee-cs-meritorious/certificate_hu_de3021e166f5dbd9.webp&#34;
               width=&#34;760&#34;
               height=&#34;587&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;IEEE Computer Society&lt;/p&gt;
&lt;h2 id=&#34;meritorious-service-award&#34;&gt;Meritorious Service Award&lt;/h2&gt;
&lt;p&gt;Presented to&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For service as General Chair for the 2010 International Parallel and Distributed Processing Symposium (IPDPS)&lt;/p&gt;
&lt;p&gt;James D. Isaak&lt;br&gt;
2010 President&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In 2010, &lt;strong&gt;David A. Bader&lt;/strong&gt; received the &lt;a href=&#34;https://www.computer.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Computer Society&lt;/a&gt; &lt;a href=&#34;https://www.computer.org/volunteering/awards/meritorious-service&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Meritorious Service Award&lt;/a&gt; for service as General Chair for the 2010 &lt;a href=&#34;http://www.ipdps.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Parallel and Distributed Processing Symposium (IPDPS)&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech launches institute for data and high performance computing</title>
      <link>http://localhost:1313/blog/20100309-idh/</link>
      <pubDate>Tue, 09 Mar 2010 12:56:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100309-idh/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Rick Smith&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Georgia Institute of Technology has launched the Georgia Tech Institute for Data and High Performance Computing (IDH) in recognition of the need to advance and coordinate institute research and education activities in this area, both in its application to key areas of science and engineering as well as in the advancement of the technology itself.&lt;/p&gt;
&lt;p&gt;“As we look to high performance computing to drive advanced breakthroughs in science, health, energy and other industries, leveraging Georgia Tech’s strongest assets – world class researchers in computing, experts across nearly every problem domain, and low barriers to collaboration – is what will set us apart,” said Dr. Mark Allen, Senior Vice Provost for Research and Innovation at Georgia Tech. “The creation of the Institute for Data and High Performance Computing provides the organizational foundation to harness our strategic capabilities and attack the most challenging problems that face society today.”&lt;/p&gt;
&lt;p&gt;A key mission of IDH will be to enhance Georgia Tech’s scientific contributions, reputation and impact, focusing on the exploitation of HPC technology coupled with the development of novel computational methods. The institute will promote the development of software and tools to enhance multidisciplinary research and enable discovery and innovation. In addition, it will work closely with the Office of Information Technology to ensure effective, faculty-driven governance concerning the acquisition and use of HPC resources on campus.&lt;/p&gt;
&lt;p&gt;The institute’s interim director will be Dr. Richard Fujimoto, Regents’ Professor and head of Computational Science &amp;amp; Engineering in the College of Computing. One important objective for Dr. Fujimoto will be to focus on developing new innovations in computational methods into useable tools and software to advance research in the application domain. Creating computational artifacts that provide value to application researchers and can be exported beyond the Tech campus provides a critical avenue to maximize the impact of Georgia Tech research innovations.&lt;/p&gt;
&lt;p&gt;“Georgia Tech has made substantial infrastructure and personnel investments in high performance computing, and achieved many important successes, over the last five years,” said Dr. Fujimoto. “I fully anticipate that IDH will enable us to advance beyond prototypes to new levels of accomplishment in the high performance computing area.”&lt;/p&gt;
&lt;p&gt;Key participants in the preliminary groundwork leading to the creation of the institute include &lt;strong&gt;Dr. David Bader&lt;/strong&gt;, Professor in the College of Computing, and Dr. Ron Hutchins, Chief Technology Officer for Georgia Tech. Their expertise and community-building activities helped bring together the various Georgia Tech constituencies with interests in the HPC area, and they will remain active players in IDH as it develops and grows.&lt;/p&gt;
&lt;p&gt;The Georgia Tech Institute for Data and High Performance Computing began operations March 1.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wraltechwire.com/2010/03/09/georgia-tech-launches-institute-for-data-and-high-performance-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wraltechwire.com/2010/03/09/georgia-tech-launches-institute-for-data-and-high-performance-computing/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Creates Institute for Data and High Performance Computing Research</title>
      <link>http://localhost:1313/blog/20100301-gtidh/</link>
      <pubDate>Mon, 01 Mar 2010 19:45:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100301-gtidh/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20100301-gtidh/image_hu_643cf40ac6c492dc.webp 400w,
               /blog/20100301-gtidh/image_hu_90f67cd36f38d674.webp 760w,
               /blog/20100301-gtidh/image_hu_de02f686a997d86a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20100301-gtidh/image_hu_643cf40ac6c492dc.webp&#34;
               width=&#34;134&#34;
               height=&#34;122&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Today the Georgia Institute of Technology Office of the Provost announced the formation of the Georgia Tech Institute for Data and High Performance Computing (IDH) in recognition of the need to advance and coordinate institute research and education activities in this area. High performance computing (HPC) continues to grow as a strategically important area for Georgia Tech, both in its application to key areas of science and engineering as well as in the advancement of the technology itself.&lt;/p&gt;
&lt;p&gt;“As we look to high performance computing to drive advanced breakthroughs in science, health, energy and other industries, leveraging Georgia Tech’s strongest assets – world class researchers in computing, experts across nearly every problem domain, and low barriers to collaboration – is what will set us apart,” said Dr. Mark Allen, Senior Vice Provost for Research and Innovation at Georgia Tech. “The creation of the Institute for Data and High Performance Computing provides the organizational foundation to harness our strategic capabilities and attack the most challenging problems that face society today.”&lt;/p&gt;
&lt;p&gt;A key mission of IDH will be to enhance Georgia Tech&amp;rsquo;s scientific contributions, reputation and impact, focusing on the exploitation of HPC technology coupled with the development of novel computational methods. The institute will promote the development of software and tools to enhance multidisciplinary research and enable discovery and innovation. In addition, it will work closely with the Office of Information Technology to ensure effective, faculty-driven governance concerning the acquisition and use of HPC resources on campus.&lt;/p&gt;
&lt;p&gt;The institute’s interim director will be Dr. Richard Fujimoto, Regents’ Professor and head of Computational Science &amp;amp; Engineering in the College of Computing. One important objective for Dr. Fujimoto will be to focus on developing new innovations in computational methods into useable tools and software to advance research in the application domain. Creating computational artifacts that provide value to application researchers and can be exported beyond the Tech campus provides a critical avenue to maximize the impact of Georgia Tech research innovations.&lt;/p&gt;
&lt;p&gt;“Georgia Tech has made substantial infrastructure and personnel investments in high performance computing, and achieved many important successes, over the last five years,” said Dr. Fujimoto. “I fully anticipate that IDH will enable us to advance beyond prototypes to new levels of accomplishment in the high performance computing area.”&lt;/p&gt;
&lt;p&gt;Key participants in the preliminary groundwork leading to the creation of the institute include Dr. &lt;strong&gt;David Bader&lt;/strong&gt;, Professor in the College of Computing, and Dr. Ron Hutchins, Chief Technology Officer for Georgia Tech. Their expertise and community-building activities helped bring together the various Georgia Tech constituencies with interests in the HPC area, and they will remain active players in IDH as it develops and grows.&lt;/p&gt;
&lt;p&gt;The Georgia Tech Institute for Data and High Performance Computing will begin operation immediately.&lt;/p&gt;
&lt;p&gt;For more information about Georgia Tech&amp;rsquo;s activities in HPC, visit &lt;a href=&#34;http://hpc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hpc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.news.gatech.edu/2010/02/26/georgia-tech-creates-institute-data-and-high-performance-computing-research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.news.gatech.edu/2010/02/26/georgia-tech-creates-institute-data-and-high-performance-computing-research&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Creates Institute for Data and High Performance Computing Research</title>
      <link>http://localhost:1313/blog/20100301-hpcwire/</link>
      <pubDate>Mon, 01 Mar 2010 19:38:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100301-hpcwire/</guid>
      <description>&lt;p&gt;Today the Georgia Institute of Technology Office of the Provost
announced the formation of the Georgia Tech Institute for Data and High Performance
Computing (IDH) in recognition of the need to advance and coordinate institute research
and education activities in this area. High performance computing (HPC) continues to grow
as a strategically important area for Georgia Tech, both in its application to key areas of
science and engineering as well as in the advancement of the technology itself.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;As we look to high performance computing to drive advanced breakthroughs in science,
health, energy and other industries, leveraging Georgia Tech&amp;rsquo;s strongest assets &amp;ndash; world-class
researchers in computing, experts across nearly every problem domain, and low barriers to
collaboration &amp;ndash; is what will set us apart,&amp;rdquo; said Dr. Mark Allen, senior vice provost for
Research and Innovation at Georgia Tech. &amp;ldquo;The creation of the Institute for Data and High
Performance Computing provides the organizational foundation to harness our strategic
capabilities and attack the most challenging problems that face society today.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A key mission of IDH will be to enhance Georgia Tech&amp;rsquo;s scientific contributions, reputation
and impact, focusing on the exploitation of HPC technology coupled with the development of
novel computational methods. The institute will promote the development of software and
tools to enhance multidisciplinary research and enable discovery and innovation. In
addition, it will work closely with the Office of Information Technology to ensure effective,
faculty-driven governance concerning the acquisition and use of HPC resources on campus.&lt;/p&gt;
&lt;p&gt;The institute&amp;rsquo;s interim director will be Dr. Richard Fujimoto, Regents&amp;rsquo; Professor and head of
Computational Science &amp;amp; Engineering in the College of Computing. One important objective
for Dr. Fujimoto will be to focus on developing new innovations in computational methods
into useable tools and software to advance research in the application domain. Creating
computational artifacts that provide value to application researchers and can be exported
beyond the Tech campus provides a critical avenue to maximize the impact of Georgia Tech
research innovations.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Georgia Tech has made substantial infrastructure and personnel investments in high
performance computing, and achieved many important successes, over the last five years,&amp;rdquo;
said Dr. Fujimoto. &amp;ldquo;I fully anticipate that IDH will enable us to advance beyond prototypes to
new levels of accomplishment in the high performance computing area.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Key participants in the preliminary groundwork leading to the creation of the institute
include Dr. &lt;strong&gt;David Bader&lt;/strong&gt;, Professor in the College of Computing, and Dr. Ron Hutchins, Chief
Technology Officer for Georgia Tech. Their expertise and community-building activities
helped bring together the various Georgia Tech constituencies with interests in the HPC
area, and they will remain active players in IDH as it develops and grows.&lt;/p&gt;
&lt;p&gt;The Georgia Tech Institute for Data and High Performance Computing will begin operation
immediately.&lt;/p&gt;
&lt;p&gt;For more information about Georgia Tech&amp;rsquo;s activities in HPC, visit &lt;a href=&#34;http://hpc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hpc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Find this article at:
&lt;a href=&#34;http://www.hpcwire.com/industry/academia/Georgia-Tech-Creates-Institute-for-Data-and-High-Performance-Computing-Research-85825822.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/industry/academia/Georgia-Tech-Creates-Institute-for-Data-and-High-Performance-Computing-Research-85825822.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ISC&#39;10 Announces Four Special Sessions</title>
      <link>http://localhost:1313/blog/20100217-hpcwire/</link>
      <pubDate>Wed, 17 Feb 2010 19:17:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100217-hpcwire/</guid>
      <description>&lt;p&gt;The 25th International Supercomputing Conference &amp;ndash; ISC&#39;10 &amp;ndash; the
leading HPC event in 2010, introduces four new special sessions to address some of the
biggest challenges in science, industry and technology today. The sessions on HPC
Computational Life Sciences, New Markets, Networking and Energy form the first part of the
ISC&#39;10 conference special sessions.&lt;/p&gt;
&lt;p&gt;The four-day conference and trade fair will once again be held at the Congress Center
Hamburg, Germany, from May 30 – June 3, 2010, bringing together some 2,000 IT
managers, researchers and enthusiasts to exchange knowledge and ideas, foster contacts and
do business. The complete program comprises educational tutorial sessions, a conference
and an exhibition consisting of 140 industrial and research exhibits. All conference
proceedings are conducted in English.&lt;/p&gt;
&lt;p&gt;The four new special sessions are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;High Performance Computational Life Sciences&lt;/strong&gt; – The Challenge for HPC Systems:
As computational life sciences become a strategic discipline at the frontier between
molecular biology and computer science, impacting medicine, biotechnology, as well as
society, this special session on Monday, May 31, provides a forum on latest research. The
session is chaired by Prof. Dr. &lt;strong&gt;David A. Bader&lt;/strong&gt;, Executive Director of High-Performance
Computing, Georgia Institute of Technology, USA, and Prof. Dr. Matthias Rarey, Managing
Director of the Center of Bioinformatics, University of Hamburg, Germany.&lt;/p&gt;
&lt;p&gt;Leading European and U.S. scientists will cover a broad spectrum of topics, from
macromolecular structures to bioinformatics infrastructure to pharmaceutical and medical
applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evolving New HPC Markets: China, Middle East &amp;amp; Russia&lt;/strong&gt;: Participants in this
Tuesday, June 1, session will learn more about the new evolving HPC markets that show
tremendous growth potential. Chaired by Dr. Happy Sithole, Director, Center for High
Performance Computing, South Africa; global and regional experts will look at various new
trends, for example, the entrance of China into the petascale arena, the installation of a very
large HPC system in the Middle-East/Saudi Arabia and the remarkable growth in the
number of installations in Russia and the upcoming installations in Africa.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;High Performance Computing &amp;amp; Networking:&lt;/strong&gt; This session will discuss the potential
of network innovations and virtualization technology in support of high-performance
computing applications. The session will also examine recent proposals for low-latency
communication, such as low-latency Ethernet. Taking place on Wednesday, June 2, this
session is chaired by Prof. Dr. Georg Carle, Professor for Network Architectures &amp;amp; Services,
Technische Universität München, Germany.&lt;/p&gt;
&lt;p&gt;Perspectives will be provided by cloud computing experts from the research and industrial
fields.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supercomputers for Modeling the Climate &amp;amp; the Roles of Energy Production in
Climate Change:&lt;/strong&gt; In this Thursday, June 3, session chaired by Prof. Dr. David Blaskovich,
Program Director for HPC in Government &amp;amp; Research, IBM, USA, three scientists will
discuss the use of HPC to simulate the role of energy production, i.e., carbon dioxide
emissions, in climate change, and the implications of tapping alternative energy sources. A
fourth presentation by the chairman will concentrate on the adaptation of alternative energy
production at a large data center as an example of the institutional use of non-polluting
energy to power large data centers. An overview of the ISC&#39;10 conference program can be
found at &lt;a href=&#34;http://www.supercomp.de/isc10/Program/Overview&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.supercomp.de/isc10/Program/Overview&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Advance registration is now open for ISC&#39;10 and in addition to reduced rates, a number of
one-day options for both the conference and exhibition are offered by the organizers. For
complete details on registration rates and passes overview, go to
&lt;a href=&#34;http://www.supercomp.de/isc10/Registration/Registration-Fees&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.supercomp.de/isc10/Registration/Registration-Fees&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;about-isc10&#34;&gt;About ISC&#39;10&lt;/h3&gt;
&lt;p&gt;Now in its 25th year, ISC is the world&amp;rsquo;s oldest and Europe&amp;rsquo;s most important conference for
the HPC community, offering a strong four-day technical program with a wide range of
expert speakers and exhibits from leading research centers and vendors. A number of events
complement the technical program, including Tutorials, Scientific Day, Birds of a Feather
(BoF) sessions, a research poster session, Exhibitor Forum and the popular &amp;ldquo;Hot Seat
Sessions&amp;rdquo; featuring leaders from industry and research centers. The conference has
experienced tremendous growth over the last few years, with an estimated 2,000
participants from around the world expected to convene in Hamburg in 2010.&lt;/p&gt;
&lt;p&gt;ISC&#39;10 is open to IT-decision makers, members of the HPC global community and other
interested parties. The ISC exhibition allows analysts, decision-makers from the automotive,
defense, aeronautical, gas &amp;amp; oil, banking and other industries; solution providers, data
storage suppliers, distributors, hardware and software manufacturers, the media, scientists
and universities to see and learn firsthand about new products, applications and
technological advances in the supercomputing industry today. For registration, conference
and exhibition information, visit &lt;a href=&#34;http://www.isc10.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isc10.org/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The supercomputer on your desktop</title>
      <link>http://localhost:1313/blog/20100216-computerworld/</link>
      <pubDate>Tue, 16 Feb 2010 12:26:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100216-computerworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Brandon&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;High-performance computing (HPC) has almost always required a supercomputer &amp;ndash; one of those room-size monoliths you find at government research labs and universities. And while those systems aren&amp;rsquo;t going away, some of the applications traditionally handled by the biggest of Big Iron are heading to the desktop.&lt;/p&gt;
&lt;p&gt;One reason is that processing that took an hour on a standard PC about eight years ago now takes six seconds, according to Ed Martin, a manager in the automotive unit at computer-aided design software maker Autodesk Inc. Monumental improvements in desktop processing power, graphics processing unit (GPU) performance, network bandwidth and solid-state drive speed combined with 64-bit throughput have made the desktop increasingly viable for large-scale computing projects.&lt;/p&gt;
&lt;p&gt;Thanks to those developments, a transition to &amp;ldquo;a supercomputer on your desk&amp;rdquo; is in full force.&lt;/p&gt;
&lt;p&gt;Earthquake simulations, nuclear-stockpile simulations and DNA research are staying put on traditional supercomputers for now. But as processor technology advances to multiple cores in the next 10 years, even those activities, or portions of them, could conceivably make their way to the desktop.&lt;/p&gt;
&lt;p&gt;n the meantime, here are some examples of high-performance applications that are already running on smaller computers.&lt;/p&gt;
&lt;h3 id=&#34;building-better-drugs-for-anesthesia&#34;&gt;Building better drugs for anesthesia&lt;/h3&gt;
&lt;p&gt;Today, doctors know how to administer anesthesia-inducing drugs, and they know the effects, but they do not actually know what the drugs&amp;rsquo; molecules are doing when the patient drifts off to sleep. This analysis requires intense computational power to see not only when the anesthetic enters the respiratory system, but also how it starts making changes.&lt;/p&gt;
&lt;p&gt;At Temple University, researchers have developed models that measure the effects of applying anesthesia on molecules within nerve cells. The models currently run on a supercomputer, but plans are underway to perform the calculations on an Nvidia GPU cluster with four nodes. This will both save money and give researchers more flexibility to conduct tests when they&amp;rsquo;re ready to do so (instead of having to wait for their scheduled time to use a supercomputer).&lt;/p&gt;
&lt;p&gt;In that scenario, each GPU has the computational power of a small HPC cluster. GPU calculations involve mathematical calculations on the scale of those normally used to, say, render pixels in a video game.&lt;/p&gt;
&lt;p&gt;Dr. Axel Kohlmeyer, a researcher on the project, says the best way to understand the simulation is to imagine a box filled with rubber balls, where each ball is a slightly different size and moves at a slightly different rate, interconnected with springs. Some springs are stronger or weaker than others, and some of the balls move faster or react differently. In the simulation, Kohlmeyer can follow the movements of all molecules to see the effects of anesthetics in the human body.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Groups of particles will form and go where they like to be as determined by the magnitude of their interactions,&amp;rdquo; says Kohlmeyer, explaining how the simulation evolves to the point where the interactions become balanced. Temperature variants produce vibrations and introduce new molecular activity. &amp;ldquo;The computational model is actually simple, but the challenge is you need so many millions of interactions. We do not want to just know the interactions at one point, but rather how they change over time.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Having to repeat the calculations very often is another part of the challenge, he adds.&lt;/p&gt;
&lt;p&gt;For Kohlmeyer, the goal is to discover when the condition of not feeling anything actually occurs in the human body. This could lead to the creation of new kinds of anesthetics or help doctors determine why problems such as memory loss can occur after surgery.&lt;/p&gt;
&lt;h3 id=&#34;bone-surgical-simulation&#34;&gt;Bone surgical simulation&lt;/h3&gt;
&lt;p&gt;Researchers at the Ohio Supercomputer Center (OSC) in Columbus, Ohio, have found that not every simulation requires a traditional supercomputer. Don Stredney, the director and interface lab research scientist for biomedical applications at OSC, found a limitation that&amp;rsquo;s common with supercomputers: Batch processes are static and run on a scheduled time frame. They cannot provide real-time interactions, so they can&amp;rsquo;t mimic a real surgical procedure. Desktop workstations that cost $6,000 to $10,000 allow his team to run simulations that show, in real-time, how a surgery changes a patient&amp;rsquo;s anatomy, he says.&lt;/p&gt;
&lt;p&gt;Stredney says his industry benefited from innovations in computer gaming because the standard consumer GPU became much more powerful, resulting in better realism at a much lower cost. Stredney says his researchers use commodity PCs running standard GPUs such as those from AMD&amp;rsquo;s ATI unit and Nvidia Corp., but not high-end GPU clusters. However, they find that when the data sets grow too large with some simulations, they need to return to the supercomputer.&lt;/p&gt;
&lt;p&gt;What drives Stredney&amp;rsquo;s group back to the supercomputer, he says, is the &amp;ldquo;exponentially increasing size of data sets, images in the gigabyte-per-slice range and multiscale data sets that are now routinely being acquired at half-terabyte levels.&amp;rdquo; Ever-larger data sets and the complex interaction required for real-time visual and auditory simulations &amp;ldquo;require more sophisticated systems,&amp;rdquo; he says.&lt;/p&gt;
&lt;h3 id=&#34;injection-molding-in-automotive-design&#34;&gt;Injection molding in automotive design&lt;/h3&gt;
&lt;p&gt;Injection-molding simulations are invaluable to car makers, Autodesk&amp;rsquo;s Martin says. Injection molding is a process for producing parts from plastic materials. Simulations show whether an injection mold &amp;ndash; such as a bumper, for instance &amp;ndash; will cause denting and how the mold will fit with other parts of the car. They also reveal any defects. Designers consider many variables: the temperature of the mold, its geometric shape and how the injection-mold process will work with certain materials.&lt;/p&gt;
&lt;p&gt;A single physical prototype of a fender can cost more than $1 million, Martin explains, so the better the simulation, the fewer prototypes that have to be built and the lower the production costs.&lt;/p&gt;
&lt;p&gt;Simulations &amp;ldquo;used to require a significant cluster-computing installation, but we are achieving the same level of power with current desktop computers,&amp;rdquo; says Martin, who says the desktop advances that played the biggest roles in making that possible were the move to multicore processing, the use of multiple GPUs and 64-bit throughput. Martin uses standard desktop computers that can be purchased at Wal-Mart, with the latest 3D-capable GPUs and Intel dual-core CPUs.&lt;/p&gt;
&lt;p&gt;The automotive industry is also using desktop software to model car designs. With such tools, the manufacturers can create complex renderings that rival &amp;ndash; or even outshine &amp;ndash; the graphics in the best video games. Interestingly, when these models are created on desktop PCs, they are often used in marketing literature and in TV commercials. Martin says it was possible to take the models created on supercomputers and transport them to PCs, but today&amp;rsquo;s PC-to-PC file transfers are simpler because the file formats are compatible across applications and the transfers take place over a standard network, making it possible for the images to reach a greater number of users.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example of modeling on the desktop: Designers often create car-paint models, examining the metallic flakes and how they look on certain plastics, or whether the paint looks dull in certain lighting conditions. The more accurate the models, the more process-intensive they are. In the past, an HPC environment was required because of the high number of polymers used in car paint &amp;ndash; around 8,000. Martin says modern desktops can now handle this kind of high-speed processing.&lt;/p&gt;
&lt;h3 id=&#34;web-based-computational-search&#34;&gt;Web-based computational search&lt;/h3&gt;
&lt;p&gt;One of the most interesting ways HPC is coming to the desktop is through the Web. The best example of this right now is on WolframAlpha, a &amp;ldquo;computational knowledge engine&amp;rdquo; on the Web that&amp;rsquo;s designed to, in the site&amp;rsquo;s own words, &amp;ldquo;collect and curate all objective data&amp;hellip; and make it possible to compute whatever can be computed about anything.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The sea change here is that, while the searches you launch at WolframAlpha.com are still conducted on a supercomputer, the results return almost instantly, right in your browser. For example, when you type the simple word moon, you trigger a complex calculation for the moon&amp;rsquo;s orbit relative to the Earth, along with average distances over a historical period.&lt;/p&gt;
&lt;p&gt;Schoeller Porter, an architect at Wolfram Research Inc., says the calculations can occur in near-real-time because of the decreased cost of HPC components. In the past, this kind of complex calculation would require setting up a batch request for a supercomputer that might take a few minutes or hours, or even a day. That wasn&amp;rsquo;t necessarily because the calculations themselves take a long time, but because the processing resources were so expensive.&lt;/p&gt;
&lt;p&gt;To test the yourself, type these calculation requests &amp;ndash; asking about the tides in Honolulu in six months &amp;ndash; into the WolframAlpha search engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;International space station jun 24&lt;/li&gt;
&lt;li&gt;skychart Timbuktu yesterday at 8:00pm&lt;/li&gt;
&lt;li&gt;NACA 2014 15 degrees&lt;/li&gt;
&lt;li&gt;tides in honolulu in 6 months&lt;/li&gt;
&lt;li&gt;y&amp;rsquo;&amp;rsquo; + sin y = x&lt;/li&gt;
&lt;li&gt;GATTAACCC&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When you run searches on WolframAlpha, you can change any variable at will from your desktop. You&amp;rsquo;ll notice that some of the queries take longer than others, depending on how long it takes the supercomputer to determine the result.&lt;/p&gt;
&lt;p&gt;While Google searches also rely on a supercomputer or cluster on the back end, they do not involve the same computational algorithms as WolframAlpha. Google searches for information and provides relevant links; WolframAlpha feeds the user actual information, not links, and that information may be pulled from a knowledge base or generated by calculations it performs.&lt;/p&gt;
&lt;h3 id=&#34;weather-forecasting&#34;&gt;Weather forecasting&lt;/h3&gt;
&lt;p&gt;Bill Magro, director of HPC software solutions at Intel Corp., says weather forecasting went through a major evolution in the past two decades, crossing a line from providing just a general idea of weather patterns to showing fine-grained details, such as the details of individual storm cells. This happened mostly because the immense data sets required for weather forecasting can now sit within the desktop&amp;rsquo;s memory, and the processing also can finally keep pace.&lt;/p&gt;
&lt;p&gt;Today, portions of weather simulations for meteorological use can run on desktop computers at local television stations, not just at the government agencies that monitor weather.&lt;/p&gt;
&lt;p&gt;Such modeling is extremely complicated because weather occurs in open 3D space and there are millions of variables for temperature, location, wind and other factors. Weather models take the spatial extent &amp;ndash; say, a 5-kilometer grid with one data point for each 5km in each direction &amp;ndash; and then &amp;ldquo;shrink it down to 1km and then down to 500 meters,&amp;rdquo; Magro explains. &amp;ldquo;Because you are shrinking in three dimensions, the computational requirement goes up by a factor of eight. It&amp;rsquo;s not that there is more data, but you are modeling with more detail. It really does come down to having enough RAM and enough computer power.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The desktop is already being used for weather simulations at some local TV stations. In the future, as processor technology advances even further, we could all run detailed personal weather forecasts on our PCs. Magro says desktop computers could be used by individuals to see models for their immediate area, or to forecast weather for vacations six months into the future, for example, based on the conditions they are experiencing and have entered into the model.&lt;/p&gt;
&lt;h3 id=&#34;more-apps-likely-to-follow&#34;&gt;More apps likely to follow&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, the executive director of high-performance computing at the Georgia Institute of Technology, envisions several additional uses for HPC on the desktop in the next 10 years.&lt;/p&gt;
&lt;p&gt;There are possibilities for discovering new trends among social networks or identifying their key influencers. Recommendations could be made for improving the energy efficiency of behaviors such as traffic routing to avoid congestion, scheduling computer applications to minimize energy usage, and monitoring a smart power grid, Bader says.&lt;/p&gt;
&lt;p&gt;Other possibilities include the construction of 3D scenes from massive collections of public photographs and live transcriptions of teleconferences with speaker IDs.&lt;/p&gt;
&lt;p&gt;Although experts agree that the largest supercomputers will be around for some time to come, it will be interesting to see what other traditional HPC applications will eventually come to reside on the desktop.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;John Brandon is a veteran of the computing industry, having worked as an IT manager for 10 years and as a tech journalist for another 10. He has written more than 2,500 feature articles and is a regular contributor to Computerworld.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2520774/the-supercomputer-on-your-desktop.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2520774/the-supercomputer-on-your-desktop.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A niche social network for researchers</title>
      <link>http://localhost:1313/blog/20100121-marketwatch/</link>
      <pubDate>Thu, 21 Jan 2010 21:44:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100121-marketwatch/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Therese Poletti, Columnist&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Entrepreneur Ijad Madisch initially wanted to be a virologist and studied how to stop certain viruses in their path.&lt;/p&gt;
&lt;p&gt;Now, the 29-year-old scientist, technologist and medical doctor is learning the positive aspects of possibly having a &amp;ldquo;viral&amp;rdquo; product on the Internet.&lt;/p&gt;
&lt;p&gt;Madisch is the founder of a niche social network called ResearchGATE. It&amp;rsquo;s like Facebook for researchers and scientists. And like Facebook, it has a clean look and it seems rather easy to use. It also mimics some of Facebook&amp;rsquo;s successful features. Since May 2008, the very specialized site has grown to 250,000 registered users.&lt;/p&gt;
&lt;p&gt;He launched the company in 2008 with an undisclosed amount of funding from an angel investor in Switzerland, Joachim Schoss and some professors and friends. Schoss is known for founding Scout24, an Internet auction site, and Myhandicap.com.&lt;/p&gt;
&lt;p&gt;Madisch, who hails from Germany, got both his PhD and MD from the Medical School of Hannover. He got the idea for ResearchGATE when he saw a friend quoting his research on his Facebook profile, which seemed like an odd place to be talking about serious scientific research.&lt;/p&gt;
&lt;p&gt;As a scientist who did his own research at Harvard University, also the stomping grounds of Facebook founder Mark Zuckerberg, he realized there could be more information sharing among researchers in specific fields. Sharing scientific methods that don&amp;rsquo;t work can also result in saving a lot of time, and can be done without disclosing too much about a particular proprietary project.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We waste a lot of time and money in research,&amp;rdquo; Madisch said. &amp;ldquo;Only the positive things are published. You don&amp;rsquo;t publish what did not work.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;He said that now, scientists are posting requests for help on certain problems daily on ResearchGATE. They can also find published literature quickly in their specific fields through a literature section. And like Facebook, you can see what papers or journals your researcher &amp;ldquo;friends&amp;rdquo; are reading. A status update feature is in the works, he said.&lt;/p&gt;
&lt;p&gt;The site also has a closed element as well, a quasi-research intranet for institutions or companies that want to stay within their own walls. ResearchGATE manages and administers those networks, but the companies or universities host their secure data themselves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor of computational science and engineering at Georgia Institute of Technology, said there are many new ways for researchers to virtually collaborate. He has not signed up for ResearchGATE yet.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I do participate with a number of research groups, but we often collaborate through private tools such as our own Wikis, and shared documents through GoogleDocs,&amp;rdquo; he said. Other tools for finding similar research, he said include Google Inc.&amp;rsquo;s GOOG, +0.72% main search engine and Google Scholar. &amp;ldquo;ResearchGate does offer a fantastic experience tailored towards the research community,&amp;rdquo; he added in an email.&lt;/p&gt;
&lt;p&gt;Madisch said the company is already generating some revenue. &amp;ldquo;We have several ways to make money without annoying the community,&amp;rdquo; he said. One is by posting jobs on a job board, where only companies are charged for placing a job. Currently, there are 1,102 jobs posted on ResearchGATE.&lt;/p&gt;
&lt;p&gt;Another revenue generator is through the managing of social networks for institutions. He hopes the Cambridge, Mass.-based company, which is very lean and currently employs 25 staffers, will be profitable sometime this year.&lt;/p&gt;
&lt;p&gt;Since its launch in 2008, the company, like Facebook, relied on feedback from its users and tried to morph along with demands and needs of its users.&lt;/p&gt;
&lt;p&gt;When asked if the site was in beta at launch, Madisch said, &amp;ldquo;Maybe it was even alpha.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Rivals have also been cropping up. The Nature Publishing Group, publishers of the well-regarded journal Nature, started Nature Network in 2007, a professional networking site for scientists. There are also a few others, including Academia.edu, which tries to answer the question, &amp;ldquo;who&amp;rsquo;s researching what&amp;rdquo; and also has many elements of Facebook.&lt;/p&gt;
&lt;p&gt;Madisch hopes to differentiate itself by offering diverse arenas of research clusters. The site has a wide range of research topics, from agricultural science, earth science, and geography science, to the most popular categories of biological sciences, computer sciences and health sciences.&lt;/p&gt;
&lt;p&gt;It will be interesting to see if these niche social networks can be sustainable businesses on their own, or if they will need to become a part of a larger business. Madish believes if his young company can get profitable, it can stand alone. He hopes ResarchGATE will ultimately have a broader impact on science, and help researchers get results faster.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.marketwatch.com/story/startup-develops-social-network-for-researchers-2010-01-21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.marketwatch.com/story/startup-develops-social-network-for-researchers-2010-01-21&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader elevated to IEEE Fellow</title>
      <link>http://localhost:1313/blog/20100101-ieee/</link>
      <pubDate>Fri, 01 Jan 2010 18:39:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100101-ieee/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20100101-ieee/certificate_hu_8f2871610315a766.webp 400w,
               /blog/20100101-ieee/certificate_hu_e341149538577bf6.webp 760w,
               /blog/20100101-ieee/certificate_hu_3e4b5484a161ea80.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20100101-ieee/certificate_hu_8f2871610315a766.webp&#34;
               width=&#34;760&#34;
               height=&#34;605&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;IEEE Certifies that &lt;strong&gt;David Albert Bader&lt;/strong&gt; has been elevated to the grade of Fellow for contributions to parallel algorithms for combinatorial problems and computational biology&lt;/p&gt;
&lt;p&gt;Pedro Ray&lt;br&gt;
President&lt;/p&gt;
&lt;p&gt;David G. Green&lt;br&gt;
Secretary&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Elected Program Director of SIAM Activity Group on Supercomputing</title>
      <link>http://localhost:1313/blog/20100101-siam/</link>
      <pubDate>Fri, 01 Jan 2010 16:00:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100101-siam/</guid>
      <description>&lt;h2 id=&#34;new-siag-officers--&#34;&gt;New SIAG officers -&lt;/h2&gt;
&lt;p&gt;The following individuals will begin two-year terms, starting January 1, 2010:&lt;/p&gt;
&lt;p&gt;The SIAM Activity Group on Supercomputing (SIAC/SC) -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cindy Phillips, Chair&lt;/li&gt;
&lt;li&gt;Boyanna Norris, Vice Chair&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, Program Director&lt;/li&gt;
&lt;li&gt;Rich Vuduc, Secretary&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives IEEE Computer Society Publications Board 2010 &#39;Sherriff&#39; Award</title>
      <link>http://localhost:1313/blog/20100101-ieeecs-pubs/</link>
      <pubDate>Fri, 01 Jan 2010 13:57:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20100101-ieeecs-pubs/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20100101-ieeecs-pubs/award_hu_6eafa4c922dd4bb0.webp 400w,
               /blog/20100101-ieeecs-pubs/award_hu_e2ea51f984514498.webp 760w,
               /blog/20100101-ieeecs-pubs/award_hu_8d84335aaf35ef06.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20100101-ieeecs-pubs/award_hu_6eafa4c922dd4bb0.webp&#34;
               width=&#34;570&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;David Alan Grier, IEEE Computer Soceity&amp;rsquo;s Vice President, Publications, presents &lt;strong&gt;David Bader&lt;/strong&gt; with the IEEE Computer Society Publications Board 2010 &amp;ldquo;Sherriff&amp;rdquo; Award.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GRAPPAling with evolutionary history</title>
      <link>http://localhost:1313/blog/20091216-isgtw/</link>
      <pubDate>Wed, 16 Dec 2009 09:50:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091216-isgtw/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Miriam Boon, iSGTW&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-this-figure-illustrates-how-gene-order-changes-among-the-eight-species-each-thin-line-represents-a-single-gene-and-its-position-in-the-different-species-most-genes-are-conserved-on-the-same-chromosomal-arm-or-muller-element-but-gene-order-is-shuffled-between-species-this-figure-appeared-in-the-july-2008-issue-of-genetics-image-courtesy-of-stephen-schaeffer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;This figure illustrates how gene order changes among the eight species. Each thin line represents a single gene and its position in the different species. Most genes are conserved on the same chromosomal arm or Muller element, but gene order is shuffled between species. This figure appeared in the July 2008 issue of Genetics. *Image courtesy of Stephen Schaeffer.*&#34; srcset=&#34;
               /blog/20091216-isgtw/NewIndex_front_hu_2cfb13c85df9324d.webp 400w,
               /blog/20091216-isgtw/NewIndex_front_hu_8cae099c3e323050.webp 760w,
               /blog/20091216-isgtw/NewIndex_front_hu_f4596ec42f8d4f79.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20091216-isgtw/NewIndex_front_hu_2cfb13c85df9324d.webp&#34;
               width=&#34;300&#34;
               height=&#34;192&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      This figure illustrates how gene order changes among the eight species. Each thin line represents a single gene and its position in the different species. Most genes are conserved on the same chromosomal arm or Muller element, but gene order is shuffled between species. This figure appeared in the July 2008 issue of Genetics. &lt;em&gt;Image courtesy of Stephen Schaeffer.&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We’ve known for several years
now that chimpanzees share 96
percent of our DNA. Our
technology tells us how closely
humans and chimps are related.
But it doesn’t tell us how we’re
related. We need new technology
for that.&lt;/p&gt;
&lt;p&gt;Enter &lt;em&gt;GRAPPA&lt;/em&gt; – or Genome
Rearrangements Analysis under
Parsimony and other Phylogenetic
Algorithms if you want a
mouthful. GRAPPA has already
been used to analyze the
evolution of organelles such as
chloroplasts and mitochondria,
running on cluster computers with
upwards of 500 processors. To
analyze more complex organisms,
however, the team that develops
GRAPPA will have to take the code
to an entirely new level – the petascale level.&lt;/p&gt;
&lt;p&gt;With the help of a $1 million grant from the National Science Foundation, computational science
researchers &lt;strong&gt;David Bader&lt;/strong&gt; of Georgia Tech and Jijun Tang of the University of Southern Carolina
have joined forces with genetics researcher Stephen Schaeffer, from Pennsylvania State
University, to do just that.&lt;/p&gt;
&lt;p&gt;“One of the first open science petascale systems will be the IBM BlueWaters resource
supercomputer,” Bader said. “Our goal is to scale GRAPPA up to use that magnificent
computer.”&lt;/p&gt;
&lt;p&gt;The trick is in the algorithms, according to Bader. “As an example, our first version of GRAPPA
eight years ago took an hour and a half to run a problem involving the chloroplasts of a dozen
species of bluebell flowers on a 512 processor linux cluster,” Bader said.&lt;/p&gt;


















&lt;figure  id=&#34;figure-a-tree-of-life-diagram-courtesy-of-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A &amp;#34;Tree of Life&amp;#34; diagram. *Courtesy of David Bader.*&#34; srcset=&#34;
               /blog/20091216-isgtw/ISGTW-091216_hu_2220afebf4d13ca1.webp 400w,
               /blog/20091216-isgtw/ISGTW-091216_hu_6df8f0095990cd1b.webp 760w,
               /blog/20091216-isgtw/ISGTW-091216_hu_711697f5e0e3734b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20091216-isgtw/ISGTW-091216_hu_2220afebf4d13ca1.webp&#34;
               width=&#34;350&#34;
               height=&#34;298&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A &amp;ldquo;Tree of Life&amp;rdquo; diagram. &lt;em&gt;Courtesy of David Bader.&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Since then, GRAPPA has
undergone a variety of
refinements. It shows. “Today
that same problem solved in a
more biologically meaningful
way takes less than five
minutes on my laptop,” Bader
said. “The biggest speed
improvement comes from
better algorithms.”&lt;/p&gt;
&lt;p&gt;Refining GRAPPA will require a
better take on both the science
and the programming, which
makes the interdisciplinary
team a perfect fit.&lt;/p&gt;
&lt;p&gt;Over the next four years, the
grant – which comes via the
American Recovery and
Reinvestment Act – will pay for
two graduate students at each
of the three participating
schools. Not only will they be creating invaluable tools for advancing science, but they’ll
also be learning very specialized skills in the process.&lt;/p&gt;
&lt;p&gt;“I think we’re the pioneers in large-scale evolutionary or reconstruction of evolutionary
trees,” Bader said. “We’re breaking new ground with each new algorithm and
implementation, where no one has studied before.”&lt;/p&gt;
&lt;p&gt;It isn’t clear how far these funds will take the team. Over the course of the grants
lifetime, they hope to continue to refine the code, doing test runs on prototypical
petascale systems. Perhaps they will be able to study fish. Or mammals. Or even
humans.&lt;/p&gt;
&lt;p&gt;Either way, a new and improved GRAPPA will be valuable in innumerable ways. Already,
it has been used to develop biochemical products, identify target drug receptors, create
safer pesticides, and study how viruses evolve in order to make better vaccines.&lt;/p&gt;
&lt;p&gt;“For instance, if you know that there’s a specific plant that has a property but that plant
is rare or difficult to find, you may be interested in what plants are phylogenetically
close or closely related in the family tree,” Bader said. “They may be more abundant or
easier to use to produce the right biochemical substance.”&lt;/p&gt;
&lt;p&gt;It is clear that Bader believes this is only the beginning of a new era. “If I look at other
communities in scientific computing, they’ve matured their methods and techniques over
the course of decades to centuries,” Bader said. “We’ve only known about the structure
of DNA for fifty years, and we’ve only had the ability to sequence full genomes in the
last several years. So we’re really at the infancy of what we see in the area of
understanding biological sciences through computational methods.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20091218072926/http://www.isgtw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.isgtw.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sciencenode.org/feature/feature-grappaling-evolutionary-history.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sciencenode.org/feature/feature-grappaling-evolutionary-history.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Five Georgia Tech Faculty Members Elected as IEEE Fellows</title>
      <link>http://localhost:1313/blog/20091214-ieee-fellow/</link>
      <pubDate>Mon, 14 Dec 2009 23:12:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091214-ieee-fellow/</guid>
      <description>&lt;p&gt;The Georgia Institute of Technology is one of only six U.S. universities to have five of its faculty members elevated to the rank of IEEE Fellow, the most at any academic institution in the United States. The five Georgia Tech faculty members promoted to IEEE Fellow, effective January 1, 2010, are &lt;strong&gt;David A. Bader&lt;/strong&gt;, Ian T. Ferguson, Richard A. Hartlein, David C. Keezer, and Emmanouil M. Tentzeris.&lt;/p&gt;
&lt;p&gt;Chosen by the &lt;a href=&#34;https://www.ieee.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE&lt;/a&gt; board of directors, the IEEE Fellows class of 2010 consists of 309 engineering professionals from around the world. IEEE is the world&amp;rsquo;s leading professional association for the advancement of technology, and the IEEE grade of Fellow is conferred by the board of directors upon a person with an extraordinary record of accomplishments in any of the IEEE fields of interest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, executive director for High-Performance Computing and professor in the Computational Science and Engineering Division of the College of Computing, was elevated to Fellow &amp;ldquo;for contributions to parallel algorithms for combinatorial problems and computational biology.&amp;rdquo; Bader serves as director of the Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor, which supports broadening the impact of the Cell Broadband Engine into multiple sectors and industries, including scientific computing, digital content creation, bioinformatics, finance, gaming, and entertainment. He serves on the Internet2 Research Advisory Council and is the general chair for the 24th IEEE International Parallel and Distributed Processing Symposium, to be held April 19-23, 2010 in Atlanta. An associate editor for the ACM Journal of Experimental Algorithmics, IEEE DSOnline, and Parallel Computing, Bader has published over 100 articles in refereed journals and conferences and recently finished two consecutive terms as associate editor for IEEE Transactions on Parallel and Distributed Systems. He also served as chair of the IEEE Computer Society&amp;rsquo;s Technical Committee on Parallel Processing from July 2003-June 2007.&lt;/p&gt;
&lt;p&gt;Ian T. Ferguson, an adjunct professor in the School of Electrical and Computer Engineering (ECE), was elected to Fellow &amp;ldquo;for the development of semiconductor materials and devices for infrared and ultraviolet sensor applications.&amp;rdquo; Ferguson became chair of the Department of ECE at the University of North Carolina at Charlotte in August 2009, after an eight-year tenure as a professor with the School of ECE at Georgia Tech. His research focuses on the development of wide band-gap materials and devices using gallium nitride and zinc oxide and using these materials for illumination, solar power, spintronic, and nuclear detection applications. While at Tech, Ferguson was named a Faculty Fellow in the Sam Nunn Security Program, based in the Center for International Strategy, Technology, and Policy, for his work in the Georgia Tech Focused Research Program in Pioneer Research in Nuclear Detection. He is the founder of the International Conference on Solid State Lighting and is the author of more than 240 refereed journal and conference publications. Ferguson is also a Fellow of SPIE, the international society for optics and photonics.&lt;/p&gt;
&lt;p&gt;Richard A. Hartlein, director and management board chair for the National Electric Energy Testing Research and Applications Center (NEETRAC), was elected to Fellow &amp;ldquo;for contributions to standards and analytical techniques for underground power cable systems.&amp;rdquo; Hartlein works with the NEETRAC team and over 30 NEETRAC industrial members that provide nearly 60 percent of the electricity used in the United States to solve problems in electric energy transmission and distribution. His research focuses on underground power cable systems, including the development of qualification test programs for distribution and transmission cable systems; design and evaluation of efficient, durable, and economical power cables; investigation of cable system failures; and cable systems operations under extreme conditions. Hartlein has served as chair of the IEEE Insulated Conductors Committee (ICC), and led various ICC working groups in the development of guides and standards related to the testing and evaluation of underground cable system components.&lt;/p&gt;
&lt;p&gt;David C. Keezer, professor in the School of ECE, was elevated to Fellow &amp;ldquo;for contributions to high-speed digital test technology.&amp;rdquo; His areas of research and educational interests are in test methods for high performance electronic systems, design of high speed logic systems, and advanced electronics packaging methods. Keezer leads the High-Speed Test Laboratory, where he and his team develop new high-performance automated test methods and instrumentation. The lab extends automated test capability above 10 Gbps by leveraging state-of-the-art field programmable gate arrays, microelectromechanical systems, gallium arsenide, and indium phosphide technologies. An associate editor for IEEE Transactions on Electronics Packaging Manufacturing, Keezer served on the technical program committee for the IEEE International Test Conference for over 10 years and was the general and technical program chair for the IEEE GHz/Gbps Test Workshop. He has published over 140 articles in refereed journals and conferences and is an IEEE Computer Society Golden Core Member, an honor awarded to longstanding members who have provided outstanding service to the organization.&lt;/p&gt;
&lt;p&gt;Emmanouil M. Tentzeris, professor in the School of ECE, was promoted to Fellow &amp;ldquo;for contributions to three-dimensional conformal integrated devices for wireless communications and sensing.&amp;rdquo; Tentzeris currently serves as associate director for radio frequency identification (RFID)/sensors research at the Georgia Electronic Design Center, which develops high-speed communications technologies, and was the associate director for RF research and RF alliance leader for the Microsystems Packaging Research Center from 2003-2006. The author of over 320 refereed journal and conference papers, three books, and 17 book chapters, Tentzeris founded and leads a new IEEE Microwave Theory and Techniques Society (MTT-S) technical committee on RFID technologies. He is an IEEE MTT-S Distinguished Lecturer for 2010-2012 and has held numerous leadership roles in several IEEE societies, conferences, and workshops. Tentzeris is the associate editor for IEEE Transactions on Advanced Packaging, IEEE Transactions on Microwave Theory and Techniques, and the International Journal on Antennas and Propagation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Does the Air Force Want Thousands of PlayStations?</title>
      <link>http://localhost:1313/blog/20091208-abcnews/</link>
      <pubDate>Tue, 08 Dec 2009 20:42:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091208-abcnews/</guid>
      <description>&lt;p&gt;By KI MAE HEUSSNER&lt;/p&gt;
&lt;p&gt;Guess what&amp;rsquo;s on the U.S. Air Force&amp;rsquo;s wish list this holiday season.&lt;/p&gt;
&lt;p&gt;Sony&amp;rsquo;s popular PlayStation 3 gaming console. Thousands of them.&lt;/p&gt;
&lt;p&gt;The Air Force Research Laboratory in Rome, N.Y., recently issued a request for proposal indicating its
intention to purchase 2,200 PlayStation 3 (PS3) consoles.&lt;/p&gt;
&lt;p&gt;But the military researchers don&amp;rsquo;t plan to play &amp;ldquo;Call of Duty: Modern Warfare 2&amp;rdquo; or any of the season&amp;rsquo;s
other blockbuster games. They plan to string the consoles together into a massive supercomputer and
study how well they can enhance the military&amp;rsquo;s high-performance computing systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PS3s Offer High Performance at a Good Price&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The PS3s offer some outstanding performance for the price,&amp;rdquo; said Richard Linderman, senior scientist
for advanced computing architectures at the Air Force Research Laboratory. &amp;ldquo;It&amp;rsquo;s an opportunity to
leverage the large gaming market and get those kinds of cost efficiencies which are more along the lines
of high-performance computing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;What makes the PlayStation so interesting to Linderman and his bargain-hunting colleagues is the PS3&amp;rsquo;s
mega-powerful Cell processor, which was created jointly by IBM, Sony and Toshiba.&lt;/p&gt;
&lt;p&gt;According to a document accompanying the Air Force RFP, a server configured with two 3.2GHz cell
processors can cost up to $8,000, while two Sony PS3s cost just a fraction of that price at about $600.
The two cell processors are about 33 percent more powerful than the PS3s, but the document went on to
say that the PS3s are still more cost-effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Researchers Across the Country Harness Power of PlayStation 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In May 2008, the Air Force acquired 336 PlayStation 3 consoles, loaded them on to large metal bread
racks and linked them together in its first experimental cluster.&lt;/p&gt;
&lt;p&gt;Once the researchers configured the hardware, they installed the Linux operating system on them,
turning the gaming consoles into a military-grade supercomputer.&lt;/p&gt;
&lt;p&gt;Linderman said they&amp;rsquo;ve found the cluster&amp;rsquo;s performance to be &amp;ldquo;excellent.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s not marketing hype,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;Linderman said their first PS3 cluster was used in applications such as high-definition video processing
and &amp;ldquo;neuromorphic&amp;rdquo; computing, which mimics the way the human brain perceives and processes images
and information. When the new cluster of 2,200 PS3 consoles arrive in the next month or so, he said
they will likely be used for similar projects.&lt;/p&gt;
&lt;p&gt;But the Air Force researchers aren&amp;rsquo;t the only ones to harness the power of the PlayStation consoles.
From coast to coast, academic and military computer scientists are stringing the consoles together in
various projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PS3 Cluster Research Includes Aircraft Monitoring and Fincial Risk Assessment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor and executive director of high performance computing at the Georgia Institute
of Technology, has been involved in a number of projects involving PlayStation clusters.&lt;/p&gt;
&lt;p&gt;When the PlayStation launched in 2006, he said, its processor far surpassed those of its generation.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Sony wanted a processor that they could use inside a game box that would be able to render the games
but also incorporate real-world physics, emotion and really new aspects to game playing,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;The same chip that enabled high-octane game play also powered Toshiba&amp;rsquo;s high-end HD TVs and
technology created by IBM for oil and gas exploration.&lt;/p&gt;
&lt;p&gt;At Georgia Tech, Bader has researched the possibility of using PS3 clusters in aircraft monitoring and
financial risk assessment.&lt;/p&gt;
&lt;p&gt;One project proposed using PlayStation 3 consoles on board commercial airplanes, he said. Consoles
would not only provide in-flight entertainment for each passenger, but also serve as sensors around the
aircraft that would alert the pilot to potential problems and failures.&lt;/p&gt;
&lt;p&gt;Astrophysicists at the University of Massachusetts at Dartmouth are using a cluster of PS3 consoles to
research gravitational waves and black holes.&lt;/p&gt;
&lt;p&gt;And even the U.S. Immigration and Customs Enforcement agency&amp;rsquo;s Cyber Crimes Center has used
linked PS3s to solve Internet crimes.&lt;/p&gt;
&lt;p&gt;Copyright © 2009 ABC News Internet Ventures&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abcnews.go.com/print?id=9272180&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abcnews.go.com/print?id=9272180&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS Members Elevated to Fellows</title>
      <link>http://localhost:1313/blog/20091202-ieee-cs-fellow/</link>
      <pubDate>Wed, 02 Dec 2009 08:27:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091202-ieee-cs-fellow/</guid>
      <description>&lt;p&gt;Seventy IEEE Computer Society members will be elevated to IEEE Fellow grade in 2010. The grade of Fellow recognizes unusual distinction in the profession.&lt;/p&gt;
&lt;p&gt;The IEEE Board of Directors elevated 309 members to Fellow status. Computer Society members recommended for Fellow status in 2010 include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Tinku Acharya&lt;/li&gt;
&lt;li&gt;Raj Acharya&lt;/li&gt;
&lt;li&gt;Charu Aggarwal&lt;/li&gt;
&lt;li&gt;Srinivas Aluru&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Grady Booch&lt;/li&gt;
&lt;li&gt;Athman Bouguettaya&lt;/li&gt;
&lt;li&gt;Lionel Briand&lt;/li&gt;
&lt;li&gt;Douglas Burger&lt;/li&gt;
&lt;li&gt;Srimat Chakradhar&lt;/li&gt;
&lt;li&gt;Stefano Chiaverini&lt;/li&gt;
&lt;li&gt;Thomas Cloonan&lt;/li&gt;
&lt;li&gt;Laurent Cohen&lt;/li&gt;
&lt;li&gt;Ray Dolby&lt;/li&gt;
&lt;li&gt;Ahmed El-Magarmid&lt;/li&gt;
&lt;li&gt;Elmootazbellah Elnozahy&lt;/li&gt;
&lt;li&gt;Mário Figueiredo&lt;/li&gt;
&lt;li&gt;William Gropp&lt;/li&gt;
&lt;li&gt;Baining Guo&lt;/li&gt;
&lt;li&gt;Richard Hartley&lt;/li&gt;
&lt;li&gt;Yutaka Hata&lt;/li&gt;
&lt;li&gt;Joseph Hellerstein&lt;/li&gt;
&lt;li&gt;James Hendler&lt;/li&gt;
&lt;li&gt;John Impagliazzo&lt;/li&gt;
&lt;li&gt;Yannis Ioannidis&lt;/li&gt;
&lt;li&gt;Dimitrios Ioannou&lt;/li&gt;
&lt;li&gt;David Kaeli&lt;/li&gt;
&lt;li&gt;Andrew Kahng&lt;/li&gt;
&lt;li&gt;Matti Karjalainen&lt;/li&gt;
&lt;li&gt;Nikola Kasabov&lt;/li&gt;
&lt;li&gt;David Keezer&lt;/li&gt;
&lt;li&gt;Fanny Klett&lt;/li&gt;
&lt;li&gt;Andrew Laine&lt;/li&gt;
&lt;li&gt;Kwei-Jay Lin&lt;/li&gt;
&lt;li&gt;Chih-Min (Jimmy) Lin&lt;/li&gt;
&lt;li&gt;John C.S. Lui&lt;/li&gt;
&lt;li&gt;Kevin Lynch&lt;/li&gt;
&lt;li&gt;Margaret Martonosi&lt;/li&gt;
&lt;li&gt;Peter Marwedel&lt;/li&gt;
&lt;li&gt;Peter Maxwell&lt;/li&gt;
&lt;li&gt;Mark Maybury&lt;/li&gt;
&lt;li&gt;Dejan Milojicic&lt;/li&gt;
&lt;li&gt;Joseph Mitola&lt;/li&gt;
&lt;li&gt;Prasant Mohapatra&lt;/li&gt;
&lt;li&gt;Eliot Moss&lt;/li&gt;
&lt;li&gt;Robin Murphy&lt;/li&gt;
&lt;li&gt;Ashwini Nanda&lt;/li&gt;
&lt;li&gt;Lynne Parker&lt;/li&gt;
&lt;li&gt;Larry Peterson&lt;/li&gt;
&lt;li&gt;Jonathon Phillips&lt;/li&gt;
&lt;li&gt;Keshav Pingali&lt;/li&gt;
&lt;li&gt;Chunming Qiao&lt;/li&gt;
&lt;li&gt;Long Quan&lt;/li&gt;
&lt;li&gt;Anand Raghunathan&lt;/li&gt;
&lt;li&gt;Al Reddy&lt;/li&gt;
&lt;li&gt;Michael Scott&lt;/li&gt;
&lt;li&gt;Timoleon Sellis&lt;/li&gt;
&lt;li&gt;Malcolm Slaney&lt;/li&gt;
&lt;li&gt;Asim Smailagic&lt;/li&gt;
&lt;li&gt;Aravind Srinivasan&lt;/li&gt;
&lt;li&gt;Heinrich Stuttgen&lt;/li&gt;
&lt;li&gt;Jie Tian&lt;/li&gt;
&lt;li&gt;Nian-Feng Tzeng&lt;/li&gt;
&lt;li&gt;Nitin Vaidya&lt;/li&gt;
&lt;li&gt;Amitabh Varshney&lt;/li&gt;
&lt;li&gt;Yi-Min Wang&lt;/li&gt;
&lt;li&gt;Shumpei Yamazaki&lt;/li&gt;
&lt;li&gt;Qing Yang&lt;/li&gt;
&lt;li&gt;Feng Zhao&lt;/li&gt;
&lt;li&gt;Xinhua Zhuang&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Board of Directors confers the title of Fellow upon a person of outstanding and extraordinary qualifications and experience in IEEE-designated fields, who has made important individual contributions to one or more of those fields. For more information online visit &lt;a href=&#34;http://www.ieee.org/fellowprogram&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ieee.org/fellowprogram&lt;/a&gt;. To view the full list of Computer Society Fellows, go to &lt;a href=&#34;http://www.ieee.org/web/membership/fellows/Societies/COMP.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ieee.org/web/membership/fellows/Societies/COMP.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At the time the nomination is submitted, a nominee must:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have accomplishments that have contributed importantly to the advancement or application of engineering, science and technology, bringing the realization of significant value to society;&lt;/li&gt;
&lt;li&gt;hold Senior Member or Life Senior Member grade at the time the nomination is submitted;&lt;/li&gt;
&lt;li&gt;have been a member in good standing in any grade for a period of five years or more preceding 1 January of the year of elevation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The nominee cannot be a member of the IEEE Fellow Committee, an IEEE Society/Technical Council Fellow Evaluating Committee Chair, or a member of IEEE Society/Technical Council Fellow Evaluating Committees reviewing the nomination.&lt;/p&gt;
&lt;h3 id=&#34;about-the-ieee-computer-society&#34;&gt;About the IEEE Computer Society&lt;/h3&gt;
&lt;p&gt;With nearly 85,000 members, the IEEE Computer Society is the world’s leading organization of computing professionals. Founded in 1946, and the largest of the 39 societies of the Institute of Electrical and Electronics Engineers (IEEE), the Computer Society is dedicated to advancing the theory and application of computer and information-processing technology. The Society serves the information and career-development needs of today’s computing researchers and practitioners with technical journals, magazines, conferences, books, conference publications, certifications, and online courses.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20091206201750/https://www.computer.org/portal/web/pressroom/2009/fellows&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computer.org/portal/web/pressroom/2009/fellows&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nuke labs show the future of hybrid computing</title>
      <link>http://localhost:1313/blog/20091119-register/</link>
      <pubDate>Thu, 19 Nov 2009 08:32:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091119-register/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Timothy Prickett Morgan&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Hybrid Multicore Consortium is on a mission that perhaps all of computing - on the desktop and in the data center - will one day embark on: making hybrid computing architectures as easy to program and use as monolithic platforms have been.&lt;/p&gt;
&lt;p&gt;There is a growing consensus - but by no means a complete one - that the future of energy-efficient and yet powerful systems will be based on the coupling of general purpose, multicore CPUs with various kinds of co-processors that also have hundreds of cores to do specific kinds of accelerations needed by particular applications. The trouble with these hybrid computing architectures, which can bring a lot of flops to bear, is that even the smartest people in the world complain about how hard it is to program them.&lt;/p&gt;
&lt;p&gt;That is why the Oak Ridge, Lawrence Berkeley, and Los Alamos national laboratories and the &lt;strong&gt;Georgia Institute of Technology&lt;/strong&gt; in the United States and the Swiss Federal Institute of Technology in Zurich have banded together to create the Hybrid Multicore Consortium, which is open to members of the HPC, server, accelerator, and software development communities.&lt;/p&gt;
&lt;p&gt;According to Jeff Nichols, who runs the Oak Ridge lab, the idea behind the consortium is to mimic the sharing of ideas that the HPC community did in the wake of IBM&amp;rsquo;s BlueGene massively parallel Linux-on-Power systems, which required a different kind of programming model from earlier federated RISC/Unix machines or x64/Linux clusters to squeeze the most performance out of them.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We believe that the development of code for these hybrid architectures will need a lift, and we cannot do it by ourselves,&amp;rdquo; Nichols said in announcing the consortium at SC09 supercomputer conference in Portland, Oregon, and referring to the massive amount of brainpower that the propellerheads in the major supercomputing centers of the world have.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Roadrunner is the first hybrid multicore system of any significant size,&amp;rdquo; Nichols said, referring to the hybrid Opteron-Cell blade cluster rated at just above 1 petaflops built by IBM that is installed at the Los Alamos lab. &amp;ldquo;The challenge is significant, and the programming model to get performance on these machines is going to have to be different.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Andy White, who heads up Los Alamos and who babysits Roadrunner, nodded his head and smiled wryly at that sentiment. &amp;ldquo;When we began thinking about this in 2002, this was something of a bold vision,&amp;rdquo; White explained. &amp;ldquo;We are excited about helping mainstream this technology.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;White said that he and Horst Simon, the director of the Berkeley Lab, have a running bet about when all of 50 machines at the top of the Top 500 supercomputer rankings will be hybrid architectures like the one embodied in Roadrunner (but certainly based on different technologies). Simon says it will happen in 2014, White in 2018. All of the players in the Hybrid Multicore Consortium are betting that the ramp to exaflops of computing power - that&amp;rsquo;s 1,000 times the oomph of Roadrunner - is going to require hybrid architectures.&lt;/p&gt;
&lt;p&gt;Maybe the real bet is when all HPC gear (and quite possibly all servers and all PCs) will be sold with a variety of co-processors, whether they are embedded on CPU chips or on off-chip co-processors. With the advance of Moore&amp;rsquo;s Law, it will be possible to embed all kinds of co-processors as well as memory and switching onto chips. While that electronic integration will solve plenty of signaling and thermal issues, the programming nightmares will remain.&lt;/p&gt;
&lt;p&gt;Nichols said that the Hybrid Multicore Consortium will wield a kind of soft power. It is not funding a specific set of technologies, and indeed, it will not be directly involved with any kind of funding. But the expectation is that hybrid architectures will come into being through the normal government and industry budgeting process and that the consortium can be a kind of clearinghouse for the ideas that work and the ideas that don&amp;rsquo;t. What they are looking to establish is what Nichols called &amp;ldquo;evolutionary co-design.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are all doing this, and we are all dealing with a half dozen different design approaches,&amp;rdquo; said Simon. &amp;ldquo;We need to work as a group and communicate to the vendors what we like and what we don&amp;rsquo;t like.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In addition to the supercomputing labs mentioned above, Advanced Micro Devices, Cray, Convey, Hewlett-Packard, Intel, Micron, Nvidia, Tilera, and Xilinx have all expressed interest in joining the consortium.&lt;/p&gt;
&lt;p&gt;While the Hybrid Multicore Consortium is clearly something that the HPC community needs, perhaps a more rigorous approach to standards and some direct government funding for the development of hybrid computing architectures is in order. Uncle Sam ponied up big cash in the 1980s to keep the semiconductor industry alive and indigenous and has pumped huge sums of money into supercomputing centers in the 1990s and 2000s to keep HPC a national priority.&lt;/p&gt;
&lt;p&gt;The consortium needs hard power rather than soft power to compel specific investments to discover the hybrid hardware and software technologies that work and to support these in the market. We all stand to benefit from the innovation or at least that is theme of all the talk about exascale computing here at SC09.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theregister.co.uk/2009/11/19/nuke_lab_hybrid_consortium/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.theregister.co.uk/2009/11/19/nuke_lab_hybrid_consortium/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Awards $1M to Support Petascale Computational Tools for Genome Rearrangement Research</title>
      <link>http://localhost:1313/blog/20091118-genomeweb/</link>
      <pubDate>Wed, 18 Nov 2009 07:53:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091118-genomeweb/</guid>
      <description>&lt;p&gt;The National Science Foundation has funded three research teams under a four-year
$1 million project to help ensure that certain bioinformatics tools are compatible with
upcoming petascale computers.&lt;/p&gt;
&lt;p&gt;NSF awarded the grants under the American Recovery and Reinvestment Act to support
the development of algorithms that will infer evolutionary relationships from genomic
rearrangement events — a task that could take &amp;ldquo;centuries&amp;rdquo; to analyze on today&amp;rsquo;s fastest
parallel computers, according to the agency.&lt;/p&gt;
&lt;p&gt;As a result, these algorithms will be designed to run on machines that can process more
than a thousand trillion calculations per second. Currently, only two such systems are
installed in the world, according to the latest &amp;ldquo;Top500&amp;rdquo; ranking of the world&amp;rsquo;s fastest
supercomputers — Cray&amp;rsquo;s &amp;ldquo;Jaguar,&amp;rdquo; located at the Department of Energy&amp;rsquo;s Oak Ridge
Leadership Computing Facility, which clocked in at 1.75 petaflops per second; and IBM&amp;rsquo;s
&amp;ldquo;Roadrunner&amp;rdquo; system at Los Alamos National Laboratory, which recorded a performance
of 1.04 petaflops per second.&lt;/p&gt;
&lt;p&gt;NSF awarded the grants to &lt;strong&gt;David Bader&lt;/strong&gt; from the Georgia Institute of Technology, Jijun
Tang of the University of South Carolina; and Stephen Schaeffer at Pennsylvania State
University.&lt;/p&gt;
&lt;p&gt;According to the NSF awards database, Bader received a grant for $400,000, Schaeffer
received $275,000, and Tang&amp;rsquo;s team received $324,968.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Genome sequences are now available for many organisms, but making &amp;lsquo;biological sense&amp;rsquo;
of the available genomic data requires high-performance computing methods and an
evolutionary perspective, whether scientists are trying to understand how genes of new
functions arise, why genes are organized as they are in chromosomes, or why these
arrangements are subject to change,&amp;rdquo; Bader said in a statement.&lt;/p&gt;
&lt;p&gt;According to the grant abstract, the project to develop new algorithms and highperformance
software for data analysis will allow these problems to &amp;ldquo;be addressed at
scale&amp;rdquo; for the first time.&lt;/p&gt;
&lt;p&gt;Fruit fly genomes will be the &amp;ldquo;primary source of data to assess models and methods
developed.&amp;rdquo; This effort leverages past work on genome rearrangement analysis through
parsimony and other phylogenetic algorithms.&lt;/p&gt;
&lt;p&gt;Specifically, the starting point for the new algorithms will be Bader&amp;rsquo;s Genome
Rearrangements Analysis under Parsimony and other Phylogenetic Algorithm, or
GRAPPA, an open-source package for breakpoint analysis initially released in 2000,
which is able to reconstruct the evolutionary relatedness among species.&lt;/p&gt;
&lt;p&gt;Fruit flies are good models for studying rearrangement &amp;ldquo;because the genome sizes are
relatively small for animals, the mechanism that alters gene order is reasonably well
understood, and the evolutionary relationships among the 12 sequenced genomes are
known,&amp;rdquo; Schaeffer said in a statement.&lt;/p&gt;
&lt;p&gt;The scientists said that the fruit fly gene order diversity results can be extended for
research on mammalian genomes.&lt;/p&gt;
&lt;p&gt;The NSF program to accelerate scientific discovery and engineering through petascale
simulations and analysis, called PetaApps for short, was launched in 2007 by the NSF&amp;rsquo;s
Office of Cyberinfrastructure.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.genomeweb.com/print/927978/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.genomeweb.com/print/927978/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Funds Genomics Petascale Project</title>
      <link>http://localhost:1313/blog/20091117-petaapps/</link>
      <pubDate>Tue, 17 Nov 2009 22:24:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091117-petaapps/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Matthew Dublin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A team of researchers led by Georgia Tech&amp;rsquo;s &lt;strong&gt;David Bader&lt;/strong&gt; are poised to bring genomic
evolution to petascale heights. Equipped with a four-year $1M grant courtesy of the
National Science Foundation&amp;rsquo;s PetaApps program, Bader and his team are setting their
sights on developing algorithms to take advantage of petascale computing platforms for
studying genome rearrangement events. Bader says that they will be writing their own
open source software development framework from scratch and eventually releasing it
through the GNU GPU license on Sourceforge. The team says they will start off the peta
project by ramping up GRAPPA, an open-source application that reconstructs the
evolutionary relatedness among species first released by Bader in 2000, to take
advantage of petascale computers. &amp;ldquo;GRAPPA is currently the most accurate method for
determining genome rearrangement, but it has only been applied to small genomes with
simple events because of the limitation of the algorithms and the lack of computational
power,&amp;rdquo; stated Bader in an announcement. The teamed used the latest version GRAPPA
to analyze a dataset of a dozen bellflower genomes to determin the flower&amp;rsquo;s evolutionary
relatedness one billion times faster than the original, non-parallelized version.&lt;/p&gt;
&lt;p&gt;Worth checking out is Bader&amp;rsquo;s 2007 book entitled &amp;ldquo;&lt;em&gt;Petascale Computing: Algorithms and
Applications&lt;/em&gt;,&amp;rdquo; which covers possible applications for petascale systems, including
molecular dynamics and biomolecules, as well as multithreaded algorithm design,
performance analysis, Charm++, and the Cactus framework.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research#.XV3-huhKjIU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research#.XV3-huhKjIU&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Funds Petascale Algorithms for Genomic Relatedness Research</title>
      <link>http://localhost:1313/blog/20091117-petaapps-full/</link>
      <pubDate>Tue, 17 Nov 2009 22:24:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091117-petaapps-full/</guid>
      <description>&lt;p&gt;Scientists at three universities will use funding from the American Recovery and Reinvestment Act to develop computational biology tools that researchers will use with next-generation computers to study genomic evolution, according to Georgia Tech.&lt;/p&gt;
&lt;p&gt;The $1 million grant from the National Science Foundation&amp;rsquo;s PetaApps program, which funds development of computer technologies for petascale machines that can conduct trillions of calculations per second, will include Georgia Tech, the University of South Carolina, and Pennsylvania State University.&lt;/p&gt;
&lt;p&gt;These researchers will develop new algorithms in an open-source software framework that will use parallel, petascale computing to study ancestral genomics in an open source code called Genome Rearrangements Analysis under Parsimony and other Phylogenetic Algorithms (GRAPPA).&lt;/p&gt;
&lt;p&gt;&amp;ldquo;GRAPPA is currently the most accurate method for determining genome rearrangement, but it has only been applied to small genomes with simple events because of the limitation of the algorithms and the lack of computational power,&amp;rdquo; explained &lt;strong&gt;David Bader&lt;/strong&gt;, a lead investigator on the grant and executive director of high-performance computing at Georgia Tech&amp;rsquo;s College of Computing.&lt;/p&gt;
&lt;p&gt;GRAPPA was recently used to determine the evolutionary relation of a dozen bellflower genomes one billion times faster than a method that did not use parallel processing or optimization.&lt;/p&gt;
&lt;p&gt;The researchers in this program will use it to test their algorithms by analyzing a collection of fruit fly genomes. They expect their algorithms will provide &amp;ldquo;a relatively simple system to understand the mechanisms that underlie gene order diversity, which can later be extended to more complex mammalian genomes, such as primates,&amp;rdquo; according to Georgia Tech.&lt;/p&gt;
&lt;p&gt;They think that the algorithms will make genome rearrangement analysis reliable and efficient.
&amp;ldquo;Ultimately this information can be used to identify microorganisms, develop better vaccines, and help researchers better understand the dynamics of microbial communities and biochemical pathways,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research#.XV3-huhKjIU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research#.XV3-huhKjIU&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Petascale computing tools could provide deeper insight into genomic evolution</title>
      <link>http://localhost:1313/blog/20091117-petascalegenomic/</link>
      <pubDate>Tue, 17 Nov 2009 00:09:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091117-petascalegenomic/</guid>
      <description>&lt;p&gt;Technological advances in high-throughput DNA sequencing have opened up the possibility of determining how living things are related by analyzing the ways in which their genes have been rearranged on chromosomes. However, inferring such evolutionary relationships from rearrangement events is computationally intensive on even the most advanced computing systems available today.&lt;/p&gt;
&lt;p&gt;Research recently funded by the American Recovery and Reinvestment Act of 2009 aims to develop computational tools that will utilize next-generation petascale computers to understand genomic evolution. The four-year $1 million project, supported by the National Science Foundation&amp;rsquo;s &lt;a href=&#34;https://www.nsf.gov/pubs/2007/nsf07559/nsf07559.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PetaApps program&lt;/a&gt;, was awarded to a team of universities that includes the Georgia Institute of Technology, the University of South Carolina and The Pennsylvania State University.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Genome sequences are now available for many organisms, but making biological sense of the genomic data requires high-performance computing methods and an evolutionary perspective, whether you are trying to understand how genes of new functions arise, why genes are organized as they are in chromosomes, or why these arrangements are subject to change,&amp;rdquo; said lead investigator &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor in the Computational Science and Engineering Division of Georgia Tech&amp;rsquo;s College of Computing.&lt;/p&gt;


















&lt;figure class=&#34;center&#34; id=&#34;figure-microscale-rearrangements-of-genes-among-12-drosophila-species-each-gene-is-indicated-by-a-colored-line-showing-how-gene-order-is-shuffled-during-the-evolution-of-these-species-credit-image-courtesy-of-stephen-schaeffer&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Microscale rearrangements of genes among 12 Drosophila species. Each gene is indicated by a colored line, showing how gene order is shuffled during the evolution of these species. *Credit: Image courtesy of Stephen Schaeffer*&#34; srcset=&#34;
               /blog/20091117-petascalegenomic/1-petascalecom_hu_3476033325ac1205.webp 400w,
               /blog/20091117-petascalegenomic/1-petascalecom_hu_3b5cd9c9768a98a2.webp 760w,
               /blog/20091117-petascalegenomic/1-petascalecom_hu_a817c34727b67e9e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20091117-petascalegenomic/1-petascalecom_hu_3476033325ac1205.webp&#34;
               width=&#34;400&#34;
               height=&#34;196&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Microscale rearrangements of genes among 12 Drosophila species. Each gene is indicated by a colored line, showing how gene order is shuffled during the evolution of these species. &lt;em&gt;Credit: Image courtesy of Stephen Schaeffer&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Even on today&amp;rsquo;s fastest parallel computers, it could take centuries to analyze genome rearrangements for large, complex organisms. That is why the research team &amp;ndash; which also includes Jijun Tang, an associate professor in the Department of Computer Science and Engineering at the University of South Carolina; and Stephen Schaeffer, an associate professor of biology at Penn State &amp;ndash; is focusing on future generations of petascale machines, which will be able to process more than a thousand trillion, or 10&lt;sup&gt;15&lt;/sup&gt;, calculations per second. Today, most personal computers can only process a few hundred thousand calculations per second.&lt;/p&gt;
&lt;p&gt;The researchers plan to develop new algorithms in an open-source software framework that will utilize the capabilities of parallel, petascale computing platforms to infer ancestral rearrangement events. The starting point for developing these new algorithms will be GRAPPA, an open-source code co-developed by Bader and initially released in 2000 that reconstructed the evolutionary relatedness among species.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;GRAPPA is currently the most accurate method for determining genome rearrangement, but it has only been applied to small genomes with simple events because of the limitation of the algorithms and the lack of computational power,&amp;rdquo; explained Bader, who is also executive director of high-performance computing at Georgia Tech.&lt;/p&gt;
&lt;p&gt;On a dataset of a dozen bellflower genomes, the latest version of GRAPPA determined the flowers&amp;rsquo; evolutionary relatedness one billion times faster than the original implementation that did not utilize parallel processing or optimization.&lt;/p&gt;
&lt;p&gt;The researchers will test the performance of their new algorithms by analyzing a collection of fruit fly genomes.&lt;/p&gt;


















&lt;figure class=&#34;center&#34; id=&#34;figure-the-research-team-will-test-the-performance-of-the-petascale-computational-algorithms-they-develop-by-analyzing-a-collection-of-fruit-fly-genomes-image-courtesy-of-wikimedia-commons&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The research team will test the performance of the petascale computational algorithms they develop by analyzing a collection of fruit fly genomes. *(Image courtesy of Wikimedia Commons)*&#34; srcset=&#34;
               /blog/20091117-petascalegenomic/peta-apps-Drosophila_hu_934befef7943f3b9.webp 400w,
               /blog/20091117-petascalegenomic/peta-apps-Drosophila_hu_f079928400a82489.webp 760w,
               /blog/20091117-petascalegenomic/peta-apps-Drosophila_hu_1e35cd041ef501f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20091117-petascalegenomic/peta-apps-Drosophila_hu_934befef7943f3b9.webp&#34;
               width=&#34;324&#34;
               height=&#34;252&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The research team will test the performance of the petascale computational algorithms they develop by analyzing a collection of fruit fly genomes. &lt;em&gt;(Image courtesy of Wikimedia Commons)&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&amp;ldquo;Fruit flies &amp;ndash; formally known as Drosophila &amp;ndash; are an excellent model system for studying genome rearrangement because the genome sizes are relatively small for animals, the mechanism that alters gene order is reasonably well understood, and the evolutionary relationships among the 12 sequenced genomes are known,&amp;rdquo; said Schaeffer.&lt;/p&gt;
&lt;p&gt;The analysis of genome rearrangements in Drosophila will provide a relatively simple system to understand the mechanisms that underlie gene order diversity, which can later be extended to more complex mammalian genomes, such as primates.&lt;/p&gt;
&lt;p&gt;The researchers believe these new algorithms will make genome rearrangement analysis more reliable and efficient, while potentially revealing new evolutionary patterns. In addition, the algorithms will enable a better understanding of the mechanisms and rate of gene rearrangements in genomes, and the importance of the rearrangements in shaping the organization of genes within the genome.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Ultimately this information can be used to identify microorganisms, develop better vaccines, and help researchers better understand the dynamics of microbial communities and biochemical pathways,&amp;rdquo; added Bader.&lt;/p&gt;


















&lt;figure class=&#34;center&#34; id=&#34;figure-georgia-tech-professor-david-a-bader-is-leading-an-effort-to-develop-computational-tools-that-will-utilize-next-generation-petascale-computers-to-understand-genomic-evolution-georgia-tech-photo-rob-felt&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Georgia Tech professor David A. Bader is leading an effort to develop computational tools that will utilize next-generation petascale computers to understand genomic evolution. *(Georgia Tech Photo: Rob Felt)*&#34; srcset=&#34;
               /blog/20091117-petascalegenomic/peta-apps-bader-md_hu_92d86c5057f5062f.webp 400w,
               /blog/20091117-petascalegenomic/peta-apps-bader-md_hu_19fe95ffb0305e5f.webp 760w,
               /blog/20091117-petascalegenomic/peta-apps-bader-md_hu_e74cf973b0c4c38e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20091117-petascalegenomic/peta-apps-bader-md_hu_92d86c5057f5062f.webp&#34;
               width=&#34;252&#34;
               height=&#34;387&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Georgia Tech professor David A. Bader is leading an effort to develop computational tools that will utilize next-generation petascale computers to understand genomic evolution. &lt;em&gt;(Georgia Tech Photo: Rob Felt)&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://phys.org/news/2009-11-petascale-tools-deeper-insight-genomic.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://phys.org/news/2009-11-petascale-tools-deeper-insight-genomic.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cacm.acm.org/news/52230-petascale-tools-could-provide-deeper-insight-into-genomic-evolution/fulltext&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cacm.acm.org/news/52230-petascale-tools-could-provide-deeper-insight-into-genomic-evolution/fulltext&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nsf.gov/news/news_summ.jsp?cntn_id=116029&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nsf.gov/news/news_summ.jsp?cntn_id=116029&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://gtresearchnews.gatech.edu/newsrelease/peta-apps.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://gtresearchnews.gatech.edu/newsrelease/peta-apps.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newswise.com/articles/petascale-tools-could-provide-deeper-insight-into-genomic-evolution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newswise.com/articles/petascale-tools-could-provide-deeper-insight-into-genomic-evolution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.genomeweb.com/informatics/nsf-funds-petascale-algorithms-genomic-relatedness-research&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech focuses on experimental systems and computational sciences at SC09</title>
      <link>http://localhost:1313/blog/20091111-sc09/</link>
      <pubDate>Wed, 11 Nov 2009 08:04:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20091111-sc09/</guid>
      <description>&lt;p&gt;The Georgia Institute of Technology, an emerging leader in high-performance computing research and education, will be showcasing scientific research at the technical edge at next week&amp;rsquo;s SC09, the international conference on high-performance computing, networking, storage and analysis scheduled for Nov. 14-20, 2009, at the Oregon Convention Center in Portland, Oregon. An SC09 best paper nomination for work in computational biology, a new GPU based experimental HPC system in the works, and expert presence across a range of hardware, software and application domains are just some of the ways Georgia Tech will feature multidisciplinary, cross-industry research efforts focusing on computational scientific discovery and sustainable high-performance computing (HPC).&lt;/p&gt;
&lt;p&gt;At Georgia Tech, we have a history of tackling new frontiers with the realities of future needs in mind,&amp;quot; said Dr. Mark Allen, senior vice provost for Research and Innovation at Georgia Tech. &amp;ldquo;As we look to high-performance computing to drive advanced breakthroughs in science, health, energy and other industries, true leaders in this field will emerge only by rethinking current systems and developing new computational methods that address today&amp;rsquo;s challenges with massive data processing and energy-efficiency. Georgia Tech welcomes SC09 attendees to visit our booth, meet our researchers, and observe our work to expand the capabilities of the computational science community.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The papers and presentations outlined below will feature leading-edge Georgia Tech research, including the recently announced NSF Track 2 award, new research grants under the NSF Industry &amp;amp; University Cooperative Research program, and numerous NSF PetaApps awards.&lt;/p&gt;
&lt;h3 id=&#34;technical-papersposter-sessionspanelsbirds-of-a-feather&#34;&gt;Technical Papers/Poster Sessions/Panels/Birds-of-a-Feather&lt;/h3&gt;
&lt;p&gt;Technical papers, panels, poster sessions and Birds-of-a-Feather discussions featuring researchers from Georgia Tech include (activities held at the Oregon Convention Center unless otherwise noted):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TECHNICAL PAPER: A Massively Parallel Adaptive Fast-Multipole Method on Heterogeneous Architectures (Best Paper Nominee) - presented on Tuesday, Nov. 17, from 3:30 p.m. - 4:30 p.m. in Room PB255&lt;br&gt;
Georgia Tech&amp;rsquo;s George Biros, Ilya Lashuk, Aparna Chandramowlishwaran, Harper Langston, Tuan-Anh Nguyen, Rahul Sampath, Aashay Shringarpure, and Rich Vuduc are co-authors on this paper that presents new scalable algorithms and a new implementation of our kernel-independent fast multipole method, employing both distributed memory parallelism (via MPI) and shared memory/streaming parallelism (via GPU acceleration) to rapidly evaluate two-body non-oscillatory potentials.&lt;/li&gt;
&lt;li&gt;TECHNICAL PAPER: SCAMPI: A Scalable Cam-based Algorithm for Multiple Pattern Inspection - presented on Wednesday, Nov. 18, from 2:00 p.m. - 2:30 p.m. in Room PB255&lt;br&gt;
Georgia Tech&amp;rsquo;s Virat Agarwal (joint appointment with IBM T.J. Watson Research Center) is a co-author on this paper that presents SCAMPI, a ground-breaking string searching algorithm that is fast, space-efficient, scalable and resilient to attacks.&lt;/li&gt;
&lt;li&gt;TECHNICAL PAPER: Age Based Scheduling for Asymmetric Multiprocessors - presented on Thursday, Nov. 19, from 2:30 p.m. - 3:00 p.m. in Room PB255&lt;br&gt;
Georgia Tech&amp;rsquo;s Nagesh B. Lakshminarayana, Jaekyu Lee and Hyesoon Kim are co-authors on this paper that proposes a new policy, Age based scheduling, that improves scheduling multithreaded applications in asymmetric multiprocessors.&lt;/li&gt;
&lt;li&gt;PANEL: Preparing the World for Ubiquitous Parallelism - Friday, Nov. 20, from 10:30 a.m. - 12:00 p.m. in Room PB252&lt;br&gt;
Georgia Tech&amp;rsquo;s Matthew Wolf will moderate a panel discussion, composed of a diverse set of industry and academic representatives, that presents and discusses the abstractions, models, and (re-)training necessary to move parallel programming into a broad audience.&lt;/li&gt;
&lt;li&gt;POSTER: Cellule: Lightweight Execution Environment for Virtualized Accelerators - presented on Tuesday, Nov. 17, from 5:15 p.m. - 7:00 p.m. in the Oregon Ballroom Lobby&lt;br&gt;
Georgia Tech&amp;rsquo;s Vishakha Gupta, Priyanka Tembey, Ada Gavrilovska and Karsten Schwan present this poster that presents Cellule, which uses virtualization to create a high performance, low noise self-contained execution environments for the Cell processor.&lt;/li&gt;
&lt;li&gt;BIRDS-OF-A-FEATHER: Cray XMT Massively Multithreaded Architecture- Tuesday, Nov. 17, from 10:30 a.m. - 12:00 p.m. in the Roosevelt Conference Room, Doubletree Hotel&lt;br&gt;
Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt;, David Ediger,Karl Jiang, and Jason Riedy will participate in this Birds-of-a-Feather to discuss characterizing and analyzing massive Spatio-Temporal Interaction Networks and Graphs (STING)&lt;/li&gt;
&lt;li&gt;BIRDS-OF-A-FEATHER: Accelerating Discovery in Science and Engineering through Petascale Simulations and Analysis: The NSF PetaApps Program - Tuesday, Nov. 17, from 5:30 p.m. - 7 p.m. in Room D133-134&lt;br&gt;
Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; and George Biros will participate in this Birds-of-a-Feather update on recently-awarded NSF PetaApps research projects.&lt;/li&gt;
&lt;li&gt;SC09 EDUCATION PROGRAM: &amp;ldquo;Think Parallel&amp;rdquo; broadcast - Wednesday, Nov. 18 at 3:00 p.m.&lt;br&gt;
Georgia Tech&amp;rsquo;s Matthew Wolf will be featured in a &amp;ldquo;Think Parallel&amp;rdquo; web broadcast, a bi-weekly half-hour interview of experts in the education, government, and industry to explore how best to integrate parallel architectures, algorithms, and programming into the undergraduate computer science curriculum.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;booth-events-and-activities&#34;&gt;Booth Events and Activities&lt;/h3&gt;
&lt;p&gt;Georgia Tech researchers and staff will be on hand at Booth 132 to demonstrate and discuss the latest innovations in high-performance computing research. The Georgia Tech research display will feature updates on current research projects, video conversations with Georgia Tech experts in high performance computing, and an interactive display unlike any other - a virtual field trip to the world&amp;rsquo;s largest aquarium, the Georgia Aquarium. Utilizing a high bandwidth (1Gbps) channel connecting the Aquarium to the SC09 show floor, visitors to the Georgia Tech booth will be able to interact with researchers, fish and other marine creatures live through this one-of-a-kind tradeshow experience.&lt;/p&gt;
&lt;h3 id=&#34;sc09-leadership-activities&#34;&gt;SC09 Leadership Activities&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;George Biros and Rich Vuduc, Computational Science and Engineering division, are members of the Technical Papers Applications Area Committee.&lt;/li&gt;
&lt;li&gt;George Biros is a member of the Doctoral Showcase Committee.&lt;/li&gt;
&lt;li&gt;Ada Gavrilovska, School of Computer Science, is a member of the Technical Papers Architecture/Network Area Committee.&lt;/li&gt;
&lt;li&gt;Jeffrey Vetter, joint appointment to Georgia Tech&amp;rsquo;s Computer Science and Engineering division and the Oak Ridge National Laboratory, is a member of the Technical Papers Storage Area Committee.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Computational Science and Engineering division, is a member of the Posters Algorithms Committee.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/pub_releases/2009-11/giot-gtf111109.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eurekalert.org/pub_releases/2009-11/giot-gtf111109.php&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Book Review: Petascale Computing: Algorithms and Applications</title>
      <link>http://localhost:1313/blog/20090702-hpcwire/</link>
      <pubDate>Thu, 02 Jul 2009 19:07:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090702-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John E. West, for HPCwire&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Petascale Computing: Algorithms and Applications&lt;/em&gt;, edited by &lt;strong&gt;David A. Bader&lt;/strong&gt; (Chapman
&amp;amp; Hall/CRC, 2007), is the first book in CRC&amp;rsquo;s Computational Science Series, edited by Horst
Simon at Lawrence Berkeley National Lab. Although the book is a collection of papers, Bader
has done an excellent job of creating a compilation that holds together and covers a broad
topic very well. At the same time, &lt;em&gt;Petascale Computing&lt;/em&gt; remains accessible to anyone with
HPC or scientific application experience.&lt;/p&gt;
&lt;p&gt;While this is a book that just about anyone involved in large-scale
technical computing will find valuable, I especially commend the book to
center leadership and program managers who, having read it, will find
themselves in a better position to ask the questions that matter when
planning future hardware and software efforts for their teams.&lt;/p&gt;
&lt;p&gt;This book grew out of a February 2006 workshop held at Schloss
Dagstuhl in Germany (a remarkably-beautiful &amp;ldquo;country house&amp;rdquo; that looks
like a palace to me), and consists of 24 standalone chapters that cover a
wide variety of application areas and algorithm frontiers. The focus of the
book is always on the petascale, with the individual chapters analyzing
either specific applications, and what makes them variously well- or illsuited
to large-scale computations, or the algorithms and frameworks that
will enable performance on the new machines. Although the book was
written before the first system met the petaflops benchmark, we are still
only just at the beginning of the journey through petaflops into exaflops, and the material is
very fresh.&lt;/p&gt;
&lt;p&gt;Specifically, the book&amp;rsquo;s chapters loosely cover: scalable algorithms for large-scale
concurrency (for example, multithreaded graph-theoretic algorithms), specific applications
(i.e., in weather and climate, molecular dynamics, biology, &amp;hellip;), tools and programming
approaches (Charm++, Chapel, fault-tolerant MPI, and so on), and, throughout,
performance analysis.&lt;/p&gt;
&lt;p&gt;Given the diversity of the book&amp;rsquo;s material, it would be time-consuming, and pointless given
the availability of information on the Internet, to provide a treatment of each of the topics in
the individual chapters. For that I refer the reader to his or her favorite bookstore or online
resource. Instead let me talk about a few of the chapters that I think provide a good overview
of the value of the book for practitioners at the extremes of technical computing.&lt;/p&gt;
&lt;p&gt;The first chapter opens with a detailed examination of the performance of five applications
that are candidates for petascale processing (both in terms of their architecture and in terms
of the problems they are designed to solve) on five current supercomputing systems. The
applications are benchmarked on each system, but the information isn&amp;rsquo;t simply tabulated
and presented. Rather the authors dive into each application and talk about the
characteristics of each code/hardware combination that drive the measured performance
results. This leads in each case to a specific discussion about either hardware or software
technologies that will be needed to serve the petascale demands of the scientists that will rely
on the results. This chapter also incidentally provides the reader exposure to a range of
effective tools and performance evaluation techniques that will inform his own analyses.&lt;/p&gt;
&lt;p&gt;As an example, consider these passages from the analysis of the performance characteristics
of GTC, a 3D particle-in-cell code for studying magnetic confinement plasmas. After
describing how the code decomposes the computational domain, the authors go on to make
explicit the hardware implications of the decomposition &amp;ndash; a step that is probably
unnecessary for the professional computationalist, but which will nevertheless prove very
valuable for the many other HPC professionals thinking about the next generation of
hardware and software and planning the investments that will get us there:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Figure 1.1 (a) shows the regular communication structure exhibited by GTC. This particle-in-cell calculation uses a one-dimensional domain decomposition across the toroidal computational grid, causing each processor to exchange data with its two neighbors as particles cross the left and right boundaries&amp;hellip;.Therefore, on average each MPI task communicates with 4 neighbors, so much simpler interconnect networks will be adequate for its requirements.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;After looking fairly closely at the characteristics of each of the five applications, the authors
bring the whole set of results together and draw broad conclusions about these codes and
their potential for successful petascale deployment based on the evidence gathered. It&amp;rsquo;s a
great chapter to open the book with.&lt;/p&gt;
&lt;p&gt;An early example of a chapter that makes a deep dive into a specific application domain is
chapter six on the numerical prediction of &amp;ldquo;high-impact&amp;rdquo; local weather. This chapter
exemplifies one of the real strengths of the book: each domain-specific chapter provides
enough context and detail to bring along non-experts in the domain, while still managing to
cover enough detail that the reader achieves a working knowledge of the high-level drivers in
scientific applications. As a result readers exit these chapters with knowledge of why an
application does what it does sufficient to think critically about the impact that specific
petascale hardware and software features will have on performance.&lt;/p&gt;
&lt;p&gt;In chapter six, for example, the authors briefly introduce operational weather forecasting
and describe the current state of the practice, weather forecasting at a resolution of 25 km.
As a motivation for increasing this resolution, the authors describe what will be needed to
both expand the geographic area covered by predictions and enable prediction in sufficient
detail to predict items of high local interest, such as thunderstorms and tornadoes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Even at 12-km horizontal grid spacing, important weather systems that are directly responsible for meteorological hazards including thunderstorms, heavy precipitation and tornadoes cannot be directly resolved because of their small sizes. For individual storm cells, horizontal grid resolutions of at least 1-km grid are generally believed to be necessary, which even high resolutions are needed to resolve less organized storms and the internal circulation within the storm cells.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The authors then go on to describe the features of these codes that drive hardware
requirements, for example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An evaluation of the ARPS on scalar and vector-based computers indicates that the nested do-loop structures were able to realize a significant percentage of the peak performance of vector platforms, but on commodity-processor-based platforms the utilization efficiency is typically on 5-15%. The primary difference lies with the memory and memory access speeds. Since weather forecast models are largely memory bound, they contain far more loads/stores than computations and, as currently written, do not reuse in-cache data efficiently.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;And this analysis continues for data distribution, load balancing issues, scalability, and so
on.&lt;/p&gt;
&lt;p&gt;There is also discussion of specific architectures and approaches for reaching the petascale,
with emphasis on how these will influence application design and optimization. For example
chapter 8 considers reaching beyond the petascale by functionally dividing amenable
computations onto separate supercomputers, an approach the authors call &amp;ldquo;distributed
petascale computing.&amp;rdquo; Likewise, chapter 10 talks about the MDGRAPE special-purpose
hardware project. Later chapters, for example chapter 21, talk about a specific annotationbased
approach for performance portability, an important topic to address if indeed
petascale architectures end up being as diverse as many expect.&lt;/p&gt;
&lt;p&gt;In chapter 13 the authors frame the discussions about massive concurrency and enormous
computer systems in a different way: distributing applications over tens or hundreds of
thousands of processors is going to create a driver for applications to recover from hardware
faults. This chapter discusses FT-MPI, a fault-tolerant MPI implementation, along with a
diskless checkpointing approach that, combined, can provide the application developer a
good starting point for developing more robust applications. In any discussion of these
technologies, the question of performance overhead will naturally (and properly) arise, and
the authors present results in the context of a PCG algorithm.&lt;/p&gt;
&lt;p&gt;In fact, throughout this book the focus stays on performance &amp;ndash; not just the numbers, but
what drives the numbers to fall out the way they do, and, perhaps just as important, why we
need the performance in the first place. The end result educates and informs our journey
through petascale and into exascale, while serving to motivate us to travel as fast as we can
toward the goal.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.com/gp/product/1584889098?ie=UTF8&amp;amp;tag=hp0a5-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=1584889098&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Petascale Computing: Algorithms and Applications (Chapman &amp;amp; Hall/Crc Computational Science Series)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2009/07/02/book_review_petascale_computing_algorithms_and_applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2009/07/02/book_review_petascale_computing_algorithms_and_applications/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Petascale Coming Down the Pike</title>
      <link>http://localhost:1313/blog/20090529-genometechnology/</link>
      <pubDate>Fri, 29 May 2009 07:41:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090529-genometechnology/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Matthew Dublin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Given the combination of the everincreasing power of
compute hardware
and researchers’ desire to unlock the mysteries of life, it’s
no surprise that high-performance
computing in the early 21st century
is now talking in terms of a whole
new scale of computation. While the
life sciences community has for some
time now been concerned with terrifying amounts of data in terabytescale proportions — that’s 1,024 gigabytes — there is an even larger
scale on the computational horizon:
petascale computing. One petabyte
is 1,024 terabytes, and to provide
some perspective, Google processes
an average of about 20 petabytes of
data per day.&lt;/p&gt;
&lt;p&gt;Gee-whiz factor aside, should petascale and near-petascale systems
even be on the radar screen of the
life sciences community? So far, there
is a resounding yes from many in the
molecular simulation research community. “With petaflop-scale performance, [molecular] simulations
will reach the time scale of a submillisecond,” says Makoto Taiji, a
team leader at the RIKEN Yokohama
Institute. “This time scale will cover
various interesting biological events,
including large fluctuations in proteins. … Petascale computing will
provide scientific breakthroughs.”&lt;/p&gt;
&lt;p&gt;Taiji and his team use RIKEN’s petaflop-capable supercomputer called
MDGRAPE-3 to conduct a range of
molecular dynamic simulations, including “post-docking” — a protocol he uses to choose drug candiate
compunds with more precision after
using the normal molecular docking
technique. “We have already found a
few successful seed compounds for
real drug targets confirmed by the experimental assays,” says Taiji. “Their
optimization is [underway], and we
are trying to use our machine also
for the optimizations of the seeds.”
In the case of MDGRAPE-3, which is
not really a programmable machine,
porting popular molecular dynamics
software to run on its architecture is
not that difficult, but it does require
a deep knowledge of the software and
people power. So far, Taiji says, they
do not have enough researchers and
programmers to study that many molecular dynamics software packages,
although they have managed to port
Amber and CHARMM, two popular
simulation applications.&lt;/p&gt;
&lt;p&gt;On the other side of the globe, Nick
Grishin, a professor of biophysics
at the University of Texas Southwest Medical Center and a Howard
Hughes Medical Institute investigator, recently used Ranger, the sixth
most powerful supercomputer in the
world according to the Top500 list,
to solve some very difficult threedimensional protein folding problems. Housed at the Texas Advanced
Computing Center, Ranger came online in February 2008 and Grishin
got to use its roughly 62,976 processing cores to help secure top honors in
the most recent Critical Assessment
of Techniques for Protein Structure
Prediction competition, a worldwide
contest to predict the structures of a
select number of unknown proteins.&lt;/p&gt;
&lt;p&gt;“It’s embarrassing to say, but because our algorithms are stochastic,
they are not particularly fast to run,
and protein chains are very long so
it just takes an incredible amount
of computer resource to compute
those energies,” Grishin says. Without Ranger, Grishin says he never
would have been able to accomplish
the task. “A typical cluster is just too
small; it needs to be many more processors. A hundred or 200 processors
is clearly not enough for this kind of
job. … And the more computations
we make, the more likelihood there
is that we will hit the right energy
function and have something with
some medical importance,” he says.&lt;/p&gt;
&lt;h3 id=&#34;still-a-rarity&#34;&gt;Still a rarity&lt;/h3&gt;
&lt;p&gt;Despite the growing number of
petascale machines, it’s not as if just
anyone can waltz down the hallway
of an institute and find one to use.
These systems are still relatively rare;
there are only two supercomputer
sites currently capable of achieving
one petaflop peak performance in
the US. Los Alamos National Laboratory unveiled its Roadrunner supercomputer only last year, which is
listed on the Top500 supercomputing
sites list as the world’s most powerful supercomputer with a whopping
129,600 processing cores. In late
January of this year, the Oak Ridge
National Laboratory announced that
its Cray XT supercomputer, known
as Jaguar, is now capable of a peak
performance of 1.6 petaflops. “Highperformance computing affects all
areas of computational science, including biological research … [and]
more and more petascale systems
will be coming online,” says Jack
Dongarra, a professor of electrical
engineering and computer science at
the University of Tennessee. Dongarra is one of the developers of the LINPACK Benchmark, a series of dense
linear equations used
to measure a compute
system’s processing
capacity. Dongarra
helps run the Top500
list, a semiannual listing of the most powerful computing sites
in the world compiled
by computer scientists in the US and
Germany. According to Dongarra, all
high-performance systems will reach
petascale in the very near future.
“The projections say that all of the
Top500 fastest computers will be at
petascale in 2015,” he says.&lt;/p&gt;
&lt;p&gt;More petaflop machines are already
on the way. The National Center for
Supercomputing Applications has
teamed up with IBM to create Blue
Waters, a beast of a machine that
contains more than 200,000 processing cores and is capable of sustained
multi-petaflop performance. Although
exact performance figures are still being kept confidential by IBM, all involved claim that Blue Waters will far
exceed the performance capabilities of
the two formerly mentioned machines
by a long shot when it comes online in
the summer of 2011.&lt;/p&gt;
&lt;p&gt;“I think petascale computing comes
at a very good time for biology, especially genomics, which has to deal
with … increasingly large data sets
trying to do a lot of correlation between the data that’s held in several
massive datasets,” says Thomas Dunning, director of the NCSA at University of Illinois, Urbana-Champaign.
“This is the time that biology is now
going to need this kind of computing
capability — and the good thing is
that it’s going to be here.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a professor of computer science at the Georgia Institute of Technology, has been heavily involved in promoting awareness
of petascale computing. In 2006,
he co-chaired a workshop that attempted both to lay out a roadmap of
recommendations for making petascale computing a reality for the life
sciences and also to address its many
challenges, the biggest of which is
scaling algorithms to run on these
mega architectures.&lt;/p&gt;
&lt;p&gt;“First and foremost, this is a scale
of system that has not been seen
before,” Bader says. “Just in June, we
saw Roadrunner using accelerators
like the Cell processor and now we
see the Cray XT-5 system at Oak
Ridge, so I think that can lead to
more experience on how to scale
algorithms that can run on all those
processors.”&lt;/p&gt;
&lt;p&gt;In addition to scalability, reliability is
another major hurdle. When a system
crash causes you to lose a few hundred
gigabytes on a simulation or analysis job, that really hurts — but just
think of the gnashing of teeth when
you’re talking terabytes or petabytes
of data gone haywire. Efforts such as
the Berkeley Lab Checkpoint/Restart
project have focused on how to ensure a high level of reliability in these
monolithic systems. At the start of the
year, the group released a new and
improved version of its software, an
open-source solution that uses checkpointing to take hourly snapshots of
MPI-enabled applications running
jobs on large-scale compute systems.
The software “works transparently
and users do not need to make source
code changes to their applications to
work with BLCR,” says Eric Roman,
a member of the Future Technologies
Group at Lawrence Berkeley National
Laboratory. “On a petascale system
with possibly thousands of users and
applications, this feature should not
be overlooked.”&lt;/p&gt;
&lt;h3 id=&#34;planning-for-peta&#34;&gt;Planning for peta&lt;/h3&gt;
&lt;p&gt;Given that eco-consciousness now
pervades the computing world, supercomputing sites are following suit
and are continuing to make moves
toward more environmentally friendly infrastructure. For Blue Waters,
energy-efficient design is not so much
a choice as a necessity. “I think we’ve
gotten to a point where that has to be
a priority — where if you don’t pay
careful attention to that, the power
budget can just become overwhelming, so in fact we’re at the point right
now where that has to be part of the
consideration,” says NCSA’s Dunning.
“For example, Blue Waters will be a
water-cooled, not air-cooled, machine
and the simple reason for that is that
it’s 40 percent more efficient.” NCSA
also brought in a specialized team to
look at ways to minimize the footprint
of the building itself so that most of
the power coming into the building
is actually used to run the computer
rather than all the ancillary things
needed for the facility.&lt;/p&gt;
&lt;p&gt;Bader also hopes to see petascale
computing assisting with big ideas in
genomics. “When I think of petascale
machines, I think of doing complex
operations. So once I can assemble
whole genomes and sequence whole
genomes and get much richer data
sets and combine that with microarray data and other data sources,
what I want to be able to do is understand the evolution of whole genomes
and compare both the organisms and
genes across whole and entire genomes,” he says. “And that’s a problem
that needs both an army of data and
also the computational requirements
of a petascale system. So rather than
just taking our current techniques
and running them a bit faster, I think
that developing new algorithms &amp;hellip; is
really where we’re headed.”&lt;/p&gt;
&lt;p&gt;And Bader reminds researchers
that this isn’t something only computer scientists should be thinking
about. “Biologists will need to be
aware of this technology because
if you push out the road map 10
years, these are the capability class
machines that they’ll have in their
laboratories,” Bader says. “There are
a lot of biological problems that are
still in their infancy and we [now
understand] to solve those problems
we’ll have to bring in a lot of data
collected from a lot of sites and a
lot of laboratories. … [There will]
be a growing number [of biologists]
who need access to massive volumes
of data and the computational capabilities to solve their particular
scientific inquiry.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.genomeweb.com/informatics/petascale-coming-down-pike&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.genomeweb.com/informatics/petascale-coming-down-pike&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives IEEE Computer Society Certificate of Appreciation</title>
      <link>http://localhost:1313/blog/20090528-tcpp/</link>
      <pubDate>Thu, 28 May 2009 18:29:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090528-tcpp/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20090528-tcpp/award1_hu_f5a31313bc3b8748.webp 400w,
               /blog/20090528-tcpp/award1_hu_139533b42f8e7706.webp 760w,
               /blog/20090528-tcpp/award1_hu_77964504631b3274.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20090528-tcpp/award1_hu_f5a31313bc3b8748.webp&#34;
               width=&#34;570&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20090528-tcpp/award2_hu_c7d241916c4fd4c9.webp 400w,
               /blog/20090528-tcpp/award2_hu_213340c6b0e47e52.webp 760w,
               /blog/20090528-tcpp/award2_hu_ff67d5384a8d228b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20090528-tcpp/award2_hu_c7d241916c4fd4c9.webp&#34;
               width=&#34;564&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;IEEE Computer Society&lt;br&gt;
Technical Committee on Parallel Processing (TCPP)&lt;/p&gt;
&lt;p&gt;Present this Certificate of Appreciation to &lt;strong&gt;David A. Bader&lt;/strong&gt; for his outstanding contributions in Coordinating the TCPP-Announce Membership List Service on the Twenty Eighth of May in the Year Two Thousand and Nine.&lt;/p&gt;
&lt;p&gt;Sushil K. Prasad&lt;br&gt;
Chair, IEEE-CS TCPP&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Video: Georgia Tech High Performance Computing: David A. Bader</title>
      <link>http://localhost:1313/blog/20090520-hpcbader/</link>
      <pubDate>Wed, 20 May 2009 06:54:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090520-hpcbader/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IwyoRe_Zim4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://img.youtube.com/vi/IwyoRe_Zim4/0.jpg&#34; alt=&#34;Georgia Tech High Performance Computing: David A. Bader&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Science and Engineering Grad Student Wins Best Poster</title>
      <link>http://localhost:1313/blog/20090414-ipdps/</link>
      <pubDate>Tue, 14 Apr 2009 07:41:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090414-ipdps/</guid>
      <description>&lt;p&gt;Kamesh Madduri won the best poster award in the Ph. D. Forum at the 22nd IEEE International Parallel and Distributed Processing Symposium (IPDPS) held April 14-18 in Miami. Madduri’s research in computational science and high-performance computing beat out 72 other submissions to win one of two prizes in the competition.&lt;/p&gt;
&lt;p&gt;The main track of IPDPS is highly competitive and contains peer-reviewed papers submitted from researchers worldwide. Only 105 of the submitted 410 papers were accepted for presentation. As a testament to Georgia Tech’s strength in parallel and multicore computing, four papers from Computational Science and Engineering division were presented as regular papers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;SNAP, Small-world Network Analysis and Partitioning: An Open-Source Parallel Graph Framework for the Exploration of Large-Scale Networks&lt;/em&gt; &lt;br&gt;
&lt;strong&gt;David Bader&lt;/strong&gt; (CSE); Kamesh Madduri (CSE)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;High Performance MPEG-2 Software Decoder on the Cell Broadband Engine&lt;/em&gt;&lt;br&gt;
&lt;strong&gt;David Bader&lt;/strong&gt; (CSE); Sulabh Patel (Electronic Arts, USA)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Financial Modeling on the Cell Broadband Engine&lt;/em&gt;&lt;br&gt;
Virat Agarwal (CSE); Lurng-Kuo Liu (IBM T.J. Watson Research Center, USA); &lt;strong&gt;David Bader&lt;/strong&gt; (CSE)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Performance Characterizations and Optimization of Parallel I/O on the Cray XT&lt;/em&gt;&lt;br&gt;
Weikun Yu (ORNL), Jeffrey Vetter (ORNL &amp;amp; CSE) and Sarp Oral (ORNL)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IPDPS attracts over 600 of the top scientific researchers in the field for a week packed with technical talks and demonstrations.  In 2009, the meeting will be held in Rome, Italy, and in 2010, &lt;strong&gt;David Bader&lt;/strong&gt; (CSE) will serve as the General Chair when the symposium comes to Atlanta.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20081003013214/http://www.cc.gatech.edu/news/cse-grad-student-wins-best-poster-at-ipdps&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/cse-grad-student-wins-best-poster-at-ipdps&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicore and Parallelism: Catching Up</title>
      <link>http://localhost:1313/blog/20090330-drdobbs/</link>
      <pubDate>Mon, 30 Mar 2009 07:48:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/20090330-drdobbs/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jonathan Erickson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; is Executive Director of High-Performance Computing in the College of Computing at Georgia Institute of Technology and author of Petascale Computing: Algorithms and Applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; David, you&amp;rsquo;ve been actively involved with multicore and parallelism since the early 1990s and the rest of the computing world seems just now catching up. Any idea of why it&amp;rsquo;s taken so long?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Overnight the computing world has changed, and we are starting a new day when parallelism is essential not just for solving grand challenge applications in science and engineering, but for speeding up everything: our 3G cellphone apps, our desktop spreadsheets and office productivity tools to our web browsers, media players, and Web services. Over the past few decades the parallel computing community, which has been a niche from academia, industry, and national labs, has performed a terrific job of harnessing massive parallelism to solve fundamental science questions in areas such as climate prediction, renewable energy, and molecular dynamics, as evidenced by the first petascale computing systems with phenomenal capabilities. The Los Alamos Roadrunner system harnesses thousands of IBM Cell Broadband Engine heterogeneous multicore processors, the same line of PowerPC processor in the Sony PlayStation 3, to solve some of our nation&amp;rsquo;s most critical energy and material science problems. Oak Ridge&amp;rsquo;s Jaguar Cray XT5 supercomputer also broke through the petascale record and is the world&amp;rsquo;s most powerful supercomputer today for science. The parallel processing community magnificent successes over the past several decades have gone mostly unnoticed by the masses, because we&amp;rsquo;ve been able to inject our new technologies into mainstream computing unbeknownst to the user. Processors have employed astonishing amounts of parallelism using superscalar microprocessors and pushed the limits of instruction-level parallelism, and systems have employed accelerators ubiquitously from floating-point math units to graphics processors (GPUs). The end user could continue to write her sequential algorithms, and the responsibility for exploiting the implicit concurrency was assumed by the architecture, compiler, and runtime systems. We could maintain this illusion and exploit concurrency at a fine-grain within the system in order to achieve the speedup scaling with Moore&amp;rsquo;s Law that everyone has come to expect.&lt;/p&gt;
&lt;p&gt;Let me give you analogy of the situation today. Over the past century, our transportation systems have improved from riding horses to driving cars. And our car engines have gotten faster and smarter over the last few decades. We still have pretty much the same driver&amp;rsquo;s seat from the Model T to our fastest race cars &amp;ndash; complete with a steering wheel, accelerator and brakes. Imagine that our car engines reached their physical limits, so the industry has decided to provide the only solution available and sell us cars that have two engines, four engines, and even some with a thousand engines. And because of the added weight to our vehicles, these engines actually slow down our cars, even though they provide tremendous capability. We would have to rethink what it means to drive a car, and perhaps our entire transportation systems. The same is true in computing. Our ability to exploit the concurrency in application code has reached its limit, and we are now telling the application developers that it is their responsibility to rethink their applications and explicitly reveal the parallelism.&lt;/p&gt;
&lt;p&gt;My experience with parallel computing goes back to the early 1980s, and I&amp;rsquo;ve found that the most challenging task often is designing efficient parallel algorithms. I am optimistic that many applications can be parallelized, yet there is an enormous gap in our capabilities for designing new parallel algorithms and for educating our programmers. We are taking a leadership position at Georgia Tech and completely revamping the curriculum. We&amp;rsquo;ve launched a new academic department and programs in Computational Science and Engineering that are focused at the interface between computing and the sciences and engineering and addressing the software needs for productively using hybrid, multicore and manycore processors. We are producing computational thinkers who are experts in multicore computing and parallelism and can readily transform computing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; One of your early projects is SWARM. Can you tell us what it is and what problems it addresses? Is there anything like it out there?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Continued performance on multicore and manycore processors now requires the exploitation of concurrency at the algorithmic level. SWARM is a freely available as open-source software package from my group at Georgia Tech for &amp;ldquo;SoftWare and Algorithms for Running on Multicore and Manycore processors&amp;rdquo;. SWARM brings together 15 years of research on shared-memory algorithms, and addresses the key issues in algorithm design for multicore processors. And we are grateful for the generous support from the National Science Foundation, IBM, Microsoft, Sun Microsystems, and Intel, that has enabled our research and development in parallel algorithms. SWARM, with continued development since 1994, is a portable open-source parallel library of basic primitives that fully exploit multicore and manycore processors. Using this framework, we have implemented efficient parallel algorithms for important, new primitive operations such as prefix-sums, pointer-jumping, symmetry breaking, and list ranking; for combinatorial problems such as sorting and selection; for parallel graph theoretic algorithms such as spanning tree, minimum spanning tree, graph decomposition, and tree contraction; and for computational genomics applications such as maximum parsimony.&lt;/p&gt;
&lt;p&gt;Several other multicore programming productivity efforts have caught my eye. For instance, Cilk++ from Cilk Arts answers the multicore challenge and provides a smooth transition path from today&amp;rsquo;s legacy applications to tomorrow&amp;rsquo;s multicore implementations. We have been early adopters of Cilk++ and are impressed with its productivity gains. Other language efforts are also quite amazing. Sun Microsystems is developing a new high performance computing language with high programmability called Fortress with new features for implicit parallelism, transactions, and a flexible, space-aware, mathematical syntax. Intel is developing an innovative product called Concurrent Collections that provides mechanisms for constructing high-level parallel programs while allowing the programmer to ignore lower-level threading constructs. And IBM has an outstanding compilers team focused on improving OpenMP for heterogeneous multicore processors and developing new high productive languages such as X10.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; You once said that &amp;ldquo;In order to develop high-performance practical parallel algorithms, you must be a good sequential algorithmist.&amp;rdquo; Could you expand on that?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Yes, this statement comes from two observations I&amp;rsquo;ve made during my professional career in designing high-performance parallel algorithms. First, one must have expertise in designing sequential algorithms before mastering parallel algorithms. It is important to start with an efficient sequential algorithm and identify its bottlenecks, before taking the challenging step of designing an efficient parallel algorithm. Second, in a good parallel implementation, the performance is often dominated by the work performed by each thread or core. For example, in multicore computing, one must still know how to achieve performance from the algorithm subtask running on each core for a speedup of the entire application. And it goes both ways. Through the study of parallel algorithms, we have occasionally discovered an asymptotic improvement to a sequential algorithm. And an algorithmic improvement can offer dramatically higher speedups than relying on technological factors alone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; What is your current research interest?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; At Georgia Tech my group is pursuing an ongoing effort to design and implement novel parallel graph algorithms to efficiently solve advanced small-world network analysis queries, enabling analysis of networks that were previously considered too large to be feasible. For tractable analysis of large-scale networks, we are developing SNAP (&amp;ldquo;Small-world Network Analysis and Partitioning&amp;rdquo;), an open-source graph analysis framework that builds on our SWARM library and supplements existing static graph algorithms with relevant ideas from dynamic graph algorithms, social network analysis, and parallel and multicore processing.&lt;/p&gt;
&lt;p&gt;Large-scale network analysis broadly applies to real-world application domains such as social networks (friendship circles, organizational networks), the Internet (network topologies, the web-graph, peer-to-peer networks), transportation networks, electrical circuits, genealogical research and computational biology (protein-interaction networks, population dynamics, food webs). There are several analytical tools for visualizing social networks, determining empirical quantitative indices, and clustering. Real-world networks from diverse sources have been observed to share features such as a low average distance between the vertices (the small-world property), heavy-tailed degree distributions modeled by power laws, and high local densities. Modeling these networks based on experiments and measurements, and the study of interesting phenomena and observations, continue to be active areas of research.&lt;/p&gt;
&lt;p&gt;SNAP is a modular infrastructure that provides an optimized collection of algorithmic building blocks (efficient implementations of key graph-theoretic analytic approaches) to end-users. SNAP provides a simple and intuitive interface for the network analyst, effectively hiding the parallel programming complexity involved in low-level algorithm design from the user while providing a productive high-performance environment for complex network queries.&lt;/p&gt;
&lt;p&gt;In order to achieve scalable parallel performance, we exploit typical network characteristics of small-world networks, such as the low graph diameter, sparse connectivity, and skewed degree distribution. The SNAP parallel algorithms, coupled with aggressive algorithm engineering for small-world networks, give significant running time improvements over current state-of-the-art social network analysis packages. We are extending the SNAP framework with efficient data structures and algorithms aimed specifically at manipulation and analysis of large dynamic networks, with applications in social networks, healthcare, and critical infrastructures such as the Internet and the power grid.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20150902190335/http://www.drdobbs.com/parallel/multicore-and-parallelism-catching-up/216401662&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.drdobbs.com/parallel/multicore-and-parallelism-catching-up/216401662&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PlayStation 3 Processor Speeds Financial-Risk Calculation</title>
      <link>http://localhost:1313/blog/20081118-spectrum-ps3/</link>
      <pubDate>Tue, 18 Nov 2008 16:40:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20081118-spectrum-ps3/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Samuel K. Moore&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Of the many things that have gone wrong on Wall Street this past year, the use and misuse of computational algorithms meant to give financiers a clear picture of the risk of big losses was one of them. One important calculation, called Value-at-Risk (VaR), is a way of assessing the probability that an investment portfolio will lose a specified value over a certain period of time. Though VaR’s reputation is much maligned, experts say firms have little choice but to continue, if not accelerate, their use of computational algorithms as the need to calculate risk and value has become more acute. The surviving financial firms might get a little help from code produced for Cell, the processor behind Sony’s PlayStation 3 as well as a number of high-end IBM servers and supercomputers.&lt;/p&gt;
&lt;p&gt;By taking advantage of Cell’s unique architecture, computer scientists at Georgia Tech, in Atlanta, say they have found a way to accelerate the generation of random numbers up to 33-fold compared with what’s possible using commercially available Intel or AMD processors. Random-number generation is key to many so-called Monte Carlo simulations, but as a proof of concept, the researchers built their random-number generator into a program that efficiently runs a VaR algorithm. &lt;strong&gt;David A. Bader&lt;/strong&gt;, executive director of high-performance computing at Georgia Tech, says his group has been working with several financial firms, whose names he would not disclose due to legal agreements, on their use of the VaR software. Bader says the source code behind the random-number generators for Cell will be made available during the SC08 supercomputing conference this week in Austin, Texas.&lt;/p&gt;
&lt;p&gt;”What we’re able to do is generate not just a single random number fast, but streams of random numbers that can be fed into Monte Carlo simulations,” says Bader, who is also a professor of computational science and engineering.&lt;/p&gt;
&lt;p&gt;The basis of many scientific and financial computational algorithms, Monte Carlo methods all start with some random numbers, then perform an operation on them, and repeat the process to arrive at a result. (Imagine playing the game Battleship, for example.) Often the most time-consuming part of a Monte Carlo computation is the generation of a good set of random numbers (or pseudorandom numbers, as computers don’t produce true random numbers). Bader and his graduate students Aparna Chandramowlishwaran and Virat Agarwal adapted two well-known random-number algorithms to Cell: the 64-bit linear congruential generator (LCG) and the 32-bit Mersenne twister. With the LCG, their speed increased 33-fold and with the Mersenne twister, between 14-fold and 29-fold.&lt;/p&gt;
&lt;p&gt;The key to accelerating both algorithms lies in Cell’s architecture, says Bader. Cell comprises nine processor cores. One, the Power Processing Unit, is a general-purpose processor that can run an operating system and do other control tasks, such as coordinating the work done by the other eight processors. Those eight—called the Synergistic Processing Units (SPUs)—are designed to efficiently perform tasks common to multimedia applications: video compression and decompression, encryption and decryption, rendering and modifying graphics, and the like. The Georgia Tech innovation was in finding a way to parallelize the LCG and Mersenne twister algorithms in a way that played to the SPUs’ strengths. For both algorithms, that meant adapting the core algorithm to run on all eight SPUs but initializing each of the processor cores with a different set of parameters in order to guarantee that their outputs, when combined into one stream of numbers, would not have repeating or similar sections.&lt;/p&gt;
&lt;p&gt;Though it can be a rate-limiting step in other Monte Carlo calculations, generating a good set of random numbers is far from being the main drag on VaR, says Dan Rosen, financial engineer and president of R2 Financial Technologies. ”Really, it’s what you do with those random numbers,” says the Toronto-based algorithms expert. Most of the computational work involves calculating the values of stocks and other instruments in each of hundreds or thousands of different scenarios needed to produce an accurate understanding of risk. Consequently, most of the algorithmic research in VaR is dedicated to speeding the valuation process or reducing the number of scenarios needed to get to a given level of accuracy, says Rosen.&lt;/p&gt;
&lt;p&gt;Even so, it seems that financial firms are interested in what Bader’s group has done. ”We are working with several of the major financial institutions that do this type of calculation,” says Bader. Despite their troubles, ”financial institutions aren’t going to stop all calculation,” he says. ”In fact, they’re doing quite the reverse—increasing the big calculations they’re doing to better understand pricing.”&lt;/p&gt;
&lt;p&gt;Still, the ability to model risk faster is of little use if the model doesn’t reflect reality. ”While speed on a technical level might be valuable, it’s not going to substitute for what should be included in the model,” says Robert Hoyt, professor of risk management at the University of Georgia, in Athens. ”I think what’s going on will infuse a dose of reality—that models always have limitations, and in some cases, serious limitations.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://spectrum.ieee.org/semiconductors/processors/playstation-3-processor-speeds-financialrisk-calculation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://spectrum.ieee.org/semiconductors/processors/playstation-3-processor-speeds-financialrisk-calculation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech Enters the Spotlight at SC08</title>
      <link>http://localhost:1313/blog/20081112-hpcwire/</link>
      <pubDate>Wed, 12 Nov 2008 14:21:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20081112-hpcwire/</guid>
      <description>&lt;p&gt;The Georgia Institute of Technology, an emerging leader in highperformance
computing research and education, will command a significant presence at next
week&amp;rsquo;s SC08, the international conference on high-performance computing, networking,
storage and analysis scheduled for Nov. 15-21, 2008, at the Austin Convention Center in
Austin, Texas. Georgia Tech will co-chair one workshop, participate in four panel (or &amp;ldquo;Birdsof-
a-Feather&amp;rdquo;) discussions, present three technical papers and one research poster, and host
16 booth presentations and video interviews on emerging high-performance computing
projects and application areas.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At Georgia Tech, we believe a strong and expansive high-performance computing research
community drives the bigger scientific discoveries and better engineering capabilities at the
heart of human progress,&amp;rdquo; said Dr. Mark Allen, senior vice provost for Research and
Innovation at Georgia Tech. &amp;ldquo;Through this premier industry event, researchers, academics
and industry professionals have the opportunity to discuss and demonstrate new innovations
and breakthroughs in high-impact areas such as biomedicine, nanoscience, astrophysics and
exascale computing. Georgia Tech welcomes SC08 attendees to visit our booth, meet our
researchers, observe our work and understand our mission to positively affect quality of life
through advanced computing capabilities.&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;technical-workshopspanelsbirds-of-a-feather&#34;&gt;Technical Workshops/Panels/Birds-of-a-Feather&lt;/h3&gt;
&lt;p&gt;Technical workshops, panels and Birds-of-a-Feather discussions featuring research experts
from Georgia Tech&amp;rsquo;s College of Computing include (activities listed in date/time order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WORKSHOP: Supercomputing, Multicore Architectures and Biomedical Informatics &amp;ndash; Monday, Nov. 15, from 8:30 a.m. - 5 p.m. in Room 15.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor and executive director of high-performance computing at Georgia Tech, is co-chairing this workshop to begin building a community of researchers with shared interests in understanding the impact of emerging architectures on computationally demanding biomedical applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;WORKSHOP: Bridging Multicore&amp;rsquo;s Programmability Gap &amp;ndash; Monday, Nov. 15, from 8:30 a.m. - 5 p.m. in Room 16A/16B.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; is a speaker in this workshop to address the emerging &amp;ldquo;Programmability Gap&amp;rdquo; between multicore-based systems and current languages, compilers and software development techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PANEL: Can Developing Applications for Massively Parallel Systems with Heterogeneous Processors Be Made Easy(er)? &amp;ndash; Tuesday, Nov. 18, from 3:30 - 5 p.m. in Ballroom G.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; is a speaker on this panel that will look at what needs to be done in order to make the application development for massively parallel systems with heterogeneous processors easier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BIRDS-OF-A-FEATHER: Exascale Software Challenges &amp;ndash; Tuesday, Nov. 18, from 5:30 - 7 p.m. in Room 19A/19B.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; will participate in this Birds-of-a-Feather discussion to focus on understanding challenges and developing promising approaches in developing robust, scalable and efficient software to run at exascale.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;BIRDS-OF-A-FEATHER: Unleashing the Power of the Cell BE for HPC Applications &amp;ndash; Wednesday, Nov. 19, from 12:15 - 1:15 p.m. in Room 18A/18B/18C/18D
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; is leading this Birds-of-a-Feather session to stimulate an open discussion on the techniques and tools that can enable HPC applications to exploit the power of the Cell/B.E. multicore processor.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;technical-papersposter-sessions&#34;&gt;Technical Papers/Poster Sessions&lt;/h3&gt;
&lt;p&gt;Technical papers and poster sessions featuring researchers from Georgia Tech include
(activities listed in date/time order):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TECHNICAL PAPER: Wide-Area Performance Profiling of 10GigE and Infiniband Technologies &amp;ndash; presented on Tuesday, Nov. 18, from 2 - 2:30 p.m. in Ballroom F.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s Jeffrey S. Vetter is a co-author on this paper that presents an experimental study of two solutions to throughput challenges for wide-area high-performance applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TECHNICAL PAPER: Dendro: Parallel Algorithms for Multigrid and AMR Methods on 2:1 Balanced Octrees &amp;ndash; presented on Tuesday, Nov. 18, from 2:30 - 3 p.m. in Ballroom E.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s George Biros and Rahul S. Sampath are co-authors on this article that presents Dendro, a suite of parallel algorithms for the discretization and solution of partial differential equations involving second-order elliptic operators.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TECHNICAL PAPER: Early Evaluation of BlueGene/P &amp;ndash; presented on Tuesday, Nov. 18, from 4 - 4:30 p.m. in Ballroom E.
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s Jeffrey S. Vetter is a co-author on this paper that reports on the scalability and performance of the BlueGene/P &amp;ndash; the second-generation BlueGene architecture from IBM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;POSTER: Modeling Assertions for Petascale Applications and Systems &amp;ndash; presented on Tuesday, Nov. 18, from 5:15 - 7 p.m in the Rotunda Lobby
&lt;ul&gt;
&lt;li&gt;Georgia Tech&amp;rsquo;s Jeffrey S. Vetter is a co-author on this poster that addresses programming and scaling challenges to emerging Petaflops platforms at the DOE leadership computing sites.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;booth-events-and-activities&#34;&gt;Booth Events and Activities&lt;/h3&gt;
&lt;p&gt;Georgia Tech researchers and staff will be on hand at Booth 2821 to demonstrate and discuss
the latest innovations in high-performance computing research. The Georgia Tech research
display will feature live research presentations, video conversations with Georgia Tech
experts in high performance computing, and an interactive display unlike any other &amp;ndash; a
virtual field trip to the world&amp;rsquo;s largest aquarium, the Georgia Aquarium. Utilizing a high
bandwidth (1Gbps) channel connecting the Aquarium to the SC08 show floor, visitors to the
Georgia Tech booth will be able to interact with researchers, fish and other marine creatures
live through this one-of-a-kind tradeshow experience. Additional events and activities
include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PRESENTATION: Sony-Toshiba-IBM Center of Competence for the Cell Broadband Engine Processor &amp;ndash; presented by &lt;strong&gt;David A. Bader&lt;/strong&gt; on Tuesday, Nov. 18, from 1 - 2 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: High Performance Computing and Grid Computing for Large Scale Data Analysis and Integration &amp;ndash; presented by Joel Saltz, Tony Pan, Tashin Kurc and Ashish Sharma on Tuesday, Nov. 18, from 2 - 3 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Financial Modeling on the Cell Broadband Engine &amp;ndash; presented by Virat Agarwal on Tuesday, Nov. 18, from 3 - 4 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Concurrent Collections: A Model for Parallel Programming &amp;ndash; presented by Aparna Chandramowlishwaran on Tuesday, Nov. 18, from 4 - 5 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Dynamics of Inextensible Vesicles Suspended in a Two-Dimensional Stokes Flow &amp;ndash; presented by Abtin Rahimian on Tuesday, Nov. 18, from 5 - 6 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: On the Design of Fast Pseudo-Random Number Generators for the Cell Broadband Engine and an Application to Risk Analysis &amp;ndash; presented by Aparna Chandramowlishwaran on Wednesday, Nov. 19, from 10 - 11 a.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Dendro: Parallel Algorithms for Multigrid and AMR Methods on 2:1 Balanced Octrees &amp;ndash; presented by Rahul Sampath on Wednesday, Nov. 19, from 11 a.m. - 12 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Optimizing Discrete Wavelet Transform on the Cell Broadband Engine &amp;ndash; presented by Seunghwa Kang on Wednesday, Nov. 19, from 12 - 1 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Kernel-Independent Fast Multipole Method with Scalable Octree Construction &amp;ndash; presented by Ilya Lashuk on Wednesday, Nov. 19, from 1 - 2 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Numerical Relativity and XiRel: One SC Application &amp;ndash; presented by Deirdre Shoemaker on Wednesday, Nov. 19, from 2 - 3 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Large-Scale Graph Problems on the Cray XMT &amp;ndash; presented by David Ediger on Wednesday, Nov. 19, from 3 - 4 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;PRESENTATION: Multi-Threaded Maximum Flow Algorithm on Shared-Memory Platforms &amp;ndash; presented by Bo Hong on Wednesday, Nov. 19, from 4 - 5 p.m. at Booth 2821.&lt;/li&gt;
&lt;li&gt;VIDEO INTERVIEW: Jeffrey Skolnick, professor and director, Center for the Study of Systems Biology talks about his team&amp;rsquo;s development of tools for the prediction of protein structure and function from sequence, functional genomics, automatic assignment of enzymes to metabolic pathways, and prediction of protein structure and folding pathways.&lt;/li&gt;
&lt;li&gt;VIDEO INTERVIEW: Haesun Park, professor and associate chair, Division of Computational Science and Engineering, discusses her work in massive data analytics as part of the $3 million Foundations of Data and Visual Analytics (FODAVA) award Dr. Park and her team won from the National Science Foundation and the Department of Homeland Security.&lt;/li&gt;
&lt;li&gt;VIDEO INTERVIEW: Uzi Landman, Regents&amp;rsquo; professor and Fuller E. Callaway Chair in Computational Materials Science, will talk about his work at the nanoscale.&lt;/li&gt;
&lt;li&gt;VIDEO INTERVIEW: Pablo Laguna, professor and director, Center for Relativistic Astrophysics, shares the importance and impact of supercomputing on the study of black holes in order to prove Einstein&amp;rsquo;s theory of relativity.&lt;/li&gt;
&lt;li&gt;VIDEO INTERVIEW: Karsten Schwan, professor and director, Center for Experimental Research in Computer Systems, will talk about the emergence and impact of exascale computing both directly and indirectly on everyday life.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sc08-leadership-activities&#34;&gt;SC08 Leadership Activities&lt;/h3&gt;
&lt;p&gt;Georgia Tech&amp;rsquo;s &lt;strong&gt;David A. Bader&lt;/strong&gt; is co-chairing the SC08&amp;rsquo;s Biomedical Informatics Technology
Thrust with Joel Saltz of Emory University.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Kalyan Perumulla (Adjunct of the Computation Science and Engineering
(CSE) division), Ada Gavrilovska, and Karsten Schwan, serve on the SC08 Technical Program
Committee.&lt;/p&gt;
&lt;p&gt;Jeffrey Vetter is a member of the SC08 Tutorials Committee and Disruptive Technologies
Committee.&lt;/p&gt;
&lt;p&gt;Thomas Zacharia is the Invited Speakers chair.&lt;/p&gt;
&lt;p&gt;Four Georgia Tech CSE graduate students are serving as SC08 Student Volunteers: Swathi
Bhat, Aparna Chandramowlishwaran, Manisha Gajbe, and Seunghwa Kang.&lt;/p&gt;
&lt;p&gt;CSE Visiting Professor Zhihui Du (Associate Professor, Tsinghua University, China), was
awarded a conference participation grant through the SC08 Broader Engagements (BE)
Program.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20090627142705/http://www.hpcwire.com/specialfeatures/sc08/offthewire/Georgia_Tech_Enters_the_Spotlight_at_SC08.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/specialfeatures/sc08/offthewire/Georgia_Tech_Enters_the_Spotlight_at_SC08.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Executive Guide to SC08: Tuesday</title>
      <link>http://localhost:1313/blog/20081110-hpcwire/</link>
      <pubDate>Mon, 10 Nov 2008 17:07:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/20081110-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Michael Feldman&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Tuesday marks the first full day of the conference technical program. This year’s &lt;a href=&#34;http://sc08.supercomputing.org/?pg=keynote.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conference keynote&lt;/a&gt; will be given by Michael Dell, chairman and CEO of Dell, Inc. Dell’s selection reflects both the changing face of the industry, and the conference’s location – Dell is headquartered about 20 miles north of Austin in Round Rock, Texas.&lt;/p&gt;
&lt;h3 id=&#34;computing-at-scale&#34;&gt;Computing at Scale&lt;/h3&gt;
&lt;p&gt;There are many sessions on Tuesday that address the programming and operational challenges faced by managers of large computing systems today.&lt;/p&gt;
&lt;p&gt;The HPC Systems papers session includes presentations on the performance characteristics of Roadrunner, the largest system on the June TOP500 list, developing performance-optimal algorithms for multicore systems, and a comparison of the performance characteristics of several common HPC platforms.&lt;/p&gt;
&lt;p&gt;The Grid Resource Management papers session addresses many of the issues that arise in providing services over large scale, often distributed, computational resources. The session includes presentations on management of resources for massively multiplayer online games, strategies for predictable execution of jobs on shared computers, and auction-based grid reservations.&lt;/p&gt;
&lt;p&gt;Late in the day is a panel focused on an important issue that stands between users and effective use of very large scale parallel computers. &lt;a href=&#34;http://scyourway.nacse.org/conference/view/pan102&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Can Developing Applications for Massively Parallel Systems with Heterogeneous Processors Be Made Easy(er)?&lt;/a&gt; will feature some of today’s most recognized voices in this area: David Patterson, Marc Snir, &lt;strong&gt;David Bader&lt;/strong&gt;, Vivek Sarkar, and John Shalf.&lt;/p&gt;
&lt;h3 id=&#34;application-horizons&#34;&gt;Application Horizons&lt;/h3&gt;
&lt;p&gt;Tuesday features three Masterworks sessions on emerging HPC applications that have the potential to reshape our cultural understanding and how the business of business gets done.&lt;/p&gt;
&lt;p&gt;If you’re new to SC, the Masterworks sessions will be a real treat for you. They focus on the intersection of the real world and HPC, and provide an opportunity to learn from the people at the leading edge of the application of HPC to real world problems in business, finance, energy, the arts, and more.&lt;/p&gt;
&lt;p&gt;Humanities Scholarship in the Petabyte Age explores the transformation happening in the humanities as the artifacts of study — books, sculpture, recordings, and so on — are digitized and for the first time become available to the whole world for further study.&lt;/p&gt;
&lt;p&gt;The marriage of HPC and finance is explored in two sessions late in the day. High Performance Computing in the Financials: Where Rocket Science Meets ‘The Street’ will begin with an introduction on how banks use HPC today, and then discuss their unique computational requirements, and Golden Tree Asset Management will discuss the uses of HPC in finance.&lt;/p&gt;
&lt;h3 id=&#34;expanded-access&#34;&gt;Expanded Access&lt;/h3&gt;
&lt;p&gt;The final group of papers in the technical program on Tuesday includes Workflows. Papers in this session will review the state of the art in domain-based workflow composition, methods for leveraging workflow systems in solution of large problems, and double auctions for workflow scheduling in grid environments.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2008/11/10/executive_guide_to_sc08_tuesday/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2008/11/10/executive_guide_to_sc08_tuesday/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How the Large Hadron Collider Might Change the Web</title>
      <link>http://localhost:1313/blog/20080904-sciam/</link>
      <pubDate>Thu, 04 Sep 2008 12:59:04 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080904-sciam/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Mark Anderson&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When the Large Hadron Collider (LHC) begins smashing protons together this fall inside its 17-mile- (27-kilometer-) circumference underground particle racetrack near Geneva, Switzerland, it will usher in a new era not only of physics but also of computing.&lt;/p&gt;
&lt;p&gt;Before the year is out, the LHC is projected to begin pumping out a tsunami of raw data equivalent to one DVD (five gigabytes) every five seconds. Its annual output of 15 petabytes (15 million gigabytes) will soon dwarf that of any other scientific experiment in history.&lt;/p&gt;
&lt;p&gt;The challenge is making that data accessible to a scientist anywhere in the world at the execution of a few commands on her laptop. The solution is a global computer network called the LHC Computing Grid, and with any luck, it may be giving us a glimpse of the Internet of the future.&lt;/p&gt;
&lt;p&gt;Once the LHC reaches full capacity sometime next year, it will be churning out snapshots of particle collisions by the hundreds every second, captured in four subterranean detectors standing from one and a half to eight stories tall.* It is the grid&amp;rsquo;s job to find the extremely rare events—a bit of missing energy here, a pattern of particles there—that could solve lingering mysteries such as the origin of mass or the nature of dark matter.&lt;/p&gt;
&lt;p&gt;A generation earlier, research fellow Tim Berners-Lee of the European Organization for Nuclear Research (CERN) set out to create a global &amp;ldquo;pool of information&amp;rdquo; to meet a similar challenge. Then, as now, hundreds of collaborators across the planet were all trying to stay on top of rapidly evolving data from CERN experiments. Berners-Lee&amp;rsquo;s solution became the World Wide Web.&lt;/p&gt;
&lt;p&gt;But the fire hose of data that is the LHC requires special treatment. &amp;ldquo;If I look at the LHC and what it&amp;rsquo;s doing for the future,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high performance computing at the Georgia Institute of Technology, &amp;ldquo;the one thing that the Web hasn&amp;rsquo;t been able to do is manage a phenomenal wealth of data.&amp;rdquo; Bandwidth alone is a major bottleneck. Bader said that for researchers running supercomputer simulations, it&amp;rsquo;s cheaper to write the data to terabyte hard drives and ship them from one supercomputer center to another via FedEx than it is to transfer the gigantic data sets over the net.&lt;/p&gt;
&lt;p&gt;The LHC Computing Grid handles data in stages, referred to as tiers. &amp;ldquo;Tier 0,&amp;rdquo; located at CERN, is a massively parallel computer network composed of 100,000 of today&amp;rsquo;s fastest CPUs that stores and manages the raw data (1s and 0s) from the experiments. It ships portions of data over dedicated 10-gigabit-per-second fiber-optic lines to 11 &amp;ldquo;Tier 1&amp;rdquo; sites across North America, Asia and Europe. Brookhaven National Laboratory in Upton, N.Y., for example, receives data from the ALICE experiment, which collides lead ions.&lt;/p&gt;
&lt;p&gt;From those sites the data is parceled out for easier access among 140 Tier 2 computer networks based at universities, government labs and even private companies around the globe. Tier 2 is where scientists will actually access data and perform the kinds of hands-on numerical analysis needed to translate the raw 1s and 0s into energies and trajectories of particles.&lt;/p&gt;
&lt;p&gt;The crucial element that will make the data accessible, said project leader Ian Bird of CERN&amp;rsquo;s information technology (IT) department in Geneva, is a type of software known as &amp;ldquo;middleware&amp;rdquo;. The information a user wants may be spread among petabytes of data on different servers and stored in different formats. An open-source middleware platform called Globus is designed to gather that information seamlessly as though it&amp;rsquo;s sitting in a folder on one&amp;rsquo;s own desktop PC.&lt;/p&gt;
&lt;p&gt;The trial by fire that LHC programmers will be putting Globus through—and the modifications that emerge as a result—may be the first practical outgrowth of the LHC grid. If project scientists can tame massive, worldwide fields of networked data and computing cycles in particle physics, their solutions could well apply across the Internet—in much the same way that Berners-Lee&amp;rsquo;s specialized HTML invention morphed into the very backbone of modern technological society.&lt;/p&gt;
&lt;p&gt;Bader imagines future middleware allowing home computers to provide instant weather forecasts by accessing information from nearby environmental sensors. Or it might help sift through a life&amp;rsquo;s accumulation of personal medical records or years of home video footage looking for dimly remembered events.&lt;/p&gt;
&lt;p&gt;Ironically, CERN&amp;rsquo;s next great contribution to the Internet could be all but transparent to the end user. In a perfect world, Globus or its successors would simply make everything on a given grid straightforwardly and transparently accessible from any computer. &amp;ldquo;If Globus is a success,&amp;rdquo; Bader said, &amp;ldquo;then you won&amp;rsquo;t hear about it.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Correction (9/3/08): This article originally stated that the LHC will produce millions of snapshots of particle collisions; &amp;ldquo;millions&amp;rdquo; refers to the number of collisions, only a fraction of which will be recorded.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.scientificamerican.com/article/how-lhc-may-change-internet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.scientificamerican.com/article/how-lhc-may-change-internet/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputing: New Processor Architecture Holds Promise for Protein, Gene Studies</title>
      <link>http://localhost:1313/blog/20080901-genometechnology/</link>
      <pubDate>Mon, 01 Sep 2008 07:18:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080901-genometechnology/</guid>
      <description>&lt;p&gt;A supercomputing architecture that first
appeared in prototype
form more than 10 years
ago has been given a
new lease on life, thanks in part to
a recent $4 million Department of
Defense grant issued to seed the
new Center for Adaptive Supercomputing Software. The joint project
teams up Pacific Northwest
National Laboratory and supercomputer maker Cray, as well as
several institutions including
Georgia Institute of Technology
and Sandia National Laboratories.&lt;/p&gt;
&lt;p&gt;The initiative aims to develop
software that takes advantage of
the multithreaded processing
capabilities of Cray’s XMT supercomputer. Unlike traditional
supercomputer processing architecture, where each processor gets
a portion of memory for each calculation in a piece-by-piece fashion, the new processors are each
capable of multiple, simultaneous
data crunching and use a much
larger pool of memory per processing core. This design means
that many disparate sets
of complex data can be
digested at once, instead
of each portion of data
being handled piece by
piece.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, a computer scientist at Georgia
Tech, demonstrated the
architecture’s application
to biology by identifying proteins
that, when knocked out, disrupt
the cancer-causing networks in a
particular cell. Bader and his team
used a social networking algorithm to mine a huge collection of
publicly available human proteome datasets. “This is similar to
finding important people in a
social network, sometimes called
‘connectors,’” Bader says. “Looking
for these proteins is like looking
for a needle in a haystack — and it
is usually computationally intensive that won’t work well on
current [high-performance]
machines, but this new architecture is really designed for this type
of problem.”&lt;/p&gt;
&lt;p&gt;Normally, multiple database
searches of this kind would take
hours and hours to complete on a
typical cluster or supercomputer.
But with an algorithm specially
ported to this multithreaded processing architecture, the same job
takes mere seconds to complete,
says Bader. “These sorts
of problems have overwhelmed modest size
clusters, and if you start
adding processors to a
cluster, it takes longer
and longer to run
because the communication costs dominate,” he
says. “This is really the
first architecture where you can
pose a biological hypothesis, test it
out, and run it in short seconds or
minutes versus hours to days, or
maybe never.”&lt;/p&gt;
&lt;p&gt;Bader and his colleagues believe
the concept could offer a lot to largescale life science computing problems. “I think as we gather more
genomics data we can use such a
system to make scientific discoveries,” he says. “I would hope, looking
at three to five years, that we keep
investing in these novel types of
architecture and looking at the scientific results we can achieve, especially in the areas of solving
genomic problems and understanding of the genome.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;— Matthew Dublin&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Systems Biology</title>
      <link>http://localhost:1313/blog/20080818-gatech-researchhorizons/</link>
      <pubDate>Fri, 15 Aug 2008 18:52:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080818-gatech-researchhorizons/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Abby Vogel&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Research Horizons &lt;br&gt;
Summer 2008 &lt;br&gt;
Vol. 25, No. 3&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Oil reserves may raise false US hopes</title>
      <link>http://localhost:1313/blog/20080715-oilreserves/</link>
      <pubDate>Tue, 15 Jul 2008 06:53:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080715-oilreserves/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Sheila McNulty in Houston&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;No one knows the extent of US oil and natural gas reserves in the offshore and Arctic areas that
are off-limits to drilling. The last time they were surveyed was in the 1980s and the technology then
used is no longer considered accurate, say industry experts.&lt;/p&gt;
&lt;p&gt;“The youngest seismic [tests] in some of these areas is 25 years old,” said Bobby Ryan, Chevron’s
vice-president for global exploration.&lt;/p&gt;
&lt;p&gt;So even though President George W. Bush on Monday lifted a presidential ban on drilling on the
US outer continental shelf, it does not mean a big jump in US production is in the offing.
Not only must Congress lift a separate ban imposed in the 1980s but the industry must survey the
area with new technology to see what is there.&lt;/p&gt;
&lt;p&gt;When the latest data on protected areas were gathered, they were collected in pockets instead of
across a wide area, as is done now, which makes them much less comprehensive and gives less
precise results. The analysis of the data was also relatively primitive. So estimates of an extra 2m-
3m barrels of oil a day – roughly the equivalent of the daily output of Venezuela – might be some
way off.&lt;/p&gt;
&lt;p&gt;With oil prices rising sharply, polls show most Americans want Congress to lift the offshore ban if it
would help reduce prices at the pump. John McCain, the Republican presidential candidate,
advocates scrapping the ban while Barack Obama, the Democratic candidate, opposes it, saying
any extra oil and gas produced would take years to develop and further encourage fossil fuel use.
Regular efforts in Congress to lift the ban have been quashed, but growing public support will give
the move a better chance.&lt;/p&gt;
&lt;p&gt;Industry believes that surveys with new technologies should be conducted before any decisions
about opening up production in the protected areas are made.
New technology would give a far better picture of what is under the ground or ocean than before. It
gives indicators about rock type and quality, and whether there may be pockets that might contain
fossil fuel deposits, among other things.&lt;/p&gt;
&lt;p&gt;But the failure rate is still high until exploration wells can be drilled in promising areas. Two out of
three exploration wells in the Gulf of Mexico turned out to be dry holes.
“It’s still a risky business,” Mr Ryan said. “You still have to drill a well to know for sure. You are still
wildcatting.”&lt;/p&gt;
&lt;p&gt;Tom McClure, responsible for the upstream petroleum segment for IBM Deep Computing, said the
technology kept getting better, with computers becoming faster so they could process more data
and make better sense of it. “Imaging techniques are even better than they were five years ago.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, professor in computational science and engineering at Georgia Institute of
Technology, has been working with IBM on developing its PowerXCell processor, a
supercomputing chip originally designed for the Playstation 3, to search for oil reserves in what is
known as “ultradeep water” – 5,000ft or more deep.&lt;/p&gt;
&lt;p&gt;“The chip in the Play- station 3 is equivalent to what was in a super computer five years ago,” Mr
Bader said.&lt;/p&gt;
&lt;p&gt;But the industry is not going to use this technology to study protected areas unless they are open
to production. “It costs a lot of money to take a look,” Mr McClure said. “Nobody wants to take a
look unless you could get a return.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Copyright The Financial Times Limited 2008&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ft.com/content/c985b0c8-52a6-11dd-9ba7-000077b07658&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ft.com/content/c985b0c8-52a6-11dd-9ba7-000077b07658&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multithreaded supercomputer seeks software for data-intensive computing</title>
      <link>http://localhost:1313/blog/20080714-multithreaded/</link>
      <pubDate>Mon, 14 Jul 2008 07:07:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080714-multithreaded/</guid>
      <description>

















&lt;figure  id=&#34;figure-the-center-for-adaptive-supercomputing-software-will-take-advantage-of-the-multithreaded-processors-in-the-cray-xmt-image-courtesy-of-cray-inc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Center for Adaptive Supercomputing Software will take advantage of the multithreaded processors in the Cray XMT. *Image courtesy of Cray, Inc.*&#34; srcset=&#34;
               /blog/20080714-multithreaded/image_hu_893b45674e7cb6df.webp 400w,
               /blog/20080714-multithreaded/image_hu_675c441e3c5b8b08.webp 760w,
               /blog/20080714-multithreaded/image_hu_f3f23ffd28d71ba4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20080714-multithreaded/image_hu_893b45674e7cb6df.webp&#34;
               width=&#34;760&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Center for Adaptive Supercomputing Software will take advantage of the multithreaded processors in the Cray XMT. &lt;em&gt;Image courtesy of Cray, Inc.&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The newest breed of supercomputers have hardware set up not just for speed, but also to better tackle large networks of seemingly random data. And now, a multi-institutional group of researchers has been awarded $4.0 million to develop software for these supercomputers. Applications include anywhere complex webs of information can be found: from internet security and power grid stability to complex biological networks.&lt;/p&gt;
&lt;p&gt;The difference between the new breed and traditional supercomputers is how they access data, a difference that significantly increases computing power. But old software won&amp;rsquo;t run on the new hardware any more than a PC program will run on a Mac. So, the Department of Defense provided the funding this month to seed the Center for Adaptive Supercomputing Software, a joint project between the Department of Energy&amp;rsquo;s Pacific Northwest National Laboratory and Cray, Inc, in Seattle.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The system will allow much faster analysis of complex problems, like understanding and predicting how the power grid behaves &amp;ndash; one of the most complex engineering systems ever built,&amp;rdquo; said Moe Khaleel, director of Computational Sciences and Mathematics at PNNL, which is leading the project.&lt;/p&gt;
&lt;p&gt;Other researchers in the software collaboration hail from Sandia National Laboratories, Georgia Institute of Technology, Washington State University and the University of Delaware.&lt;/p&gt;
&lt;p&gt;These new machines are built with so-called &amp;ldquo;multithreaded processors&amp;rdquo; that enable multiple, simultaneous processing compared with the linear and slower approach of conventional systems. The Center will focus on applications for the multithreaded Cray XMT, one of which Cray delivered to PNNL in September 2007.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Traditional supercomputers are not well suited for certain kinds of data analysis, so we want to explore this advanced architecture,&amp;rdquo; said PNNL computational scientist Daniel Chavarría.&lt;/p&gt;
&lt;p&gt;In previously published work, PNNL computational scientist Jarek Nieplocha used a predecessor of the Cray XMT to run typical software programs that help operators keep the power grid running smoothly. Adapted to the advanced hardware, these programs ran 10 times faster on the multithreaded machine. &amp;ldquo;That was the best speed ever reported. We&amp;rsquo;re getting closer to being able to track the grid in real time,&amp;rdquo; said Nieplocha.&lt;/p&gt;
&lt;p&gt;In biology, another complex web is woven by genes (or their protein products) working together inside people&amp;rsquo;s cells. “We have discovered genes implicated in breast cancer using a massively multithreaded algorithm on the Cray XMT,” said Georgia Tech computational scientist and engineer &lt;strong&gt;David A. Bader&lt;/strong&gt;. “It’s like finding a needle in a haystack. The algorithm searches for genes whose removal quickly causes networks and pathways in the cell to breakdown.&amp;quot;&lt;/p&gt;
&lt;p&gt;The processors and computer memory in the advanced computers interact in a novel way. In traditional supercomputers, each processing chip gets a dollop of memory to use for its computations. To perform a calculation, the chip dips into the memory, does its work, then accesses the memory again for its next calculation, like an elephant dipping its trunk into a bag of peanuts and eating them one at a time. Each processor-memory unit is linked together over a network, and performance improvements come with more and faster processors and sleek network connections.&lt;/p&gt;
&lt;p&gt;The Cray XMT multithreaded system lumps all the memory together, and the processors freely access the much larger memory pool. But like an elephant with many trunks, each processor has multiple threads: it dips into memory with one thread, and while that thread is performing the calculation at hand, another thread goes into the memory, and another.&lt;/p&gt;
&lt;p&gt;By the time all the threads have dipped, the original thread has finished its calculation and is ready for another trip to the memory bank. A many-trunked elephant would have a distinct speed advantage plowing through a bag of peanuts over its hungrier zoo-mate, just as a multithreaded system does.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The processors are doing useful work all the time, so the computer can be faster,&amp;rdquo; said Chavarría. Each Cray XMT processor has 128 hardware threads with which to access the shared memory.&lt;/p&gt;
&lt;p&gt;Conceptually, this advantage translates into the machines being able to handle complex, random networks of data. Mainstream machines split up the data, assigning parcels of data to individual processing units. For example, a supercomputer trying to model how a community of microbes behaves would subdivide the community spatially.&lt;/p&gt;
&lt;p&gt;The computer would then analyze what goes on within each subdivision, but it couldn&amp;rsquo;t reach across other subdivisions to find out what happened to the microbe that wandered off to the other side of its habitat. Multithreaded machines, however, can examine the whole space at once, essentially assigning each thread to a microbe.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If all of your microbes move to the other side of the territory, it doesn&amp;rsquo;t matter, because the threads still have access,&amp;rdquo; said Chavarría.&lt;/p&gt;
&lt;p&gt;Another advantage multithreaded machines have over mainsteam computers is in power consumption. Although the Cray has not yet been tested, other multithreaded machines have shown reduced energy usage compared to traditional architectures.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The Computational Science &amp;amp; Engineering division at the Georgia Institute of Technology (CSE) was established in 2005 to strengthen and better reflect the critical role that computation plays in the science and engineering disciplines. CSE supports interdisciplinary research and education in computer science and applied mathematics. The Georgia Tech CSE program is designed to innovate and create new expertise, technologies, and practitioners in areas including high performance and grid computing, modeling and simulation, and data analysis and mining.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin company, for the U.S. Department of Energy’s National Nuclear Security Administration. With main facilities in Albuquerque, N.M., and Livermore, Calif., Sandia has major R&amp;amp;D responsibilities in national security, energy and environmental technologies, and economic competitiveness.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pnnl.gov/news/release.aspx?id=320&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.pnnl.gov/news/release.aspx?id=320&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Cell Processor Builds Its Mojo</title>
      <link>http://localhost:1313/blog/20080710-hpcwire/</link>
      <pubDate>Thu, 10 Jul 2008 14:14:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080710-hpcwire/</guid>
      <description>&lt;p&gt;With the arrival of the new QS22 blade, IBM and its partners are pushing hard to flesh out the
software ecosystem for the Cell Broadband Engine. The QS22 contains the double-precision
enhanced version of the processor, and as such, represents the first mainstream Cell platform
for HPC. Since the QS22 is also at the heart of the new Roadrunner petaflop machine for Los
Alamos, there&amp;rsquo;s been even more interest in exploring the application possibilities for the Cell.&lt;/p&gt;
&lt;p&gt;This week Georgia Tech announced they are renewing the Sony/Toshiba/IBM (STI) Center of
Competence effort that was initiated last year. The center is working with the three STI
partners plus a number of other firms and academic groups to help develop libraries,
application kernels, and productivity tools for the Cell architecture. The upcoming work will
include a new set of application research projects.&lt;/p&gt;
&lt;p&gt;One of the more interesting projects will involve using the processor to perform real-time
monitoring of an aircraft&amp;rsquo;s structural components while in flight. The computer system would
be hooked up to a network of sensors distributed around the aircraft to measure vibrations at
key locations. The Cell processor would be called upon to perform the heavy-duty FFTs
required to process the structural fatigue that is taking place across the fuselage, wings, and
engine assemblies. The application is meant to act as an early warning system for pilots, giving
them enough time to take the appropriate action (most likely, landing the plane) before a
catastrophic failure occurs. The system is being considered for both military and commercial
aircraft.&lt;/p&gt;
&lt;p&gt;To help develop this application, Georgia Tech is working with engineers at Pratt &amp;amp; Whitney
and researchers at the University of Connecticut. &lt;strong&gt;David Bader&lt;/strong&gt;, HPC director at Georgia Tech,
says that because the processor has cores to spare it would also be possible to task a Cell-based
system to simultaneously provide processing for both an in-flight entertainment system and
the structural diagnostics function, allowing commercial airlines to get double-duty from the
Cell hardware.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This is one example of the vision of the STI Center at Georgia Tech,&amp;rdquo; Bader told me. &amp;ldquo;We&amp;rsquo;re
really looking to make innovations at the application level.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Other application areas being looked at include data/file compression codes, financial services
apps, encryption libraries, multimedia encoders/decoders, and bioinformatics codes. The STI
Center is also working on software productivity enhancement tools that involve a crossplatform
profiler, performance estimation and tuning system with IDE type features. In
addition, they will be developing an automatic translator that takes a C/C++ application and
spits out the appropriate code for the Cell&amp;rsquo;s PowerPC and SPU (Synergistic Processing Unit)
cores.&lt;/p&gt;
&lt;p&gt;Even though some commercial partners are involved in these efforts, all software produced will
be open source, with some of the libraries and tools likely ending up in IBM&amp;rsquo;s own Cell SDK. At
this point in the processor&amp;rsquo;s life, this is the only rational model, since new architectures like
Cell will need a heavy infusion of software to become a popular platform for system integrators
and third-party software developers. It&amp;rsquo;s the typical &amp;ldquo;chicken-and-egg&amp;rdquo; problem all new
architectures must confront.&lt;/p&gt;
&lt;p&gt;One of the elements that is being talked about in the nascent Cell community is a standardized
development environment. Today the different form factors &amp;ndash; IBM QS2X blades, the
PlayStation game console, Toshiba&amp;rsquo;s Cell Reference Set, and the various platforms from
Mercury Computer Systems &amp;ndash; use different compiler/runtime systems. A standard compiler
and runtime environment would help create a richer environment for developers and allow
users to share code more easily. Bader says there is some consensus now to come up with a
common runtime environment that would be compatible across all the major form factors.&lt;/p&gt;
&lt;p&gt;But the big knock on Cell is that it&amp;rsquo;s just hard to program. The processor implements a rather
unique heterogeneous architecture &amp;ndash; one general-purpose PowerPC core that manages 8 SIMD
SPU cores. The SPU cores use a local store in place of cache and the SPUs talk with main
memory via DMA. At this point, it&amp;rsquo;s not possible to just recompile an existing application for
this processor. Developers need to break apart the program and rejigger it for the new
architecture.&lt;/p&gt;
&lt;p&gt;Bader says they&amp;rsquo;re trying to dispell the myth that it&amp;rsquo;s hard to program. Rather, he thinks it&amp;rsquo;s &amp;ldquo;a
processor ahead of its time.&amp;rdquo; He claims that the Cell is not really difficult to program directly,
once the underlying architecture is understood. Once that point is reached, Bader says
programmers comment on how flexible the architecture is. If you&amp;rsquo;re not disposed to code to the
bare metal, there are a few higher level environments available, including an OpenMP compiler
from IBM, the RapidMind development platform, and the Gadae compiler. As time goes on,
Bader believes &amp;ldquo;we&amp;rsquo;ll see a lot more of these frameworks that are portable across the Cell and
other processors.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080714012255/http://www.hpcwire.com/blogs/24391682.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hpcwire.com/blogs/24391682.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sony Group, Toshiba and IBM Renew Cell Broadband Engine Center with Georgia Tech</title>
      <link>http://localhost:1313/blog/20080709-ibm-cell-renew/</link>
      <pubDate>Wed, 09 Jul 2008 22:41:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080709-ibm-cell-renew/</guid>
      <description>&lt;p&gt;The Georgia Tech College of Computing today announced the renewal of the Sony Corporation/Sony Computer Entertainment Inc. (Sony Group)-Toshiba-IBM Center of Competence (STI Center), based on Georgia Tech’s exceptional work in multiple areas of research and evangelism for the Cell Broadband Engine™ (Cell/B.E.) technology. Through Georgia Tech’s efforts, the STI Center has been responsible for creating and disseminating software optimized for Cell/B.E. systems, and for performing research on the design of Cell/B.E. systems, algorithms and applications. In conjunction with this renewal of the STI Center, Georgia Tech is announcing a series of new research projects that are being undertaken at the center to develop applications and productivity tools based on the Cell/B.E. microprocessor.&lt;/p&gt;
&lt;p&gt;Georgia Tech also announced today that it will host the Second Annual Cell/B.E. Processor Workshop from July 10-11, 2008, focusing on software, tools and applications for the Cell/B.E. processor, including high performance computing applications and programmability tools. The two-day workshop is sponsored by Sony Group, Toshiba and IBM and will be held at the Klaus Advanced Computing Building on Georgia Tech’s campus. More information on the workshop may be found at http:/sti.cc.gatech.edu/.&lt;/p&gt;
&lt;p&gt;The STI Center of Competence was created at Georgia Tech to test the boundaries and demonstrate the extreme performance of the Cell/B.E. architecture. “Today, we are carrying out the vision we always intended - to generate breakthrough innovations using Cell/B.E. technologies working hand-in-hand with researchers at Sony Group, Toshiba and IBM,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, professor and executive director of High-Performance Computing in the Georgia Tech College of Computing. “We are very encouraged that our initial research results are showing the multi-faceted applicability of this technology.”&lt;/p&gt;
&lt;p&gt;One of the key research challenges that the collaborators will address through continued applied research is the use of Cell/B.E. technology to better monitor an aircraft’s structural safety in commercial and military airplanes. Researchers will develop Cell/B.E. based data-processing software that will expeditiously and accurately monitor structural components in flight by measuring and recording an aircraft’s vibrations through a distributed network of sensors. Although a commercial signal processing application for airplanes is a long term plan, researchers are working to develop a solid software foundation in the labs.&lt;/p&gt;
&lt;p&gt;“IBM has invested in a strategy that applies the use of technology to solve grand challenges with our trusted university partners,” said Jai Menon, IBM Fellow, vice president, Technical Strategy and University Relations. “In our collaboration with Georgia Tech, we are working together to better predict airline mechanical failures to make flying in airlines safer for passengers like you and me.”&lt;/p&gt;
&lt;p&gt;The other joint research projects in productivity enhancements include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A useful signal processing kernel needed for oil and gas exploration and seismic monitoring;&lt;/li&gt;
&lt;li&gt;Data compression, used for file compression or reducing the size of messages sent between computers required in multiple industries;&lt;/li&gt;
&lt;li&gt;Financial services applications for consolidated debt optimization, as well as European and American options pricing;&lt;/li&gt;
&lt;li&gt;Encryption libraries for securing communications for privacy;&lt;/li&gt;
&lt;li&gt;High-speed multimedia codecs, such as MPEG2 and JPEG2000 encoders and decoders;&lt;/li&gt;
&lt;li&gt;Bioinformatics, such as DNA sequence alignment and comparison;&lt;/li&gt;
&lt;li&gt;Software productivity enhancement tools that involve a cross-platform profiler, performance estimation and tuning system with IDE type features.&lt;/li&gt;
&lt;li&gt;Single-source automatic translator for generating PPU and SPU codes from a monolithic C/C++ application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“We anticipate a paradigm shift in computing and our collaboration with the Georgia Tech College of Computing will create innovative applications for Cell/B.E. processors,&amp;quot; said Yasu Yokote, general manager, CELL Application Development Center, Sony Corporation. &amp;ldquo;For a year STI Center created at Georgia Tech, they created software productivity enhancement tools, which are valuable for moving legacy code bases to CELL/B.E. and will generate tremendous value to all Cell-based products.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Within a year of the opening of the Center of Competence at Georgia Tech, researchers are already generating outstanding results on Cell/B.E.,” said Mitsuo Saito, Chief Fellow, Toshiba Corporation Semiconductor Company. &amp;ldquo;The future will see growing demand for multi-core processor applications, and we are delighted that the Center is playing a key role in anticipating and responding to such demand.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;about-the-georgia-tech-college-of-computing&#34;&gt;About the Georgia Tech College of Computing&lt;/h2&gt;
&lt;p&gt;The Georgia Tech College of Computing is a national leader in the research and creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked 9th nationally by U.S. News and World Report, the College’s unconventional approach to education is pioneering the new era of computing by expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human centered solutions. For more information about the College of Computing, its academic divisions and research centers, please visit &lt;a href=&#34;http://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;about-the-cell-broadband-engine&#34;&gt;About the Cell Broadband Engine&lt;/h2&gt;
&lt;p&gt;The revolutionary Cell/B.E. processor is a breakthrough design featuring a central processing core, based on IBM&amp;rsquo;s industry leading Power Architecture™ technology, and eight synergistic processors. Cell/B.E. &amp;ldquo;supercharges&amp;rdquo; computation-intensive applications, offering fast performance for computer entertainment and handhelds, virtual-reality, wireless downloads, real-time video chat, interactive TV shows and other &amp;ldquo;image-hungry&amp;rdquo; computing environments. The processor was created through a collaboration between Sony Group, Toshiba Corporation (Toshiba) and IBM.&lt;/p&gt;
&lt;p&gt;All company/product names and service marks may be trademarks or registered trademarks of their respective companies. Cell Broadband Engine is a trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;h3 id=&#34;contacts&#34;&gt;Contacts&lt;/h3&gt;
&lt;p&gt;Georgia Tech College of Computing &lt;br&gt;
Stefany Wilson, 404-312-6620 &lt;br&gt;
&lt;a href=&#34;mailto:stefany@cc.gatech.edu&#34;&gt;stefany@cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.businesswire.com/news/home/20080708006273/en/Sony-Group-Toshiba-IBM-Renew-Cell-Broadband&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.businesswire.com/news/home/20080708006273/en/Sony-Group-Toshiba-IBM-Renew-Cell-Broadband&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ga. Tech finds creative uses for gaming chip</title>
      <link>http://localhost:1313/blog/20080709-ajc/</link>
      <pubDate>Wed, 09 Jul 2008 07:37:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080709-ajc/</guid>
      <description>&lt;p&gt;&lt;em&gt;By David Ho&lt;/em&gt;, Cox New York Correspondent&lt;/p&gt;
&lt;p&gt;A video game microchip may be the key to a system that keeps aging commercial and military airplanes structurally safe, Georgia Tech&amp;rsquo;s College of Computing plans to announce Thursday.&lt;/p&gt;
&lt;p&gt;The research is among several projects being unveiled this week by an alliance of the institute&amp;rsquo;s scientists and tech companies Sony Corp., Toshiba Corp. and IBM Corp.&lt;/p&gt;
&lt;p&gt;The work focuses on the Cell processor, which is the heart of the Sony PlayStation 3 game console but also has been used in fields such as medical research and oil exploration.&lt;/p&gt;
&lt;p&gt;Georgia Tech&amp;rsquo;s project plans to add to the list of Cell applications, first by developing software that uses the chip and a network of sensors to monitor how a plane&amp;rsquo;s components vibrate in flight.&lt;/p&gt;
&lt;p&gt;With aircraft in service getting older, understanding their condition and predicting mechanical problems becomes increasingly important, said &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing at Georgia Tech.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If you could have essentially an early warning system, you may be able to tell the pilot: &amp;lsquo;There is an issue with the aircraft&amp;rsquo;s health and you have an hour to land this plane before you&amp;rsquo;ll have a catastrophic failure,&amp;rsquo; &amp;quot; Bader said.&lt;/p&gt;
&lt;p&gt;The Cell processor&amp;rsquo;s ability to process complex data combined with its gaming heritage also means that seat-back computers on planes could perform double duty as entertainment and safety devices, he said.&lt;/p&gt;
&lt;p&gt;While a commercial product is the project&amp;rsquo;s long-term goal, researchers are now focusing on developing the software in the lab, the university said.&lt;/p&gt;
&lt;p&gt;The jointly developed Cell processor, built around technology from Armonk, N.Y.-based IBM, also powers some supercomputers and is often used to process detailed 3-D images.&lt;/p&gt;
&lt;p&gt;Georgia Tech is holding a workshop this week that focuses on ways Cell can be used in new industries and markets.&lt;/p&gt;
&lt;p&gt;Researchers at the school&amp;rsquo;s Sony-Toshiba-IBM Center of Competence are announcing seven pilot projects involving the chip. They include work on seismic monitoring, electronic financial services, secure communications and biology research.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080801225855/http://www.ajc.com/business/content/printedition/2008/07/09/compute.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ajc.com/business/content/printedition/2008/07/09/compute.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NPR interviews David Bader on the chip in the PlayStation 3</title>
      <link>http://localhost:1313/blog/20080709-wabe/</link>
      <pubDate>Wed, 09 Jul 2008 06:53:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080709-wabe/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;wabe_20080709.mp3&#34;&gt;NPR radio interview on Atlanta&amp;rsquo;s 90.1 WABE station&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this radio interview, Atlanta&amp;rsquo;s NPR station 90.1 WABE&amp;rsquo;s John Lemley interviews &lt;strong&gt;David Bader&lt;/strong&gt; on the IBM Cell microchip in the Sony PlayStation 3 that could save lives.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech studying Playstation chip for plane safety</title>
      <link>http://localhost:1313/blog/20080709-playstation/</link>
      <pubDate>Wed, 09 Jul 2008 06:42:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080709-playstation/</guid>
      <description>&lt;p&gt;&lt;em&gt;By David Ho&lt;/em&gt; &lt;br&gt;
&lt;em&gt;Cox News Service&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A video game microchip may be the key to a system that keeps aging commercial and
military airplanes structurally safe, GeorgiaTech&amp;rsquo;s College of Computing plans to announce Thursday.&lt;/p&gt;
&lt;p&gt;The research is among several projects being unveiled this week by an alliance of the institute&amp;rsquo;s
scientists and the technology companies Sony Corp., Toshiba Corp. and IBM Corp.&lt;/p&gt;
&lt;p&gt;The work focuses on the Cell processor, which is the heart of the Sony PlayStation 3 game console but
also has been used in fields such as medical research and oil exploration.&lt;/p&gt;
&lt;p&gt;Georgia Tech&amp;rsquo;s project plans to add to the list of Cell applications, first by developing software that uses
the chip and a network of sensors to monitor how a plane&amp;rsquo;s components vibrate in flight.
With aircraft in service getting older, understanding their condition and predicting mechanical problems
becomes increasingly important, said &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing at
Georgia Tech.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If you could have essentially an early warning system, you may be able to tell the pilot: &amp;lsquo;There is an
issue with the aircraft&amp;rsquo;s health and you have an hour to land this plane before you&amp;rsquo;ll have a catastrophic
failure,&amp;rsquo; &amp;ldquo;Bader said.&lt;/p&gt;
&lt;p&gt;The Cell processor&amp;rsquo;s ability to process complex data combined with its gaming heritage also means that
seat-back computers on planes could perform double duty as entertainment and safety devices, he said.&lt;/p&gt;
&lt;p&gt;While a commercial product is the project&amp;rsquo;s long-term goal, researchers are now focusing on developing
the software in the lab, the university said.&lt;/p&gt;
&lt;p&gt;The jointly developed Cell processor, built around technology from Armonk, N.Y.-based IBM, also
powers some supercomputers and is often used to process detailed 3-D images.&lt;/p&gt;
&lt;p&gt;Georgia Tech is hosting a workshop this week that focuses on ways Cell can be used in new industries
and markets.&lt;/p&gt;
&lt;p&gt;Researchers at the school&amp;rsquo;s Sony-Toshiba-IBM Center of Competence are announcing seven pilot
projects involving the chip. They include work on seismic monitoring, electronic financial services,
secure communications and biology research.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;David Ho is a New York correspondent for Cox Newspapers.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Featured Keynote Speaker: David Bader at Third Annual High Performance Computing Day at Lehigh University</title>
      <link>http://localhost:1313/blog/20080404-lehigh-keynote/</link>
      <pubDate>Fri, 04 Apr 2008 06:38:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080404-lehigh-keynote/</guid>
      <description>&lt;h3 id=&#34;third-annual-high-performance-computing-day-at-lehigh&#34;&gt;Third Annual High Performance Computing Day at Lehigh&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.lehigh.edu/computing/hpc/hpcday.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.lehigh.edu/computing/hpc/hpcday.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Friday, April 4, 2008&lt;br&gt;
Featured Keynote Speaker&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt; &amp;lsquo;90, &amp;lsquo;91G&lt;br&gt;
Petascale Phylogenetic Reconstruction of Evolutionary Histories&lt;br&gt;
&lt;a href=&#34;http://www.lehigh.edu/computing/hpc/hpcday/2008/hpckeynote.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.lehigh.edu/computing/hpc/hpcday/2008/hpckeynote.html&lt;/a&gt; &lt;br&gt;
Executive Director of High Performance Computing&lt;br&gt;
College of Computing, Georgia Institute of Technology&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://webapps.lehigh.edu/hpc/2008/hpcday_2008.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://webapps.lehigh.edu/hpc/2008/hpcday_2008.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intel, Microsoft fund university chip research</title>
      <link>http://localhost:1313/blog/20080319-intelmicrosoft/</link>
      <pubDate>Wed, 19 Mar 2008 15:32:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080319-intelmicrosoft/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Tom Abate, Chronicle Staff Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After more than two decades of boosting chip performance by using two hardware tricks that work
hand-in-glove, Intel Corp. has realized that one trick makes computers too hot, so it has been
forced to redesign its microprocessors in a way that requires the invention of a whole new approach
to PC software.&lt;/p&gt;
&lt;p&gt;On Tuesday, Intel, joined by Microsoft Corp., said it will invest $20 million over the next five years
to fund software research at UC Berkeley and the University of Illinois at Urbana-Champaign to
program a way around this unexpected dead-end in chip design.&lt;/p&gt;
&lt;p&gt;In a nutshell, it was only a few years ago that Intel realized its approach to chip design - making a
single microprocessor smaller and faster - would turn out processors too hot to handle, said UC
Professor Kurt Keutzer, who is part of the local team.&lt;/p&gt;
&lt;p&gt;Intel is already at work on a new approach to chip design that would put a slew of mini-processors,
or cores, on a single sliver of silicon, the idea being that each could run slower and cooler but
together they would still get the job done.&lt;/p&gt;
&lt;p&gt;But before these new multicore chips can do anything useful for consumers or businesses, Intel and
Microsoft must develop a new approach to software, called parallel processing. That is the part of
the puzzle Intel and Microsoft hope the computer whizzes at Berkeley and Illinois can solve.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Right now, it&amp;rsquo;s a chicken-and-egg problem,&amp;rdquo; said UC Berkeley Professor David Patterson, another
scientist on the project. &amp;ldquo;This is a new way of processing that we don&amp;rsquo;t know how to do yet.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Potential applications for multicore computers include faster and more realistic video processing,
better speech recognition and quicker searching of databases.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s another dimension to this dilemma that has the potential to disrupt the chip-making
industry. Other firms, like Nvidia and IBM, also are building multicore processors, and if they solve
the software-and-hardware problem first, that could tilt the balance of power in personal
computing away from the industry&amp;rsquo;s reigning co-rulers, Intel and Microsoft.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This is a really new time in the history of computing, truly a paradigm shift,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, a
professor of computing at the Georgia Institute of Technology who is an expert in parallel
processing but is not directly associated with the Intel-Microsoft project.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;When there is a paradigm shift, there is a risk to established companies,&amp;rdquo; agreed Marc Snir, the
University of Illinois computer scientist who will lead efforts there to make sure that the &amp;ldquo;Wintel&amp;rdquo;
duo doesn&amp;rsquo;t miss the paradigm shift.&lt;/p&gt;
&lt;p&gt;But Intel - whose motto used to be &amp;ldquo;Only the paranoid survive&amp;rdquo; - is already feeling the heat from its
Santa Clara neighbor, Nvidia, which is testing multicore chips with a new way to program them
called Cuda.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;When we hear Intel say &amp;lsquo;multicore&amp;rsquo; we say &amp;lsquo;Welcome to the party,&amp;rsquo; &amp;quot; said Nvidia processor expert
Ujesh Desai.&lt;/p&gt;
&lt;p&gt;During a teleconference Tuesday, the parties involved in the research project explained how the
intellectual fruits of this university-industry partnership would be shared. Computer scientists at
both universities will be free to publish any and all findings, while Microsoft and Intel will be get
nonexclusive licenses to any ideas they want, and be able to negotiate for more-exclusive terms on
technologies they deem particularly useful.&lt;/p&gt;
&lt;p&gt;Patterson, the UC scientist, said the research deal represents a great win for Berkeley, which was
chosen over 25 institutions, including Stanford, to lead the project.&lt;/p&gt;
&lt;p&gt;Patterson likened Berkeley&amp;rsquo;s involvement in the parallel-processing software to its leadership in
past technology leaps, like its pivotal role in the development of the Unix operating system and
other technologies that &amp;ldquo;led to multibillion-dollar industries.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;E-mail Tom Abate at &lt;a href=&#34;mailto:tabate@sfchronicle.com&#34;&gt;tabate@sfchronicle.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sfgate.com/business/article/Intel-Microsoft-fund-university-chip-research-3223678.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.sfgate.com/business/article/Intel-Microsoft-fund-university-chip-research-3223678.php&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article appeared on page C - 1 of the San Francisco Chronicle&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>H-P to unveil big revamp in its famed labs</title>
      <link>http://localhost:1313/blog/20080304-marketwatch/</link>
      <pubDate>Tue, 04 Mar 2008 17:12:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080304-marketwatch/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Therese Poletti&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;No technology company wants to end up with great research that it fails to commercialize.&lt;/p&gt;
&lt;p&gt;Silicon Valley is too familiar with the failure of the research lab previously known as Xerox PARC to capitalize on its early innovations for the personal computer in the 1980s. Their work provided the seeds for the point-and-click user interface commercialized first by Apple Inc. AAPL, -4.62% and then Microsoft Corp. MSFT, -3.19%, and Xerox got only Apple shares.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;All the MBAs read about &amp;lsquo;Fumbling the Future&amp;rsquo; at Xerox PARC,&amp;rdquo; said David Patterson, an inventor and professor in the computer science department at the University of California, Berkeley, referring to a book on Xerox PARC. Even though PARC went on to innovate in its core businesses, such as developing laser printing, its storied failure lives on in the Valley.&lt;/p&gt;
&lt;p&gt;Amid this backdrop, Hewlett-Packard Co. will unveil a big push to ensure that its famed research group, H-P Labs, about a mile from PARC, contributes more to the printer and computer giant&amp;rsquo;s bottom line. H-P Labs, PARC, SRI and IBM&amp;rsquo;s IBM, -3.54% Almaden Research Center are the elders among the Valley&amp;rsquo;s research institutions that are now all confronting new ways to turn some of their research into commercially viable projects at a faster pace.&lt;/p&gt;
&lt;p&gt;This Thursday, at an event hosted by H-P HPQ, -5.92% Chief Executive Mark Hurd, executives will sketch out new directions for H-P Labs.&lt;/p&gt;
&lt;p&gt;This revamp is expected to be the biggest reorganization of H-P Labs in more than a decade, and details are expected to provide the key areas of focus for the future for one of the tech industry&amp;rsquo;s most prestigious research laboratories. The legacy of H-P Labs, founded in 1966, is also entwined with that of founders Bill Hewlett and Dave Packard, who were inventors themselves and whose offices are enshrined at H-P Labs&amp;rsquo; local home, an airy modern building on Page Mill Road, a stone&amp;rsquo;s throw from headquarters in Palo Alto, Calif.&lt;/p&gt;
&lt;p&gt;Thursday will also serve as a sort of debut event for Prith Banerjee, a former academic and entrepreneur who left life in the ivory tower to join the new H-P under the regime of Hurd&amp;rsquo;s cost-obsessed and results-oriented H-P. So far, one source said, Banerjee has been removing layers of management in the labs, and projects without results in sight are getting shelved.&lt;/p&gt;
&lt;p&gt;In an era of cost-cutting and ruthless focus on the bottom line, it is perhaps miraculous that H-P still conducts basic research into areas such as quantum science, nanotechnology and information management.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;With the turmoil that they went through, it is a real credit that they kept the labs together,&amp;rdquo; said Paul Saffo, a veteran technology forecaster. &amp;ldquo;H-P Labs has always been one of the great corporate R&amp;amp;D labs.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In recent years, some innovations out of H-P Labs include its major advancements in developing computer chip circuitry at the atomic scale, software to automate data centers and building what is believed to be the first utility computing center, where customers can get computing power based on their changing needs.&lt;/p&gt;
&lt;p&gt;I can only guess what some of H-P&amp;rsquo;s big focuses will be, based on a chat I had with Banerjee last year. He wants to identify big themes, really big bets, and throw a lot of resources at those top areas. H-P will likely continue to focus on nanotechnology, reducing the power and increasing automation in computer data centers, and advances in digital printing, among others. Eventually, H-P will focus on about 30 projects, Banerjee said last year.&lt;/p&gt;
&lt;p&gt;Banerjee, who appears to be well-liked among his H-P Labs colleagues and respected among academics, made it clear in a few interviews after he joined H-P that he plans to focus all research with a specific market in mind.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I&amp;rsquo;d start with making sure that when a research project begins there&amp;rsquo;s a reason why &amp;ndash; not just a scientific reason, but a market research, why this work should proceed,&amp;rdquo; Banerjee said last year. He is also expected to have staffers from the product divisions join the research teams within the labs, to move faster toward creating real products, when feasible.&lt;/p&gt;
&lt;p&gt;H-P spokesman David Harrah declined to provide any specifics on what H-P Labs will announce, only to say that H-P will discuss the direction of the labs. The event originally coincided with a press conference at Apple in Cupertino, where the company will launch its roadmap for developers to help them create software applications for the popular iPhone. H-P originally had scheduled its event an hour before Apple&amp;rsquo;s and later changed it to the afternoon, to accommodate Hurd&amp;rsquo;s schedule, Harrah said.&lt;/p&gt;
&lt;p&gt;I suspect they moved the time after they saw the competing event from Apple, but that&amp;rsquo;s just me.&lt;/p&gt;
&lt;p&gt;Like many other tech companies faced with the dilemma of how to innovate for the future and keep generating revenues to fuel the bottom line and fund that research, H-P is also likely to adopt a model of more partnerships with universities, given Banerjee&amp;rsquo;s background at the University of Illinois, where he was the dean of the college of engineering.&lt;/p&gt;
&lt;p&gt;Forming partnerships is a trend that many tech firms have embraced in recent years. Microsoft, for example, which formed its now-respected Microsoft Research in 1991, will open up a new research lab later this year in Cambridge, Mass., allowing for more collaboration with nearby universities such as MIT and Harvard. Intel Corp. INTC, -3.89% has formed small lab-lets with engineers on university campuses, such as Berkeley.&lt;/p&gt;
&lt;p&gt;It is a positive trend that lets the tech business innovate at a faster pace while at the same time government spending on basic research continues to decline.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;There has to be a shorter time span between innovative ideas and that tech transfer,&amp;rdquo; said &lt;strong&gt;David Bader&lt;/strong&gt;, Executive Director of High-Performance Computing at Georgia Institute of Technology in Atlanta, which works with IBM, Sony and Toshiba on the Cell chip. &amp;ldquo;It changes the traditional model of doing research and doing a slow handoff to industry and seeing it in a product in 10 to 20 years. Instead it&amp;rsquo;s down to two and half to three years.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;As it winnows out its projects, H-P also has to walk a fine line of focusing on research that bears fruit more quickly, and not scare away their star researchers who may be disappointed with the loss of their pie-in-the-sky projects.&lt;/p&gt;
&lt;p&gt;I am not sure completely cutting far-out projects with no end in sight is such a good thing, because sometimes this is where hidden gems may lie, but hopefully Banerjee is harvesting the best out of H-P Labs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.marketwatch.com/story/h-p-to-announce-revamping-of-famed-h-p-labs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.marketwatch.com/story/h-p-to-announce-revamping-of-famed-h-p-labs&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM selects Bader for its 2008 Technical Leadership Forum</title>
      <link>http://localhost:1313/blog/20080205-ibm-tlf/</link>
      <pubDate>Tue, 05 Feb 2008 20:17:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20080205-ibm-tlf/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20080205-ibm-tlf/letter_hu_8a6440e02a8c41c8.webp 400w,
               /blog/20080205-ibm-tlf/letter_hu_c1fd00d49be0ad70.webp 760w,
               /blog/20080205-ibm-tlf/letter_hu_42318384cf1c01dc.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20080205-ibm-tlf/letter_hu_8a6440e02a8c41c8.webp&#34;
               width=&#34;592&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Will Faster Supercomputers Help Solve World’s Problems?</title>
      <link>http://localhost:1313/blog/20071213-realtruth/</link>
      <pubDate>Thu, 13 Dec 2007 08:01:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20071213-realtruth/</guid>
      <description>&lt;p&gt;Over twice as fast as today’s most commonly used supercomputers, petascale processors—which are able to
perform 1,000 trillion calculations per second—are slated to be released as early as 2008.&lt;/p&gt;
&lt;p&gt;With the power of 100,000 desktop computers, the newest generation of supercomputers is predicted to have a
profound impact in the fields of business and industry. Not only will stock brokers be able to use their processing
power to better predict swings in the stock market, but the automotive industry will be able to use the higher
processing speed to limit the need for building vehicle models—thus mitigating most defects before a prototype
is built.&lt;/p&gt;
&lt;p&gt;Advancements in supercomputing are also projected to aid in scientific discovery and speed up research. Experts
maintain that as supercomputing technology advances, new models will be able to more quickly analyze massive
data sets and test complex scientific models. Already supercomputers are used where experimentation is
impossible or too dangerous, costly or time consuming.&lt;/p&gt;
&lt;p&gt;In an interview with iTnews, &lt;strong&gt;David A. Bader&lt;/strong&gt;, editor of the first book on petascale computing, said that scientists
have high hopes for this next level of computing. He said that not only will the new machines make calculations
easier, but they will be
applied “to address our national and global priorities, such as sustainability of our natural environment by
reducing our carbon footprint and by decreasing our dependencies on fossil fuels, improving human health and
living conditions, understanding the mechanisms of life from molecules and systems to organisms and
populations, preventing the spread of disease, predicting and tracking severe weather, recovering from natural
and human-caused disasters, maintaining national security, and mastering nanotechnologies.”&lt;/p&gt;
&lt;p&gt;But despite petascale computers’ ability to help solve mankind’s persistent problems, experts have already
turned their sights on the next generation of supercomputers. Exascale computing, estimated to be released in
2018, will be capable of running one million trillion calculations per second.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rcg.org/realtruth/news/071213-002-science.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://rcg.org/realtruth/news/071213-002-science.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Petascale computers: the next supercomputing wave</title>
      <link>http://localhost:1313/blog/20071129-itnews/</link>
      <pubDate>Thu, 29 Nov 2007 17:05:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20071129-itnews/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Liz Tay&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The author of the world&amp;rsquo;s first published collection on petascale techniques, &lt;strong&gt;David A. Bader&lt;/strong&gt;, discusses petascale, exascale and the future of computing.&lt;/p&gt;
&lt;p&gt;Supercomputing has come a long way in the past half-century. Far from CDC&amp;rsquo;s single-operation scalar processors in the 1960s, present day terascale computers in development by companies like Intel boast up to 100 processor cores and the ability to perform one trillion operations per second.&lt;/p&gt;
&lt;p&gt;Now, academics have turned their attention to petascale computers that are said to be capable of performing one quadrillion – that&amp;rsquo;s &lt;em&gt;one million billion&lt;/em&gt; – operations per second. Running at nearly ten times the speed of today&amp;rsquo;s fastest supercomputers, petascale computing is expected to open the doors to solving global challenges such as environmental sustainability, disease prevention, and disaster recovery.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Petascale Computing: Algorithms and Applications&amp;rdquo; was launched this month as the world&amp;rsquo;s first published collection on petascale techniques for computational science and engineering. Edited by &lt;strong&gt;David A. Bader&lt;/strong&gt;, associate professor of computing and executive director of high-performance computing at Georgia Tech, the book is geared towards generating new discussion about the future of computing.&lt;/p&gt;
&lt;p&gt;To gain a little insight into petascale technologies, iTnews spoke with Bader about high performance computing and the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What is petascale computing?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Petascale Computing is the present state-of-the-art in High Performance Computing that leverages the most cutting edge large-scale resources to solve grand challenge problems in science and engineering.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What will be the primary use of petascale computers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Science has withstood centuries of challenges by building upon the community&amp;rsquo;s collective wisdom and knowledge through theory and experiment. However, in the past half-century, the research community has implicitly accepted a fundamental change to the scientific method.&lt;/p&gt;
&lt;p&gt;In addition to theory and experiment, computation is often cited as the third pillar as a means for scientific discovery.&lt;/p&gt;
&lt;p&gt;Computational science enables us to investigate phenomena where economics or constraints preclude experimentation, evaluate complex models and manage massive data volumes, model processes across interdisciplinary boundaries, and transform business and engineering practices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How significant will the development of Petascale Computing be to the advancement of science and technology?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Increasingly, cyber-infrastructure is required to address our national and global priorities, such as sustainability of our natural environment by reducing our carbon footprint and by decreasing our dependencies on fossil fuels, improving human health and living conditions, understanding the mechanisms of life from molecules and systems to organisms and populations, preventing the spread of disease, predicting and tracking severe weather, recovering from natural and human-caused disasters, maintaining national security, and mastering nanotechnologies.&lt;/p&gt;
&lt;p&gt;Several of our most fundamental intellectual questions also require computation, such as the formation of the universe, the evolution of life, and the properties of matter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What are the challenges faced by Petascale Computing?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While petascale architectures certainly will be held as magnificent feats of engineering skill, the community anticipates an even harder challenge in scaling up algorithms and applications for these leadership-class supercomputing systems.&lt;/p&gt;
&lt;p&gt;Several areas are important for this task: scalable algorithm design for massive concurrency, computational science and engineering applications, petascale tools, programming methodologies, performance analyses, and scientific visualisation.&lt;/p&gt;
&lt;p&gt;High end simulation is a tool for computational science and engineering applications. To be useful tools for science, such simulations must be based on accurate mathematical descriptions of the processes and thus they begin with mathematical formulations, such as partial differential equations, integral equations, graph-theoretic, or combinatorial optimisation.&lt;/p&gt;
&lt;p&gt;Because of the ever-growing complexity of scientific and engineering problems, computational needs continue to increase rapidly. But most of the currently available hardware, software, systems, and algorithms are primarily focused on business applications or smaller scale scientific and engineering problems, and cannot meet the high-end computing needs of cutting-edge scientific and engineering work.&lt;/p&gt;
&lt;p&gt;This book [&amp;ldquo;Petascale Computing: Algorithms and Applications&amp;rdquo;] primarily addresses the concerns of petascale scientific applications, which are highly compute- and data-intensive, cannot be satisfied in today&amp;rsquo;s typical cluster environment, and tax even the largest available supercomputer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When can we expect petascale computers, and who from?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Realising that cyber infrastructure is essential to research innovation and competitiveness, several nations are now in [what has been called] a &amp;rsquo;new arms race to build the world&amp;rsquo;s mightiest computer&#39;.&lt;/p&gt;
&lt;p&gt;These petascale computers, expected around 2008 to 2012, will perform ten to the power of 15 operations per second, nearly an order of magnitude faster than today&amp;rsquo;s speediest supercomputer.&lt;/p&gt;
&lt;p&gt;In fact, several nations are in a worldwide race to deliver high-performance computing systems that can achieve 10 petaflops or more within the next five years. Several leading computing vendors, such as IBM, Cray, Dawning, NEC, Sun Microsystems, and others, are in a race to deploy petascale computing systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do you think technology will progress in 2008 and beyond? What is likely to be the next set of challenges we will face?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We expect to see the first peak petascale systems in 2008, and sustained petascale systems soon thereafter. Many grand challenge investigations in computational science and engineering will be solved by these magnificent systems.&lt;/p&gt;
&lt;p&gt;Already, discussions are taking place on the complex applications that will require the next generations of supercomputers: exascale systems that are 1000 times more capable than these petascale systems.&lt;/p&gt;
&lt;p&gt;Certainly amazing technological innovations and application development will be necessary to make exascale computing a reality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yours is the first book on petascale techniques to ever have been published. What do you hope it will achieve?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My goal in developing this book was to inspire members of the high-performance computing community to solve computational grand challenges that will help our society, protect our environment, and improve our understanding in fundamental ways, all through the efficient use of petascale computing.&lt;/p&gt;
&lt;p&gt;This is the premier book that captures the first wave of applications anticipated to run on these petascale computers. Already, international interest in this milestone book is very high, since it is the first book ever to discuss applications of petascale computing.&lt;/p&gt;
&lt;p&gt;This book in petascale computing will be a resource for training these [future] generations of students and researchers in how to leverage state-of-the-art high performance computing systems to solve grand challenges of national and global priority, such as sustainability of our natural environment by reducing our carbon footprint and by decreasing our dependencies on fossil fuels, improving human health and living conditions, understanding the mechanisms of life from molecules and systems to organisms and populations, preventing the spread of disease, predicting and tracking severe weather, recovering from natural and human-caused disasters, maintaining national security, and mastering nanotechnologies.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.itnews.com.au/feature/petascale-computers-the-next-supercomputing-wave-98316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.itnews.com.au/feature/petascale-computers-the-next-supercomputing-wave-98316&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Students Explore Video Game Programming and Architecture in New Course</title>
      <link>http://localhost:1313/blog/20071127-videogameprogramming/</link>
      <pubDate>Tue, 27 Nov 2007 07:49:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/20071127-videogameprogramming/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20071127-videogameprogramming/image_hu_6635a080431c8fd2.webp 400w,
               /blog/20071127-videogameprogramming/image_hu_8cd9fa98c4252711.webp 760w,
               /blog/20071127-videogameprogramming/image_hu_a000b4bee70ccf8d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20071127-videogameprogramming/image_hu_6635a080431c8fd2.webp&#34;
               width=&#34;283&#34;
               height=&#34;202&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20071127-videogameprogramming/Abhishek-Venkatesh-Xbox1_hu_e2b91885228502ad.webp 400w,
               /blog/20071127-videogameprogramming/Abhishek-Venkatesh-Xbox1_hu_21c395d92ccbef1c.webp 760w,
               /blog/20071127-videogameprogramming/Abhishek-Venkatesh-Xbox1_hu_b7c618b02beda03.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20071127-videogameprogramming/Abhishek-Venkatesh-Xbox1_hu_e2b91885228502ad.webp&#34;
               width=&#34;283&#34;
               height=&#34;235&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;ECE introduced a new video game programming class to students this fall: Multicore and GPU Programming for Video Games. The course focuses on the architecture and programming of multicore  photo of Brian Davidsonprocessors and graphical processing units (GPUs). ECE faculty members Aaron Lanterman and Hsien-Hsin “Sean” Lee helped develop the class (ECE 4893A/CS4803MPG) along with &lt;strong&gt;David Bader&lt;/strong&gt;, an associate professor in the College of Computing. The three professors are co-teaching the class this semester.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Exploiting clever techniques such as multicore and GPU computing is the way to achieve continual advances in computer speed, now that clock speeds are topping out,” explained Dr. Lanterman. “There are many courses elsewhere that cover GPU programming in the context of scientific computation, but I wanted to photo of Abhishek Venkateshdo a course addressing it in its ‘native context’ of video games. An astonishing amount of cheap computing power is now available that can be applied to medical imaging, bioinformatics, and finance, but that power is only available so cheaply because people wanted to play cooler versions of Doom and Quake, which turned the technology into a mass-market item.”&lt;/p&gt;
&lt;p&gt;With approximately $7.4 billion in revenue from software sales in 2006*, the video game industry is booming. ECE’s new course addresses this growing industry’s need for engineers with specialized CPU/GPU programming skills, such as writing multithreaded code for the multiple-core architectures of new and upcoming PCs and game consoles.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Although our computer engineering majors are less likely to work for a company like Electronic Arts than the computer science students are, they may find themselves designing circuits at NVIDIA, AMD/ATI, IBM, or Intel,” said Dr. Lanterman. “Nothing pushes the envelope for low-cost, high-performance computation more than video games, so it’s good for our students to understand the computational demands involved and the techniques manufacturers use to meet them. In addition, many of the underlying algorithms we discuss, such as collision detection, also have applications in robotics.”&lt;/p&gt;
&lt;p&gt;Rather than focusing solely on game design, ECE 4893A uses gaming systems to provide a practical arena for considering issues in computer architecture, such as how hardware features influence game design. The students focus specifically on architectures available in inexpensive consumer hardware, including Sony PlayStation 3, Xbox 360, and Windows PCs with NVIDIA and ATI graphics cards.&lt;/p&gt;
&lt;p&gt;To develop games for the PC and Xbox 360 environments, the students use Microsoft XNA Game Studio Express.One of the many issues considered involves the trade-offs between asymmetric multicore architectures (such as the STI Cell BE used in the PlayStation 3) and symmetric multicore architectures (such as the triple PowerPC used in the Xbox 360).&lt;/p&gt;
&lt;p&gt;Student response to the course thus far has been overwhelmingly positive. “The class covers a broad spectrum, so every project has involved a different technology, and each one has involved a significant learning curve,” said Dr. Lanterman. “The students have been more enthusiastic than I’ve ever seen in any course I’ve taught, often going beyond the ‘call of duty’ in their projects.”&lt;/p&gt;
&lt;p&gt;For more information about Multicore and GPU Programming for Video Games, visit &lt;a href=&#34;http://users.ece.gatech.edu/~lanterma/mpg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://users.ece.gatech.edu/~lanterma/mpg/&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Revenue from software sales statistic courtesy of the NPD Group, a leading marketing information provider.*&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20151011102433/http://www.ece.gatech.edu/highlights/volume-04-issue-04/news/lanterman.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ece.gatech.edu/highlights/volume-04-issue-04/news/lanterman.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First book on petascale computing launched at SC07</title>
      <link>http://localhost:1313/blog/20071113-petascale/</link>
      <pubDate>Tue, 13 Nov 2007 20:21:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20071113-petascale/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech and Chapman &amp;amp; Hall/CRC Press today announced the launch of &amp;ldquo;Petascale Computing: Algorithms and Applications&amp;rdquo;, the first published collection on petascale techniques for computational science and engineering, at the SC07 conference. Edited by &lt;strong&gt;David A. Bader&lt;/strong&gt;, associate professor of computing and executive director of high-performance computing at Georgia Tech, this collection represents an academic milestone in the high-performance computing industry and is the first work to be released through Chapman &amp;amp; Hall/CRC Press&amp;rsquo; new Computational Science series.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;High-performance computing will enable breakthrough science and engineering in the 21st century,&amp;rdquo; said Bader. &amp;ldquo;My goal in developing this book was to inspire members of the high-performance computing community to solve computational grand challenges that will help our society, protect our environment, and improve our understanding in fundamental ways, all through the efficient use of petascale computing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Featuring contributions from the world&amp;rsquo;s leading experts in computational science, &amp;ldquo;Petascale Computing: Algorithms and Applications&amp;rdquo; discusses expected breakthroughs in the computational science and engineering field and covers a breadth of topics in petascale computing, including architectures, software, programming methodologies, tools, scalable algorithms, performance evaluation and application development. Covering a wide range of issues critical to the advancement of the high-performance computing/supercomputing industry, this edited collection illustrates the application of petascale computing to space and Earth science missions, biological systems and climate science, among others, and details the simulation of multiphysics, cosmological evolution, molecular dynamics and biomolecules.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In the same way as petascale computing will open up new and unprecedented opportunities for research in computational science, I expect this current book to lead to a deeper understanding and appreciation of research in computational science and engineering,&amp;rdquo; said Horst Simon, associate laboratory director for computing sciences, Lawrence Berkeley National Laboratory and editor of Chapman &amp;amp; Hall/CRC Press&amp;rsquo; new Computational Science book series.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The College of Computing at Georgia Tech is a young and rising leader in high-performance computing, computational science and engineering (CSE), and real-world computing. Focusing on research and education that impacts and influence social and scientific progress, the College of Computing Georgia Tech is unlocking 21st century grand challenges through fundamental and real world research, and educating tomorrow&amp;rsquo;s computational science innovators with advanced degrees in CSE. In November 2006, the College of Computing was recognized for its innovation and leadership role in this industry through its selection as the first Sony-Toshiba-IBM Center of Competence focused on the Cell Broadband Engine™ (Cell BE) microprocessor.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/pub_releases/2007-11/giot-fbo111207.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EurekaAlert&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercharging Multi-Core Designs</title>
      <link>http://localhost:1313/blog/20070917-intel/</link>
      <pubDate>Mon, 17 Sep 2007 22:20:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070917-intel/</guid>
      <description>&lt;p&gt;&amp;ldquo;Multi-core is a disruptive technology — and I mean that in a good
way — because it’s only when you have disruption of the status
quo that new innovations can affect technology with revolutionary
advances,” says &lt;strong&gt;David A. Bader&lt;/strong&gt;, executive director of high-performance
computing at Georgia Tech. Software developers who optimize
their code for this groundbreaking technology can deliver new levels of functionality
more cost-effectively. However, to achieve this optimization they’ll
need to tap into a new set of resources. “Making the most of multi-core systems
will require new tools, new algorithms and new ways of looking at programming,”
says Bader. Advanced visual software tools are helping developers balance
workload among all cores and consolidate more features and horsepower within
a smaller form factor.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Advertising insert to EE Times&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High Performance Computing and Web Science Initiatives Receive Seed Money from Provost&#39;s Office</title>
      <link>http://localhost:1313/blog/20070809-frp/</link>
      <pubDate>Thu, 09 Aug 2007 22:04:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070809-frp/</guid>
      <description>&lt;p&gt;The Office of the Provost has awarded Focused Research Program (FRP) funding for research proposals in high performance computing for $30,000 and web science for $29,313. The proposals were coordinated by College of Computing Associate Professor &lt;strong&gt;David A. Bader&lt;/strong&gt; and co-coordinated by Associate Professors Amy Bruckman and Milena Mihail, respectively.&lt;/p&gt;
&lt;p&gt;An FRP is a grant given by the Georgia Tech Provost&amp;rsquo;s Office designed to provide start-up support for research programs that could not be carried out through individual effort or within the resources of a single unit. Normally, an FRP involves a group of faculty members from more than one unit and is intended to provide start-up support for coordinated research projects that have the potential to develop into research centers within one to two years.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dr. Bader&lt;/strong&gt; coordinated the proposal entitled &amp;ldquo;Focused Research Program in High-Performance Computing&amp;rdquo; on behalf of researchers in the College of Computing, the College of Engineering and the College of Sciences. Drs. Bruckman and Mihail&amp;rsquo;s proposal, entitled &amp;ldquo;Focused Research Program in Web Science,&amp;rdquo; is a collaboration among researchers in 11 units: Interactive Computing; Computer Science; International Affairs; Architecture; Public Policy; Literature, Communications and Culture; Electrical and Chemical Engineering; Industrial and Systems Engineering; Management; and Mathematics.&lt;/p&gt;
&lt;p&gt;In all, fifteen proposals across campus for FRPs were submitted, five of which were approved.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://hg.gatech.edu/node/67890&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://hg.gatech.edu/node/67890&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060450/http://www.cc.gatech.edu/news/high-performance-computing-and-web-science-initiatives-receive-seed-money-from-provosts-office&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/high-performance-computing-and-web-science-initiatives-receive-seed-money-from-provosts-office&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Elected to Serve on Advisory Board for Internet2</title>
      <link>http://localhost:1313/blog/20070727-internet2/</link>
      <pubDate>Fri, 27 Jul 2007 20:14:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070727-internet2/</guid>
      <description>&lt;p&gt;College of Computing Associate Professor and Executive Director of High-Performance Computing &lt;strong&gt;David A. Bader&lt;/strong&gt; has been elected to the 2007 Internet2 Advisory Council in its first-ever election held this summer. Formed in 1996, Internet2 is a non-profit consortium contributing to the advancement of networking research with projects like the Abiline Network and the National Lambda Rail (NLR) project, which has deployed the highest bandwidth research network in the country.&lt;/p&gt;
&lt;p&gt;The election for Advisory Council members closed on June 28, with 57% of eligible member institutions from industry and research casting ballots. Bader, who was nominated by members of the Governance and Nominations Committee, will sit on the Research Advisory Council (RAC) of Internet2 along with two other researchers from MIT and Carnegie Mellon.&lt;/p&gt;
&lt;p&gt;The RAC is responsible for advising the Internet2 Board (consisting primarily of university presidents and CIOs, as well as leaders from industry and research) on matters relating to Internet2&amp;rsquo;s support for research, both network-focused research and disciplinary research that makes use of the network as a tool. The RAC provides a forum for strategic questions about how best to support the development of resources for scientific, humanities, clinical, computational, and other research communities domestically and internationally. The RAC will also provide advice on the nature and extent of research undertaken by Internet2 staff.&lt;/p&gt;
&lt;p&gt;Internet2 is a not-for-profit advanced networking consortium comprising more than 200 U.S. universities in cooperation with 70 leading corporations, 45 government agencies, laboratories and other institutions of higher learning as well as over 50 international partner organizations. Beyond just providing network capacity, Internet2 and its members actively engage in the development of important new technology including middleware, security, network research and performance measurement capabilities which are critical to the progress of the Internet. For more information and the complete announcement, visit the Internet2 website.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060238/http://www.cc.gatech.edu/news/david-bader-elected-to-serve-on-advisory-board-for-internet2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/david-bader-elected-to-serve-on-advisory-board-for-internet2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computation Directorate at Lawrence Livermore National Laboratory welcomes Bader</title>
      <link>http://localhost:1313/blog/20070726-llnl/</link>
      <pubDate>Thu, 26 Jul 2007 20:24:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070726-llnl/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20070726-llnl/letter_hu_bae13153cbc6ccc4.webp 400w,
               /blog/20070726-llnl/letter_hu_e2040a20c1388157.webp 760w,
               /blog/20070726-llnl/letter_hu_96041ecea4608e8c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070726-llnl/letter_hu_bae13153cbc6ccc4.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Professor Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;On behalf of the Institute for Scientific Computing Research and the Knowledge Inference Center, I would like to welcome you to Lawrence Livermore National Laboratory. We are honored by your visit and hope that you will find your experience here to be both enjoyable and enriching.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;D. Marie Manlapaz&lt;br&gt;
Visit Program Coordinator&lt;br&gt;
Institute for Scientific Computing Research&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NAE selects two from Tech for ‘Frontiers’ symposium</title>
      <link>http://localhost:1313/blog/20070716-nae-frontiers/</link>
      <pubDate>Mon, 16 Jul 2007 07:07:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070716-nae-frontiers/</guid>
      <description>&lt;p&gt;The National Academy of Engineering (NAE) has
selected two Georgia Tech faculty — College of
Computing Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt; and
Mechanical Engineering Assistant Professor
Samuel Graham — to participate in the NAE’s
annual Frontiers of Engineering symposium, a
three-day event that will bring together engineers
ages 30 to 45, who are performing cutting-
edge engineering research and technical
work in a variety of disciplines. Participants
were nominated by fellow engineers or organizations
and chosen from 260 applicants.
The symposium will be held Sept. 24-26 at
Microsoft Research in Redmond, Wash., and will
examine trustworthy computer systems, safe
water technologies, modeling and simulating
human behavior, biotechnology for fuels and
chemicals and the control of protein conformations.
For more information about Frontiers of
Engineering, visit &lt;a href=&#34;https://www.nae.edu/frontiers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nae.edu/frontiers&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creative Young Engineers Selected to Participate in NAE&#39;s 2007 Frontiers of Engineering Symposium</title>
      <link>http://localhost:1313/blog/20070715-foe/</link>
      <pubDate>Sun, 15 Jul 2007 22:33:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070715-foe/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The thirteenth annual US Frontiers of Engineering Symposium was held in September, 2007, in Redmond Washington.  Chaired by Julia M. Phillips of Sandia National Laboratories, the meeting included sessions on Engineering trustworthy computer systems, Control of protein conformations, Biotechnology for fuels and chemicals, Modeling and simulating human behavior, and Safe water technologies.&lt;/p&gt;
&lt;p&gt;US Frontiers of Engineering is an annual meeting that brings together 100 of the nation&amp;rsquo;s outstanding young engineers (ages 30-45) from industry, academia, and government to discuss pioneering technical and leading-edge research in various engineering fields and industry sectors. Participation is by invitation following a competitive nomination and selection process.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.naefrontiers.org/17273/Creative-Young-Engineers-Selected-to-Participate-in-NAEs-2007-Frontiers-of-Engineering-Symposium&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.naefrontiers.org/17273/Creative-Young-Engineers-Selected-to-Participate-in-NAEs-2007-Frontiers-of-Engineering-Symposium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech &#39;CellBuzz&#39; Cluster in Production Use</title>
      <link>http://localhost:1313/blog/20070710-hpcwire/</link>
      <pubDate>Tue, 10 Jul 2007 09:54:17 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070710-hpcwire/</guid>
      <description>&lt;p&gt;Georgia Tech is one of the first universities to deploy the IBM BladeCenter QS20 Server for production use,
through Sony-Toshiba-IBM (STI) Center of Competence for the Cell Broadband Engine (&lt;a href=&#34;http://sti.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sti.cc.gatech.edu/&lt;/a&gt;) in the College of
Computing at Georgia Tech. The QS20 uses the same ground-breaking Cell/B.E. processor appearing in products such as Sony
Computer Entertainment&amp;rsquo;s PlayStation3 computer entertainment system, and Toshiba&amp;rsquo;s Cell Reference Set, a development tool for
Cell/B.E. applications.&lt;/p&gt;
&lt;p&gt;The Georgia Tech installation includes a cluster of 28 Cell/B.E. processors (14 blades) and supports the operation of Cell-optimized
multi-core applications in areas such as digital content creation, gaming and entertainment, security, scientific and technical
computing, biomedicine, and finance. Georgia Tech grants users access on the cluster to test drive the Cell/B.E. processor and
support independent software vendors (ISVs) that develop products and tools for the Cell/B.E. processor. The Georgia Tech
Cell/B.E. processor installation uses Altair Engineering&amp;rsquo;s PBS Professional job scheduling software that increases the utilization of
the IBM Blade Center QS20.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Cell/B.E. processor represents the future of computing using heterogeneous multi-core processors, and we are proud to help
drive the continued advancement of computationally-intensive applications that will directly impact the global growth of our industry
and evolution of our society,&amp;rdquo; said &lt;strong&gt;David A. Bader&lt;/strong&gt;, Associate Professor and Executive Director of High-Performance Computing in
the College of Computing at Georgia Tech.&lt;/p&gt;
&lt;p&gt;Accounts on the Georgia Tech CellBuzz Cluster can be requested by visiting the Georgia Tech &amp;ndash; STI Cell/B.E. web page at
&lt;a href=&#34;http://sti.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sti.cc.gatech.edu/&lt;/a&gt; and clicking on the &amp;ldquo;CellBuzz Cluster&amp;rdquo; link.&lt;/p&gt;
&lt;h3 id=&#34;about-the-college-of-computing-at-georgia-tech&#34;&gt;About the College of Computing at Georgia Tech&lt;/h3&gt;
&lt;p&gt;The College of Computing at Georgia Tech is a national leader in the research and creation of real-world computing breakthroughs
that drive social and scientific progress. With its graduate program ranked 11th nationally by U.S. News and World Report, the
College&amp;rsquo;s unconventional approach to education is pioneering the new era of computing by expanding the horizons of traditional
computer science students through interdisciplinary collaboration and a focus on human centered solutions. For more information
about the College of Computing at Georgia Tech, its academic divisions and research centers, please visit &lt;a href=&#34;https://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cc.gatech.edu&lt;/a&gt; .&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech High Performance Computing Pioneer Joins Nation&#39;s Brightest Engineers to Tackle Green Issues</title>
      <link>http://localhost:1313/blog/20070625-naefoe/</link>
      <pubDate>Mon, 25 Jun 2007 21:28:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070625-naefoe/</guid>
      <description>&lt;p&gt;College of Computing Associate Professor &lt;strong&gt;David A. Bader&lt;/strong&gt; is one of eighty-three of the nation&amp;rsquo;s brightest young engineers who have been selected to take part in the National Academy of Engineering&amp;rsquo;s (NAE) 13th annual U.S. Frontiers of Engineering symposium. The two and a half-day event will bring together engineers ages 30 to 45 who are performing exceptional engineering research and technical work in a variety of disciplines. The participants — from industry, academia, and government — were nominated by fellow engineers or organizations. Participants were chosen from more than 260 nominations.&lt;/p&gt;
&lt;p&gt;Bader is a pioneer in the use of high-performance computing for problems in bioinformatics and computational genomics. &amp;ldquo;High-performance computing is essential for solving 21st century problems such as the design of safe water technologies, developing sustainable biofuels, and engineering proteins, which will be key themes at this symposium,”  he said.&lt;/p&gt;
&lt;p&gt;The symposium will be held September 24-26 at Microsoft Research in Redmond, Wash., and will examine trustworthy computer systems, safe water technologies, modeling and simulating human behavior, biotechnology for fuels and chemicals, and the control of protein conformations. Dr. Henrique Malvar, Microsoft Distinguished Engineer and managing director, Microsoft Research, will be a featured speaker. His research at Microsoft has focused on audio and video signal enhancement and compression, multirate signal processing, and signal decompositions. Prior to joining Microsoft Research, Malvar headed research and advanced technology at PictureTel and the Digital Signal Processing Research Group at Universidade de Brasília.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It is exciting to witness the energy when outstanding engineers from many different fields come together in this unique venue,&amp;rdquo; said NAE President William A. Wulf. &amp;ldquo;Frontiers of Engineering is a proven mechanism for traversing engineering disciplines. By exposing bright young minds to developments in areas other than their own — and giving them lots of time to interact — Frontiers enables advances in approaches and thinking that would not have occurred otherwise.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Sponsors for the 2007 U.S. Frontiers of Engineering are the Air Force Office of Scientific Research, the U.S. Department of Defense (DDR&amp;amp;E-Research), DARPA, the National Science Foundation, Microsoft Corp., Cummins Inc., and numerous individual donors.&lt;/p&gt;
&lt;p&gt;The National Academy of Engineering is an independent, nonprofit institution that serves as an adviser to government and the public on issues in engineering and technology. Its members consist of the nation&amp;rsquo;s premier engineers, who are elected by their peers for their distinguished achievements. Established in 1964, NAE operates under the congressional charter granted to the National Academy of Sciences in 1863.&lt;/p&gt;
&lt;p&gt;A meeting program and more information about Frontiers of Engineering is available at the &lt;a href=&#34;https://web.archive.org/web/20080202060345/http://www.nae.edu/frontiers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;symposium website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060345/http://www.cc.gatech.edu/news/georgia-tech-high-performance-computing-pioneer-joins-nations-brightest-engineers-to-tackle-green-issues&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/georgia-tech-high-performance-computing-pioneer-joins-nations-brightest-engineers-to-tackle-green-issues&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Hosts Workshop to Drive Innovation in Cell Broadband Engine Processor Research</title>
      <link>http://localhost:1313/blog/20070531-cellworkshop2/</link>
      <pubDate>Thu, 31 May 2007 21:59:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070531-cellworkshop2/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech today announced it will host the Georgia Tech Cell Broadband Engine™ (Cell/B.E.) Processor Workshop from June 18-19, 2007, focusing on applications for the Cell/B.E. processor, including gaming, virtual reality, home entertainment, tools and programmability and high performance scientific and technical computing.&lt;/p&gt;
&lt;p&gt;The two-day workshop is sponsored by Sony Computer Entertainment Inc. (SCEI), Toshiba and IBM and will be held at the Klaus Advanced Computing Building on Georgia Tech’s campus. Keynote speakers at the event include Bijan Davari, IBM Fellow and Vice President, Next Generation Computing Systems and Technology; Dominic Mallinson, Vice President, US Research and Development, SCEI and Yoshio Masubuchi, General Manager, Broadband System LSI Development Center, Toshiba’s semiconductor company. More information on the workshop may be found at &lt;a href=&#34;http://sti.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sti.cc.gatech.edu/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“We are very excited to be able to support the growth of this breakthrough technology by bringing some of the top minds in the industry together at Georgia Tech to stimulate discussion about the future of Cell/B.E. technology,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, Associate Professor and Executive Director of High-Performance Computing in the College of Computing at Georgia Tech. “The Cell/B.E. processor represents the future of computing using heterogeneous multi-core processors, and we are proud to help drive the continued advancement of computationally-intensive applications that will directly impact the global growth of our industry and evolution of our society.”&lt;/p&gt;
&lt;p&gt;The revolutionary Cell/B.E. processor is a breakthrough design featuring a central processing core, based on IBM&amp;rsquo;s industry leading Power Architecture™ technology, and eight synergistic processors.  Cell/B.E. &amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and handhelds, virtual-reality, wireless downloads, real-time video chat, interactive TV shows and other &amp;ldquo;image-hungry&amp;rdquo; computing environments. The processor was created through a collaboration of IBM, Sony Corporation, SCEI and Toshiba Corporation (Toshiba).&lt;/p&gt;
&lt;p&gt;The College of Computing also announced today that it is one of the first universities to deploy the IBM BladeCenter® QS20 Server for production use. The QS20 uses the same ground-breaking Cell/B.E. processor appearing in products such as Sony Computer Entertainment’s PLAYSTATION®3 computer entertainment system, and Toshiba’s Cell Reference Set, a development tool for Cell/B.E. applications. The Georgia Tech installation includes a cluster of 28 Cell/B.E. processors (14 blades) and supports the operation of Cell-optimized multi-core applications in areas such as digital content creation, gaming and entertainment, security, scientific and technical computing, biomedicine, and finance. Georgia Tech will grant users access on the cluster to test drive the Cell/B.E. processor and support independent software vendors (ISVs) that develop products and tools for the Cell/B.E. processor. The Georgia Tech Cell/B.E. processor installation will use Altair Engineering’s PBS Professional job scheduling software that increases the utilization of the IBM Blade Center® QS20.&lt;/p&gt;
&lt;p&gt;Directed by Bader, the STI Cell Center of Competence at Georgia Tech has a mission to grow the community of Cell/B.E. processor users and developers by performing research and service in support of the Cell/B.E. processor, and further enable students at the College to grow their skills and experience around Cell/B.E. technology to apply in future career opportunities. The Center will sponsor discussion forums and workshops, provide remote access to Cell/B.E processor based blade hardware installed at Georgia Tech, create and disseminate software optimized for Cell/B.E. processor based systems, and perform research on the design of Cell/B.E. processor based systems, algorithms, and applications. A collaboration with SCEI, Toshiba and IBM supports the Center’s activities and research efforts in support of broadening the Cell/B.E. processor’s impact into multiple sectors and industries, including scientific computing, digital content creation, bioinformatics, finance, gaming and entertainment.&lt;/p&gt;
&lt;h2 id=&#34;about-the-college-of-computing-at-georgia-tech&#34;&gt;About the College of Computing at Georgia Tech&lt;/h2&gt;
&lt;p&gt;The College of Computing at Georgia Tech is a national leader in the research and creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked 11th nationally by U.S. News and World Report, the College’s unconventional approach to education is pioneering the new era of computing by expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human centered solutions. For more information about the College of Computing at Georgia Tech, its academic divisions and research centers, please visit &lt;a href=&#34;https://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more information, contact:&lt;br&gt;
Stefany Wilson&lt;br&gt;
College of Computing at Georgia Tech&lt;br&gt;
404.894.7253&lt;br&gt;
&lt;a href=&#34;mailto:stefany@cc.gatech.edu&#34;&gt;stefany@cc.gatech.edu&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202055838/http://www.cc.gatech.edu/news/college-of-computing-hosts-workshop-to-drive-innovation-in-cell-broadband-engine-processor-research&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/college-of-computing-hosts-workshop-to-drive-innovation-in-cell-broadband-engine-processor-research&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing at Georgia Tech Hosts Workshop to Drive Innovation in Cell Broadband Engine Processor Research</title>
      <link>http://localhost:1313/blog/20070531-cellworkshop/</link>
      <pubDate>Thu, 31 May 2007 07:38:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070531-cellworkshop/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech today announced it will host the
Georgia Tech Cell Broadband Engine™ (Cell/B.E.) Processor Workshop from June 18-19, 2007, focusing on
applications for the Cell/B.E. processor, including gaming, virtual reality, home entertainment, tools and
programmability and high performance scientific and technical computing.&lt;/p&gt;
&lt;p&gt;The two-day workshop is sponsored by Sony Computer Entertainment Inc. (SCEI), Toshiba and IBM and will
be held at the Klaus Advanced Computing Building on Georgia Tech’s campus. Keynote speakers at the
event include Bijan Davari, IBM Fellow and Vice President, Next Generation Computing Systems and
Technology; Dominic Mallinson, Vice President, US Research and Development, SCEI and Yoshio
Masubuchi, General Manager, Broadband System LSI Development Center, Toshiba’s semiconductor
company. More information on the workshop may be found at &lt;a href=&#34;http://sti.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sti.cc.gatech.edu/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;“We are very excited to be able to support the growth of this breakthrough technology by bringing some of the
top minds in the industry together at Georgia Tech to stimulate discussion about the future of Cell/B.E.
technology,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, Associate Professor and Executive Director of High-Performance
Computing in the College of Computing at Georgia Tech. “The Cell/B.E. processor represents the future of
computing using heterogeneous multi-core processors, and we are proud to help drive the continued
advancement of computationally-intensive applications that will directly impact the global growth of our
industry and evolution of our society.”&lt;/p&gt;
&lt;p&gt;The revolutionary Cell/B.E. processor is a breakthrough design featuring a central processing core, based on
IBM&amp;rsquo;s industry leading Power Architecture™ technology, and eight synergistic processors. Cell/B.E.
&amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and
handhelds, virtual-reality, wireless downloads, real-time video chat, interactive TV shows and other
&amp;ldquo;image-hungry&amp;rdquo; computing environments. The processor was created through a collaboration of IBM, Sony
Corporation, SCEI and Toshiba Corporation (Toshiba).&lt;/p&gt;
&lt;p&gt;The College of Computing also announced today that it is one of the first universities to deploy the IBM
BladeCenter® QS20 Server for production use. The QS20 uses the same ground-breaking Cell/B.E.
processor appearing in products such as Sony Computer Entertainment’s PLAYSTATION®3 computer
entertainment system, and Toshiba’s Cell Reference Set, a development tool for Cell/B.E. applications. The
Georgia Tech installation includes a cluster of 28 Cell/B.E. processors (14 blades) and supports the operation
of Cell-optimized multi-core applications in areas such as digital content creation, gaming and entertainment,
security, scientific and technical computing, biomedicine, and finance. Georgia Tech will grant users access
on the cluster to test drive the Cell/B.E. processor and support independent software vendors (ISVs) that
develop products and tools for the Cell/B.E. processor. The Georgia Tech Cell/B.E. processor installation will
use Altair Engineering’s PBS Professional job scheduling software that increases the utilization of the IBM
Blade Center® QS20.&lt;/p&gt;
&lt;p&gt;Directed by Bader, the STI Cell Center of Competence at Georgia Tech has a mission to grow the community
of Cell/B.E. processor users and developers by performing research and service in support of the Cell/B.E.
processor, and further enable students at the College to grow their skills and experience around Cell/B.E.
technology to apply in future career opportunities. The Center will sponsor discussion forums and workshops,
provide remote access to Cell/B.E. processor based blade hardware installed at Georgia Tech, create and
disseminate software optimized for Cell/B.E. processor based systems, and perform research on the design
of Cell/B.E. processor based systems, algorithms, and applications. A collaboration with SCEI, Toshiba and
IBM supports the Center’s activities and research efforts in support of broadening the Cell/B.E. processor’s
impact into multiple sectors and industries, including scientific computing, digital content creation,
bioinformatics, finance, gaming and entertainment.&lt;/p&gt;
&lt;h3 id=&#34;about-the-college-of-computing-at-georgia-tech&#34;&gt;About the College of Computing at Georgia Tech&lt;/h3&gt;
&lt;p&gt;The College of Computing at Georgia Tech is a national leader in the research and creation of real-world
computing breakthroughs that drive social and scientific progress. With its graduate program ranked 11th
nationally by U.S. News and World Report, the College’s unconventional approach to education is pioneering
the new era of computing by expanding the horizons of traditional computer science students through
interdisciplinary collaboration and a focus on human centered solutions. For more information about the
College of Computing at Georgia Tech, its academic divisions and research centers, please visit
&lt;a href=&#34;https://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;IBM, BladeCenter, and Power Architecture are trademarks of IBM Corporation in the United States and/or
other countries.&lt;/p&gt;
&lt;p&gt;PLAYSTATION is a registered trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;p&gt;All other company/product names and service marks may be trademarks or registered trademarks of their
respective companies.&lt;/p&gt;
&lt;p&gt;Cell Broadband Engine is a trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;http://www.ibm.com/legal/copytrade.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ibm.com/legal/copytrade.shtml&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;contacts&#34;&gt;Contacts&lt;/h3&gt;
&lt;p&gt;For College of Computing at Georgia Tech &lt;br&gt;
Brendan Streich, 404-260-3519 &lt;br&gt;
&lt;a href=&#34;mailto:bstreich@gcigroup.com&#34;&gt;bstreich@gcigroup.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NAE invites Bader for 2007 U.S. Frontiers of Engineering Symposium</title>
      <link>http://localhost:1313/blog/20070522-nae/</link>
      <pubDate>Tue, 22 May 2007 20:32:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070522-nae/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20070522-nae/letter1_hu_43c0a2a2e3a7a52d.webp 400w,
               /blog/20070522-nae/letter1_hu_c932e79d172870aa.webp 760w,
               /blog/20070522-nae/letter1_hu_670b721173b6b572.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070522-nae/letter1_hu_43c0a2a2e3a7a52d.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20070522-nae/letter2_hu_335e44aad19e2917.webp 400w,
               /blog/20070522-nae/letter2_hu_8765268fe0c8cb88.webp 760w,
               /blog/20070522-nae/letter2_hu_6fddb6d4e40cc018.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070522-nae/letter2_hu_335e44aad19e2917.webp&#34;
               width=&#34;593&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dear &lt;strong&gt;Dr. Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Upon the recommendation of the organizing committee, I am please to invite you to participate in the National Academy of Engineering&amp;rsquo;s 2007 U.S. Frontiers of Engineering Symposium. The symposium will be held at Microsoft Research in Redmond, WA beginning Monday, September 24 and ending around noon on Wednesday, September 26.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Advances in engineering are moving with incredible spped across many facets of the profession. The goal of the Frontiers of Engineering symposium is to bring together outstanding leaders of these engineering developments. The convening of top-notch people from disparate fields and challening them to think about the developments at the frontiers of areas different from their own could lead to a variety of desirable results, including collaborative work, the transfer of new techniques and approaches across fields, and the establishment of contacts among the next generation of engineering leaders.&lt;/p&gt;
&lt;p&gt;The total number of participants at the symposium will be 100 engineering, generally 30-45 years old. This number includes the organizing committee and speakers. Participants will be from industry, universities, and government labs, and will represent a range of engineering fields, including aerospace, chemical, electrical, environemntal, civil, materials, and mechanical engineering. Sponsoring agency representatives and a few senior engineers will also be invited to attend.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Wm. A. Wulf&lt;br&gt;
President&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech to Host Cell BE Workshop</title>
      <link>http://localhost:1313/blog/20070521-hpcwire/</link>
      <pubDate>Mon, 21 May 2007 09:49:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070521-hpcwire/</guid>
      <description>&lt;p&gt;Georgia Tech will be hosting a two-day workshop on software and applications for
the Cell Broadband Engine, to be held on Monday, June 18 and Tuesday, June 19, at the Klaus Advanced
Computing Building, (&lt;a href=&#34;http://www.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/&lt;/a&gt;
) at Georgia Institute of Technology, in Atlanta, GA, United States. The workshop is sponsored by Georgia
Tech and the Sony, Toshiba, IBM, (STI) Center of Competence for the Cell BE.&lt;/p&gt;
&lt;p&gt;The theme of the workshop will include Gaming, Virtual Reality, Home Entertainment, Tools and&lt;/p&gt;
&lt;p&gt;Programmability, and High-Performance Scientific and Technical Computing.&lt;/p&gt;
&lt;p&gt;There is no registration fee. However, we ask that you register online at &lt;a href=&#34;http://sti.cc.gatech.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sti.cc.gatech.edu/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Participants are responsible for their travel expenses, including airfare and hotel accommodation.&lt;/p&gt;
&lt;h3 id=&#34;confirmed-keynote-speakers&#34;&gt;Confirmed Keynote Speakers:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bijan Davari (IBM Research), IBM Fellow and Vice President, Next Generation Computing Systems and Technology&lt;/li&gt;
&lt;li&gt;Dominic Mallinson (Sony Computer Entertainment Inc.), SCEI Vice President, US Research and Development&lt;/li&gt;
&lt;li&gt;Yoshio Masubuchi (Toshiba Corp.), General Manager, Broadband System LSI Development Center&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We look forward to seeing you in Atlanta!&lt;/p&gt;
&lt;h3 id=&#34;workshop-co-chairs&#34;&gt;Workshop Co-Chairs:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Tech, &lt;a href=&#34;http://www.cc.gatech.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/~bader&lt;/a&gt; &lt;a href=&#34;mailto:bader@cc.gatech.edu&#34;&gt;bader@cc.gatech.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Santosh Pande, Georgia Tech, &lt;a href=&#34;http://www.cc.gatech.edu/~santosh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/~santosh&lt;/a&gt; &lt;a href=&#34;mailto:santosh@cc.gatech.edu&#34;&gt;santosh@cc.gatech.edu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;worshop-steering-committee&#34;&gt;Worshop Steering Committee:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Doreen Anding, IBM&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Tech&lt;/li&gt;
&lt;li&gt;Gordon Ellison, IBM&lt;/li&gt;
&lt;li&gt;Austin Noronha, Sony&lt;/li&gt;
&lt;li&gt;Santosh Pande, Georgia Tech&lt;/li&gt;
&lt;li&gt;Hiroshi Sekiguchi, Toshiba&lt;/li&gt;
&lt;li&gt;Kenji Suzuki, Toshiba&lt;/li&gt;
&lt;li&gt;Bob Szabo, IBM&lt;/li&gt;
&lt;li&gt;Thomas Swidler, Sony&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Joins IBM Technical Leadership Forum</title>
      <link>http://localhost:1313/blog/20070504-ibm-tlf/</link>
      <pubDate>Fri, 04 May 2007 21:24:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070504-ibm-tlf/</guid>
      <description>&lt;p&gt;What do Harley-Davidson, Lehman Brothers, eBay, Volkswagen, Electronic Arts, and Georgia Tech all have in common?  Each is a member of the highly-selective IBM Technical Leadership Forum, which is comprised of high-tech representatives from around the world who serve as an advisory board for IBM on information technologies.&lt;/p&gt;
&lt;p&gt;College of Computing Associate Professor &lt;strong&gt;David A. Bader&lt;/strong&gt; was selected as the first academic member of the IBM Technical Leadership Forum and attended the forum’s recent meeting held at the IBM Executive Briefing Center in Mainz, Germany, on May 3-4, 2007. The meeting included about 32 members of the forum as well as a dozen IBM VP’s, CTO’s, and Distinguished Engineers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; is an associate professor in the Computational Science and Engineering division of the College of Computing and is executive director of high-performance computing.&lt;/p&gt;
&lt;p&gt;The IBM System Storage Executive Briefing Center specializes in storage and systems research and development.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060243/http://www.cc.gatech.edu/news/david-bader-joins-ibm-technical-leadership-forum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/david-bader-joins-ibm-technical-leadership-forum&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The College of Computing Honors Exceptional Students, Faculty, and Staff</title>
      <link>http://localhost:1313/blog/20070417-coc/</link>
      <pubDate>Tue, 17 Apr 2007 21:15:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070417-coc/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech hosted its 16th Annual Awards Celebration on April 17, 2007. Master of Ceremony and CoC Associate Dean and Honors &amp;amp; Awards Chair Merrick Furst led the College in congratulating students, faculty, and staff on another exciting and productive year.&lt;/p&gt;
&lt;h2 id=&#34;the-2006-2007-undergraduate-awards&#34;&gt;The 2006-2007 Undergraduate Awards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Outstanding Freshman - Christopher Adam Sladky&lt;/li&gt;
&lt;li&gt;Outstanding Sophomore - Michael E. Hale&lt;/li&gt;
&lt;li&gt;Outstanding Junior - Megan Leigh Elmore&lt;/li&gt;
&lt;li&gt;Outstanding Undergraduate - David Schachter&lt;/li&gt;
&lt;li&gt;Outstanding Undergraduate Research - Steven Dalton&lt;/li&gt;
&lt;li&gt;Outstanding Undergraduate Teaching Assistant - David Schachter&lt;/li&gt;
&lt;li&gt;Outstanding Undergraduate Research Assistant - Steven French&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Dave &amp;amp; Carrie Armento Scholarship - Gary Mann&lt;br&gt;
Established by David D. Armento to be awarded to an African-American computer science major who is a resident of the State of Georgia.&lt;/p&gt;
&lt;p&gt;The Bierne M. Prager Scholarship - Jason Power&lt;br&gt;
Established to award an undergraduate student from Shelby County, Tennessee.&lt;/p&gt;
&lt;h2 id=&#34;the-2006-2007-graduate-awards&#34;&gt;The 2006-2007 Graduate Awards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Outstanding Graduate Teaching Assistant - Jessica Paradise-Elliott&lt;/li&gt;
&lt;li&gt;Outstanding Graduate Research Assistant - Shwetak Patel, Qi Zhao&lt;/li&gt;
&lt;li&gt;CS7001 Research Project Award - Hwa-You Hsu, Wanchun Li, Ogechi Nnadi, Christopher Parnin, Hina Shah, Yanbing Yu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Donald V. Jackson Fellowship - Angela Navarro&lt;br&gt;
This Fellowship supports the master’s program and is awarded to a well-rounded computer science student who best embodies values of academic excellence and leadership.&lt;/p&gt;
&lt;p&gt;The Marshall D. Williamson Fellowship - Roozbeh Mottaghi&lt;br&gt;
This Fellowship supports the master’s program and is awarded to a well-rounded student who best embodies values of academic excellence and leadership.&lt;/p&gt;
&lt;p&gt;2005-2006 Outstanding Doctoral Dissertation Award:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vivek Kwatra - (graduated Summer 2005), Example-based Rendering of Textural Phenomena, Advisors: Aaron Bobick and Irfan Essa&lt;/li&gt;
&lt;li&gt;Yaxin Liu - (graduated Spring 2005), Decision-Theoretic Planning Under Risk-Sensitive Planning Objectives, Advisors: Sven Koenig and Craig Tovey&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outstanding-support-and-service-staff-awards&#34;&gt;Outstanding Support and Service Staff Awards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Francine Lyken&lt;/li&gt;
&lt;li&gt;John David Sturgill&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;institute-awards&#34;&gt;Institute Awards&lt;/h2&gt;
&lt;p&gt;Don Bratcher Human Relations Award - Gregory D. Abowd&lt;/p&gt;
&lt;h3 id=&#34;10-years-of-service&#34;&gt;10 Years of Service&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Michelle Bernard&lt;/li&gt;
&lt;li&gt;Amy Bruckman&lt;/li&gt;
&lt;li&gt;Marcus Johnson&lt;/li&gt;
&lt;li&gt;Thomas Pilsch&lt;/li&gt;
&lt;li&gt;David Smith&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;25-years-of-service&#34;&gt;25 Years of Service&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Vivian Chandler&lt;/li&gt;
&lt;li&gt;Cathy Dunnahoo&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;faculty-awards&#34;&gt;Faculty Awards&lt;/h2&gt;
&lt;p&gt;Outstanding Junior Faculty Research Award - Charles Isbell&lt;br&gt;
Outstanding Senior Faculty Research Award - Eric Vigoda&lt;/p&gt;
&lt;p&gt;The Raytheon Faculty Fellowship - Santosh Vempala&lt;br&gt;
The Raytheon Faculty Fellowship is a $15,000 seed grant designed to foster and encourage collaborative activities among CoC faculty members and to further
promote a sense of community within the college. The recipient can be academic as well as research faculty members in the CoC who seek to collaborate on new
research initiatives in any area. Thanks to the ongoing support of the college by the Raytheon Company, this award is presented annually.&lt;/p&gt;
&lt;p&gt;Outstanding Instructor - Robert Waters&lt;/p&gt;
&lt;p&gt;William A. “Gus” Baird Faculty Teaching Award - Amy Bruckman&lt;br&gt;
This award is given to a faculty member who is recognized by his or her peers for excellence in teaching. It is fitting that “Gus” – “the man who wanted nothing more than to teach” – be remembered each time this award is presented. “Gus” received his master’s degree from Georgia Tech in 1980 and was a favorite College of Computing undergraduate instructor for nearly 15 years.&lt;/p&gt;
&lt;p&gt;The Peter A. Freeman Faculty Award - Leo Mark&lt;br&gt;
The Freeman Award is presented annually to a member of the College of Computing faculty who has demonstrated the entrepreneurial spirit for which the founding Dean, Peter A. Freeman, was known.&lt;/p&gt;
&lt;p&gt;The Dean’s Award - &lt;strong&gt;David Bader&lt;/strong&gt;&lt;br&gt;
The Dean’s Award is selected by the Dean and presented to a member of the college who has contributed significantly to the success of the college this year.&lt;/p&gt;
&lt;p&gt;Lifetime Achievement Award – James D. Foley&lt;br&gt;
The Lifetime Achievement Award is the most prestigious award given by the Association for Computing Machinery Special Interest Group on Computer/Human Interaction (SIGCHI) Jim Foley was one of the computer graphics pioneers who helped to establish Human Interactive Computing as a discipline.&lt;/p&gt;
&lt;p&gt;Social Impact Award – Gregory Abowd&lt;br&gt;
This award is given by the Association for Computing Machinery Special Interest Group on Computer/Human Interaction (SIGCHI) to individuals who promote the application of humancomputer interaction research to pressing social needs. Gregory’s research explores applications of ubiquitous computing technologies, combining both human-centered and technology-driven research themes.&lt;/p&gt;
&lt;h2 id=&#34;corporate-scholarships&#34;&gt;Corporate Scholarships&lt;/h2&gt;
&lt;p&gt;AT&amp;amp;T Graduate Fellowship - Lana Yarosh&lt;/p&gt;
&lt;p&gt;Cisco Internet Generation Scholarships:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lawrence Jarvis&lt;/li&gt;
&lt;li&gt;Alex Marquez&lt;/li&gt;
&lt;li&gt;Cisco Leadership Award&lt;/li&gt;
&lt;li&gt;Courtland Goodson&lt;/li&gt;
&lt;li&gt;Dolapo Kukoyi&lt;/li&gt;
&lt;li&gt;Jennifer Whitlow&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google Anita Borg Scholarship&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helene Brashear&lt;/li&gt;
&lt;li&gt;Marshini Chetty&lt;/li&gt;
&lt;li&gt;Andrea Grimes&lt;/li&gt;
&lt;li&gt;Julie Kientz&lt;/li&gt;
&lt;li&gt;Susan Wyche (Finalist)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IBM PhD Fellowship Award - David Hilley&lt;/p&gt;
&lt;p&gt;PEO Fellowship - Julie Kientz&lt;/p&gt;
&lt;p&gt;Lockheed Martin Scholarship - Jordan Garner&lt;/p&gt;
&lt;p&gt;Rockwell Collins Scholarships:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Denis Bueno&lt;/li&gt;
&lt;li&gt;Joanna Leidy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intel Scholars:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matthew Benford&lt;/li&gt;
&lt;li&gt;Sauvik Das&lt;/li&gt;
&lt;li&gt;Alexander Dodson&lt;/li&gt;
&lt;li&gt;Catie Donnelly&lt;/li&gt;
&lt;li&gt;Brittany Duncan&lt;/li&gt;
&lt;li&gt;Meagan Elmore&lt;/li&gt;
&lt;li&gt;Katherine Flinn&lt;/li&gt;
&lt;li&gt;Matthew Fong&lt;/li&gt;
&lt;li&gt;Jordan Garner&lt;/li&gt;
&lt;li&gt;Anne Hewitt&lt;/li&gt;
&lt;li&gt;Lawrence Jarvis&lt;/li&gt;
&lt;li&gt;Terris Johnson&lt;/li&gt;
&lt;li&gt;Christina Lacey&lt;/li&gt;
&lt;li&gt;Amanda Ladd&lt;/li&gt;
&lt;li&gt;Vanessa Larco&lt;/li&gt;
&lt;li&gt;Uzoma Okafor&lt;/li&gt;
&lt;li&gt;Kady Rosier&lt;/li&gt;
&lt;li&gt;Aaron St.Clair&lt;/li&gt;
&lt;li&gt;Sweta Vajjhala&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intel Graduated Scholars Fall 2006&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mina Doroudi&lt;/li&gt;
&lt;li&gt;Andrew Durso&lt;/li&gt;
&lt;li&gt;Priyanka Mahalanabis&lt;/li&gt;
&lt;li&gt;Logan Thomas&lt;/li&gt;
&lt;li&gt;Megan Thomas&lt;/li&gt;
&lt;li&gt;Dana Van Devender&lt;/li&gt;
&lt;li&gt;Jenelle Walker&lt;/li&gt;
&lt;li&gt;Crystal Wrenn&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intel Mentors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nicholas Diakopoulos&lt;/li&gt;
&lt;li&gt;Steven Dow&lt;/li&gt;
&lt;li&gt;Andrea Forte&lt;/li&gt;
&lt;li&gt;Manish Mehta&lt;/li&gt;
&lt;li&gt;Mario Romero&lt;/li&gt;
&lt;li&gt;Erika Shehan&lt;/li&gt;
&lt;li&gt;Alan Wagner&lt;/li&gt;
&lt;li&gt;Ping Wang&lt;/li&gt;
&lt;li&gt;Steve Webb&lt;/li&gt;
&lt;li&gt;Jose Zagal&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SAIC Scholars:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cory Campbell&lt;/li&gt;
&lt;li&gt;Jill Donnelly&lt;/li&gt;
&lt;li&gt;Carolina Gomez&lt;/li&gt;
&lt;li&gt;Kathryn Long&lt;/li&gt;
&lt;li&gt;Candace Mitchell&lt;/li&gt;
&lt;li&gt;Jahmeilah Richardson&lt;/li&gt;
&lt;li&gt;Michael Slaughter&lt;/li&gt;
&lt;li&gt;Kelli Stancil&lt;/li&gt;
&lt;li&gt;Jennifer Stoll&lt;/li&gt;
&lt;li&gt;Pavani Yalla&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SAIC Mentors&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nova Ahmed&lt;/li&gt;
&lt;li&gt;Guofei Gu&lt;/li&gt;
&lt;li&gt;Zsolt Kira&lt;/li&gt;
&lt;li&gt;Dushmanta Mohapatra&lt;/li&gt;
&lt;li&gt;Bryan Payne&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CC Ambassadors exists to introduce prospective students and parents, alumni and industry professionals to the College of Computing community. CC Ambassadors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Katie Collins&lt;/li&gt;
&lt;li&gt;Skji Conklin&lt;/li&gt;
&lt;li&gt;Catie Donnelly&lt;/li&gt;
&lt;li&gt;Brittany Duncan&lt;/li&gt;
&lt;li&gt;Megan Elmore&lt;/li&gt;
&lt;li&gt;Dawn Finney&lt;/li&gt;
&lt;li&gt;Katie Flinn&lt;/li&gt;
&lt;li&gt;Matthew Fong&lt;/li&gt;
&lt;li&gt;Jordan Garner&lt;/li&gt;
&lt;li&gt;Matt Goforth&lt;/li&gt;
&lt;li&gt;Noah Goodwin&lt;/li&gt;
&lt;li&gt;Michael Hale&lt;/li&gt;
&lt;li&gt;Blake Israel&lt;/li&gt;
&lt;li&gt;Nayan Jain&lt;/li&gt;
&lt;li&gt;Lawrence Jarvis&lt;/li&gt;
&lt;li&gt;Terris Johnson&lt;/li&gt;
&lt;li&gt;Vallerie Killian&lt;/li&gt;
&lt;li&gt;Vanessa Larco&lt;/li&gt;
&lt;li&gt;Priyanka Mahalanabis&lt;/li&gt;
&lt;li&gt;Micajah McGarity&lt;/li&gt;
&lt;li&gt;Candice Mitchell&lt;/li&gt;
&lt;li&gt;Charlie Morn&lt;/li&gt;
&lt;li&gt;Heather Phillips&lt;/li&gt;
&lt;li&gt;Heather Pritchard&lt;/li&gt;
&lt;li&gt;Peter Rosegger&lt;/li&gt;
&lt;li&gt;Joshua Silver&lt;/li&gt;
&lt;li&gt;Michael Slaughter&lt;/li&gt;
&lt;li&gt;Kelli Stancil&lt;/li&gt;
&lt;li&gt;Jeffrey Starker II&lt;/li&gt;
&lt;li&gt;Jennifer Whitlow&lt;/li&gt;
&lt;li&gt;Bennett Wilson&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202061050/http://www.cc.gatech.edu/news/the-college-of-computing-honors-exceptional-students-faculty-and-staff-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/the-college-of-computing-honors-exceptional-students-faculty-and-staff-1&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Awarded Georgia Tech College of Computing Dean&#39;s Award</title>
      <link>http://localhost:1313/blog/20070417-gatech/</link>
      <pubDate>Tue, 17 Apr 2007 16:08:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070417-gatech/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20070417-gatech/certificate_hu_9d30f7b118cdd3db.webp 400w,
               /blog/20070417-gatech/certificate_hu_551ce340e0f3df0b.webp 760w,
               /blog/20070417-gatech/certificate_hu_5561bbfcfb09e0a9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070417-gatech/certificate_hu_9d30f7b118cdd3db.webp&#34;
               width=&#34;760&#34;
               height=&#34;578&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Georgia Tech College of Computing&lt;/p&gt;
&lt;h2 id=&#34;the-deans-award&#34;&gt;The Dean&amp;rsquo;s Award&lt;/h2&gt;
&lt;p&gt;Presented to &lt;strong&gt;David Bader&lt;/strong&gt; on Tuesday, April 17, 2007&lt;/p&gt;
&lt;p&gt;Richard A. DeMillo&lt;br&gt;
The John P. Imlay, Jr. Dean of Computing&lt;/p&gt;
&lt;p&gt;Merrick L. Furst&lt;br&gt;
Distinguished Professor, Associate Dean&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSE Leadership in Petascale Computing</title>
      <link>http://localhost:1313/blog/20070401-ipdps/</link>
      <pubDate>Sun, 01 Apr 2007 21:07:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070401-ipdps/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Executive Director of High-Performance Computing, and Associate Professor in the Computational Science and Engineering Division of the College of Computing at Georgia Tech, recently delivered a keynote talk on “Petascale Computing for Large-Scale Graph Problems“ at the 8th IEEE International Workshop on Parallel and Distributed Scientific and Engineering Computing (PDSEC), on March 30 in Long Beach, CA, held in conjunction with the 21st IEEE International Parallel and Distributed Processing Symposium (IPDPS).&lt;/p&gt;
&lt;p&gt;“Graph theoretic problems are representative of fundamental kernels in traditional and emerging computational sciences such as chemistry, biology, and medicine, as well as applications in national security,” said Bader, “Yet they pose serious challenges for parallel machines due to non-contiguous, concurrent accesses to global data structures with low degrees of locality. We consider several graph theoretic kernels for connectivity and centrality and discuss how the features of petascale architectures will affect algorithm development, ease of programming, performance, and scalability.”&lt;/p&gt;
&lt;p&gt;IPDPS is considered the premier academic conference in the areas of parallel and distributed computing.  This year&amp;rsquo;s symposium held from March 26-30 in Long Beach, CA, was met with record attendance with over 650 attendees and included 109 peer-reviewed papers in its highly-competitive main track, four keynote talks and over 20 workshops. Being highly regarded by the professional community as the annual meeting for top research results in the field, IPDPS had strong international participation with approximately equal attendance from the U.S., Europe and Asia.&lt;/p&gt;
&lt;p&gt;The College of Computing had strong participation at the symposium, with 20 activities related to the College:&lt;/p&gt;
&lt;h2 id=&#34;technical-papers&#34;&gt;TECHNICAL PAPERS&lt;/h2&gt;
&lt;p&gt;International Parallel and Distributed Processing Symposium (IPDPS) 2007:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spam-Resilient Web Rankings via Influence Throttling&lt;br&gt;
James Caverlee, Steve Webb and Ling Liu&lt;/li&gt;
&lt;li&gt;RF2ID: A Reliable Middleware Framework for RFID Deployment&lt;br&gt;
Nova Ahmed, Rajnish Kumar, Robert Steven French and Umakishore Ramachandran&lt;/li&gt;
&lt;li&gt;On the Design and Analysis of Irregular Algorithms on the Cell Processor: A Case Study of List Ranking&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Virat Agarwal and Kamesh Madduri&lt;/li&gt;
&lt;li&gt;Optimizing Multiple Distributed Stream Queries Using Hierarchical Network Partitions&lt;br&gt;
Sangeetha Seshadri, Vibhore Kumar, Brian F. Cooper and Ling Liu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 6th High-Performance Computational Biology (HiCOMB) Workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Graph-Theoretic Analysis of the Human Protein-Interaction Network Using Multicore Parallel Algorithms&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt; and Kamesh Madduri&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NSF Next Generation Software Workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DOSA: Design Optimizer for Scientific Applications&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt; and Viktor K. Prasanna&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Security in Systems and Networks (SSN) Workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improving Secure Communication Policy Agreements by Building Coalitions&lt;br&gt;
Srilaxmi Malladi, Sushil K. Prasad and Shamkant B. Navathe&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Performance Optimization for High-Level Languages and Libraries (POHLL) Workshop&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-Guided Empirical Optimization for Multimedia Extension Architectures: A Case Study&lt;br&gt;
Chun Chen, Jaewook Shin, Shiva Kintali, Jacqueline Chame and Mary Hall&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Multi-Threaded Architectures and Applications (MTAAP) Workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SWARM: A Parallel Programming Framework for Multicore Processors&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;, Varun Kanade and Kamesh Madduri&lt;/li&gt;
&lt;li&gt;Advanced Shortest Paths Algorithms on a Massively-Multithreaded Architecture&lt;br&gt;
Joseph R. Crobak, Jonathan W. Berry, Kamesh Madduri and &lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keynote-talk&#34;&gt;KEYNOTE TALK&lt;/h2&gt;
&lt;p&gt;Parallel and Distributed Scientific and Engineering Computing (PDSEC) Workshop:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Petascale Computing for Large-Scale Graph Problems&lt;br&gt;
&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;co-chair&#34;&gt;CO-CHAIR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, The 6th High-Performance Computational Biology (HiCOMB) Workshop&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;program-committee-memberships&#34;&gt;PROGRAM COMMITTEE MEMBERSHIPS&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, IPDPS 2007&lt;/li&gt;
&lt;li&gt;Sudhakar Yalamanchili, IPDPS 2007&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, The 16th Heterogeneity in Computing Workshop (HCW)&lt;/li&gt;
&lt;li&gt;Alberto Apostolico, The 6th High-Performance Computational Biology (HiCOMB) Workshop&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Multi-Threaded Architectures and Applications (MTAAP) Workshop&lt;/li&gt;
&lt;li&gt;Jeff Vetter, Multi-Threaded Architectures and Applications (MTAAP) Workshop&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, a faculty member in the Computational Science and Engineering Division, and Executive Director of High-Performance Computing at Georgia Tech, chairs the Institute of Electrical and Electronics Engineers&amp;rsquo; (IEEE) Technical Committee on Parallel Processing (TCPP), which sponsors IPDPS.  Bader will also host the Monday evening general membership reception on behalf of TCPP.&lt;/p&gt;
&lt;p&gt;For more information about IPDPS 2007, visit &lt;a href=&#34;http://www.ipdps.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.ipdps.org&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060213/http://www.cc.gatech.edu/news/cse-leadership-in-petascale-computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/cse-leadership-in-petascale-computing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multicore Processors for Science and Engineering</title>
      <link>http://localhost:1313/blog/20070220-multicoreprocessors/</link>
      <pubDate>Tue, 20 Feb 2007 07:56:00 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070220-multicoreprocessors/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Pam Frost Gorder&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s no question that multicore processors have gone mainstream. These computer chips, which have more than one CPU, first hit the consumer market less than two years ago. Today, practically every new computer has a dual-core (two-CPU) chip, and Intel just launched a quad-core chip with four CPUs. One of 2006&amp;rsquo;s most in-demand holiday gifts was Sony&amp;rsquo;s PlayStation 3, which boasts a “cell” chip with nine CPUs for faster and more realistic video gaming.&lt;/p&gt;
&lt;p&gt;Multicore systems might offer advantages to gamers, but what about researchers? &lt;strong&gt;David A. Bader&lt;/strong&gt;, who directs a new research center at Georgia Tech devoted to cell technology, says that making the most of multicore systems will require new tools, new algorithms, and a new way of looking at programming.&lt;/p&gt;
&lt;h3 id=&#34;embrace-concurrency&#34;&gt;Embrace Concurrency&lt;/h3&gt;
&lt;p&gt;“We&amp;rsquo;ve known for some time that Moore&amp;rsquo;s law was ending, and we would no longer be able to keep improving performance,” Bader says. “The steady progression from symmetric multiprocessing to putting many functional units on a chip to multicore has been a long time coming.” Software ran faster year after year, not because of software innovations, but because chip makers kept adding transistors to the standard single-processor architecture. Now, he says, clock speeds are capped out at around 4 GHz: “If we want faster speeds, we have to embrace concurrency and make use of multiple processors on the chip at once.”&lt;/p&gt;
&lt;p&gt;Bader heads the Sony-Toshiba-IBM (STI) Center of Competence at Georgia Tech, where researchers will develop applications for the Cell Broadband Engine (Cell BE) microprocessor—the chip that powers the PlayStation 3, as well as IBM&amp;rsquo;s QS20 blade servers. The Cell BE is already being developed for aerospace, defense, and medical imaging; the new research center will focus on scientific computing and bioinformatics. Bader also received a Microsoft research grant to develop algorithms that exploit multicore processors. He&amp;rsquo;ll adapt a library package called Swarm (SoftWare and Algorithms for Running on Multicore; &lt;a href=&#34;https://www.lesc.ic.ac.uk/markets/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.lesc.ic.ac.uk/markets/&lt;/a&gt;, &lt;a href=&#34;http://sourceforge.net/projects/multicore-swarm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://sourceforge.net/projects/multicore-swarm/&lt;/a&gt;) that he began building in 1994.&lt;/p&gt;
&lt;p&gt;Although a huge industry push toward multicore systems exists today, there wasn&amp;rsquo;t one when Bader first began working on Swarm. Another computer scientist who started thinking about multicore even earlier—albeit, on the hardware side of things—is Stanford University&amp;rsquo;s Kunle Olukotun. He recalls that when he and his colleagues started talking about multicore architectures in the early 1990s, they received a cool reception. “Back then, people thought that single-core processors still had a lot of life in them,” he says. But by 2001, he was working with Sun Microsystems to commercialize his first multicore chip, the Niagara. They designed it to work 10 times faster than existing devices with half the power consumption.&lt;/p&gt;
&lt;p&gt;In the end, what drove the industry to multicore technology wasn&amp;rsquo;t just the need for more processing speed, Olukotun says—it was the need for less heat and more energy efficiency. Less heat because the fastest chips were heating up faster than the average fan could cool them down, and more energy efficiency because single-core chips rely on tightly packed power-hungry transistors to get the job done.&lt;/p&gt;
&lt;p&gt;Compared to several single-core chips, a multicore chip is easier to cool because the CPUs are simpler and use fewer transistors. This means they use less power and dissipate less heat overall. As for performance, each multicore CPU can work on a different task at the same time. Parallel processing used to require more than one chip or clever algorithms to simulate parallel processing from the software side. In a multicore processor, however, parallelism is already built in.&lt;/p&gt;
&lt;h3 id=&#34;modeling-and-more-on-multicore&#34;&gt;Modeling (and More) on Multicore&lt;/h3&gt;
&lt;p&gt;So what will multicore processors mean for researchers? Bader says the built-in parallelization should seem natural to people who model computationally intensive problems because they&amp;rsquo;re already accustomed to using parallelized codes on computer clusters or supercomputers. But the average scientist will need tools. “[Scientists] may use Matlab or Mathematica or other standard packages, and they&amp;rsquo;re going to have to rely on those frameworks to make use of parallelism. Others who are developing scientific codes are going to have to think differently about those problems and ways of revealing concurrency,” he says.&lt;/p&gt;
&lt;h3 id=&#34;parallelization-tools&#34;&gt;Parallelization Tools&lt;/h3&gt;
&lt;p&gt;Computer scientists are working on tools to help parallelize the sequential algorithms used in research today. Each of these projects recently received funding from the US National Science Foundation to speed their development:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Most large-scale scientific programs have irregular data structures, such as tree and graph structures, that are hard to parallelize. Keshav Pingali, the W.A. “Tex” Moncrief Chair of Grid and Distributed Computing at the University of Texas at Austin, heads the Galois project, which aims to develop a compiler that can parallelize irregular algorithms for multicore processors. Galois uses “optimistic parallelization” to address what happens when more than one core accesses the same data at the same time. It runs a program in parallel, but if the data becomes corrupted, it aborts the computations and re-runs them sequentially.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Eric Polizzi, assistant professor of electrical engineering and computer engineering at the University of Massachusetts Amherst, is developing a parallel linear system solver called SPIKE, and integrating it into a nanoelectronics simulator, where it can help with quantum calculations (&lt;a href=&#34;https://www.ecs.umass.edu/ece/polizzi/Softwares.html%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.ecs.umass.edu/ece/polizzi/Softwares.html)&lt;/a&gt;. He plans to create a toolkit that can automatically provide nonexpert users with the most efficient version of SPIKE for their hardware schemes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rastislav Bodik, assistant professor of computer science at the University of California, Berkeley, is developing a different tool to make parallelization easier. Scientists start with their own sequential code and then “sketch” an outline of what they&amp;rsquo;d like the implementation to be, and a compiler does the rest (&lt;a href=&#34;https://www.cs.berkeley.edu/~bodlk/research/sketching.html%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cs.berkeley.edu/~bodlk/research/sketching.html)&lt;/a&gt;. “We had scientific programmers in mind when we designed the sketching language,” Bodik says.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is that most programmers—truly, most humans—think sequentially, so most codes are written to run sequentially. Parallelizing them can require heroic effort, Bader says: “We could rely on compilers to convert the code that we already have. But except for a few situations where data has a very simple structure, we haven&amp;rsquo;t seen compilers as a magic bullet to get performance.”&lt;/p&gt;
&lt;p&gt;When Olukotun and his colleagues designed Niagara, they optimized it to run commercial server applications that were already highly multithreaded, meaning they split tasks into “threads” of execution—instruction sequences that run in parallel. Now he&amp;rsquo;s working on a new technique called thread-level speculation that lets users parallelize sequential algorithms automatically for multicore architectures.&lt;/p&gt;
&lt;h3 id=&#34;web-trends&#34;&gt;Web Trends&lt;/h3&gt;
&lt;p&gt;For a brief look at current events, including program announcements and news items related to science and engineering, check out the following Web sites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Advanced Technological Education (ATE; &lt;a href=&#34;https://www.cs.berkeley.edu/~bodlk/research/sketching.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cs.berkeley.edu/~bodlk/research/sketching.html&lt;/a&gt;, &lt;a href=&#34;https://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07530%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07530)&lt;/a&gt;. This program emphasizes two-year colleges and their role in providing technicians to high-technology fields. Deadline for proposals is 11 October 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Canada Creates National High-Performance Computing Network (&lt;a href=&#34;https://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07530&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07530&lt;/a&gt;, &lt;a href=&#34;https://www.innovation.ca/media/index.cfm?websiteid=482%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.innovation.ca/media/index.cfm?websiteid=482)&lt;/a&gt;. Using C$88 million dollars from the Canada Foundation for Innovation (CFI) and the Natural Sciences and Engineering Research Council of Canada (NSERC), Canada will create its first national high-performance computing (HPC) network that unifies all HPC efforts across the country. All seven of Canada&amp;rsquo;s HPC Consortia will participate in the project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cornell Awarded Grant to Lead NSF&amp;rsquo;s Broadening Participation in Computing Program (&lt;a href=&#34;https://www.tc.comell.edu/News/2006/061214.htm%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.tc.comell.edu/News/2006/061214.htm)&lt;/a&gt;. A new grant will fund the Worlds for Information Technology and Science (WITS) project, which will look for ways to attract women and minorities to computing with service learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Industry/University Cooperative Research Centers Program (I/UCRC; &lt;a href=&#34;https://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07537%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07537)&lt;/a&gt;. The I/UCRC program&amp;rsquo;s goal is to foster partnerships among industry, academia, and government. Letters of intent are due 29 June 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mathematical Sciences: Innovations at the Interface with Computer Sciences (MSPA-MCS; &lt;a href=&#34;https://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07534%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07534)&lt;/a&gt;. The MSPA-MCS program fosters collaboration between mathematicians, statisticians, engineers, and scientists. Proposal deadline is 12 March 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Integrative Graduate Education and Research Traineeship Program (IGERT; &lt;a href=&#34;https://www.nsf.gov/pubs/2007/nsf07540/nsf07540.htm%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/pubs/2007/nsf07540/nsf07540.htm)&lt;/a&gt;. The US National Science Foundation (NSF) is soliciting proposals for the IGERT program, which was developed to help meet the educational needs of US PhD students in the science and engineering fields pursuing research and education careers. Preliminary proposal deadline is 5 April 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;National Science, Technology, Engineering, and Mathematics Education Digital Library (NSDL; &lt;a href=&#34;https://www.nsf.gov/pubs/2007/nsf07538/nsf07538.htm%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.nsf.gov/pubs/2007/nsf07538/nsf07538.htm)&lt;/a&gt;. The NSF is accepting proposals for its NSDL program, which seeks to create an online network and national library for science, technology, engineering, and mathematics education. Proposal deadline is 11 April 2007.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The idea with speculation is that the parallelization may not always work, and you can detect at runtime whether it&amp;rsquo;s working. When there is no parallelism in the application, you still get the same result as if you had a single-core processor. You can think of it as a safety net.” He sees colleagues in the sciences using more dynamic algorithms that are difficult to parallelize. “Be it seismic analysis for oil exploration or molecular dynamics for protein folding or probabilistic inference, these types of algorithms could really take advantage of the speculation,” Olukotun says.&lt;/p&gt;
&lt;h3 id=&#34;petaflops-by-2010&#34;&gt;Petaflops by 2010&lt;/h3&gt;
&lt;p&gt;Multicore technology is taking hold in supercomputing, where the goal is to reach petaflop (one quadrillion calculations per second) capability by the end of the decade. In 2006, the makers of the Top500 Supercomputer Sites list (&lt;a href=&#34;https://www.top500.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.top500.org&lt;/a&gt;) reported that 100 of its denizens now use dual-core chips, and that this number is expected to grow. At the 2006 International Supercomputing Conference (ISC) in Dresden, Germany, multicore computing was called one of the year&amp;rsquo;s advances—and one of two technologies that would guide supercomputing to the petaflop goal. As Thomas Sterling, professor of computer science at Caltech, wrote in his ISC review, 2006 marked a turning point in the quest for such machines (&lt;a href=&#34;https://www.hpcwire.com/hpc/709078.html%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.hpcwire.com/hpc/709078.html)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In particular, Sterling pointed to the multicore systems in the Top500 list, including the top-ranked IBM BlueGene/L system, which achieved 280.6 Tflops (trillions of calculations per second). “While the majority of such systems are dual-core,” he wrote, “next-generation systems are rapidly moving to quad-core. And it is expected that this trend will continue with Moore&amp;rsquo;s law over several iterations. However, it is recognized that the shift to multicore brings with it its own challenges. […] Even for the world of supercomputing, this trend to multicore will impose a demand for increasing parallelism. If, as is expected, this trend continues, then the amount of parallelism required of user applications may easily increase by two orders of magnitude over the next decade.”&lt;/p&gt;
&lt;p&gt;Scientists who already run large simulations or process massive amounts of data in parallel can look forward to some improvements from multicore systems. “Big science” problems in Earth science, atmospheric science, and molecular biology are among those that would benefit.&lt;/p&gt;
&lt;p&gt;Jim Gray, manager of the Microsoft Research eScience Group, works with the Sloan Digital Sky Survey (SDSS), which holds the world&amp;rsquo;s largest astronomical database (approximately 3 Tbytes of catalog data and 40 Tbytes of raw data). The SkyServer (&lt;a href=&#34;http://skyserver.sdss.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://skyserver.sdss.org&lt;/a&gt;) lets astronomers and educators access the catalog database. He says that, so far, processor speed isn&amp;rsquo;t as important to Sky-Server data access as the speed of the disks that store the data. The project generally has more CPUs available than it needs. Still, Gray says, “The SDSS image processing pipeline is 10,000 instructions per byte. That used to be a room full of machines, but the 4-GHz processors with four cores will run at more than 1 Mbyte per second, so we only need a few multicore machines to process the data.”&lt;/p&gt;
&lt;p&gt;Johns Hopkins University research scientist Ani Thakar adds that he and his SDSS colleagues are working hard to keep their CPUs busier, in part by parallelizing data access and “bringing the analysis to the data as far as possible, rather than the other way around” to minimize disk input/output. “I think in the near future, our fraction of CPU usage will steadily increase and we will be able to benefit considerably from the multicore design,” Thakar says.&lt;/p&gt;
&lt;h3 id=&#34;thinking-in-parallel&#34;&gt;Thinking in Parallel&lt;/h3&gt;
&lt;p&gt;For decades, programmers have been trained to write sequential algorithms. To Bader, the ability to write parallel code is a different kind of skill, one that has nothing to do with a programmer&amp;rsquo;s intelligence, but rather his or her ability to think broadly. “I&amp;rsquo;m convinced that it&amp;rsquo;s an art,” he says. “You either get it, or you don&amp;rsquo;t.” He&amp;rsquo;s training students at Georgia Tech to think in parallel—and to think of how their programs connect to larger issues in science and engineering.&lt;/p&gt;
&lt;p&gt;“I think this is a really exciting time—the first time in 20 years that we&amp;rsquo;ve seen really disruptive technologies in computing,” Bader says. “Multicore is a disruptive technology—and I mean that in a good way—because it&amp;rsquo;s only when you have disruption of the status quo that new innovations can impact technology with revolutionary advances.”&lt;/p&gt;
&lt;p&gt;Universities that embrace this philosophy could reap an added benefit. Bader says Georgia Tech has seen a boost in computer science enrollment; nationally, the number of students interested in the major is falling. “Computer science has in some sense become stagnant because many students today don&amp;rsquo;t see how computer science impacts the world,” he says. Georgia Tech has reorganized its computer science program to create a computational science and engineering division to tie programming to the idea of solving real-world problems. So have the University of California, Berkeley, and the University of Texas at Austin, and Bader predicts that more universities nationwide will soon follow. Multicore computing is helping to kick-start the change.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intel announces it&#39;s built a better microprocessor</title>
      <link>http://localhost:1313/blog/20070212-sfchronicle/</link>
      <pubDate>Mon, 12 Feb 2007 20:34:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070212-sfchronicle/</guid>
      <description>

















&lt;figure  id=&#34;figure-the-intel-teraflop-research-chip-is-seen-tuesday-feb-6-2007-near-san-jose-ore-intel-corp-has-designed-a-computer-chip-that-promises-to-do-as-many-calculations-as-quickly-as-an-entire-data-center-while-consuming-as-much-energy-as-a-light-bulb-ap-photothe-oregonian-fredrick-d-joe&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Intel Teraflop Research Chip is seen Tuesday, Feb. 6, 2007, near San Jose, Ore. Intel Corp. has designed a computer chip that promises to do as many calculations as quickly as an entire data center, while consuming as much energy as a light bulb. *(AP Photo/The Oregonian, Fredrick D. Joe)*&#34; srcset=&#34;
               /blog/20070212-sfchronicle/Intel2007_hu_9248344586009125.webp 400w,
               /blog/20070212-sfchronicle/Intel2007_hu_37154edfeac0710b.webp 760w,
               /blog/20070212-sfchronicle/Intel2007_hu_50e5234395c6339c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070212-sfchronicle/Intel2007_hu_9248344586009125.webp&#34;
               width=&#34;492&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Intel Teraflop Research Chip is seen Tuesday, Feb. 6, 2007, near San Jose, Ore. Intel Corp. has designed a computer chip that promises to do as many calculations as quickly as an entire data center, while consuming as much energy as a light bulb. &lt;em&gt;(AP Photo/The Oregonian, Fredrick D. Joe)&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;by Tom Abate, Chronicle Staff Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Scientists at Intel Corp. have made an experimental microprocessor the size of a fingertip that has the same computational power that it took a 2,500-square-foot supercomputer to deliver just 11 years ago.&lt;/p&gt;
&lt;p&gt;The new chip could give personal computers extraordinary capabilities that are now available to only a handful of research computers, such as video games that look as realistic as television shows and machines capable of understanding speech.&lt;/p&gt;
&lt;p&gt;The announcement follows a series of different chip advances by Intel, IBM and Hewlett-Packard, all touting different ways that firms are trying to make processors smaller, faster and more energy-efficient in order to bring better graphics and sound to desktop and, ultimately, handheld devices.&lt;/p&gt;
&lt;p&gt;Intel Chief Technology Officer Justin Rattner formally introduced the super-processor Sunday on the opening day of an international conference of chip scientists who will be meeting in San Francisco through Thursday. He said it could take five years before the super-processor is ready for commercial use.&lt;/p&gt;
&lt;p&gt;In an advance briefing last week, Rattner used the super-processor to perform more than a trillion mathematical calculations per second. To put that into perspective, it would take light, traveling 186,282 miles per second, 62.1 days to travel a trillion miles.&lt;/p&gt;
&lt;p&gt;The trillion calculation threshold &amp;ndash; called a &amp;ldquo;teraflop&amp;rdquo; &amp;ndash; was a big deal a decade ago, when supercomputers first achieved it. Today, the scientific computers that tackle big computational problems like global warming have gone on to even faster speeds. But the excitement here is the prospect of bringing teraflop performance to business and ultimately consumer machines.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re going to put supercomputer-like capabilities onto a single chip at the desktop,&amp;rdquo; Rattner said.&lt;/p&gt;
&lt;p&gt;Jim McGregor, semiconductor analyst for the market research firm In-Stat, said Intel engineered this super-processor to draw roughly the same electricity as today&amp;rsquo;s PC microprocessors, which will mean that it should be easier to incorporate into future PCs without having to redesign the cooling systems that keep machines from overheating.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;To create supercomputer capability at that low power consumption is stunning,&amp;rdquo; McGregor said, adding: &amp;ldquo;But there are still a lot of hurdles to overcome.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Foremost among these will be the need to create new software capable of directing these super-processors. To understand that challenge, it&amp;rsquo;s necessary to consider how Intel&amp;rsquo;s new super-processor differs from today&amp;rsquo;s microprocessors.&lt;/p&gt;
&lt;p&gt;Think of the microprocessor as a brain. For years, chipmakers made these silicon brains faster by shrinking the size of the transistors that form their fundamental working unit. More transistors in the same space offered greater computing capability. Moore&amp;rsquo;s Law, named after Intel co-founder Gordon Moore, predicted that this dynamic would continue at a steady rate. But as transistors have gotten tinier and tinier, chipmakers have started to have more and more trouble continuing the shrinking act.&lt;/p&gt;
&lt;p&gt;Intel&amp;rsquo;s new super-processor takes a radically different approach to boosting performance. It doesn&amp;rsquo;t mess with the size of the transistor.&lt;/p&gt;
&lt;p&gt;Instead of thinking of the processor as a single brain, they designed it as 80 computing cores. Each core is like a mini-microprocessor, trained to do a small part of some larger task. Intel scientists are using special software algorithms to break huge computational problems into many small pieces &amp;ndash; and then using the 80 cores to solve these bits of the problem, all at the same time &amp;ndash; to achieve the milestone of a trillion calculations per second. This type of divide-and-conquer problem-solving is called parallel processing. It has been used for years to control house-size supercomputers. In 1996, for instance, Intel built the first supercomputer to hit the trillion-calculation threshold. It was a parallel processor. But it was huge, consisting of 104 computer cabinets that filled a 2,500-square-foot room.&lt;/p&gt;
&lt;p&gt;The parallel processing software that ran these room-size supercomputers was written from scratch, said &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of high-performance computing at Georgia Tech.&lt;/p&gt;
&lt;p&gt;That was fine when creating a climate-modeling program for a research lab. But Intel&amp;rsquo;s super-chip is designed for use in ordinary electronic devices, and so its software will have to be written by regular programmers, Bader said. And they will have to rethink how they do things to take advantage of the 80 cores.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The average programmer is trained to think sequentially, but they are going to have to learn to see tasks in parallel,&amp;rdquo; said Bader.&lt;/p&gt;
&lt;p&gt;The Redwood City startup Rapport, Inc. is another semiconductor firm that has created a high-performance chip by dividing it into many cores. Rapport president Frank Sinton said his company&amp;rsquo;s chips are designed to deliver low power consumption and high performance, and are currently intended for mobile devices.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;People want more video, better-quality audio, faster speeds and longer battery life,&amp;rdquo; Sinton said. Designing a single chip to behave like a cluster of processors is a good way to do this &amp;ndash; provided software exists to efficiently divide the tasks.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We are experts in programming,&amp;rdquo; Sinton said.&lt;/p&gt;
&lt;p&gt;While briefing reporters last week, Intel&amp;rsquo;s Rattner said his company has been working to develop software tools to make parallel processing easier while at the same time trying to get code-warriors thinking about possible uses for these forthcoming super-chips.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re trying to turn on programmers to the fact that this kind of power is coming,&amp;rdquo; he said.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM selects Bader for its 2007 Technical Leadership Forum</title>
      <link>http://localhost:1313/blog/20070205-ibm-tlf/</link>
      <pubDate>Mon, 05 Feb 2007 20:17:51 -0400</pubDate>
      <guid>http://localhost:1313/blog/20070205-ibm-tlf/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20070205-ibm-tlf/letter_hu_1b75aefd31995fc2.webp 400w,
               /blog/20070205-ibm-tlf/letter_hu_40e4772ab38d0ba1.webp 760w,
               /blog/20070205-ibm-tlf/letter_hu_1ad0d92c19ff5a4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20070205-ibm-tlf/letter_hu_1b75aefd31995fc2.webp&#34;
               width=&#34;586&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Pioneering Petascale Computing in the Biological Sciences - Workshop Report</title>
      <link>http://localhost:1313/blog/20061214-sdsc/</link>
      <pubDate>Thu, 14 Dec 2006 07:41:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061214-sdsc/</guid>
      <description>&lt;p&gt;Supercomputers help scientists build virtual worlds to explore blood flow for stroke prevention, design new proteins for life-saving drugs, and diagnose brain disorders. But even with today&amp;rsquo;s largest machines, researchers are only beginning to capture the key features of many complex problems.&lt;/p&gt;
&lt;p&gt;To take supercomputers to the next level of power and realism, today&amp;rsquo;s frontier goal is &amp;ldquo;petascale&amp;rdquo; computing, announced by the National Science Foundation (NSF) in the Leadership-Class System Acquisition - Creating a Petascale Computing Environment for Science and Engineering. The initiative calls for building a supercomputer by around 2011 that is capable of sustained performance of one petaflop for important scientific applications &amp;ndash; that&amp;rsquo;s 10^15 or one thousand trillion floating point operations per second. If all 6.5 billion people in the world worked together on a problem, each using a calculator and doing one calculation per second, it would take them 150,000 times longer than a petaflops supercomputer.&lt;/p&gt;
&lt;p&gt;But will today&amp;rsquo;s application codes in the biological sciences, the geosciences, and other disciplines &amp;ndash; the programs that simulate problems and analyze data &amp;ndash; be able to scale up to take full advantage of the next generation of supercomputers?&lt;/p&gt;
&lt;p&gt;Hand in hand with the effort to develop petascale-level high performance architectures, preparing applications to take advantage of these extraordinarily powerful machines will require a concerted effort in extreme application scaling to target and develop petascale-capable codes that can run on parallel supercomputers with hundreds of thousands of processors.&lt;/p&gt;
&lt;p&gt;To help with this effort, the NSF Biological Sciences Directorate sponsored a workshop entitled &amp;ldquo;Petascale Computing in the Biological Sciences&amp;rdquo; in which biological and computer scientists teamed to identify applications and plan development efforts to reach petascale biology applications. Organized by Allan Snavely of the San Diego Supercomputer Center (SDSC), &lt;strong&gt;David Bader&lt;/strong&gt; of the Georgia Institute of Technology, and Gwen Jacobs of Montana State University , the workshop was held at NSF headquarters in Washington D.C. August 29-30, 2006. The final workshop report is now available online in PDF format.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A key workshop goal was to examine the new opportunities for progress in the biological sciences that will be made possible by having a petascale computational capability,&amp;rdquo; said Snavely, Director of SDSC&amp;rsquo;s Performance Modeling and Characterization (PMaC) laboratory. &amp;ldquo;We also worked together to identify the steps for a smooth path for the biology community to take advantage of these amazing resources when they come on line.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The primary finding of the workshop, says Snavely, is that it is vital for collaborating teams of biologists and computer scientists to work closely together, starting now, in order to enable petascale computations of scientific importance to begin running on the petascale facility in just a few years.&lt;/p&gt;
&lt;p&gt;In addition to the final workshop report, workshop presentations are also available online.&lt;/p&gt;
&lt;p&gt;SDSC hosted a related workshop, also organized by Snavely, &amp;quot; PetaScale Computation for the Geosciences Workshop,&amp;quot; on April 4-5, 2006. Presentations for this workshop are available at . The second part of this two-part workshop will be held in January, 2007 at the National Center for Atmospheric Research (NCAR), and a final workshop report will then be issued.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sdsc.edu/News%20Items/PR121406_petascale_b.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.sdsc.edu/News%20Items/PR121406_petascale_b.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM Announces Winners of Shared University Research Awards</title>
      <link>http://localhost:1313/blog/20061212-ibm-sur/</link>
      <pubDate>Tue, 12 Dec 2006 12:49:04 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061212-ibm-sur/</guid>
      <description>&lt;p&gt;IBM (NYSE: IBM) today announced that ten universities spanning multiple geographies have been chosen as winners of the latest IBM Shared University Research (SUR) awards. For the first time, each of the universities will be using the Cell Broadband Engine™ (Cell/B.E.) technology to enable students and faculty to drive innovation, collaborate and foster skill development in the creation of digital media, software platform performance and medical imaging solutions.
As research helps drive innovation and growth, new skills are required to staff the emerging disciplines and technologies, leading to tremendous opportunities to drive Cell/B.E. technology into multiple areas.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Because of its ability to handle compute-intensive applications, we are seeing tremendous demand to incorporate Cell/B.E. microprocessor technology in a host of products, solutions and opportunities outside of gaming,&amp;rdquo; said Lilian Wu, Program Executive, IBM University Relations and Innovation. &amp;ldquo;All of these universities have very unique ideas on how they think Cell/B.E. technology can be applied to help solve different problems, as well as using the technology to encourage skill development among its students and faculty. IBM is proud to collaborate with these universities to make these innovation ideas possible.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The ten winning universities include:&lt;/p&gt;
&lt;h4 id=&#34;north-america&#34;&gt;North America&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Georgia Institute of Technology&lt;/strong&gt; (Atlanta, Georgia): The College of Computing at Georgia Institute of Technology will undertake a research project to test high performance computing, gaming and digital content applications on Cell/B.E. technology, as well as port and optimize key    Cell/B.E. libraries for data, video and image processing.&lt;/li&gt;
&lt;li&gt;University of California San Diego (San Diego, California): UCSD&amp;rsquo;s Experimental Game Lab will use Cell/B.E. technology to accelerate computation in their applications, making more aspects of game environments a part of a user&amp;rsquo;s real-time interactive experience.&lt;/li&gt;
&lt;li&gt;University of Illinois at Urbana-Champaign: The University will look into developing programming models for the Cell Broadband Engine along with applying Cell/B.E. technology in the continuing development of high performance computing applications including molecular dynamics and    cosmology simulations.&lt;/li&gt;
&lt;li&gt;University of Minnesota (Minneapolis/St. Paul, Minnesota): The University of Minnesota will investigate Cell/B.E. implementation on numerical algorithms for fluid dynamics.&lt;/li&gt;
&lt;li&gt;University of Virginia (Charlottesville, Virginia): The University&amp;rsquo;s Institute for Advanced Technology in the Humanities will be using Cell/B.E. technology to develop a real-time 3D rendering model of the City of Rome in    AD400, for both classroom and research in the Institute&amp;rsquo;s new 3D theater.&lt;/li&gt;
&lt;li&gt;University of Washington (Seattle, Washington): The University&amp;rsquo;s Department of Bioengineering will explore the use of Cell/B.E. technology in various medical imaging modalities.  Specifically, they will design a fully programmable ultrasound machine architecture that can be scaled from sophisticated high-end systems to low-cost units for use in doctors&amp;rsquo; offices and in the home.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;europe-middle-east-and-asia&#34;&gt;Europe, Middle East and Asia&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Barcelona Supercomputing Center at the Technical University of Catalonia (Barcelona, Spain): The Center will investigate innovative programming models for scientific and technical computing for life sciences, earth sciences and engineering.&lt;/li&gt;
&lt;li&gt;Tsinghua University (Beijing, China): Tsinghua University in China will implement Cell/B.E. technology to test real-time multi-view video coding and rendering, taking multi-view images from the real world and modeling them for the virtual world to implement interactive streaming applications.
United Arab Emirates University (Al-Ain, UAE): The College of Information Technology at the United Arab Emirates University will develop a set of new applications for the Cell/B.E. technology in the areas of seismic imaging and parallel oil reservoir simulations which are of particular importance in the oil industry.&lt;/li&gt;
&lt;li&gt;University of Dublin Trinity College (Dublin, Ireland): The University will be implementing Cell/B.E. technology with the goal to create realistic animation of human motion, which is critical to the development of computer and video games and movies.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The revolutionary Cell/B.E. processor is a breakthrough design featuring a central processing core, based on IBM&amp;rsquo;s industry-leading Power Architecture™ technology, and eight synergistic processing elements (SPE). Cell/B.E. &amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and handhelds, virtual reality, wireless downloads, real-time video chat, interactive TV shows and other &amp;ldquo;image-hungry&amp;rdquo; computing environments. The groundbreaking Cell/B.E. processor appears in products such as Sony Computer Entertainment&amp;rsquo;s PLAYSTATION®3, Toshiba&amp;rsquo;s Cell/B.E Reference Set, a development tool for Cell/B.E. products, and already is included in the IBM BladeCenter® QS20, or &amp;ldquo;Cell Blade.&amp;rdquo; The Cell/B.E. processor is also used through joint collaboration with Mercury Computer Systems, Inc., targeted at aerospace and defense, semiconductor, medical imaging, and other markets.&lt;/p&gt;
&lt;p&gt;IBM&amp;rsquo;s highly selective SUR program awards computing equipment, software, and services globally to higher education institutions in order to facilitate research projects of mutual interest, including: the architecture of business and processes, real-time data analysis, privacy and security, supply chain management, information based medicine, deep computing, event-driven computing, and storage solutions. The SUR awards also support the advancement of university projects by connecting top researchers in academia with IBM researchers, along with representatives from product development and solution provider communities. IBM supports more than 50 SUR awards per year worldwide.&lt;/p&gt;
&lt;h4 id=&#34;about-ibm&#34;&gt;About IBM&lt;/h4&gt;
&lt;p&gt;For more information on IBM&amp;rsquo;s SUR award project, please visit: &lt;a href=&#34;http://www-304.ibm.com/jct09002c/university/scholars/sur/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www-304.ibm.com/jct09002c/university/scholars/sur/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For more information on IBM Technology Collaboration Solutions, please visit &lt;a href=&#34;http://www.ibm.com/technology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ibm.com/technology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www-03.ibm.com/press/us/en/pressrelease/20756.wss&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www-03.ibm.com/press/us/en/pressrelease/20756.wss&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.hg.gatech.edu/node/51601&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.hg.gatech.edu/node/51601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tech Wins IBM Shared University Research Award:
&lt;a href=&#34;https://web.archive.org/web/20080202061034/http://www.cc.gatech.edu/news/tech-wins-ibm-shared-university-research-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/tech-wins-ibm-shared-university-research-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pioneering Petascale Computing in Biological Sciences</title>
      <link>http://localhost:1313/blog/20061208-hpcwire/</link>
      <pubDate>Fri, 08 Dec 2006 07:38:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061208-hpcwire/</guid>
      <description>&lt;p&gt;Supercomputers help scientists build virtual worlds to explore blood flow for stroke prevention, design new
proteins for life-saving drugs, and diagnose brain disorders. But even with today&amp;rsquo;s largest machines,
researchers are only beginning to capture the key features of many complex problems.&lt;/p&gt;
&lt;p&gt;To take supercomputers to the next level of power and realism, today&amp;rsquo;s frontier goal is &amp;ldquo;petascale&amp;rdquo; computing,
announced by the National Science Foundation (NSF) in the Leadership-Class System Acquisition - Creating
a Petascale Computing Environment for Science and Engineering. The initiative calls for building a
supercomputer by around 2011 that is capable of sustained performance of one petaflop for important
scientific applications &amp;ndash; that&amp;rsquo;s 10^15 or one thousand trillion floating point operations per second. If all 6.5
billion people in the world worked together on a problem, each using a calculator and doing one calculation
per second, it would take them 150,000 times longer than a petaflops supercomputer.&lt;/p&gt;
&lt;p&gt;But will today&amp;rsquo;s application codes in the biological sciences, the geosciences, and other disciplines &amp;ndash; the
programs that simulate problems and analyze data &amp;ndash; be able to scale up to take full advantage of the next
generation of supercomputers?&lt;/p&gt;
&lt;p&gt;Hand in hand with the effort to develop petascale-level high performance architectures, preparing
applications to take advantage of these extraordinarily powerful machines will require a concerted effort in
extreme application scaling to target and develop petascale-capable codes that can run on parallel
supercomputers with hundreds of thousands of processors.&lt;/p&gt;
&lt;p&gt;To help with this effort, the NSF Biological Sciences Directorate sponsored a workshop entitled &amp;ldquo;Petascale
Computing in the Biological Sciences&amp;rdquo; in which biological and computer scientists teamed to identify
applications and plan development efforts to reach petascale biology applications. Organized by Allan
Snavely of the San Diego Supercomputer Center (SDSC), &lt;strong&gt;David Bader&lt;/strong&gt; of the Georgia Institute of
Technology, and Gwen Jacobs of Montana State University, the workshop was held at NSF headquarters in
Washington D.C. August 29-30, 2006. The final workshop report is now available online in PDF format at
&lt;a href=&#34;http://www.sdsc.edu/PMaC/BioScience_Workshop/Publications/PetascaleBIOworkshopreport.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sdsc.edu/PMaC/BioScience_Workshop/Publications/PetascaleBIOworkshopreport.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A key workshop goal was to examine the new opportunities for progress in the biological sciences that will
be made possible by having a petascale computational capability,&amp;rdquo; said Snavely, Director of SDSC&amp;rsquo;s
Performance Modeling and Characterization (PMaC) laboratory. &amp;ldquo;We also worked together to identify the
steps for a smooth path for the biology community to take advantage of these amazing resources when they
come on line.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The primary finding of the workshop, says Snavely, is that it is vital for collaborating teams of biologists and
computer scientists to work closely together, starting now, in order to enable petascale computations of
scientific importance to begin running on the petascale facility in just a few years.&lt;/p&gt;
&lt;p&gt;In addition to the final workshop report, workshop presentations are also available at
&lt;a href=&#34;http://www.sdsc.edu/PMaC/BioScience_Workshop/biosciences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sdsc.edu/PMaC/BioScience_Workshop/biosciences.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SDSC hosted a related workshop, also organized by Snavely, &amp;ldquo;PetaScale Computation for the Geosciences
Workshop,&amp;rdquo; on April 4-5, 2006. Presentations for this workshop are available at
&lt;a href=&#34;http://www.sdsc.edu/PMaC/GeoScience_Workshop/geosciences.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sdsc.edu/PMaC/GeoScience_Workshop/geosciences.html&lt;/a&gt;. The second part of this two-part
workshop will be held in January, 2007 at the National Center for Atmospheric Research (NCAR), and a final
workshop report will then be issued.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Wins Cell Processor Center</title>
      <link>http://localhost:1313/blog/20061201-buzz/</link>
      <pubDate>Fri, 01 Dec 2006 21:50:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061201-buzz/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech has been designated as the first Sony-Toshiba-IBM Center of Competence to build a community of programmers and broaden industry support for the Cell Broadband Engine microprocessor.&lt;/p&gt;
&lt;p&gt;Cell BE &amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and handhelds, virtual reality, wireless downloads, real-time video chat, interactive TV shows and other &amp;ldquo;image-hungry&amp;rdquo; computing environments.&lt;/p&gt;
&lt;p&gt;The Cell BE processor already appears in such products as Sony&amp;rsquo;s PlayStation 3, Toshiba&amp;rsquo;s Cell Reference Set and the IBM BladeCenter QS20.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The College of Computing at Georgia Tech firmly believes that the Sony-Toshiba-IBM Cell BE processor represents the future of computing using heterogeneous multi-core processors, and we are pleased to work with three leading technology companies in a broad collaboration that will demonstrate the extreme performance of Cell,&amp;rdquo; said &lt;strong&gt;David A. Bader&lt;/strong&gt;, associate professor and executive director of high-performance computing in the College of Computing.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;By supporting the growth of the industry-changing Cell BE processor technology, the College of Computing at Georgia Tech will drive the continued advancement of computationally intensive applications that will directly impact the global growth of our industry and the evolution of our society,&amp;rdquo; Bader said.&lt;/p&gt;
&lt;p&gt;Directed by Bader, the STI Cell Center of Competence, located in the Christopher W. Klaus Advanced Computing Building, will provide remote access to Cell blade hardware installed at Georgia Tech, create and disseminate software optimized for Cell BE systems and perform research on the design of Cell BE systems, algorithms and applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080129224755/http://gtalumni.org/buzzwords/dec06/article88.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://gtalumni.org/buzzwords/dec06/article88.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Wins Microsoft Research Award</title>
      <link>http://localhost:1313/blog/20061120-microsoft/</link>
      <pubDate>Mon, 20 Nov 2006 22:10:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061120-microsoft/</guid>
      <description>&lt;p&gt;College of Computing Associate Professor and Executive Director of High-Performance Computing &lt;strong&gt;David Bader&lt;/strong&gt; received a $75,000 award from Microsoft Research to investigate the design and optimization of algorithms that fully exploit multi-core processors. The project, “Enabling MS Visual Studio Programmers to Design Efficient Parallel Algorithms for Multi-Core Processors,” is one of approximately six projects selected by Microsoft Research in the 2006-2007 Parallel and Concurrent Programming, a targeted research effort that funds work dedicated to pursuits in concurrency, parallelism and multi-core technology.&lt;/p&gt;
&lt;p&gt;Since the inception of the desktop computer, software performance has improved at an exponential rate, primarily driven by the steady technological improvements of microprocessors. Due to fundamental physical limitations and power constraints, there is currently a radical change in commodity microprocessor architecture to multi-core designs. Continued performance now requires the exploitation of concurrency at the algorithmic level. Thus, a crisis is occurring to provide software engineers with productive and easy-to-use programming methodologies that can take full advantage of multi-core processors. This crisis is primarily driven by three causes: the microprocessor industry must shift to multi-core processor architectures; computer science education has focused primarily on teaching sequential algorithms; and automated methods, such as those in compilers, cannot deduce algorithmic concurrency from most sequential codes.&lt;/p&gt;
&lt;p&gt;This award supports Bader’s design of a parallel programming methodology for Microsoft client systems with commodity multi-core processors by providing a productive, easy-to-use portable library package called SWARM (SoftWare and Algorithms for Running on Multicore) for Visual Studio.  Bader, a pioneer in the area of parallel algorithm engineering techniques, was chosen for the award because he has extensive experience designing the world’s fastest parallel algorithms for shared memory, symmetric multiprocessor and multithreaded architectures.&lt;/p&gt;
&lt;p&gt;“There is no more free lunch,” said Bader. “Continued performance improvements of client codes cannot rely solely on the ride we’ve taken with Moore’s Law for the past four decades. Instead, we must take stake out a new path for training programmers to use multicore (parallel) algorithms. We will need to educate programmers and developers on the design, analysis, and implementation, of parallel algorithms by providing a parallel programming methodology and example codes, and also provide libraries and tools that offer a rich foundation of efficient parallel primitives and kernels.”&lt;/p&gt;
&lt;p&gt;Bader plans to deliver his methodologies and parallel algorithm libraries as a package for Microsoft Visual Studio 2005 using the SDK v3 extensibility toolkit. Several releases of the SWARM package are planned: an initial release when the build environment is stable and basic primitives are implemented; a second major release with parallel primitives that support two to three higher-level algorithms; and the third release at the end of the project with a complete set of fundamental kernels and additional higher-level algorithms. The source code will be freely-available under the Microsoft Permissive License (Ms-PL) that allows licensees to view, modify, and redistribute the source code for either commercial or non-commercial purposes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060253/http://www.cc.gatech.edu/news/david-bader-wins-microsoft-research-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/david-bader-wins-microsoft-research-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing Designated First STI Center of Competence Focused on Cell Processor</title>
      <link>http://localhost:1313/blog/20061115-sti/</link>
      <pubDate>Wed, 15 Nov 2006 21:47:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061115-sti/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech today announced its designation as the first Sony-Toshiba-IBM (STI) Center of Competence focused on the Cell Broadband Engine™ (Cell BE) microprocessor. IBM® Corp., Sony Corporation and Toshiba Corporation selected to partner with the College of Computing at Georgia Tech to build a community of programmers and broaden industry support for the Cell BE processor.&lt;/p&gt;
&lt;p&gt;The revolutionary Cell BE processor is a breakthrough design featuring a central processing core, based on IBM&amp;rsquo;s industry leading Power Architecture™ technology, and eight synergistic processors.  Cell BE &amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and handhelds, virtual-reality, wireless downloads, real-time video chat, interactive TV shows and other &amp;ldquo;image-hungry&amp;rdquo; computing environments.  The groundbreaking Cell BE processor appears in products such as Sony Computer Entertainment’s PLAYSTATION®3, Toshiba’s Cell Reference Set, a development tool for Cell products, and already is included in the IBM BladeCenter® QS20, or “Cell Blade”, as well as through joint collaboration with Mercury Computer Systems, Inc., targeted at aerospace and defense, semiconductor, medical imaging, and other markets.&lt;/p&gt;
&lt;p&gt;“The College of Computing at Georgia Tech firmly believes that the Sony-Toshiba-IBM Cell BE processor represents the future of computing using heterogeneous multi-core processors, and we are pleased to work with three leading technology companies in a broad collaboration that will demonstrate the extreme performance of Cell,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, Associate Professor and Executive Director of High-Performance Computing in the College of Computing at Georgia Tech. “By supporting the growth of the industry-changing Cell BE processor technology, the College of Computing at Georgia Tech will drive the continued advancement of computationally-intensive applications that will directly impact the global growth of our industry and the evolution of our society.”&lt;/p&gt;
&lt;p&gt;Directed by Bader, the new STI Cell Center of Competence at Georgia Tech has a mission to grow the community of Cell BE users and developers by performing research and service in support of the Cell BE processor, and further enable students at the College to grow their skills and experience around Cell BE technology to apply in future career opportunities. The Center will sponsor discussion forums and workshops, provide remote access to Cell blade hardware installed at Georgia Tech, create and disseminate software optimized for Cell BE systems, and perform research on the design of Cell BE systems, algorithms, and applications. The award from Sony-Toshiba-IBM will support the Center’s activities and research efforts in support of broadening Cell BE’s impact into multiple sectors and industries, including scientific computing, digital content creation, bioinformatics, finance, gaming and entertainment.&lt;/p&gt;
&lt;p&gt;“We are looking forward to seeing a paradigm shift in computing, and anticipate that our collaboration with the College of Computing at Georgia Tech will create innovative applications for Cell processors,” said Masa Chatani, Senior General Manager, Cell Development Center, Sony Corporation and also CTO of Sony Computer Entertainment Inc. “We expect that it will generate tremendous value not limited to PLAYSTATION 3 but to all Cell-based computers. We are looking forward to seeing a new computing paradigm.”&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We look forward to seeing the Center of Competence at Georgia Tech generating outstanding technology based on Cell BE,” said Tomotaka Saito, General Manager, Broadband System LSI Division, System LSI Division I, Toshiba’s Semiconductor Company. &amp;ldquo;The future will see growing demand for multi-core processor applications, and we want to see the Center playing a key role in anticipating and responding to such demand.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;“The joint collaboration by IBM, Sony and Toshiba on the Cell processor has led to tremendous advancements in computing applications and innovations,” said Sharon Nunes, Vice President, Business Development and Strategic Growth Initiatives, IBM Systems &amp;amp; Technology Group.  “We are pleased to be collaborating with the College of Computing at Georgia Tech to enable a team of engineers, professors and students to create breakthrough solutions, share information among various industries and other universities, and further the Cell ecosystem overall.”&lt;/p&gt;
&lt;h2 id=&#34;about-the-college-of-computing-at-georgia-tech&#34;&gt;About the College of Computing at Georgia Tech&lt;/h2&gt;
&lt;p&gt;The College of Computing at Georgia Tech is a national leader in the research and creation of real-world computing breakthroughs that drive social and scientific progress. With its graduate program ranked 11th nationally by U.S. News and World Report, the College’s unconventional approach to education is pioneering the new era of computing by expanding the horizons of traditional computer science students through interdisciplinary collaboration and a focus on human centered solutions. For more information about the College of Computing at Georgia Tech, its academic divisions and research centers, click here.&lt;/p&gt;
&lt;h2 id=&#34;media-contact&#34;&gt;Media Contact&lt;/h2&gt;
&lt;p&gt;For College of Computing at Georgia Tech&lt;br&gt;
Stefany Wilson&lt;br&gt;
404-894-7253&lt;br&gt;
&lt;a href=&#34;mailto:stefany@cc.gatech.edu&#34;&gt;stefany@cc.gatech.edu&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;heading&#34;&gt;# &lt;/h1&gt;
&lt;p&gt;IBM, BladeCenter, Power Architecture and QS20 are trademarks of IBM Corporation in the United States and/or other countries.&lt;/p&gt;
&lt;p&gt;PLAYSTATION is a registered trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;p&gt;All other company/product names and service marks may be trademarks or registered trademarks of their respective companies.&lt;/p&gt;
&lt;p&gt;Cell Broadband Engine is a trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;p&gt;For more information, &lt;a href=&#34;https://web.archive.org/web/20080202055816/http://www.ibm.com/legal/copytrade.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202055816/http://www.cc.gatech.edu/news/college-of-computing-designated-first-sti-center-of-competence-focused-on-cell-processor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/college-of-computing-designated-first-sti-center-of-competence-focused-on-cell-processor&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>College of Computing at Georgia Tech Selected as First Sony-Toshiba-IBM Center of Competence Focused on the Cell Processor</title>
      <link>http://localhost:1313/blog/20061115-cell-center/</link>
      <pubDate>Wed, 15 Nov 2006 06:53:11 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061115-cell-center/</guid>
      <description>&lt;p&gt;The College of Computing at Georgia Tech today announced its designation
as the first Sony-Toshiba-IBM (STI) Center of Competence focused on the Cell Broadband Engine™ (Cell
BE) microprocessor. &lt;a href=&#34;https://www.ibm.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBM® Corp.&lt;/a&gt;, &lt;a href=&#34;http://www.sony.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sony Corporation&lt;/a&gt; and &lt;a href=&#34;https://www.toshiba.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Toshiba Corporation&lt;/a&gt; selected to partner with the
College of Computing at Georgia Tech to build a community of programmers and broaden industry support
for the Cell BE processor.&lt;/p&gt;
&lt;p&gt;The revolutionary Cell BE processor is a breakthrough design featuring a central processing core, based on
IBM&amp;rsquo;s industry leading Power Architecture™ technology, and eight synergistic processors. Cell BE
&amp;ldquo;supercharges&amp;rdquo; compute-intensive applications, offering fast performance for computer entertainment and
handhelds, virtual-reality, wireless downloads, real-time video chat, interactive TV shows and other
&amp;ldquo;image-hungry&amp;rdquo; computing environments. The groundbreaking Cell BE processor appears in products such
as Sony Computer Entertainment’s PLAYSTATION®3, Toshiba’s Cell Reference Set, a development tool for
Cell products, and already is included in the IBM BladeCenter® QS20, or “Cell Blade,” as well as through joint
collaboration with Mercury Computer Systems, Inc., targeted at aerospace and defense, semiconductor,
medical imaging, and other markets.&lt;/p&gt;
&lt;p&gt;“The College of Computing at Georgia Tech firmly believes that the Sony-Toshiba-IBM Cell BE processor
represents the future of computing using heterogeneous multi-core processors, and we are pleased to work
with three leading technology companies in a broad collaboration that will demonstrate the extreme
performance of Cell,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, Associate Professor and Executive Director of High-Performance
Computing in the College of Computing at Georgia Tech. “By supporting the growth of the industry-changing
Cell BE processor technology, the College of Computing at Georgia Tech will drive the continued
advancement of computationally-intensive applications that will directly impact the global growth of our
industry and the evolution of our society.”&lt;/p&gt;
&lt;p&gt;Directed by Bader, the new STI Cell Center of Competence at Georgia Tech has a mission to grow the
community of Cell BE users and developers by performing research and service in support of the Cell BE
processor, and further enable students at the College to grow their skills and experience around Cell BE
technology to apply in future career opportunities. The Center will sponsor discussion forums and workshops,
provide remote access to Cell blade hardware installed at Georgia Tech, create and disseminate software
optimized for Cell BE systems, and perform research on the design of Cell BE systems, algorithms, and
applications. The award from Sony-Toshiba-IBM will support the Center’s activities and research efforts in
support of broadening Cell BE’s impact into multiple sectors and industries, including scientific computing,
digital content creation, bioinformatics, finance, gaming and entertainment.&lt;/p&gt;
&lt;p&gt;“We are looking forward to seeing a paradigm shift in computing, and anticipate that our collaboration with the
College of Computing at Georgia Tech will create innovative applications for Cell processors,” said Masa
Chatani, Senior General Manager, Cell Development Center, Sony Corporation and also CTO of Sony
Computer Entertainment Inc. “We expect that it will generate tremendous value not limited to PLAYSTATION
3 but to all Cell-based computers.&amp;quot;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We look forward to seeing the Center of Competence at Georgia Tech generating outstanding technology
based on Cell BE,” said Tomotaka Saito, General Manager, Broadband System LSI Division, System LSI
Division I, Toshiba’s Semiconductor Company. &amp;ldquo;The future will see growing demand for multi-core processor
applications, and we want to see the Center playing a key role in anticipating and responding to such
demand.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;“The joint collaboration by IBM, Sony and Toshiba on the Cell processor has led to tremendous
advancements in computing applications and innovations,” said Sharon Nunes, Vice President, Business
Development and Strategic Growth Initiatives, IBM Systems &amp;amp; Technology Group. “We are pleased to be
collaborating with the College of Computing at Georgia Tech to enable a team of engineers, professors and
students to create breakthrough solutions, share information among various industries and other universities,
and further the Cell ecosystem overall.”&lt;/p&gt;
&lt;h3 id=&#34;about-the-college-of-computing-at-georgia-tech&#34;&gt;About the College of Computing at Georgia Tech&lt;/h3&gt;
&lt;p&gt;The College of Computing at Georgia Tech is a national leader in the research and creation of real-world
computing breakthroughs that drive social and scientific progress. With its graduate program ranked 11th
nationally by U.S. News and World Report, the College’s unconventional approach to education is pioneering
the new era of computing by expanding the horizons of traditional computer science students through
interdisciplinary collaboration and a focus on human centered solutions. For more information about the
College of Computing at Georgia Tech, its academic divisions and research centers, please visit
&lt;a href=&#34;https://www.cc.gatech.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cc.gatech.edu&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;IBM, BladeCenter, Power Architecture and QS20 are trademarks of IBM Corporation in the United States
and/or other countries.&lt;/p&gt;
&lt;p&gt;PLAYSTATION is a registered trademark of Sony Computer Entertainment Inc.&lt;/p&gt;
&lt;p&gt;All other company/product names and service marks may be trademarks or registered trademarks of their
respective companies.&lt;/p&gt;
&lt;p&gt;Cell Broadband Engine is a trademark of Sony Computer Entertainment Inc.
See &lt;a href=&#34;http://www.ibm.com/legal/copytrade.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ibm.com/legal/copytrade.shtml&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;contacts&#34;&gt;Contacts&lt;/h3&gt;
&lt;p&gt;For College of Computing at Georgia Tech &lt;br&gt;
Brendan Streich &lt;br&gt;
404-260-3519 &lt;br&gt;
&lt;a href=&#34;mailto:bstreich@gcigroup.com&#34;&gt;bstreich@gcigroup.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia, not Austin, gets chip center</title>
      <link>http://localhost:1313/blog/20061114-austinamericanstatesman/</link>
      <pubDate>Tue, 14 Nov 2006 17:37:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061114-austinamericanstatesman/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Bob Keefe&lt;/em&gt;&lt;br&gt;
&lt;em&gt;WEST COAST BUREAU&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Three of the biggest names in technology plan to announce today that they will start a research center at the
Georgia Institute of Technology to explore ways to expand the reach of a promising new semiconductor
design.&lt;/p&gt;
&lt;p&gt;The move would sidestep Austin, which was a contender for the center and is where the technology was
developed.&lt;/p&gt;
&lt;p&gt;Sony Corp., IBM Corp. and Toshiba Corp. compare their new Cell
microprocessor to a supercomputer on a chip that can handle some
applications 10 times as fast as traditional computer microprocessors.&lt;/p&gt;
&lt;p&gt;The technology that the companies jointly developed in Austin over
five years at a cost of $400 million is making its debut in Sony&amp;rsquo;s new
PlayStation 3 video game console. The $500 console went on sale in
Japan last week and will hit U.S. store shelves Friday.&lt;/p&gt;
&lt;p&gt;Now, the companies want to take the Cell technology much further.
With $320,000 of funding from the three Cell partners and additional
money from outside grants, researchers at Georgia Tech&amp;rsquo;s new STI
Center of Competence will explore ways to adapt the technology for
other industries, including biotech, finance and digital media creation.
The center, to be in the school&amp;rsquo;s new Christopher W. Klaus Advanced
Computing Building, will also teach students and outside companies
how to program computers and write software for the new type of
chip.&lt;/p&gt;
&lt;p&gt;In picking Atlanta&amp;rsquo;s Georgia Tech for the Center of Competence, the
Cell partners sidestepped Austin and other high-tech hubs across the
country. More than a dozen U.S. schools, including the University of Texas, were vying to land the center,
according to officials involved.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Texas universities were absolutely part of the consideration,&amp;rdquo; said Hina Shah, the Austin-based Cell
development program director at IBM. But Georgia Tech won out in the end, she said, partly because its
curriculum and areas of expertise matched up better with the interests of the three companies involved.&lt;/p&gt;
&lt;p&gt;For Georgia Tech, the center is the latest in a series of big wins and increased prominence for the College of Computing.&lt;/p&gt;
&lt;p&gt;The school has benefited from its extensive programs in high-performance computing, digital media and video game design. But since the
2002 arrival of Rich DeMillo, a former chief technical officer for Hewlett-Packard Co., as dean, the school has redesigned its curriculum to
focus less on computer science theory and more on real-world applications.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In many ways, we found them to be much more grounded about focusing on what&amp;rsquo;s needed not 10 years from now but what&amp;rsquo;s needed today
and tomorrow,&amp;rdquo; Shah said. &amp;ldquo;That made a huge difference.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Landing the center puts Georgia Tech at the forefront of a ground-breaking type of semiconductor design. &lt;strong&gt;David Bader&lt;/strong&gt;, executive director of
the school&amp;rsquo;s high-performance computing program, said he thinks that the center will be the only one of its kind in the United States.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We really see this as the future of technology and innovation,&amp;rdquo; Bader said. &amp;ldquo;This is so high-impact.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Cell chip design is in its infancy and has a lot to prove. The chip isn&amp;rsquo;t expected to make a big dent in the traditional semiconductor market
controlled by Intel Corp. and Advanced Micro Devices Inc. anytime soon. Reaching into other markets also won&amp;rsquo;t be easy.&lt;/p&gt;
&lt;p&gt;What makes Cell so promising is its potential power, especially for graphics-intensive programs such as video games, broadband Internet
video processing and other digital media applications.&lt;/p&gt;
&lt;p&gt;Just recently, Intel released its first &amp;ldquo;dual-core&amp;rdquo; and &amp;ldquo;quad core&amp;rdquo; microprocessors that essentially put two or four processors on one chip.&lt;/p&gt;
&lt;p&gt;Cell chips have already leapfrogged that capability. The chips in Sony&amp;rsquo;s PlayStation 3, for instance, essentially have nine cores: eight
sub-processors that work in connection with a central processor.&lt;/p&gt;
&lt;p&gt;Future Cell designs could have as many as 16 sub-processing cores, which could dramatically increase the speed and the number of
applications that Cell-equipped computers can handle.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This really is a new era in performance,&amp;rdquo; Jim Kahle, an IBM fellow who oversaw the chip&amp;rsquo;s design in Austin, said during the announcement of
the first Cell chips in San Francisco last year.&lt;/p&gt;
&lt;p&gt;Sony has the most riding on Cell. The Japanese giant is counting on the chip to help it regain ground in new technology development that it
lost in such areas as digital music.&lt;/p&gt;
&lt;p&gt;Sony is exploring putting Cell processors into a wide array of products, including computers, televisions and mobile phones.&lt;/p&gt;
&lt;p&gt;Toshiba plans to use Cell processors in its televisions and in other products.&lt;/p&gt;
&lt;p&gt;IBM has introduced powerful computer servers based on the design.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ga. Tech lands research facility!</title>
      <link>http://localhost:1313/blog/20061114-ajc/</link>
      <pubDate>Tue, 14 Nov 2006 17:23:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20061114-ajc/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Bob Keefe&lt;/em&gt;&lt;br&gt;
&lt;em&gt;Cox Washington Bureau&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Three of the biggest names in technology plan to announce today they will start a research center at Georgia Tech to
explore ways to expand the reach of a promising new semiconductor design.&lt;/p&gt;
&lt;p&gt;Sony Corp., IBM Corp. and Toshiba Corp. compare their new &amp;ldquo;Cell&amp;rdquo; microprocessor to a supercomputer on a chip
that can handle some applications 10 times faster than traditional computer chips.
The technology that the companies jointly developed in Austin, Texas, over five years at a cost of $400 million is
debuting in Sony&amp;rsquo;s new PlayStation3 video game console. The $500 console went on sale in Japan last week and hits
U.S. store shelves on Friday.&lt;/p&gt;
&lt;p&gt;Now, the companies want to take the Cell technology much further.
With funding from the three Cell partners and additional money from Georgia Tech and outside grants, researchers
at Tech&amp;rsquo;s new STI Center of Competence will explore ways to adapt the technology for other industries, including
biotech, finance and digital media creation.&lt;/p&gt;
&lt;p&gt;Sony, Toshiba and IBM are providing an initial investment of $320,000, while Tech is putting in $230,000 and
another $100,000 is coming from a National Science Foundation grant.&lt;/p&gt;
&lt;p&gt;At the center, to be located in the school&amp;rsquo;s new Christopher W. Klaus Advanced Computing Building, researchers
will also teach students and outside companies how to program computers and write software for the new type of
chip. There will be four faculty members involved in the project.&lt;/p&gt;
&lt;p&gt;Landing the center puts Georgia Tech at the forefront of a groundbreaking new type of semiconductor design.
&lt;strong&gt;David Bader&lt;/strong&gt;, executive director of the school&amp;rsquo;s high-performance computing program, said he believes the center will be
the only one of its kind in the United States.
&amp;ldquo;We really see this as the future of technology and innovation,&amp;rdquo; Bader said. &amp;ldquo;This is so high-impact.&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;austin-bypassed&#34;&gt;Austin bypassed&lt;/h3&gt;
&lt;p&gt;In picking Georgia Tech for the Center of Competence, the Cell partners sidestepped Austin as well as other
high-tech hubs across the country. In addition to the University of Texas, more than a dozen schools around the
country were vying to land the center, according to officials involved.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Texas universities were absolutely part of the consideration,&amp;rdquo; said Hina Shah, the Austin-based Cell development
program director at IBM. But Georgia Tech won out in the end, she said, partly because its curriculum and areas of
expertise matched up better with the interests of the three companies involved.&lt;/p&gt;
&lt;p&gt;For Georgia Tech, the center is the latest in a series of big wins and increased prominence for the College of
Computing.&lt;/p&gt;
&lt;p&gt;In part, the school benefited from its extensive programs in high-performance computing, digital media and video
game design.&lt;/p&gt;
&lt;p&gt;But since the 2002 arrival as dean of Rich DeMillo, the former chief technical officer for Hewlett-Packard Co., the
school has redesigned its curriculum to focus less on computer science theory and more on real-world applications.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In many ways, we found them to be much more grounded about focusing on what&amp;rsquo;s needed, not 10 years from now,
but what&amp;rsquo;s needed today and tomorrow,&amp;rdquo; Shah said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;That made a huge difference.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Masa Chatani, Sony&amp;rsquo;s senior general manager for Cell development, said in a statement that the &amp;ldquo;collaboration with
the College of Computing at Georgia Tech will create innovative applications for Cell processors.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Cell chip design is only in its infancy and has a lot to prove. The chip isn&amp;rsquo;t expected to make a big dent in the
traditional semiconductor market controlled by Intel Corp. and Advanced Micro Devices Inc. anytime soon.
Reaching into other markets won&amp;rsquo;t be easy either.&lt;/p&gt;
&lt;p&gt;Still, what makes Cell so promising is its potential power, especially when it comes to graphics-intensive programs
like video games, broadband Internet video processing and other digital media applications.
Just recently, Intel released its first &amp;ldquo;dual core&amp;rdquo; and &amp;ldquo;quad core&amp;rdquo; microprocessors that essentially put two or four
processors on one chip.&lt;/p&gt;
&lt;p&gt;Cell chips have already leapfrogged that capability. The chips in Sony&amp;rsquo;s PlayStation3, for instance, essentially have
nine cores —- eight unique sub-processors that work in connection with a central processor.&lt;/p&gt;
&lt;h3 id=&#34;16-cores-a-possibility&#34;&gt;16 cores a possibility&lt;/h3&gt;
&lt;p&gt;Future Cell designs could have as many as 16 sub-processing cores, which could dramatically increase the speed and
the number of applications Cell-equipped computers could handle.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;This really is a new era in performance,&amp;rdquo; Jim Kahle, an IBM fellow who oversaw the chip&amp;rsquo;s design in Austin, said in
announcing the first Cell chips in San Francisco last year.&lt;/p&gt;
&lt;p&gt;Sony has the most riding on Cell. The Japanese giant is counting on the chip to help it regain ground in new
technology development that it lost in areas like digital music.&lt;/p&gt;
&lt;p&gt;Along with its video game machines, Sony is exploring putting Cell processors into a wide array of products,
including personal computers, televisions and mobile phones.&lt;/p&gt;
&lt;p&gt;Toshiba plans to use Cell processors in its TV sets and in other products.&lt;/p&gt;
&lt;p&gt;IBM already has introduced powerful computer servers based on the design.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;AJC-20061114.pdf&#34;&gt;Atlanta Journal Constitution, 14 November 2006&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Gives Keynote On Petascale Computing</title>
      <link>http://localhost:1313/blog/20060916-hpcc/</link>
      <pubDate>Sat, 16 Sep 2006 21:02:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060916-hpcc/</guid>
      <description>&lt;p&gt;College of Computing Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt; gave an invited keynote on “Petascale Computing for Large-Scale Graph Problems” at the second international conference on High Performance Computing and Communications (HPCC ‘06) in Munich, Germany.&lt;/p&gt;
&lt;p&gt;With the rapid growth in computing and communication technology, the past decade has witnessed a proliferation of powerful parallel and distributed systems, and an ever-increasing demand for practice of high performance computing and communication (HPCC). HPCC has moved into the mainstream of computing and become a key technology in determining future research and development activities in many academic and industrial branches, especially when the solution of large and complex problems must cope with very tight timing schedules.&lt;/p&gt;
&lt;p&gt;In his keynote, Bader discusses several graph theoretic kernels for connectivity and centrality and how the features of petascale architectures will affect algorithm development, ease of programming, performance, and scalability. Graph theoretic problems are representative of fundamental kernels in traditional and emerging computational sciences such as chemistry, biology, and medicine, as well as applications in national security. However, they pose serious challenges for parallel machines due to non-contiguous, concurrent accesses to global data structures with low degrees of locality. Few parallel graph algorithms outperform their best sequential implementation due to long memory latencies and high synchronization costs.&lt;/p&gt;
&lt;p&gt;The HPCC conference series provides a forum for engineers and scientists in academia, industry, and government to address all resulting profound challenges, and to present and discuss their new ideas, research results, applications, and experience on all aspects of high performance computing and communication.&lt;/p&gt;
&lt;p&gt;In addition to three keynotes, the HPCC &amp;lsquo;06 conference held on September 13-15, included 95 peer-reviewed papers from 328 submissions, including papers from Europe, Asia and the Pacific, as well as North and South America.  HPCC is emerging as the premier academic high-performance computing conference based in Europe. Last year’s meeting was held in Sorrento (Naples), Italy.&lt;/p&gt;
&lt;p&gt;For more information about HPCC ’06, &lt;a href=&#34;https://web.archive.org/web/20080202055550/http://hpcc06.lrr.in.tum.de/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202055550/http://www.cc.gatech.edu/news/bader-gives-keynote-on-petascale-computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/bader-gives-keynote-on-petascale-computing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives 2006 IBM Faculty Award</title>
      <link>http://localhost:1313/blog/20060907-ibm-faculty-award/</link>
      <pubDate>Thu, 07 Sep 2006 23:21:52 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060907-ibm-faculty-award/</guid>
      <description>&lt;p&gt;Congratulations to Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt; who recently received a 2006 &lt;a href=&#34;https://www.ibm.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBM&lt;/a&gt; Faculty Award in recognition of his outstanding achievement and importance to industry.  The highly competitive award, valued at $40,000, was given to Bader for making fundamental contributions to the design and optimization of parallel scientific libraries for multicore processors, such as the IBM Cell. As an international leader in innovation for the most advanced computing systems, IBM recognizes the strength of collaborative research with the College of Computing at Georgia Tech’s Computational Science and Engineering (CSE) division.&lt;/p&gt;
&lt;p&gt;Bader is part of the College&amp;rsquo;s growing CSE division, established in 2005 to strengthen and better reflect the critical role that computation plays in the science and engineering disciplines at Georgia Tech and in the broader technology community. Along with theory and experimentation, computation has gained widespread acceptance as a key component in the advancement of knowledge and practice. As a division of the College of Computing, CSE supports multidisciplinary research and education in computer science that solves grand challenges in science and engineering. CSE is designed to innovate and create new expertise, technologies, and practitioners.&lt;/p&gt;
&lt;p&gt;IBM does not accept unsolicited requests or proposals for Faculty Awards. Candidates must be nominated by an IBM employee with common interests who will serve as a liaison for the collaboration. Awardees may be nominated for an award renewal, and renewal nominations engage in the same competition as first-time nominations.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202055555/http://www.cc.gatech.edu/news/bader-receives-2006-ibm-faculty-award&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/bader-receives-2006-ibm-faculty-award&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kamesh Madduri Wins Prestigious NASA Fellowship</title>
      <link>http://localhost:1313/blog/20060818-nasa/</link>
      <pubDate>Fri, 18 Aug 2006 21:37:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060818-nasa/</guid>
      <description>&lt;p&gt;College of Computing Ph.D. student Kamesh Madduri has received a NASA Graduate Student Researchers Program (GSRP) Fellowship for his proposal titled, “Performance Analysis and Optimization of NASA Scientific Applications on the NAS Supercomputers.”  The National Aeronautics and Space Administration began the competitive program in 1980 to support 100 promising students each year who are pursuing advanced degrees in science and engineering, and to cultivate research ties to the academic community.&lt;/p&gt;
&lt;p&gt;Madduri was selected based on an extremely competitive evaluation of his academic qualifications, his proposed research plan, and his planned use of NASA research facilities. GSRP fellowships, for up to $22,000, are awarded for one year and are renewable based on satisfactory progress for a total of three years. Madduri is doing his research under the supervision of Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060624/http://www.cc.gatech.edu/news/kamesh-madduri-wins-prestigious-nasa-fellowship&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/kamesh-madduri-wins-prestigious-nasa-fellowship&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumnus Bader Joins DSPlogic Advisory Board</title>
      <link>http://localhost:1313/blog/20060802-maryland-dsplogic/</link>
      <pubDate>Wed, 02 Aug 2006 17:14:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060802-maryland-dsplogic/</guid>
      <description>

















&lt;figure  id=&#34;figure-david-bader&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;David Bader&#34; srcset=&#34;
               /blog/20060802-maryland-dsplogic/article1814.large_hu_9ce69eed33723403.webp 400w,
               /blog/20060802-maryland-dsplogic/article1814.large_hu_29e5612354979e48.webp 760w,
               /blog/20060802-maryland-dsplogic/article1814.large_hu_92fd9e8c7792e1f3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20060802-maryland-dsplogic/article1814.large_hu_9ce69eed33723403.webp&#34;
               width=&#34;120&#34;
               height=&#34;165&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      David Bader
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Ph.D. alumnus &lt;a href=&#34;http://www.cs.njit.edu/~bader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/a&gt; &amp;lsquo;96, Associate Professor of Computational Science and Engineering at Georgia Tech, has joined the Technical Advisory Board of DSPlogic, a provider of FPGA-based, reconfigurable computing and signal processing products and services.
Bader has been a pioneer in the field of high performance computing for problems in bioinformatics and computational genomics, and has co-authored over 75 articles in peer-reviewed journals and conferences. His main areas of research are in parallel algorithms, combinatorial optimization, and computational biology and genomics.&lt;/p&gt;
&lt;p&gt;As a graduate student, Dr. Bader was advised by Professor &lt;a href=&#34;http://www.ece.umd.edu/meet/faculty/jaja.php3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Joseph JaJa&lt;/a&gt; (&lt;a href=&#34;http://www.ece.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECE&lt;/a&gt;&lt;a href=&#34;http://www.umiacs.umd.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;/UMIACS&lt;/a&gt;). Bader founded and served as president of the Electrical and Computer Engineering Graduate Student Association (&lt;a href=&#34;http://www.ece.umd.edu/ecegsa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ECEGSA&lt;/a&gt;) while he attended school at Maryland. Prior to his recent hiring at Georgia Tech, Bader served as an Associate Professor of ECE and Computer Science at the University of New Mexico. He received a National Science Foundation (NSF) &lt;a href=&#34;http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=5262&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Faculty Early Career Development (CAREER) Award&lt;/a&gt; for his research on High-Performance Algorithms for Scientific Applications last fall.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/alumnus-bader-joins-dsplogic-advisory-board&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/alumnus-bader-joins-dsplogic-advisory-board&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preparing Researchers To Use Petascale Computation</title>
      <link>http://localhost:1313/blog/20060730-petascale/</link>
      <pubDate>Sun, 30 Jul 2006 21:39:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060730-petascale/</guid>
      <description>&lt;p&gt;This year, the National Science Foundation (NSF) will award the acquisition of a national supercomputer for production use by 2011. The supercomputer will achieve petascale computation which is a rate several orders of magnitude more powerful than the fastest supercomputers available today. Although breakthroughs across science and engineering are anticipated with this incredible resource, few researchers are prepared to use this massive computing capability.&lt;/p&gt;
&lt;p&gt;An NSF-sponsored workshop co-organized by College of Computing Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt;, Allan Snavely (UC San Diego), and Gwen Jacobs (Montana State) will be held August 29-30, 2006 in Arlington, Virginia to identify key challenges in the biological sciences which may lead to early breakthroughs on petascale supercomputers. The objectives of this workshop are to examine the opportunities for progress in the biological sciences that could be enabled by petascale computational capability, and to determine the steps necessary to ensure the community is prepared to take advantage of such resources.&lt;/p&gt;
&lt;p&gt;Unique challenges face the biological sciences community to make efficient use of national cyberinfrastructure. For example, traditional uses of high-performance computing (HPC) systems in physics and engineering involve problems that often have well defined and regular structures. In contrast, many problems in biology are irregular in structure, are significantly more challenging for software engineers to parallelize, and often involve integer-based abstract data structures.&lt;/p&gt;
&lt;p&gt;The first goal of the workshop is to increase the dialogue between computational biologists and computer scientists to examine the performance of existing algorithms, identify critical/bottleneck sections, scalability, and discuss how and why these codes will perform differently on various architectures. The second goal of the workshop is to identify some early collaborations and pilot studies whereby biologists and computer scientists can work together on problems relevant to petascale deployment.&lt;/p&gt;
&lt;p&gt;The workshop’s outcome will be greater insight into the factors affecting the performance of strategic codes to model genomes, proteins, networks, organs, organisms and populations, and interactions between them and their environments. Biologists are using supercomputers to explore scientific problems of national strategic interest impacting health and well-being by understanding how we age, seeking cures for diseases, understanding the genomes of human and all animal and plant species and their evolutionary histories, developing agricultural products, understanding the impact of human activities on biodiversity, and the like.&lt;/p&gt;
&lt;p&gt;Bader’s help with this interdisciplinary workshop will facilitate an unprecedented exploration of these problems at greater scale and accuracy. Collaborations between biologists and computer scientists thus have the potential to improve U.S. economic competitiveness, national and world health and well being, and keep the U.S. pre-eminent in this important domain of science.&lt;/p&gt;
&lt;p&gt;For more information about Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt;, &lt;a href=&#34;https://web.archive.org/web/20080202060831/http://www.cc.gatech.edu/component/option,com_peopledb/task,view/contact_id,284451105/Itemid,238/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060831/http://www.cc.gatech.edu/news/preparing-researchers-to-use-petascale-computation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/preparing-researchers-to-use-petascale-computation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People and Positions: HPC Expert Joins DSPlogic&#39;s Technical Advisory Board</title>
      <link>http://localhost:1313/blog/20060728-hpcwire/</link>
      <pubDate>Fri, 28 Jul 2006 07:33:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060728-hpcwire/</guid>
      <description>&lt;p&gt;Dr. &lt;strong&gt;David A. Bader&lt;/strong&gt;, a Georgia Tech faculty member and international leader in high performance computing,
has joined DSPlogic&amp;rsquo;s Technical Advisory Board.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At DSPlogic we increase our customers&amp;rsquo; productivity by improving the software and tools needed for high
performance reconfigurable computing solutions,&amp;rdquo; said Michael Babst, president of DSPlogic. &amp;ldquo;We expect
growth in our core business with the addition of one of the world&amp;rsquo;s foremost parallel computing experts, Dr.
David A. Bader, to our advisory board.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Computer systems are increasingly relying upon hardware accelerators for continued performance gains.
Productivity tools, such as DSPlogic&amp;rsquo;s Reconfigurable Computing Toolbox with seamless Matlab integration,
are essential for rapidly designing complex high performance applications,&amp;rdquo; said Dr. Bader. &amp;ldquo;DSPlogic is a
leading provider of these tools and understands the importance of this technology for making advances in
health care, national security and oil exploration.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;David A. Bader is an Associate Professor in Computational Science and Engineering, a division within the
College of Computing, Georgia Institute of Technology. He received his Ph.D. in 1996 from The University
of Maryland, was awarded a National Science Foundation (NSF) Postdoctoral Research Associateship in
Experimental Computer Science. He is an NSF CAREER Award recipient, an investigator on several NSF
awards, and has been supported by the DARPA High Productivity Computing Systems program, DOE, and
NASA. He has collaborated closely with the leading high performance computer vendors including IBM,
Cray, Sun, and Intel.&lt;/p&gt;
&lt;p&gt;Dr. Bader serves on the Steering Committees of the IPDPS and HiPC conferences, and was the General
co-Chair for IPDPS (2004&amp;ndash;2005), and Vice General Chair for HiPC (2002&amp;ndash;2004). David has chaired several
major conference program committees: Program Chair for HiPC 2005, Program Vice-Chair for IPDPS 2006
and Program Vice-Chair for ICPP 2006. He has served on numerous conference program committees related
to parallel processing and computational science &amp;amp; engineering, is an associate editor for several high impact
publications including the IEEE Transactions on Parallel and Distributed Systems (TPDS), the ACM Journal
of Experimental Algorithmics (JEA), IEEE DSOnline, and Parallel Computing, is a Senior Member of the
IEEE Computer Society and a Member of the ACM.&lt;/p&gt;
&lt;p&gt;Dr. Bader has been a pioneer the field of high performance computing for problems in bioinformatics and
computational genomics. He has co-chaired a series of meetings, the IEEE International Workshop on
High-Performance Computational Biology (HiCOMB), written several book chapters, and co-edited special
issues of the Journal of Parallel and Distributed Computing (JPDC) and IEEE TPDS on high performance
computational biology. He has co-authored over 75 articles in peer-reviewed journals and conferences, and
his main areas of research are in parallel algorithms, combinatorial optimization, and computational biology and genomics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>High-Performance Computing Expert, Dr. David A. Bader, joins DSPlogic&#39;s Technical Advisory Board</title>
      <link>http://localhost:1313/blog/20060713-dsplogic/</link>
      <pubDate>Thu, 13 Jul 2006 08:22:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060713-dsplogic/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, a Georgia Tech faculty member and international leader in high-performance computing, has joined DSPlogic&amp;rsquo;s Technical Advisory Board.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;At DSPlogic we increase our customers&amp;rsquo; productivity by improving the software and tools needed for high-performance reconfigurable computing solutions,&amp;rdquo; said Michael Babst, president of DSPlogic. &amp;ldquo;We expect growth in our core business with the addition of one of the world&amp;rsquo;s foremost parallel computing experts, Dr. David A. Bader, to our advisory board.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Computer systems are increasingly relying upon hardware accelerators for continued performance gains. Productivity tools, such as DSPlogic&amp;rsquo;s Reconfigurable Computing Toolbox with seamless Matlab integration, are essential for rapidly designing complex high-performance applications,&amp;rdquo; said Dr. Bader. &amp;ldquo;DSPlogic is a leading provider of these tools and understands the importance of this technology for making advances in health care, national security and oil exploration.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;David A. Bader is an Associate Professor in Computational Science and Engineering, a division within the College of Computing, Georgia Institute of Technology. He received his Ph.D. in 1996 from The University of Maryland, was awarded a National Science Foundation (NSF) Postdoctoral Research Associateship in Experimental Computer Science. He is an NSF CAREER Award recipient, an investigator on several NSF awards, and has been supported by the DARPA High Productivity Computing Systems program, DOE, and NASA. He has collaborated closely with the leading high-performance computer vendors including IBM, Cray, Sun, and Intel.&lt;/p&gt;
&lt;p&gt;Dr. Bader serves on the Steering Committees of the IPDPS and HiPC conferences, and was the General co-Chair for IPDPS (2004&amp;ndash;2005), and Vice General Chair for HiPC (2002&amp;ndash;2004). David has chaired several major conference program committees: Program Chair for HiPC 2005, Program Vice-Chair for IPDPS 2006 and Program Vice-Chair for ICPP 2006. He has served on numerous conference program committees related to parallel processing and computational science &amp;amp; engineering, is an associate editor for several high impact publications including the IEEE Transactions on Parallel and Distributed Systems (TPDS), the ACM Journal of Experimental Algorithmics (JEA), IEEE DSOnline, and Parallel Computing, is a Senior Member of the IEEE Computer Society and a Member of the ACM.&lt;/p&gt;
&lt;p&gt;Dr. Bader has been a pioneer the field of high-performance computing for problems in bioinformatics and computational genomics. He has co-chaired a series of meetings, the IEEE International Workshop on High-Performance Computational Biology (HiCOMB), written several book chapters, and co-edited special issues of the Journal of Parallel and Distributed Computing (JPDC) and IEEE TPDS on high-performance computational biology. He has co-authored over 75 articles in peer-reviewed journals and conferences, and his main areas of research are in parallel algorithms, combinatorial optimization, and computational biology and genomics.&lt;/p&gt;
&lt;h3 id=&#34;about-dsplogic-inc&#34;&gt;About DSPlogic, Inc.&lt;/h3&gt;
&lt;p&gt;DSPlogic is a leading provider of high-quality, FPGA-based, reconfigurable computing and signal processing products and services. DSPlogic&amp;rsquo;s set of software-to-FPGA design tools helps customers rapidly develop verified algorithm implementations using today&amp;rsquo;s leading reconfigurable computing platforms. DSPlogic serves organizations ranging from small start-ups, to Fortune 500 companies, to U.S. government agencies. Go to &lt;a href=&#34;https://www.dsplogic.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.dsplogic.com&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20061018182034/http://www.dsplogic.com/home/news/20060713_drbader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.archive.org/web/20061018182034/http://www.dsplogic.com/home/news/20060713_drbader&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CERCS Researchers Receive Industry Awards</title>
      <link>http://localhost:1313/blog/20060701-ibm-sur/</link>
      <pubDate>Sat, 01 Jul 2006 22:21:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060701-ibm-sur/</guid>
      <description>&lt;p&gt;Research faculty from the Center for Experimental Research in Computer Systems (CERCS) at Georgia Tech recently received several new industry awards. CERCS, located within the College of Computing, is one of the largest experimental systems programs in the U.S. focusing on complex hardware, communications and system-level software, and applications that lead the innovation of new information and computing technologies.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prof. Ling Liu for receiving and IBM Faculty Award for her work on &amp;ldquo;Building Secure Publish-subscribe Systems for Large Scale Event Dissemination&amp;rdquo;&lt;/li&gt;
&lt;li&gt;CERCS Research Scientist Ada Gavrilovska received funding from Intel Corporation to support her research on application-specific processing on IXA routers.&lt;/li&gt;
&lt;li&gt;College of Computing Associate Professor Santosh Pande received funding from Intel Corporation to support his research on new compilation methods for the micro-cores used in machines like the IXA routers.&lt;/li&gt;
&lt;li&gt;Professor and CERCS Director Karsten Schwan, and CERCS Research Scientists Ada Gavrilovska and Matt Wolf received an award from Cisco Corporation&amp;rsquo;s University Research Program to support their proposal tiled &amp;ldquo;High Performance Dynamic Communications&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Professors Karsten Schwan and Calton Pu, along with Associate Professors &lt;strong&gt;David Bader&lt;/strong&gt;, Santosh Pande and Irfan Essa will receive an IBM SUR award to support their proposal titled &amp;ldquo;Optimizing Scientific Libraries for IBM Cell&amp;rdquo; for two Cell blades.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, CERCS celebrated College of Computing Ph.D. student Zhongtang Cai for his &amp;ldquo;Best Paper Award&amp;rdquo; at last week&amp;rsquo;s 2006 High-Performance Distributed Computing (HPDC) conference in Paris, France. Cai&amp;rsquo;s research is on new bandwidth-sensitive methods for dynamic traffic division and scheduling across multiple overlay paths.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cercs.gatech.edu/news/060701.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cercs.gatech.edu/news/060701.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creative Young Engineers Selected to Participate in NAE&#39;s 2007 U.S. Frontiers of Engineering Symposium</title>
      <link>http://localhost:1313/blog/20060618-nae/</link>
      <pubDate>Sun, 18 Jun 2006 21:43:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060618-nae/</guid>
      <description>&lt;p&gt;WASHINGTON — Eighty-three of the nation&amp;rsquo;s brightest young engineers have been selected to take part in the National
Academy of Engineering&amp;rsquo;s (NAE) 13th annual U.S. Frontiers of Engineering symposium. The 2½-day event will bring together
engineers ages 30 to 45 who are performing exceptional engineering research and technical work in a variety of disciplines.
The participants — from industry, academia, and government — were nominated by fellow engineers or organizations and
chosen from more than 260 applicants.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It is exciting to witness the energy when outstanding engineers from many different fields come together in this unique
venue,&amp;rdquo; said NAE President Wm. A. Wulf. &amp;ldquo;Frontiers of Engineering is a proven mechanism for traversing engineering
disciplines. By exposing bright young minds to developments in areas other than their own — and giving them lots of time to
interact — Frontiers enables advances in approaches and thinking that would not have occurred otherwise.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The symposium will be held Sept. 24-26 at Microsoft Research in Redmond, Wash., and will examine trustworthy computer
systems, safe water technologies, modeling and simulating human behavior, biotechnology for fuels and chemicals, and the
control of protein conformations. Dr. Henrique Malvar, Microsoft Distinguished Engineer and managing director, Microsoft
Research, will be a featured speaker. His research at Microsoft has focused on audio and video signal enhancement and
compression, multirate signal processing, and signal decompositions. Prior to joining Microsoft Research, Malvar headed
research and advanced technology at PictureTel and the Digital Signal Processing Research Group at Universidade de
Brasília.&lt;/p&gt;
&lt;p&gt;The following engineers were selected as general participants:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dongchan Ahn, Dow Corning Corp.&lt;/li&gt;
&lt;li&gt;Vitaly Aizenberg, Exxon Mobil Corp.&lt;/li&gt;
&lt;li&gt;Matthew Andrews, Alcatel-Lucent, Bell Labs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Georgia Institute of Technology&lt;/li&gt;
&lt;li&gt;Randy A. Bartels, Colorado State University&lt;/li&gt;
&lt;li&gt;Jennifer T. Bernhard, University of Illinois, Urbana-Champaign&lt;/li&gt;
&lt;li&gt;Joshua Binder, Boeing Co.&lt;/li&gt;
&lt;li&gt;Markus J. Buehler, Massachusetts Institute of Technology&lt;/li&gt;
&lt;li&gt;Wai Kin Chan, Rensselaer Polytechnic Institute&lt;/li&gt;
&lt;li&gt;Eric Pei-Yu Chiou, University of California, Los Angeles&lt;/li&gt;
&lt;li&gt;Alfred J. Crosby, University of Massachusetts, Amherst&lt;/li&gt;
&lt;li&gt;Rula A. Deeb, Malcolm Pirnie Inc.&lt;/li&gt;
&lt;li&gt;Shrikant Dhodapkar, Dow Chemical Co.&lt;/li&gt;
&lt;li&gt;Peter A. Dinda, Northwestern University&lt;/li&gt;
&lt;li&gt;Michael J. Fasolka, National Institute of Standards and Technology&lt;/li&gt;
&lt;li&gt;Andrew Fernandez, Agilent Technologies Inc.&lt;/li&gt;
&lt;li&gt;Timothy Fisher, Purdue University&lt;/li&gt;
&lt;li&gt;Raja N. Ghanem, Medtronic Inc.&lt;/li&gt;
&lt;li&gt;Reza Ghodssi, University of Maryland&lt;/li&gt;
&lt;li&gt;Rajat S. Ghosh, Alcoa Inc.&lt;/li&gt;
&lt;li&gt;Anouck Girard, University of Michigan, Ann Arbor&lt;/li&gt;
&lt;li&gt;Samuel Graham, Georgia Institute of Technology&lt;/li&gt;
&lt;li&gt;Tyrone W. Grandison, IBM Almaden Research Center&lt;/li&gt;
&lt;li&gt;Johney Green, Oak Ridge National Laboratory&lt;/li&gt;
&lt;li&gt;Michelle L. Gregory, Pacific Northwest National Laboratory&lt;/li&gt;
&lt;li&gt;John Hecht, Procter &amp;amp; Gamble Co.&lt;/li&gt;
&lt;li&gt;Hugh W. Hillhouse, Purdue University&lt;/li&gt;
&lt;li&gt;Jeffrey Kanel, Eastman Chemical Co.&lt;/li&gt;
&lt;li&gt;Jeffrey M. Karp, Harvard University&lt;/li&gt;
&lt;li&gt;Theodore J. Kim, Sandia National Laboratories&lt;/li&gt;
&lt;li&gt;Fred A. Kish, Infinera Corp.&lt;/li&gt;
&lt;li&gt;Efrosini Kokkoli, University of Minnesota, Minneapolis&lt;/li&gt;
&lt;li&gt;Ilya Kolmanovsky, Ford Research and Advanced Engineering&lt;/li&gt;
&lt;li&gt;Michael R. Krames, Philips Lumileds Lighting Co.&lt;/li&gt;
&lt;li&gt;Raj Krishnaswamy, Metabolix Inc.&lt;/li&gt;
&lt;li&gt;Rajesh Kumar, Intel Corp.&lt;/li&gt;
&lt;li&gt;Sanjay Lall, Stanford University&lt;/li&gt;
&lt;li&gt;Eugene J. LeBoeuf, Vanderbilt University&lt;/li&gt;
&lt;li&gt;K. Rustan Leino, Microsoft Research&lt;/li&gt;
&lt;li&gt;Philip Levis, Stanford University&lt;/li&gt;
&lt;li&gt;Mark E. Lewis, Cornell University&lt;/li&gt;
&lt;li&gt;Ju Li, Ohio State University&lt;/li&gt;
&lt;li&gt;Jennifer Lukes, University of Pennsylvania&lt;/li&gt;
&lt;li&gt;Yiorgos Makris, Yale University&lt;/li&gt;
&lt;li&gt;Ajay P. Malshe, University of Arkansas&lt;/li&gt;
&lt;li&gt;Michele Marcolongo, Drexel University&lt;/li&gt;
&lt;li&gt;Wade Martinson, Cargill Inc.&lt;/li&gt;
&lt;li&gt;Katherine McMahon, University of Wisconsin, Madison&lt;/li&gt;
&lt;li&gt;Swarup Medasani, HRL Laboratories LLC&lt;/li&gt;
&lt;li&gt;Edward S. Miller, DuPont Co.&lt;/li&gt;
&lt;li&gt;Wilbur L. Myrick, SAIC&lt;/li&gt;
&lt;li&gt;Priya Narasimhan, Carnegie Mellon University&lt;/li&gt;
&lt;li&gt;Ravi Narasimhan, University of California, Santa Cruz&lt;/li&gt;
&lt;li&gt;Roseanna M. Neupauer, University of Colorado, Boulder&lt;/li&gt;
&lt;li&gt;Sissy Nikolaou, Mueser Rutledge Consulting Engineers&lt;/li&gt;
&lt;li&gt;Burak Ozdoganlar, Carnegie Mellon University&lt;/li&gt;
&lt;li&gt;Jianbiao Pan, California Polytech State University&lt;/li&gt;
&lt;li&gt;Alexander Parkhomovsky, Seagate Technology LLC&lt;/li&gt;
&lt;li&gt;Arshan Poursohi, Sun Microsystems Inc.&lt;/li&gt;
&lt;li&gt;Lisa Purvis, Xerox Corp.&lt;/li&gt;
&lt;li&gt;Marcus Quigley, Geosyntec Consultants&lt;/li&gt;
&lt;li&gt;R. Michael Raab, Agrivida Inc.&lt;/li&gt;
&lt;li&gt;Lee Rosen, Praxair Inc.&lt;/li&gt;
&lt;li&gt;Rachel A. Segalman, University of California, Berkeley&lt;/li&gt;
&lt;li&gt;Deborah Shands, Aerospace Corp.&lt;/li&gt;
&lt;li&gt;Samuel Sia, Columbia University&lt;/li&gt;
&lt;li&gt;John E. Smee, QUALCOMM Inc.&lt;/li&gt;
&lt;li&gt;Lydia L. Sohn, University of California, Berkeley&lt;/li&gt;
&lt;li&gt;Randy Stiles, Lockheed Martin Space Systems Co.&lt;/li&gt;
&lt;li&gt;Michael S. Strano, University of Illinois, Urbana-Champaign&lt;/li&gt;
&lt;li&gt;Joseph Szczerba, General Motors Corp.&lt;/li&gt;
&lt;li&gt;David N. Thompson, Idaho National Laboratory&lt;/li&gt;
&lt;li&gt;Jean W. Tom, Bristol-Myers Squibb Co.&lt;/li&gt;
&lt;li&gt;Sundeep N. Vani, Archer Daniels Midland Co.&lt;/li&gt;
&lt;li&gt;James Scott Vartuli, GE Global Research&lt;/li&gt;
&lt;li&gt;S. Travis Waller, University of Texas, Austin&lt;/li&gt;
&lt;li&gt;Helen J. Wang, Microsoft Research&lt;/li&gt;
&lt;li&gt;Annemarie Ott Weist, Air Products and Chemicals Inc.&lt;/li&gt;
&lt;li&gt;Mei Wen, Arkema Inc.&lt;/li&gt;
&lt;li&gt;Joan Wills, Cummins Inc.&lt;/li&gt;
&lt;li&gt;Lifang Yuan, EDO Corp.&lt;/li&gt;
&lt;li&gt;Randy Zachery, U.S. Army Research Office&lt;/li&gt;
&lt;li&gt;Xin Zhang, Boston University&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Speakers at this year&amp;rsquo;s event are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Carina Maria Alles, DuPont Co.&lt;/li&gt;
&lt;li&gt;Jess Brown, Carollo Engineers, P.C.&lt;/li&gt;
&lt;li&gt;Amy Childress, University of Nevado, Reno&lt;/li&gt;
&lt;li&gt;Bruce Dien, National Center for Agricultural Utilization Research&lt;/li&gt;
&lt;li&gt;Edward W. Felton, Princeton University&lt;/li&gt;
&lt;li&gt;Kevin Gluck, Air Force Research Laboratory&lt;/li&gt;
&lt;li&gt;Laurent Itti, University of Southern California&lt;/li&gt;
&lt;li&gt;Matthew J. Lang, Massachusetts Institute of Technology&lt;/li&gt;
&lt;li&gt;Karl G. Linden, Duke University&lt;/li&gt;
&lt;li&gt;Sanjay Malhotra, National Cancer Institute&lt;/li&gt;
&lt;li&gt;Greg Morrisett, Harvard University&lt;/li&gt;
&lt;li&gt;Rama Ranganathan, University of Texas Southwestern Medical School&lt;/li&gt;
&lt;li&gt;Diana K. Smetters, PARC&lt;/li&gt;
&lt;li&gt;Vanessa L. Speight, Malcolm Pirnie Inc.&lt;/li&gt;
&lt;li&gt;Michael van Lent, University of Southern California&lt;/li&gt;
&lt;li&gt;Rebeccca N. Wright, Stevens Institute of Technology&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The organizers of the 2007 symposium are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Julia M. Phillips (chair), Sandia National Laboratories&lt;/li&gt;
&lt;li&gt;Ana I. Antón, North Carolina State University&lt;/li&gt;
&lt;li&gt;John Dunagan, Microsoft Research&lt;/li&gt;
&lt;li&gt;Richard T. Elander, National Renewable Energy Laboratory&lt;/li&gt;
&lt;li&gt;Christian Lebiere, Carnegie Mellon University&lt;/li&gt;
&lt;li&gt;Donald J. Leo, Defense Advanced Research Projects Agency&lt;/li&gt;
&lt;li&gt;Carol R. Rego, CDM&lt;/li&gt;
&lt;li&gt;Vijay Singh, University of Illinois, Urbana-Champaign&lt;/li&gt;
&lt;li&gt;Paul K. Westerhoff, Arizona State University&lt;/li&gt;
&lt;li&gt;Robert Wray, Soar Technology Inc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sponsors for the 2007 U.S. Frontiers of Engineering are the Air Force Office of Scientific Research, the U.S. Department of
Defense (DDR&amp;amp;E-Research), DARPA, the National Science Foundation, Microsoft Corp., Cummins Inc., and numerous
individual donors.&lt;/p&gt;
&lt;p&gt;The National Academy of Engineering is an independent, nonprofit institution that serves as an adviser to government and the
public on issues in engineering and technology. Its members consist of the nation&amp;rsquo;s premier engineers, who are elected by
their peers for their distinguished achievements. Established in 1964, NAE operates under the congressional charter granted
to the National Academy of Sciences in 1863.&lt;/p&gt;
&lt;p&gt;A meeting program and more information about Frontiers of Engineering is available at &lt;a href=&#34;http://www.nae.edu/frontiers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.nae.edu/frontiers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nae.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Academy of Engineering&lt;/a&gt;&lt;br&gt;
Date: June 18, 2006&lt;br&gt;
Contacts: Janet Hunziker, NAE Senior Program Officer&lt;br&gt;
202-334-1571; e-mail &lt;a href=&#34;mailto:jhunziker@nae.edu&#34;&gt;jhunziker@nae.edu&lt;/a&gt;&lt;br&gt;
Randy Atkins, NAE Senior Media Relations Officer&lt;br&gt;
202-334-1508; e-mail &lt;a href=&#34;mailto:atkins@nae.edu&#34;&gt;atkins@nae.edu&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CSE Faculty-Student Research Accepted at ICPP 2006</title>
      <link>http://localhost:1313/blog/20060508-icpp/</link>
      <pubDate>Mon, 08 May 2006 20:51:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060508-icpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, associate professor within the College&amp;rsquo;s Computational Science and Engingeering (CSE) division, along with Ph.D. students Kamesh Madduri and Vaddadi Chandu, have three papers accepted at this year&amp;rsquo;s 35th International Conference on Parallel Processing (ICPP). ICPP is the longest-running conference dedicated to parallel processing with a significant impact within the field, and will be hosted by Ohio State University on August 14-18, 2006. The papers include:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Designing Multithreaded Algorithms for Breadth-First Search and st-connectivity on the Cray MTA-2,&amp;rdquo; &lt;strong&gt;D.A. Bader&lt;/strong&gt; and K. Madduri&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Parallel Algorithms for Evaluating Centrality Indices in Real-world Networks,&amp;rdquo; &lt;strong&gt;D.A. Bader&lt;/strong&gt; and K. Madduri.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;ExactMP: An Efficient Parallel Exact Solver for Phylogenetic Tree Reconstruction Using Maximum Parsimony,&amp;rdquo; &lt;strong&gt;D.A. Bader&lt;/strong&gt;, V. Chandu, and M. Yan&lt;/p&gt;
&lt;p&gt;The ExactMP paper by Bader, Chandu, and Yan, designs and implements an exact solver for the problem of maximum parsimony in computing evolutionary histories and important computational biology application. ExactMP can solve moderate sized instances exactly using combinatorial optimization techniques on symmetric multiprocessor and multicore systems with large main memories. This implementation is the first parallel solver for this problem and outperforms the widely-used commercial solver. The other two papers by Bader and Madduri design and implement parallel algorithms for large-scale graph theoretic problems. For instance, the two researchers identify key vertices using the betweenness centrality metric on real-world graphs, from small-world networks, patent databases, and citation networks. The results include the first parallel algorithms designed for several important metrics.&lt;/p&gt;
&lt;p&gt;For more information about the International Conference on Parallel Processing (ICPP), &lt;a href=&#34;https://web.archive.org/web/20080509145841/http://www.cse.ohio-state.edu/~icpp2006/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060208/http://www.cc.gatech.edu/news/cse-faculty-student-research-accepted-at-icpp-2006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/cse-faculty-student-research-accepted-at-icpp-2006&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Organizes Premier International Conference</title>
      <link>http://localhost:1313/blog/20060501-ipdps/</link>
      <pubDate>Mon, 01 May 2006 20:57:08 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060501-ipdps/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;, associate professor within the College of Computing&amp;rsquo;s Computational Science and Engineering (CSE) division, was co-organizer and steering committee member of the International Parallel and Distributed Processing Symposium (IPDPS) 2006, held in Rhodes, Greece on April 25-29.&lt;/p&gt;
&lt;p&gt;IPDPS celebrated its 20th year and is considered the premier academic conference in the areas of parallel and distributed computing. Not only did David serve as a program vice-chair for the Applications Track, he also chairs the Institute of Electrical and Electronics Engineers&amp;rsquo; (IEEE) Technical Committee on Parallel Processing, which sponsors IPDPS. This year&amp;rsquo;s symposium had over 550 attendees and included 125 peer-reviewed papers, four keynotes and over 20 workshops. Being highly regarded by the professional community as the annual meeting for top research results in the field, IPDPS 2006 had strong international participation with approximately equal attendance from the U.S., Europe and Asia.&lt;/p&gt;
&lt;p&gt;For more information about IPDPS 2006, &lt;a href=&#34;https://web.archive.org/web/20080214154346/http://ipdps.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;click here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20080202060248/http://www.cc.gatech.edu/news/david-bader-organizes-premier-international-conference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/news/david-bader-organizes-premier-international-conference&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Georgia Tech, Oak Ridge team up</title>
      <link>http://localhost:1313/blog/20060202-ornl/</link>
      <pubDate>Thu, 02 Feb 2006 15:42:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20060202-ornl/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Aliya Sternstein&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After President Bush in his State of the Union address proposed spending more on supercomputing, the College of Computing at the Georgia Institute of Technology announced a new partnership with the federal government that could advance the president&amp;rsquo;s agenda.&lt;/p&gt;
&lt;p&gt;The College of Computing, the Energy Department’s Oak Ridge National Laboratory in Tennessee and the nonprofit company UT-Battelle said they would share facilities and staff for large-scale research efforts that rely on supercomputing. Such research efforts would include the development of nanotechnology and new drugs.&lt;/p&gt;
&lt;p&gt;The close proximity of the facilities&amp;rsquo; computational resources supplies the networking and power to maintain some of the largest computer systems in the world.&lt;/p&gt;
&lt;p&gt;In his speech, Bush said, “I propose to double the federal commitment to the most critical basic research programs in the physical sciences over the next 10 years. This funding will support the work of America&amp;rsquo;s most creative minds as they explore promising areas such as nanotechnology, supercomputing and alternative energy sources.”&lt;/p&gt;
&lt;p&gt;Georgia Tech has plans to make the Southeast a national destination for high-performance computing research and development.&lt;/p&gt;
&lt;p&gt;“The Southeastern United States has the ‘perfect storm’ of elements needed for success in computational science and engineering,” said &lt;strong&gt;David Bader&lt;/strong&gt;, associate professor in computational science and engineering at Georgia Tech.&lt;/p&gt;
&lt;p&gt;The region’s academic institutions, including Georgia Tech, the University of North Carolina at Chapel Hill and the University of Tennessee, run some of the nation’s leading computer science departments. Internationally recognized researchers Haesun Park and Bader have joined a new division at Georgia Tech to lead the partnership.&lt;/p&gt;
&lt;p&gt;Georgia Tech recently installed a 4,000-processor IBM eServer supercomputer, one of the fastest university computers, for solving problems in computational biology. The Georgia Tech/Oak Ridge supercomputing effort will harness information extracted from the sequencing of the human genome, providing data that researchers can use to accelerate drug design. Supercomputing is used in protein structure prediction and in the molecular modeling of protein-protein interactions.&lt;/p&gt;
&lt;p&gt;Additionally, researchers at Georgia Tech’s School of Physics are using supercomputer-based nanoscale simulations to discover new technologies that can store massive amounts of information in a compact space.&lt;/p&gt;
&lt;p&gt;Dan Reed, vice chancellor of information technology and chief information officer at UNC Chapel Hill, serves as director of the Renaissance Computing Institute, a collaboration among the state of North Carolina, UNC, Duke University and North Carolina State University. The institute is exploring the intersection of computing technology and the sciences, arts and humanities.&lt;/p&gt;
&lt;p&gt;Reed is also chairman of the board of directors for the Computing Research Association.&lt;/p&gt;
&lt;p&gt;Dave Turek, vice president of IBM Deep Computing, said prospects are good that the new public/private partnership will achieve impressive results. “It’s a reflection of ambition that’s probably going to have a very positive impact,” he said. “It’s a kind of strategy that feeds on itself, if you will.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://fcw.com/articles/2006/02/02/georgia-tech-oak-ridge-team-up.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://fcw.com/articles/2006/02/02/georgia-tech-oak-ridge-team-up.aspx&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Infocrats get unique opportunity to know latest techs at HiPC &#39;05</title>
      <link>http://localhost:1313/blog/20051219-navhindtimes/</link>
      <pubDate>Mon, 19 Dec 2005 16:58:57 -0400</pubDate>
      <guid>http://localhost:1313/blog/20051219-navhindtimes/</guid>
      <description>&lt;p&gt;Mr &lt;strong&gt;David Bader&lt;/strong&gt;, the Associate Professor of the College of Computing, Atlanta, USA said that two papers which will be submitted in Goa conference are excellent and professionals will get latest technological know-how that are taking around the globe.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Recognizes Expert Reviewers</title>
      <link>http://localhost:1313/blog/20051212-computer/</link>
      <pubDate>Mon, 12 Dec 2005 21:44:34 -0400</pubDate>
      <guid>http://localhost:1313/blog/20051212-computer/</guid>
      <description>&lt;p&gt;Computer&amp;rsquo;s editor in chief extends her thanks to the more than 200 professionals who contributed their time and expertise as reviewers of article submissions in 2005.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/1556489&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/1556489&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cray co-founder, CEO steps down</title>
      <link>http://localhost:1313/blog/20050809-cray-ceo/</link>
      <pubDate>Tue, 09 Aug 2005 13:07:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050809-cray-ceo/</guid>
      <description>&lt;p&gt;Seattle supercomputer maker Cray yesterday revealed that it lost more than Wall Street expected, including its co-founder and chief executive.&lt;/p&gt;
&lt;p&gt;Jim Rottsolk, 60, retired yesterday after 27 years of running the company. He’s being replaced by Peter Ungaro, 36, a former IBM sales executive who was promoted to Cray president in March.&lt;/p&gt;
&lt;p&gt;Rottsolk’s biggest contribution was building a company with “a culture of innovation, one that can rapidly move on new technology opportunities,” said Thomas Sterling, principal scientist at the Jet Propulsion Laboratory (JPL) and a faculty associate at the California Institute of Technology in Pasadena.&lt;/p&gt;
&lt;p&gt;“How many people can at the same time have the button-down, conservative approach that’s needed in business and to keep the cash flow and at the same time be receptive and responsive to risk?” Sterling said. “There’s not a lot of CEOs that are able to do that.”&lt;/p&gt;
&lt;p&gt;Cray remains a leader in the elite world of supercomputing, but the business has struggled since 2003. The company is restructuring, dropping some development projects and cutting 10 percent of its workers.&lt;/p&gt;
&lt;p&gt;Yesterday it said sales are picking up but it lost $24 million in the second quarter, which ended June 30. It lost $55 million during the same period last year.&lt;/p&gt;
&lt;p&gt;Sales more than doubled to $53.4 million, compared with $21.7 million a year ago. The loss came to 27 cents a share, including restructuring costs and other charges.&lt;/p&gt;
&lt;p&gt;Analysts surveyed by Thomson Financial on average had expected a 12 cent per share loss, but they generally don’t count special charges.&lt;/p&gt;
&lt;p&gt;“They missed on the [earnings per share] but it didn’t miss by a huge amount,” said Alan Robinson, research director at Delafield Hambrecht in Seattle.&lt;/p&gt;
&lt;p&gt;Cray appears to be taking the right steps to resume profitability, said Robinson, who doesn’t own Cray shares but whose firm does make a market in the stock.&lt;/p&gt;
&lt;p&gt;“I think perhaps we can say there is light at the end of the tunnel here,” he said.&lt;/p&gt;
&lt;p&gt;After closing yesterday at $1.25, up 8 cents, the stock jumped to $1.31 in after-hours trading, after the earnings report.&lt;/p&gt;
&lt;p&gt;New product bookings fell during the quarter, but the company expects higher sales and lower costs in the second half of the year.&lt;/p&gt;
&lt;p&gt;“While we had a solid revenue quarter, some of our largest and most complex installations have been delayed in getting into full production,” Ungaro said in a news release. “We will not recognize the revenue on these systems until we receive customer acceptances, which should happen over the next several months.”&lt;/p&gt;
&lt;p&gt;The company also announced that board member Stephen Kiely, a tech executive, is now chairman.&lt;/p&gt;
&lt;p&gt;Rottsolk will remain on the board and serve as a company adviser through 2005.&lt;/p&gt;
&lt;p&gt;“Jim Rottsolk’s vision and leadership were key to returning Cray to the forefront of supercomputing,” Kiely said in a news release.&lt;/p&gt;
&lt;p&gt;Cray is the latest incarnation of Tera Computer, a company that Rottsolk and computer scientist Burton Smith started in 1987 in Washington, D.C. They moved it to Seattle a year later and in 2000 bought the assets of Cray Research and renamed the company. Smith remains Cray’s chief scientist.&lt;/p&gt;
&lt;p&gt;“Burton certainly brought the brilliance and the ideas and a certain charisma, but taking nothing away from Burton’s accomplishments, Jim made it happen,” JPL’s Sterling said.&lt;/p&gt;
&lt;p&gt;Rottsolk took a long-term view of the potential of high-performance computing. He also revitalized Cray with new architectures and products created to address computing capability needs, said &lt;strong&gt;David Bader&lt;/strong&gt;, associate professor at the Georgia Institute of Technology.&lt;/p&gt;
&lt;p&gt;“Cray’s reputation is still extremely strong,” he said. “They’re a leading player in this market area.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Brier Dudley: 206-515-5687 or &lt;a href=&#34;mailto:bdudley@seattletimes.com&#34;&gt;bdudley@seattletimes.com&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Brier Dudley is a member of The Seattle Times editorial board. His columns appear regularly on editorial pages of The Times.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.seattletimes.com/business/cray-co-founder-ceo-steps-down/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.seattletimes.com/business/cray-co-founder-ceo-steps-down/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People and Positions: David Bader Joins Georgia Tech for HPC Work</title>
      <link>http://localhost:1313/blog/20050610-hpcwire/</link>
      <pubDate>Fri, 10 Jun 2005 07:30:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050610-hpcwire/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; has joined Georgia Institute of Technology&amp;rsquo;s College of Computing, effective August 15, 2005. David will
advance Georgia Tech&amp;rsquo;s capabilities in the area of computational science, high-performance computing, and biomedical
engineering.&lt;/p&gt;
&lt;p&gt;Prior to this, he was a faculty member at the University of New Mexico from 1998 &amp;ndash; 2005. He received his Ph.D. in 1996
from The University of Maryland, and was awarded a National Science Foundation (NSF) Postdoctoral Research
Associateship in Experimental Computer Science. He is an NSF CAREER Award recipient, an investigator on several NSF
awards, a distinguished speaker in the IEEE Computer Society Distinguished Visitors Program, and is a member of the IBM
PERCS team for the DARPA High Productivity Computing Systems program.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ECE Alumnus David Bader Joins Georgia Tech Faculty</title>
      <link>http://localhost:1313/blog/20050527-umd-gt/</link>
      <pubDate>Fri, 27 May 2005 21:37:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050527-umd-gt/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20050527-umd-gt/article1500.large_hu_9ce69eed33723403.webp 400w,
               /blog/20050527-umd-gt/article1500.large_hu_29e5612354979e48.webp 760w,
               /blog/20050527-umd-gt/article1500.large_hu_92fd9e8c7792e1f3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20050527-umd-gt/article1500.large_hu_9ce69eed33723403.webp&#34;
               width=&#34;120&#34;
               height=&#34;165&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Maryland Electrical and Computer Engineering (ECE) alumnus &lt;strong&gt;David Bader&lt;/strong&gt; will join the Computational Science and Engineering faculty at Georgia Tech this fall.&lt;/p&gt;
&lt;p&gt;Dr. Bader completed his Ph.D. at the University of Maryland in 1996. As a graduate student, he was advised by Professor Joseph JaJa (ECE/UMIACS). Bader founded and served as president of the Electrical and Computer Engineering Graduate Student Association while he attended school at Maryland. Prior to his recent hiring at Georgia Tech, Dr. Bader served as an Associate Professor of ECE and Computer Science at the University of New Mexico. He received a National Science Foundation (NSF) Faculty Early Career Development (CAREER) Award for his research on High-Performance Algorithms for Scientific Applications last fall.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ece.umd.edu/news/story/ece-alumnus-david-bader-joins-georgia-tech-faculty&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ece.umd.edu/news/story/ece-alumnus-david-bader-joins-georgia-tech-faculty&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computing Life&#39;s Family Tree</title>
      <link>http://localhost:1313/blog/20050501-cse/</link>
      <pubDate>Sun, 01 May 2005 07:18:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050501-cse/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Pam Frost Gorder&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Call it yet another biological gold rush. When Charles Darwin published The Origin of Species in 1859, scientists began working in earnest to document the world&amp;rsquo;s plant and animal species and build a &lt;em&gt;phylogeny&lt;/em&gt; —a map of how all those species relate to each other. More scientists came to the discipline in the 1980s, when automated DNA sequencing offered a new way to classify species and new applications for phylogenetics.&lt;/p&gt;
&lt;p&gt;Thanks to such efforts, at least a small sample of genetic code is on file in databases worldwide for some 100,000 of Earth&amp;rsquo;s organisms. The largest such database, &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/Genbank/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GenBank&lt;/a&gt;, contains more than 42.7 million genetic sequences, and counting. But scientists have yet to realize Darwin&amp;rsquo;s phylogeny; they&amp;rsquo;ve sampled genes piecemeal—a rice gene here, a mouse protein there—and have connected relatively few species.&lt;/p&gt;
&lt;p&gt;Today, the newest prospectors in the gold rush are those with enough expertise in computing to connect all that genetic data in a meaningful way. The goal is the same as it was 150 years ago: build the ultimate family tree.&lt;/p&gt;
&lt;h2 id=&#34;when-too-much-data-isnt-enough&#34;&gt;When Too Much Data Isn&amp;rsquo;t Enough&lt;/h2&gt;
&lt;p&gt;A complete genetic &amp;ldquo;tree of life&amp;rdquo; (see Figure 1) would not only provide an evolutionary map, but could also lead scientists to a new understanding of diseases, new drugs, and even new strategies for saving endangered species, explains Michael Sanderson, a biologist at the University of California, Davis.&lt;/p&gt;


















&lt;figure  id=&#34;figure-figure-1-the-tree-of-life&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1: The tree of life.&#34; srcset=&#34;
               /blog/20050501-cse/c30031_hu_d778cbeb06eead2b.webp 400w,
               /blog/20050501-cse/c30031_hu_1b2b10f71d2e3986.webp 760w,
               /blog/20050501-cse/c30031_hu_fd03568bf572fd41.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20050501-cse/c30031_hu_d778cbeb06eead2b.webp&#34;
               width=&#34;400&#34;
               height=&#34;533&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: The tree of life.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The raw material for small sections of the tree is already available. Databases such as GenBank, which the US National Center for Biotechnology Information maintains, now catalog partial genetic sequences for 6 percent of the 1.7 million known plant and animal species. With millions of individual records, GenBank is a huge data set, but because it still lacks so many species, computer scientists consider it very sparse—it holds a lot of data, but a lot more is missing.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The scale of the problem just cries out for elegant solutions at the algorithm level,&amp;rdquo; Sanderson says.&lt;/p&gt;
&lt;p&gt;With help from colleagues, Sanderson and postdoctoral researcher Amy Driskell combined some publicly available algorithms to mine a subset of data from GenBank. In Science (vol. 306, no. 5699, 2004, pp. 1172–1174), they described how they constructed two small subtrees of 69 related plant species and 70 animals, despite much missing data.&lt;/p&gt;
&lt;p&gt;To &lt;strong&gt;David A. Bader&lt;/strong&gt; —who codirects the high-performance computing thrust of the Cyberinfrastructure for Phylogenetic Research (CIPRes) project at the University of New Mexico—the Science paper comes as a &amp;ldquo;wonderful surprise&amp;rdquo; because it means that scientists can exploit sparse genetic databases as a cost-effective way to assemble the tree.&lt;/p&gt;
&lt;p&gt;Biologist Keith Crandall of Brigham Young University (see Figure 2) also feels that Sanderson&amp;rsquo;s strategy is a good one. &amp;ldquo;It&amp;rsquo;s a really neat idea because most people in the phylogeny reconstruction business assume that you need a complete data set.&amp;rdquo;&lt;/p&gt;


















&lt;figure  id=&#34;figure-figure-2-biology-professor-keith-crandall-and-doctoral-candidate-jennifer-buhay-examine-a-crawfish-in-crandalls-lab-the-pair-study-the-genetic-differences-among-crayfish-species-and-used-their-expertise-to-author-a-paper-in-science-about-better-methods-for-assembling-the-relationships-among-all-species&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 2: Biology professor Keith Crandall and doctoral candidate Jennifer Buhay examine a crawfish in Crandall&amp;#39;s lab. The pair study the genetic differences among crayfish species, and used their expertise to author a paper in Science about better methods for assembling the relationships among all species.&#34; srcset=&#34;
               /blog/20050501-cse/c30032_hu_74e7ac8823606811.webp 400w,
               /blog/20050501-cse/c30032_hu_6d009917546847f9.webp 760w,
               /blog/20050501-cse/c30032_hu_c2550a564376de78.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20050501-cse/c30032_hu_74e7ac8823606811.webp&#34;
               width=&#34;500&#34;
               height=&#34;361&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2: Biology professor Keith Crandall and doctoral candidate Jennifer Buhay examine a crawfish in Crandall&amp;rsquo;s lab. The pair study the genetic differences among crayfish species, and used their expertise to author a paper in Science about better methods for assembling the relationships among all species.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Given that in more than 20 years of trying, scientists have only been able to document partial records for a small percentage of known species, the likelihood of constructing a complete data set anytime soon seems small. In the meantime, species are going extinct—or, as Crandall puts it, &amp;ldquo;leaves are falling off the tree.&amp;rdquo; What&amp;rsquo;s more, experts speculate that many more species remain undiscovered, some of which could answer fundamental questions in biology and medicine. These notions all fuel the sense of urgency with which scientists are trying to build the tree from currently available data.&lt;/p&gt;
&lt;p&gt;In 2002, the US National Science Foundation (NSF) launched a program called Assembling the Tree of Life (ATOL), which has received consistent funding. The expected amount available for 2005 is US &lt;em&gt;14 million, with another&lt;/em&gt; 15 million set aside for 2006. Individual projects can receive up to $3 million each.&lt;/p&gt;
&lt;p&gt;So far, much of this funding has focused on constructing the tree&amp;rsquo;s deepest branches, Crandall says. To him, Sanderson&amp;rsquo;s work suggests that biologists can focus on detailing the smaller branches and twigs of the tree, whereas the data already in GenBank and elsewhere can fill in the larger structure—if scientists can find the right algorithmic strategy to connect that data.&lt;/p&gt;
&lt;h2 id=&#34;think-outside-the-matrix&#34;&gt;Think Outside the Matrix&lt;/h2&gt;
&lt;p&gt;Scientists trying to build the tree face even bigger computational problems than an overabundance of sparse data; today&amp;rsquo;s high-performance computing techniques have their roots in the physical sciences, and life-science data are different.&lt;/p&gt;
&lt;p&gt;As the CIPRes project&amp;rsquo;s Bader explains, the earliest supercomputers were designed to tackle simulations of atmospheric phenomena and nuclear weaponry—problems often computationally represented with a simple two- or three-dimensional matrix. Matrices make for efficient computing because the simulations draw on source data predictably. Related pieces of data are stored together in a cache—literally next to each other on the same computer chip—for fast retrieval.&lt;/p&gt;
&lt;p&gt;A matrix won&amp;rsquo;t work for biology&amp;rsquo;s hierarchical, tree-shaped data structure. Similar data is grouped by branch, but the branches split at unpredictable intervals. Well-known techniques for exploiting caches often do little for these codes.&lt;/p&gt;
&lt;p&gt;Bader and CIPRes director Bernard Moret have been working around the phylogeny problem with a combination of innovative algorithms and parallelization in a package called Grappa (Genome Rearrangements Analysis under Parsimony and other Phylogenetic Algorithms; &lt;a href=&#34;http://phylo.unm.edu/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://phylo.unm.edu/)&lt;/a&gt;. In 2000, they assembled the phylogeny of 13 members of the bluebell family of flowering plants on a 512-processor computer cluster, and achieved a billion-fold speedup from prior methods.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The biologists that we were working with had computations on their data set that they estimated would take 250 years to compute if they just let their current computer run using an available program,&amp;rdquo; Bader recalls. &amp;ldquo;We can now solve a more biologically meaningful version of the same problem using the same data set in just five minutes on a laptop.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Sanderson and his team built their tree using algorithms that searched for similar gene sequences among their selected species, then used the differences in those sequences to arrange the species relative to each other on the tree. He&amp;rsquo;s hoping that over the next two years, they can scale up the technique and produce trees with tens of thousands of species. Compiling all the GenBank species into one tree is a far more distant goal, but assuming he could, this would raise yet another problem: visualization.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Imagine a tree with 100,000 species on it,&amp;rdquo; he says. &amp;ldquo;How are you going to look at the thing?&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;seeing-trees&#34;&gt;Seeing Trees&lt;/h2&gt;
&lt;p&gt;Nina Amenta, computer science professor at the University of California, Davis, knows how most biologists would have to answer that question. &amp;ldquo;Say you want to look at a really big tree,&amp;rdquo; she begins. &amp;ldquo;It&amp;rsquo;s bigger than you can fit on your computer screen, bigger than you can fit on a piece of paper. What you&amp;rsquo;ll probably do is print it out on 10 pieces of paper and stick them together with Scotch tape and sit on the floor with highlighters and try to pick out the features of interest.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Hardly a high-tech solution. She and Tamara Munzner, from the University of British Columbia, have written a program that might get biologists off the floor and back to their desks. The &lt;a href=&#34;http://olduvai.sourceforge.net/tj/index.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TreeJuxtaposer&lt;/a&gt; draws a large data tree that fits on a single computer screen; users can click on parts of the tree to zoom in. The selected part of the tree expands while the rest of it remains visible, so that users can still view the portion of interest in the context of the entire data set.&lt;/p&gt;
&lt;p&gt;If scientists are working with a data set that allows more than one possible tree configuration, they can try the &lt;a href=&#34;http://comet.lehman.cuny.edu/treeviz/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tree Set Visualization program&lt;/a&gt; to help them sort through their choices. Amenta and Katherine St. John at the City University of New York designed the program to compare the characteristics of candidate trees and represent each as a point on a graph, with similar trees clustered together. The software lets biologists look at the phylogenies they&amp;rsquo;re studying in a whole new way.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Sometimes you may have a group of trees that are similar, and if you compute the average tree structure for that cluster, that&amp;rsquo;s probably a pretty good answer,&amp;rdquo; Amenta says. &amp;ldquo;But if there are trees far outside that group, you have to wonder what makes them good competing hypotheses for this one dense cluster.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;classifying-the-phylogeneticist&#34;&gt;Classifying the Phylogeneticist&lt;/h2&gt;
&lt;p&gt;On a recent March day, Crandall worked at home, catching up on articles he was peer-reviewing. Just that day, he&amp;rsquo;d read papers for Marine Biotechnology and the &lt;em&gt;Journal of the American Medical Association&lt;/em&gt;, and he&amp;rsquo;s often amazed at the diversity of the journals that come across his desk. Papers on phylogenetics are just as likely to appear in journals for mathematics, statistics, and computer science as they are in biology journals. Promising applications of the tree of life, such as drug discovery, are drawing many people to the discipline.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It&amp;rsquo;s a nice field for people to get into because there&amp;rsquo;s a lot of action here,&amp;rdquo; Crandall says. He thinks that now is a good time for people with skills in algorithm construction and data visualization to partner with biologists to make real advances in the field.&lt;/p&gt;
&lt;p&gt;Sanderson agrees. &amp;ldquo;The algorithms are just not keeping pace with the data that are available, so anyone with a better mousetrap is going to have a huge impact,&amp;rdquo; he says.&lt;/p&gt;
&lt;p&gt;Bader hopes that scientists and engineers who want to learn more will visit the Web page for the NSF-funded &lt;a href=&#34;http://www.phylo.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CIPRes program&lt;/a&gt;. At CIPRes, some 40 scientists across various disciplines are developing a national computational infrastructure to support phylogenetics and enable the construction of the tree. He envisions that someday researchers will be able to submit the smaller species trees they&amp;rsquo;ve constructed to CIPRes, just as they submit genes they&amp;rsquo;ve sequenced to GenBank.&lt;/p&gt;
&lt;p&gt;Asked to dream big about where computing could take phylogenetics in the distant future, Sanderson says he and his colleagues would like to see an automated online system that continuously polls databases such as GenBank and adds the new sequences to trees. &amp;ldquo;There are so many difficult computational problems along each step of the way that it&amp;rsquo;s probably a little overly ambitious,&amp;rdquo; he concedes. &amp;ldquo;But it&amp;rsquo;s something that we imagine might be possible.&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;you-can-be-a-phylogeneticist-too&#34;&gt;You Can be a Phylogeneticist, Too&lt;/h2&gt;
&lt;p&gt;Anyone can be a phylogeneticist at home by downloading &lt;a href=&#34;http://dogma.byu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dogma&lt;/a&gt;, a parallel computing program Keith Crandall&amp;rsquo;s colleagues developed at Brigham Young University. Like the Stanford University programs Folding@Home and Genome@home, Dogma is a screensaver that exploits volunteers&amp;rsquo; unused CPU time. Recent Dogma projects included mapping a tree of 2,500 insect species.&lt;/p&gt;
&lt;h2 id=&#34;about-the-authors&#34;&gt;About the Authors&lt;/h2&gt;
&lt;p&gt;Pam Frost Gorder is a freelance science writer based in Columbus, Ohio.&lt;/p&gt;
&lt;p&gt;Computing in Science &amp;amp; Engineering&lt;br&gt;
May/June 2005, pp. 3-6, vol. 7&lt;br&gt;
DOI Bookmark: 10.1109/MCSE.2005.48&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computer.org/csdl/magazine/cs/2005/03/c3003/13rRUxjyX7B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computer.org/csdl/magazine/cs/2005/03/c3003/13rRUxjyX7B&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Obituary: Morris Bader</title>
      <link>http://localhost:1313/blog/20050426-morningcall/</link>
      <pubDate>Tue, 26 Apr 2005 15:30:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050426-morningcall/</guid>
      <description>&lt;p&gt;Morris Bader, 72, of Bethlehem, died peacefully at home on Thursday, April 21, 2005. Born: In New York, he was a son of the late Louis and Esther Saltzman Bader. Personal: He and his wife, the former Karen Roberts, were married for 45 years. He was a graduate of Stuyvesant High School in New York City. He was a 1953 graduate of the City University of New York, formerly City College of New York and earned his Ph.D. in physical chemistry at Indiana University, Bloomington, Ind. He taught at New York University, Marietta College in Marietta, Ohio, and Moravian College. He was an emeritus professor of chemistry at Moravian College. He taught chemistry and computer science from 1962 until his retirement in 1995. He also taught physical chemistry, developed the initial computer science program, conceived and funded SOAR program for funding student and faculty summer research, and collaborated and developed a plant growth hormone. He was a scientific glassblower, making much of his own equipment. He developed scientific programs and published five computer manuals and software which sold worldwide, the profits of which were donated to assist faculty research travel to conferences. He developed the course, &amp;ldquo;Chemistry for the Non-Science Major&amp;rdquo; and his paper &amp;ldquo;A Systematic Approach to Standard Addition Methods in Instrumental Analysis&amp;rdquo; is highly cited and used widely in practice. Morris holds two patents, one for a bicycle gearing system and one for a quartz infrared cell; both manufactured. He has published numerous articles in numerical scientific computation for chemical analysis, solution of hard differential equations, and improved accuracy and error analysis in numerical computing. His chemistry publications include various chemical experiments for use by educators, and guideline and error estimates for the neglect of buoyancy in laboratory weighings. He has published in the Journal of Chemical Education and in American Laboratory of which he was a contributing editor. Memberships: He was a championship chess player and the Moravian College Chess Club Advisor. He supported the Moravian College Foreign Film Festival. His many volunteering activities included teaching swimming to toddlers at the 3rd Street Alliance, Easton, Rodale Theater, State Theatre, Easton. He was a 21 year Musikfest volunteer, Lehigh Valley Hospital-Muhlenberg Hospital. He was financial advisor to the Friendship Circle of the J.C.C., and was assistant Scoutmaster of Troops 304 and 346 in the Minsi Trails Council, Boy Scouts of America. Morris was a member of Congregation Beth Avraham, formerly Agudath Achim of Bethlehem. He was a member of the board and then president for 20 years. He read Torah and led services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Survivors:&lt;/strong&gt; In addition to his wife, Karen, he is survived by three sons, William A. of Bethlehem, Joel S. and his wife, Jennifer of Baltimore, Md, David A. and his wife, Sara Gottlieb of Albuquerque, NM; a daughter, Debra S. Eisenstein and her husband, Eric of Ithaca, NY; two sisters, Rose Hittman and her husband, Ralph, Marian Ashrey, all of New York City; and five grandchildren.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Services:&lt;/strong&gt; Graveside services were held on Friday, April 22, 2005 at Beth Avraham/Agudath Achim Cemetery, Fountain Hill, Pa. Arrangements are by the Long Funeral Home, Bethlehem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contributions:&lt;/strong&gt; In Morris&amp;rsquo; memory, memorials may be made to the Congregation Beth Avraham, 1555 Linwood Street, Bethlehem, PA 18017; or to the Bader Memorial Scholarship Prize in Chemistry, Moravian College, 1200 Main Street, Bethlehem, PA 18018.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reconstructing the Tree of Life</title>
      <link>http://localhost:1313/blog/20050218-unm-treeoflife/</link>
      <pubDate>Fri, 18 Feb 2005 09:32:49 -0400</pubDate>
      <guid>http://localhost:1313/blog/20050218-unm-treeoflife/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Greg Johnston&lt;/em&gt;&lt;/p&gt;


















&lt;figure  id=&#34;figure-illustration-by-greg-tucker&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Illustration by Greg Tucker.&#34; srcset=&#34;
               /blog/20050218-unm-treeoflife/treeoflife_hu_c7b9572c3ce86148.webp 400w,
               /blog/20050218-unm-treeoflife/treeoflife_hu_e276bcbf2e103566.webp 760w,
               /blog/20050218-unm-treeoflife/treeoflife_hu_8f51389202c43241.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20050218-unm-treeoflife/treeoflife_hu_c7b9572c3ce86148.webp&#34;
               width=&#34;307&#34;
               height=&#34;400&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Illustration by Greg Tucker.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Biologist Terry Yates distinctly remembers a question posed in 2000, when he served as director of the Division of Environmental Biology at the National Science Foundation, NSF. About a month after he started his work at NSF in Arlington, Virginia, Director Rita Colwell called a division directors’ retreat to pose a challenge: “Give me your craziest idea that would represent a major unmet need for the nation or the world.”&lt;/p&gt;
&lt;p&gt;Yates, now vice president for research and economic development at the University of New Mexico, responded. “I said, ‘I think it’s about time we try to assemble a universal Tree of Life, from microbes to mammals.’ I thought I’d get laughed out of the place until I heard responses like ‘I think that’s a good idea. Our program can’t go any further because we don’t know what organisms to sequence next.’” By reconstructing the Tree of Life, researchers and scientists would have a clearer picture of how life has evolved and continues to evolve.&lt;/p&gt;
&lt;p&gt;There are an estimated 1.7 million known species of life on Earth. “This tree can be a pretty powerful predictive tool,” says Yates. “But there is a whole computational infrastructure that has to be done—not just the machines, but also software that is more efficient and faster. To assemble a tree that has 1.7 million branches on it, computationally and in any sensible way, is going to take enormous computer power.”&lt;/p&gt;
&lt;p&gt;Yates’ idea gave rise to the NSF “Assembling the Tree of Life” research program, which funds the collection and analysis of extensive data on modern and fossil species. NSF also ran the Information Technology Research, ITR, program, a goal of which was to foster large-scale applications of information technology to new areas of science.&lt;/p&gt;
&lt;p&gt;At the UNM School of Engineering, Professor Bernard Moret and Associate Professor &lt;strong&gt;David Bader&lt;/strong&gt; had already received several ITR grants to support their work in computational phylogenetics—algorithms to reconstruct evolutionary trees such as the Tree of Life. Moret and Bader each hold joint appointments in the Departments of Computer Science and Electrical and Computer Engineering. Collaborating with Professor Tandy Warnow at the University of Texas-Austin, they led a group of biologists and computer scientists in a proposal to ITR to build a program called Cyber Infrastructure for Phylogenetic Research, CIPRES, needed to support the reconstruction of the Tree of Life.&lt;/p&gt;
&lt;p&gt;In September 2003, NSF announced that CIPRES, one of only seven proposals funded out of more than seventy submitted, would receive a five-year, $11.6 million grant. CIPRES is a collaboration of thirteen institutions, including three museums. Lead institutions are UNM, UT-Austin, University of California at Berkeley, University of California at San Diego, and Florida State University.&lt;/p&gt;
&lt;p&gt;Moret, director of the CIPRES project, is known primarily for his work in algorithm engineering. He has gained extensive expertise in recent years in computational biology. For the Tree of Life, Moret leads a team of over thirty-five computer scientists, biologists, and mathematicians, including students and postdoctoral researchers.&lt;/p&gt;
&lt;p&gt;“What the Table of Elements did for chemistry, the Tree of Life will do for biology,” says Moret. “It’s a fundamental organizing tool. Thus, while the Tree of Life is an abstract pursuit, it will help develop a deep understanding of mechanisms and models that are going to be used everywhere in biology and medicine.”&lt;/p&gt;
&lt;p&gt;Moret’s closest collaborator at UNM is David Bader. Bader’s research interests lie in computational biology, genomics, high-performance computing, and parallel computation. Bader is teamed with Fran Berman, director of the San Diego Supercomputing Center, which is the physical location for the computational infrastructure. Bader and Berman will lead the CIPRES efforts in high-performance computing.&lt;/p&gt;
&lt;p&gt;Moret and Bader have already achieved spectacular results by applying algorithm engineering and parallel computing to problems in phylogeny. In 2000, a team including Professor Robert Jansen from UT-Austin reconstructed the phylogeny of thirteen members of the bluebell family of flowering plants, an adaptable family found throughout the world.&lt;/p&gt;
&lt;p&gt;At the time, existing approaches would have required several centuries of computation on a high-powered workstation to reconstruct the evolution. However, Moret and Bader developed new code and used a UNM computing cluster of 512 processors at the Center for High Performance Computing to carry out the analysis in just one pass. Continuous refinement of the code at UNM now enables the same analysis to be run in thirty minutes on a laptop, a tremendous improvement for a process that must examine nearly fourteen billion candidate trees.&lt;/p&gt;
&lt;p&gt;The research focused on only thirteen plants. Biologists estimate that there are anywhere from ten to a hundred million undescribed species beyond the identified 1.7 million known species of living organisms. “We will need to run billions of trillions times faster than we can run today,” says Moret, “and that cannot be done through hardware technology alone.”&lt;/p&gt;
&lt;p&gt;“Building the Tree of Life is a problem for the next few decades,” explains Bader. “So many computing cycles and so much research are directed at this problem because it has direct impact on our future, but it will take time to accumulate the data and the know-how.”&lt;/p&gt;
&lt;p&gt;The effort will be international: CIPRES has collaborators in Europe, Asia, Oceania, and South America. For the Tree of Life to be successful, extensive collaboration will also need to occur between computer scientists and biologists. “We want to make sure that what we deliver over the next five years is of direct use to biologists—an infrastructure that has been thoroughly tested,” Moret says. “We must understand in detail the performance characteristics of our algorithms, not just in terms of running time—that’s the easy part—but more importantly in terms of accuracy under the models of evolution that the biologists are interested in. There is only one Tree of Life—we must get it right.”&lt;/p&gt;
&lt;p&gt;By the end of the five-year grant cycle, Bader says CIPRES will have built a computer platform allowing biologists worldwide to go to a Web site, enter data, select methods, and have their analysis carried out. The more computing-oriented scientists can choose to download the software package and run it on their own machines.&lt;/p&gt;
&lt;p&gt;Yates uses an analogy to describe the collaboration between two scientific communities. “Mother Nature has given us almost two billion years of free research and development. These are time-tested and thoroughly validated software packages. All we have to do now is run them through the computer and they will tell us what to do.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20050214021231/http://research.unm.edu/quantum/treeoflife.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://research.unm.edu/quantum/treeoflife.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prof. Bader Selected as an Associate Editor for the IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
      <link>http://localhost:1313/blog/20041201-tpds/</link>
      <pubDate>Wed, 01 Dec 2004 21:58:59 -0400</pubDate>
      <guid>http://localhost:1313/blog/20041201-tpds/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20041201-tpds/bader_hu_6e993b7a83afe3c0.webp 400w,
               /blog/20041201-tpds/bader_hu_b006c1ee15093781.webp 760w,
               /blog/20041201-tpds/bader_hu_ad598b0b0a3a6703.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20041201-tpds/bader_hu_6e993b7a83afe3c0.webp&#34;
               width=&#34;80&#34;
               height=&#34;102&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Prof David Bader&lt;/strong&gt; who is an Associate Editor for The ACM Journal of Experimental Algorithmics and the IEEE Distributed Systems Online, as well as a Member of the Founding Editorial Board for the International Journal of High Performance Computing and Networking; has been selected as an Associate Editor for the IEEE Transactions on Parallel and Distributed Systems (TPDS).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20041205162809/http://www.ece.unm.edu/event/news/single.php?id=90&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ece.unm.edu/event/news/single.php?id=90&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF Awards Major Research Instrumentation Grant to UNM</title>
      <link>http://localhost:1313/blog/20040921-unm-mri/</link>
      <pubDate>Tue, 21 Sep 2004 07:36:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/20040921-unm-mri/</guid>
      <description>&lt;p&gt;The National Science Foundation has awarded a $350,000 Major Research Instrumentation grant to the University of New Mexico for the purchase and maintenance of a state-of-the-art shared memory high-performance computer.&lt;/p&gt;
&lt;p&gt;“The new computer will allow UNM researchers to tackle a wide range of problems in computational science and engineering,” said Principal Investigator, Hua Guo, professor, Chemistry Department. “It will also provide educational opportunities for students interested in high performance computing.”&lt;/p&gt;
&lt;p&gt;Computational research plays an increasingly important role in many grand challenge-scale problems in science and engineering. At UNM, a diverse group of investigators are engaged in computationally intensive advanced research projects in physics, chemistry, biology and computer science.&lt;/p&gt;
&lt;p&gt;The project, titled “Acquisition of a High Performance Shared-Memory Computer for Computational Science and Engineering,” involves other UNM researchers including: &lt;strong&gt;David A. Bader&lt;/strong&gt;, Electrical and Computer Engineering; Marc S. Ingber, High Performance Computing Center and Mechanical Engineering; Susan Atlas, Physics and Astronomy, and Tudor Oprea, Biochemistry and Molecular Biology.&lt;/p&gt;
&lt;p&gt;In addition to traditional domains of research, there is a growing trend towards interdisciplinary collaborations to address complex problems in systems biology and nanotechnology. The research includes biologically motivated projects that span conventional departmental boundaries — computational biophysics and biochemistry, phylogenetics and virtual drug screening — as well as work in materials science and the quantum control of complex systems.&lt;/p&gt;
&lt;p&gt;“This state-of-the-art computer will be operated and maintained by the High Performance Computing Center at UNM, and in addition to supporting the projects detailed in the proposal, it will be made available for general science and engineering research and education use by members of the university community,” said Guo. “This is an effort to improve the research and educational infrastructure of high performance computing at the University of New Mexico.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20041010142607/http://www.unm.edu/~market/cgi-bin/archives/000303.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/~market/cgi-bin/archives/000303.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Distributed Systems Online Names First Editorial Board</title>
      <link>http://localhost:1313/blog/20040426-ieee-dsonline/</link>
      <pubDate>Mon, 26 Apr 2004 07:38:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20040426-ieee-dsonline/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://dsonline.computer.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;IEEE Distributed Systems Online&lt;/em&gt;&lt;/a&gt;, the IEEE’s first online-only publication has named the charter members of its editorial board. Editor-in-Chief Jean Bacon culled the 17 board members from leading professionals within academia and the distributed systems industry worldwide.&lt;/p&gt;
&lt;p&gt;“I wanted senior people, experts in each area, to ensure that we had high quality information,” says Bacon. “These people have very little time, so I encouraged them to recruit enthusiastic young colleagues to assist. On this basis, I was able to attract leaders in the field to apply overall control while their team is responsible for supplying the detail and managing Web pages regularly.&amp;quot;&lt;/p&gt;
&lt;p&gt;DS Online debuted in 2000 as an IEEE Computer Society pilot program. Featuring original, peer-reviewed articles and departments, as well as expert-managed topic areas, the monthly magazine quickly became a leading resource for distributed computing information. &amp;ldquo;Our vision in starting DS Online in 2000 was to provide a Web site that people could trust, both to keep up-to-date in their area and to find out about new ones,” says Bacon.&lt;/p&gt;
&lt;p&gt;The IEEE recognized the magazine’s importance by awarding it full publication status, making it the international organization’s only exclusively online publication. DS Online relaunched under its new status in January 2004.&lt;/p&gt;
&lt;p&gt;Bacon credits the magazine’s success to a dedicated cast of volunteers, many of whom have been with DS Online from the beginning. “I volunteered to participate on the DS Online editorial board because of the emerging shift in the means by which communities interact with one another,” says &lt;strong&gt;David A. Bader&lt;/strong&gt;, a professor at the University of New Mexico. “I rely on DS Online as the single portal for providing active information about our community to computing researchers, academics, and professionals.”&lt;/p&gt;
&lt;p&gt;Hewlett-Packard’s Dejan Milojicic, another DS Online board member, agrees. “People in the industry have less and less time to regularly read papers in IEEE transactions and journals. The dynamic style of IEEE DS Online is more suitable for busy professionals,” says Milojicic. “It is a combination of a magazine style enriched with online access. … Short, but credible, summaries of technology help researchers and technologists more easily navigate through the wealth of information available in contemporary computer science.”&lt;/p&gt;
&lt;p&gt;For the latest issue of DS Online, visit &lt;a href=&#34;http://dsonline.computer.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://dsonline.computer.org&lt;/a&gt;.
For a complete list of DS Online volunteers and their affiliations, visit &lt;a href=&#34;http://dsonline.computer.org/about_editors.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://dsonline.computer.org/about_editors.htm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Contact: Rita Scanlan, Lead editor, &lt;a href=&#34;mailto:DSOnline@computer.org&#34;&gt;DSOnline@computer.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20040605063303/http://www.computer.org/pr/Apr04/dso_edboard.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computer.org Online&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wiz All for Fiddling Under the Software Hood</title>
      <link>http://localhost:1313/blog/20031009-abqjournal/</link>
      <pubDate>Thu, 09 Oct 2003 07:01:56 -0400</pubDate>
      <guid>http://localhost:1313/blog/20031009-abqjournal/</guid>
      <description>&lt;p&gt;&lt;em&gt;by John Fleck, Journal Staff Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Richard Stallman is one of those people who isn&amp;rsquo;t famous but should be.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/443547089/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/443547089/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Richard Stallman, Founder of GNU Project, to Give Speech at UNM</title>
      <link>http://localhost:1313/blog/20031002-unm-stallman/</link>
      <pubDate>Thu, 02 Oct 2003 07:18:41 -0400</pubDate>
      <guid>http://localhost:1313/blog/20031002-unm-stallman/</guid>
      <description>&lt;p&gt;Noted scientist Richard Stallman will be the featured speaker at a seminar hosted by the University of New Mexico School of Engineering (SOE) Wednesday, Oct. 8, in the Student Union Building, Ballroom B, at 3 p.m. Stallman’s speech is titled, “Copyright vs. Community in the Age of Computer Networks.”&lt;/p&gt;
&lt;p&gt;Stallman is the founder of the GNU Project, which was launched in 1984, to develop the free operating system GNU, which gives computer users the freedom that most have lost. GNU is free software: meaning everyone is free to copy it and redistribute it and can make changes either large or small.&lt;/p&gt;
&lt;p&gt;Today, Linux-based variants of the GNU system, based on the kernel Linux developed by Linus Torvalds, are in widespread use. There are an estimated 20 million users of GNU/Linux systems worldwide.&lt;/p&gt;
&lt;p&gt;Stallman, who was elected to the National Academy of Engineering in 2002, is the principal author of the GNU Compiler Collection, a portable optimizing compiler that was designed to support diverse architectures and multiple languages. It now supports more than 30 architectures and seven programming languages. He also wrote the GNU symbolic debugger, GNU Emacs and various other GNU programs.&lt;/p&gt;
&lt;p&gt;“Arguably, Richard Stallman has had the single most important impact in computer software development in history due to his writing of the first portable editors, compilers and debuggers that are de facto standards today,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, associate professor and Regents’ Lecturer, Electrical and UNM Computer Engineering, with a joint appointment in Computer Science.&lt;/p&gt;
&lt;p&gt;Stallman has received many accolades including the Grace Hopper award from the Association for Computer Machinery in 1991 for development of the first Emacs editors. In 1990, he was awarded a MacArthur Foundation fellowship and in 1998 he received the Electronic Frontier Foundation’s pioneer award along with Torvalds.&lt;/p&gt;
&lt;p&gt;The seminar is hosted by the UNM Laboratory for High Performance Algorithm Engineering and Computational Molecular Biology, Computer Science, Electrical and Computer Engineering Departments, CIRT and the SOE. Admission is free and open to the public. Refreshments will be served.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20040214184321/http://www.unm.edu/news/Releases/03-10-02stallmanspeech.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-10-02stallmanspeech.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM to Collaborate on Two Information Technology Research Awards Through the National Science Foundation</title>
      <link>http://localhost:1313/blog/20030917-unm-itr/</link>
      <pubDate>Wed, 17 Sep 2003 07:13:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030917-unm-itr/</guid>
      <description>&lt;p&gt;The University of New Mexico will collaborate with a number of institutions on two separate Information Technology Research (ITR) “large” (over $5 million) awards announced by the National Science Foundation today. The grants, totaling more than $24 million, are two of only eight awarded from an initial field of 70.&lt;/p&gt;
&lt;p&gt;This is the second year in a row UNM is the lead institution on a large ITR grant, last year&amp;rsquo;s was the SEEK project led by Biology Professor William Michener. UNM joins Carnegie Mellon University, MIT, Cal-Berkeley, Cal-San Diego and the University of Florida as one of the six institutions ever to be the lead institution on more than one large ITR grant in the four-year history of the program.&lt;/p&gt;
&lt;p&gt;UNM leads on the $11.6 million, 13-institution effort to develop computational tools to explore evolutionary relationships among all species of living organisms forming the “Tree of Life.” Spearheaded by Project Director Bernard Moret, professor of computer science in the School of Engineering (SOE), the main collaborative institutions also include Florida State University, UC Berkeley, UC San Diego and the University of Texas-Austin.&lt;/p&gt;
&lt;p&gt;“This is an ambitious project to assemble an evolutionary Tree of Life that includes all known plants and animals,” said Terry Yates, UNM Vice Provost for Research. “It will provide a predictive and comparative framework for all fundamental and applied biology. This will basically provide the infrastructure to allow us to pursue a variety of projects that benefit society such as new drug discoveries, identify merging diseases and predict outbreaks, to discover new life forms, to improve global agriculture and many other things we couldn’t do previously because we didn’t know how these organisms were related. Developing a comprehensive understanding of life’s history will advance all biology and provide enormous benefits to society.&lt;/p&gt;
&lt;p&gt;“Assembly of a comprehensive Tree of Life is like putting a man on the moon in terms of the scope of the project. Among other things this effort of humans and resources. In addition, there’s a lot of computational challenges to handle in the assembly of roughly 1.7 million organisms.”&lt;/p&gt;
&lt;p&gt;Constructing the “Tree of Life” poses one of the most complex biological problems and represents challenges much greater than sequencing the human genome. Almost two million species of organisms have been discovered and described, yet it is estimated that tens of millions remain to be discovered. Some 60 to 70 thousand species have been studied in some detail, but the resulting data are far from complete, so relatively little is known about phylogenetic relationships of Earth&amp;rsquo;s species or among the major branches of the Tree.&lt;/p&gt;
&lt;p&gt;“Reconstructing the Tree of Life is extremely important &amp;ndash; we will get a better picture of how life has evolved on earth, a better understanding of where we come from as humans, and a sense of where life may be headed, on a very long time scale,” said Moret. “Among the many consequences of obtaining an accurate reconstruction of the Tree, our understanding of the relationships between the genetic code and cell functions will expand enormously, thus accelerating the pace of biomedical discoveries.”&lt;/p&gt;
&lt;p&gt;The relationships in the Tree of Life can be determined by comparing DNA sequences, the encoded blueprint determining the characteristics of each organism. The relative similarities between DNA sequences among different organisms allow scientists to predict the relationships of these organisms to their common ancestors.&lt;/p&gt;
&lt;p&gt;The end result is a map that describes species by their relationships to their close common ancestors and to their more distant relations, much like a family tree. The map will depict the evolutionary relationships of Earth&amp;rsquo;s taxonomic diversity &amp;ndash; including living and extinct forms &amp;ndash; over the past 3.5 billion years of its existence. Developing this map has long been a high priority for biologists, but doing so requires an extraordinary computational effort.&lt;/p&gt;
&lt;p&gt;“The computational problem is extremely difficult,” said &lt;strong&gt;David A. Bader&lt;/strong&gt;, co-investigator on the project and UNM professor of computer science. “Even with entirely novel solutions methods, an enormous amount of computational power will be required to construct the first version of such a tree.”&lt;/p&gt;
&lt;p&gt;The focus of the initiative is to establish a national resource to move the research community closer to realization of the Tree of Life. This resource will serve as an incubator to promote the development of new ideas for this enormously challenging computational task and to create a forum where experimentalists, computational biologists, and computer scientists share data, compare methods, and analyze results, thereby speeding up tool development while also sustaining current biological research projects.&lt;/p&gt;
&lt;p&gt;“In order to assemble a Tree of Life we are going to need two different things. One is a lot of data on existing species,” said Moret. “We don&amp;rsquo;t have nearly enough yet. Then, we&amp;rsquo;re going to need computational methods and computational power to take the data and make sense out of it.”&lt;/p&gt;
&lt;p&gt;“Thus the goal of our ITR project is to provide the computational infrastructure &amp;ndash; including algorithms, software, machines, and databases &amp;ndash; to support the analysis once more data have been collected,” added Moret. “We will do analyses all along the way, of course, but a full-scale attempt at reconstructing the Tree of Life will not take place for many years yet: just coming up with methods and platforms to operate at that scale will take us at least five years, not to mention that collecting enough data to support the reconstruction will require the efforts of teams of biologists all over the world for many years.”&lt;/p&gt;
&lt;p&gt;The resource will be composed of a large computational platform, a collection of interoperable high-performance software for phylogenetic analysis, and a large database of datasets (both real and simulated) and their analyses. The platform will be Internet accessible by developers, researchers and educators. The software, freely available in source form, will be usable on scales varying from laptops to supercomputers and will be packaged to be compatible with current popular tools.&lt;/p&gt;
&lt;p&gt;“We are very excited about this opportunity,” said Fran Berman, Director of the San Diego Supercomputer Center at UC San Diego. “It gives us a chance to stretch the bounds of technology to enable new science. Scientists in many fields are now confronted with large data resources and so require new kinds of tools to help them sort and understand their data. We will be working on developing critical cyberinfrastructure with the Tree of Life project.”&lt;/p&gt;
&lt;p&gt;This project will bring together researchers from many areas and foster new collaboration and styles of research in computational biology; moreover, the interaction of algorithm design, database management, large scale modeling, and biology will give fresh impetus and directions to each area. The project also aims to increase public understanding of evolutionary relationships, genomics and bioinformatics through informal education programs at its museum partners, the American Museum of Natural History, the Peabody Museum at Yale University and the Jepson Herbaria at UC Berkeley.&lt;/p&gt;
&lt;p&gt;An additional large ITR award funded this year involves UNM Computer Science Professor Stephanie Forrest. She is co-principal investigator on a $12.5 million ITR award titled, “Sensitive Information in a Wired World,” led by Stanford University Professor Dan Boneh.&lt;/p&gt;
&lt;p&gt;This project seeks to develop methods for data mining that respect and protect individual rights, but allow law enforcement and legitimate users to mine massive data sets. The research team will also develop database tools that enforce privacy policies while managing sensitive data and release tools for end-users to prevent identity theft via spoofed or malicious websites.&lt;/p&gt;
&lt;p&gt;“The idea behind the project is that more and more of our personal data and sensitive information live on the Internet and are shipped around and accessed by many intermediate parties,” said Forrest. “The question is how to protect that data. Traditional approaches to computer security focus on narrow technical concerns and this project takes a broader view, taking into account how technical solutions interact with legal and other social institutions.&lt;/p&gt;
&lt;p&gt;“The research we’ll be conducting at UNM focuses on two aspects. In the past we have studied biologically inspired methods for computer security. My role in the project is to think about biologically inspired methods for protecting data. In particular, one of the projects we’ll be working on involves privacy enhancing databases. The idea is to protect the privacy of personal information stored in databases, while still allowing legitimate activities such as epidemiological studies or searches for potential terrorists.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20031008193107/http://www.unm.edu/news/Releases/03-09-17ITR.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-09-17ITR.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Center for High Performance Computing at UNM Undergoes Reorganization Plan</title>
      <link>http://localhost:1313/blog/20030905-unm-hpc/</link>
      <pubDate>Fri, 05 Sep 2003 07:08:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030905-unm-hpc/</guid>
      <description>&lt;p&gt;Sometimes change is good, and in the case of the Center for High Performance Computing (HPC), it’s a welcomed opportunity. The HPC recently underwent a few changes including new management, mission and focus, which gives Marc Ingber, who was hired as the director earlier this year, reason to be excited.&lt;/p&gt;
&lt;p&gt;“In a sense, it’s good for the center because we can concentrate more effort on academic aspects of high performance computing,” said Ingber, who is also a professor in the Mechanical Engineering Department at the UNM School of Engineering (SOE). “The Center for HPC@UNM is the focal point for all aspects of high performance computing at UNM. We have a new, broad-based leadership team consisting of faculty from the Health Sciences Center (HSC), College of Arts &amp;amp; Sciences (A&amp;amp;S) and the School of Engineering. This group has provided the new intellectual directions for the Center.”&lt;/p&gt;
&lt;p&gt;The leadership team consists of the new associate director, Barney Maccabe from the Computer Science Department, SOE, and the faculty steering committee. The steering committee includes &lt;strong&gt;David A. Bader&lt;/strong&gt; and Wennie Shu from the Electrical and Computer Engineering Department, SOE; Vageli Coutsias from the Mathematics and Statistics Department, A&amp;amp;S; Debi Evans from the Chemistry Department, A&amp;amp;S; Richard Larson from the Pathology Department, HSC; and Tudor Oprea, from the Biochemistry and Molecular Biology Department, HSC.&lt;/p&gt;
&lt;p&gt;“The directions and goals of the Center are coming from the faculty steering committee, which is important to me,” Ingber said. “I believe in a bottom up approach. We are interested in getting as much participation from the University community as possible.&lt;/p&gt;
&lt;p&gt;“We have a strong center with national visibility built over the last 10 years. People in the community and abroad know about UNM and the HPC, which has world class faculty.”&lt;/p&gt;
&lt;p&gt;In the reorganization of the HPC, the Center has established both systems research thrusts and application research thrusts. The Center ultimately would like to merge the systems and application research with production computing using a “vertical integration strategy.” As explained by the new associate director, Barney Maccabe, vertical integration combines research in computer systems, advanced algorithms, and application science to enable new knowledge.&lt;/p&gt;
&lt;p&gt;The Center also supports a variety of educational and outreach activities to make high performance computing more accessible, not only to the UNM community, but also to regional businesses and industry.&lt;/p&gt;
&lt;p&gt;Starting in mid-September, the Center will be offering five different workshops designed to introduce faculty, students, and staff to high performance computing and facilities provided by the Center. The workshops include: Introduction to HPC@UNM; Message Passing Interface I &amp;amp; II; Fortran 90/95 and Parallel Numerical Libraries.&lt;/p&gt;
&lt;p&gt;“Researchers need increased processing speed, more memory and the ability to run larger data sets and simulations in order to investigate increasingly complex problems,” said Ingber. “At the HPC, researchers will be able to use facilities and services not available anywhere else at UNM.”&lt;/p&gt;
&lt;p&gt;For more information on the workshops offered through the Center for HPC@UNM, visit the website at &lt;a href=&#34;https://www.hpc.unm.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.hpc.unm.edu&lt;/a&gt; or call Candace Shirley at 277-9543.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20031025143953/http://www.unm.edu/news/Releases/03-09-05hpc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-09-05hpc.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Computing Faculty Collaborating with IBM to Design Next-Gen Supercomputer</title>
      <link>http://localhost:1313/blog/20030825-unm-ibm/</link>
      <pubDate>Mon, 25 Aug 2003 07:03:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030825-unm-ibm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;200404-UNM-Mirage.pdf&#34;&gt;UNM Mirage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;UNM Computing faculty &lt;strong&gt;David A. Bader&lt;/strong&gt;, Patrick Bridges, Arthur B. Maccabe and Bernard Moret, are collaborating on IBM’s Productive, Easy-to-use, Reliable, Computing Systems (PERCS) project, a new initiative to design a supercomputer several orders of magnitude faster than today’s high-end systems.&lt;/p&gt;
&lt;p&gt;IBM has received more than $53 million in funding from the Defense Advanced Research Projects Agency (DARPA) for the second phase of DARPA’s High Productivity Computing Systems (HPCS) initiative to perform this research and development effort in technology risk reduction demonstrations and a preliminary design review.&lt;/p&gt;
&lt;p&gt;The IBM PERCS project will conduct ground-breaking research in areas that include revolutionary chip technology, new computer architecture, operating systems, compiler and programming environments. PERCS is based on an integrated software-hardware co-design that will enable multi-petaflop sustained performance by 2010. A petaflop is one quadrillion calculations per second.&lt;/p&gt;
&lt;p&gt;PERCS aims at reducing the time-to-solution, starting from the inception to actual result. To this end, PERCS will include innovative middleware, compiler andprogramming environments that will be supported by hardware features to automate many phases of the program development process.&lt;/p&gt;
&lt;p&gt;“High-performance computing is a strategic area of excellence at the University of New Mexico, and this project represents UNM’s continued research leadership in advanced computing studies,” said &lt;strong&gt;Bader&lt;/strong&gt;, principal investigator for UNM’s portion of the project.&lt;/p&gt;
&lt;p&gt;Maccabe, associate director of UNM’s Center for High Performance Computing, noted that, “It is rare for universities to be involved this early in the development of a new computing system. Usually, we have to make the best use of what we are given.”&lt;/p&gt;
&lt;p&gt;Bader and Moret were part of IBM’s team on the one-year technology assessment in phase one, and will be working on architecture design and evaluation. Bridges and Maccabe join the project this year and are working on operating systems and communication.&lt;/p&gt;
&lt;p&gt;The research will be performed in cooperation with the Center for High Performance Computing, the Departments of Electrical and Computer Engineering and Computer Science, and the School of Engineering. Other university partners include MIT, University of Illinois at Urbana-Champaign, and University of Texas at Austin.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20031206001124/http://www.unm.edu/news/Releases/03-08-25percs.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-08-25percs.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader on the Challenges of Linux Clusters</title>
      <link>http://localhost:1313/blog/20030730-cio-insight/</link>
      <pubDate>Wed, 30 Jul 2003 07:57:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030730-cio-insight/</guid>
      <description>&lt;p&gt;&lt;em&gt;CIO Insight&lt;/em&gt; reporter Debra D&amp;rsquo;Agostino spoke with &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, a professor in the Electrical and Computer Engineering Department and researcher at the Center for High Performance Computing at the University of New Mexico, about the differences between Linux clusters and supercomputers, and the challenges CIOs can expect to face when evaluating the two strategies.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CIO Insight: Why are we seeing more and more companies choose Linux clusters rather than supercomputers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bader&lt;/strong&gt;: Initially, the attractive part about Linux clusters was just the price vs. performance. Linux clusters are much cheaper to acquire than supercomputers. And with the Linux cluster, the OS essentially is free. You could buy PCs, commodity PCs, get a fast network and put it together. That&amp;rsquo;s why a lot of companies, big and small, moved toward Linux clusters, although most of the commercial vendors now are offering their own versions of Linux clusters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;And is the advantage to using a Linux cluster instead of a supercomputer still mainly a matter of cost?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, in addition, while a supercomputer traditionally would have perhaps a very well engineered system, the processor might be a generation out of date. Meanwhile, a Linux cluster typically looks like a collection of the fastest PCs you can currently buy. And so you may be getting a newer generation of processors or the current generation as opposed to a previous generation, and so you&amp;rsquo;re able to buy a commodity network. A supercomputer or a Linux cluster has maybe a three-year life span as it is, so the faster you can get [current] technology into place, the greater the competitive advantage. If you&amp;rsquo;re using a technology that&amp;rsquo;s a year or two out of date and your machine has only a three-year life span, that doesn&amp;rsquo;t make a lot of sense.&lt;/p&gt;
&lt;p&gt;At this point, a Linux cluster and a supercomputer are probably very close to each other. In fact, Linux clusters are now some of the largest and fastest computers, and most of the traditional supercomputing companies, such as IBM and Sun, are modifying their operating systems so that they run Linux applications as well. Furthermore, in many of the large systems that are being delivered, the failure rate of nodes in a large Linux cluster is very similar to a supercomputer. So the distinctions between a Linux cluster and a supercomputer are really blurred at this point, and becoming more so by the day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So is this the end of supercomputers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While I think the two are blending together, at the high end, for instance, you may find you need a more tightly coupled machine or better architected machine to scale some problems to the larger sizes. And at the very high end you typically have data-intensive problems. You need to be able to have memory bandwidths on a very high scale that you may not find in commodity products.&lt;/p&gt;
&lt;p&gt;So while typical Linux clusters are often very good if you have tens or maybe hundreds of nodes, if you&amp;rsquo;re talking thousands of nodes or more, you may need a very sophisticated interconnection network, a modified operating system, a scalable scheduler to put the jobs onto the machine, and checkpoint restartability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What are some of the challenges facing CIOs who start looking to this technology?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;They&amp;rsquo;re very similar to the challenges of acquiring a supercomputer. Machine rooms need to be upgraded—the cooling, the power requirements, etc. As you go to cheaper commodities, cheaper nodes and larger clusters, you&amp;rsquo;ll start requiring more power and more cooling. That&amp;rsquo;s one challenge.&lt;/p&gt;
&lt;p&gt;Another concern is the total cost of ownership. As we have machines with lots and lots of nodes, big concerns include actually finding all the faults, making sure that everything is operating correctly, the system administration, making sure that versions are consistent across machines, security issues. While an academic or a group of enthusiasts can put together a Linux cluster themselves, really there&amp;rsquo;s a significant amount of effort that needs to take place to have a large installation. The operating system has some limitations, the networks, maintaining software across that many computers, and so forth. You need to be able to service faults in the machines, for instance; if one machine goes down, you have to be able to figure out which one it is and replace it. And if you have a long-running application, you don&amp;rsquo;t want that to hurt what you&amp;rsquo;re doing. You also need to be able to get your jobs running on the cluster. Your code has to be willing to take advantage of that and so forth.&lt;/p&gt;
&lt;p&gt;Another challenge is that applications that normally would run on a PC, for instance, may not be straightforward on a cluster. If it&amp;rsquo;s a database application, chances are there&amp;rsquo;s a port to a multiprocessor system. But if you have an internal computation, you would have to find some way to make sure that it would work on a cluster. However, if you do have applications that already have implementations that could run in parallel, then it makes the computing even more accessible. Some of the machines today have orders of magnitude more computing power than what we had just three to five years ago. And so the amount of computation is just astonishing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cioinsight.com/print/c/a/Past-News/Web-Extra-David-Bader-on-the-Challenges-of-Linux-Clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cioinsight.com/print/c/a/Past-News/Web-Extra-David-Bader-on-the-Challenges-of-Linux-Clusters&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People and Positions: Bader Elected Chair Of IEEE Committee</title>
      <link>http://localhost:1313/blog/20030711-hpcwire/</link>
      <pubDate>Fri, 11 Jul 2003 07:09:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030711-hpcwire/</guid>
      <description>&lt;p&gt;Prof. &lt;strong&gt;David A. Bader&lt;/strong&gt;, an Associate Professor and Regents&amp;rsquo; Lecturer in the
Electrical and Computer Engineering Department of The University of New
Mexico, has been elected Chair of the IEEE Computer Society&amp;rsquo;s Technical
Committee on Parallel Processing (TCPP). The Chair serves a two-year term
beginning July 1. The TCPP acts as an international forum to promote parallel
processing research and education, and participates in setting up technical
standards in this area. Issues related to the design, analysis and
implementation of parallel systems and solutions are of interest. These
include design and analysis of parallel architectures and algorithms, and
application development on parallel machines. TCPP sponsors professional
meetings, brings out publications, sets guidelines for educational programs
and coordinates academia, funding agency, and industry activities in the above
areas. The International Parallel and Distributed Processing  Symposium
(IPDPS), held annually in the Spring, serves as the flagship  activity of this
TC. Several workshops  spanning interdisciplinary areas  are sponsored by this
TC.&lt;/p&gt;
&lt;p&gt;David A. Bader is an Associate Professor and Regents&amp;rsquo; Lecturer in the
Department of Electrical and Computer Engineering of The University of New
Mexico (UNM).  He received his Ph.D. in Electrical Engineering in 1996 from
The University of Maryland, and was awarded a National Science Foundation
(NSF) Postdoctoral Research Associateship in Experimental Computer Science
before joining UNM in 1998.  He is an NSF CAREER Award recipient, an
investigator on six NSF awards including three ITR awards, a distinguished
speaker in the IEEE Computer Society Distinguished Visitors Program, and is a
member of the IBM PERCS team for the DARPA High Productivity Computing Systems
program.  Dr. Bader serves on the Steering Committees of the IPDPS and HiPC
conferences, and is the General co-Chair for IPDPS (2004-2005), and Vice
General Chair for HiPC (2002-2003). He has served on numerous conference
program committees related to parallel processing, is an associate editor for
the ACM Journal of Experimental Algorithmics in the area of parallel
algorithms, a Senior Member of the IEEE Computer Society, and a Member of the
ACM. Dr. Bader has given several Keynote Talks on high-performance computing
for problems in computational genomics.  He has co-authored over 38 articles
in peer-reviewed journals and conferences, and his main areas of research are
in parallel algorithms, combinatorial optimization, and computational biology
and genomics.&lt;/p&gt;
&lt;p&gt;Prof. &lt;strong&gt;Bader&lt;/strong&gt; is the general co-chair of IPDPS 2004 (&lt;a href=&#34;http://www.ipdps.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ipdps.org&lt;/a&gt;),
which will be held at the Eldorado Hotel in Santa Fe, April 26-30, 2004.&lt;/p&gt;
&lt;p&gt;Bader: &lt;a href=&#34;http://www.ece.unm.edu/~dbader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ece.unm.edu/~dbader&lt;/a&gt;&lt;br&gt;
TCPP:  &lt;a href=&#34;http://dsonline.computer.org/parallel/parallel_tcpp.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://dsonline.computer.org/parallel/parallel_tcpp.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Associate Professor Elected Chair of Prestigious IEEE Committee on Parallel Processing</title>
      <link>http://localhost:1313/blog/20030711-unm-tcpp/</link>
      <pubDate>Fri, 11 Jul 2003 06:59:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030711-unm-tcpp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, an associate professor and Regents’ Lecturer in the Electrical and Computer Engineering Department at the University of New Mexico, has been elected chair of the IEEE Computer Society’s Technical Committee on Parallel Processing (TCPP).&lt;/p&gt;
&lt;p&gt;He will serve a two-year term that began on July 1. The TCPP acts as an international forum to promote parallel processing research and education, and participates in setting up technical standards in the area.&lt;/p&gt;
&lt;p&gt;“New Mexico, well-known for its high performance computing research and some of the fastest computers in the world, is a great place from which to promote parallel, distributed, high performance computing education, research and technology,” said Bader. “I am honored to serve the international community.&lt;/p&gt;
&lt;p&gt;“UNM is well situated for this interest. We manage the world class Center for High Performance Computing, are lead investigators on several highly competitive National Science Foundation Information Technology Research projects and are partners with IBM in developing a revolutionary high-end computing platform under the DARPA High Productivity Computing Systems program.”&lt;/p&gt;
&lt;p&gt;Of interest are issues related to the design, analysis and implementation of parallel systems and solutions, including design and analysis of parallel architectures and algorithms and application development on parallel machines. TCPP sponsors professional meetings, releases publications, sets guidelines for educational programs and coordinates academia, funding agency and industry activities in these areas.&lt;/p&gt;
&lt;p&gt;The International Parallel and Distributed Processing  Symposium (IPDPS), held annually in the spring in locations such as Nice, France (2003), Fort Lauderdale, Fla. (2002), San Francisco, Calif. (2001) and Cancun, Mexico (2000), will be held at the Eldorado Hotel in Santa Fe in April 2004. Bader will co-chair and UNM will host the meeting that serves as the flagship activity of TCPP. Several workshops spanning interdisciplinary areas are also sponsored by committee.&lt;/p&gt;
&lt;p&gt;Bader received his Ph.D. in Electrical Engineering in 1996 from the University of Maryland and was awarded a National Science Foundation (NSF) Postdoctoral Research Associateship in Experimental Computer Science before joining UNM in 1998.&lt;/p&gt;
&lt;p&gt;He is an NSF CAREER Award recipient, an investigator on six NSF awards including three ITR awards, a distinguished speaker in the IEEE Computer Society Distinguished Visitors Program, and is a member of the IBM PERCS team for the DARPA High Productivity Computing Systems program.&lt;/p&gt;
&lt;p&gt;Bader serves on the steering committees of the IPDPS and HiPC conferences, and is the general co-chair for IPDPS (2004-2005) and vice general chair for HiPC (2002-2003). He has served on numerous conference program committees related to parallel processing, is an associate editor for the ACM Journal of Experimental Algorithmics in the area of parallel algorithms, senior member of the IEEE Computer Society and a member of the ACM.&lt;/p&gt;
&lt;p&gt;Bader has given several keynote talks and lectures on high-performance computing for problems in computational genomics.  He has co-authored more than 40 articles in peer-reviewed journals and conferences. His main areas of research are in parallel algorithms, combinatorial optimization, and computational biology and genomics.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20030821054151/http://www.unm.edu/news/Releases/03-07-11bader.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-07-11bader.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David A. Bader elected chair of IEEE Computer Society&#39;s technical committee on parallel processing</title>
      <link>http://localhost:1313/blog/20030707-abqjournal/</link>
      <pubDate>Mon, 07 Jul 2003 07:01:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030707-abqjournal/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, an associate professor and regents&amp;rsquo; lecturer in the electrical and computer engineering department at UNM, has been elected chair of the &lt;strong&gt;IEEE Computer Society&lt;/strong&gt;&amp;rsquo;s technical committee on parallel processing. The committee acts as an international forum to promote parallel processing research and education and participates in setting up international technical standards.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/436323008/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/436323008/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Applause: David A. Bader selected for IEEE Computer Society&#39;s Distinguished Visitors Program</title>
      <link>http://localhost:1313/blog/20030317-abqjournal/</link>
      <pubDate>Mon, 17 Mar 2003 07:01:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030317-abqjournal/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, an assistant professor and Regent&amp;rsquo;s Lecturer in the electrical and computer engineering department at UNM, has been selected to be a lecturer for the IEEE Computer Society&amp;rsquo;s Distinguished Visitors Program for a three-year term.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/428366766/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/428366766/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Professor Bader selected as speaker for national program</title>
      <link>http://localhost:1313/blog/20030224-unm-ieeecs-dvp/</link>
      <pubDate>Mon, 24 Feb 2003 06:48:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030224-unm-ieeecs-dvp/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;200308-UNM-Mirage.pdf&#34;&gt;UNM Mirage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, assistant professor in the Electrical and Computer Engineering Department, has been selected as an Institute of Electrical and Electronics Engineers (IEEE) Computer Society Distinguished Speaker. Bader is named to a group of about three dozen speakers from throughout the country and will serve a three-year term.&lt;/p&gt;
&lt;p&gt;“I am honored that I have been nominated and selected for this prestigious program,” Bader said. “As a distinguished speaker, I will have the support to visit with IEEE Computer Society student and professional chapters and give presentations related to my high-performance computing research.”&lt;/p&gt;
&lt;p&gt;Bader said this is an exciting way to disseminate his students’ and his latest results from research sponsored by the National Science Foundation and collaborations with industrial partners such as IBM, Sun and Myricom.&lt;/p&gt;
&lt;p&gt;Bader said he looks forward to representing UNM in North America and around the world, and plans to encourage students to pursue research-oriented careers at institutions such as UNM.&lt;/p&gt;
&lt;p&gt;“The University of New Mexico is in the company of some of the top computing departments and professionals by our participation in the program; this award truly distinguishes UNM as an international leader in computer science and engineering,” Bader said.&lt;/p&gt;
&lt;p&gt;In the program, IEEE chapters can request Bader as a speaker at any given time. Bader said he will limit his travel to once a month and will arrange travel only during days when he is not teaching.&lt;/p&gt;
&lt;p&gt;The program, initiated in 1971 by Dr. Stephen Yau, a professor and chairman of the Department of Computer Science and Engineering at Arizona State University, offers first quality speakers serving IEEE Computer Society professional and student chapters, and speakers are recognized authorities in their respective fields.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20030415061211/http://www.unm.edu/news/Releases/03-02-24bader.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/03-02-24bader.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IEEE Computer Society selects David Bader for Distinguished Visitors Program</title>
      <link>http://localhost:1313/blog/20030203-ieeecs/</link>
      <pubDate>Mon, 03 Feb 2003 18:40:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20030203-ieeecs/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20030203-ieeecs/letter_hu_bdb62b23e2e36a51.webp 400w,
               /blog/20030203-ieeecs/letter_hu_3e027d5c66f93467.webp 760w,
               /blog/20030203-ieeecs/letter_hu_54ca7fd4669ddd80.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20030203-ieeecs/letter_hu_bdb62b23e2e36a51.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Prof. Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;The IEEE Computer Society is pleased to announce that you have been
selected to participate in the &lt;strong&gt;Distinguished Visitors Program
(DVP)&lt;/strong&gt; for a three-year term starting in January 2003.&lt;/p&gt;
&lt;p&gt;On behalf of the IEEE Computer Society, congratulations and we look
forward to working with you in the year to come.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;br&gt;
Murali Varanasi&lt;br&gt;
Vice President, Chapter Activities Board&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Assembling The Tree of Life</title>
      <link>http://localhost:1313/blog/20021101-nsf-tol/</link>
      <pubDate>Fri, 01 Nov 2002 18:45:19 -0400</pubDate>
      <guid>http://localhost:1313/blog/20021101-nsf-tol/</guid>
      <description>&lt;p&gt;“Simple identification via phylogenetic classification of organisms
has, to date, yielded more patent filings than any other use of
phylogeny in industry.”  &lt;strong&gt;Bader&lt;/strong&gt; et al. (2001)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Journal Of Parallel &amp; Distributed Computing: Special Issue</title>
      <link>http://localhost:1313/blog/20021025-hpcwire/</link>
      <pubDate>Fri, 25 Oct 2002 22:40:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/20021025-hpcwire/</guid>
      <description>&lt;p&gt;CALL FOR PAPERS&lt;/p&gt;
&lt;p&gt;Journal of Parallel and Distributed Computing&lt;/p&gt;
&lt;p&gt;Special Issue on High-Performance Computational Biology&lt;/p&gt;
&lt;p&gt;Guest Editors:&lt;/p&gt;
&lt;p&gt;Prof. Srinivas Aluru&lt;br&gt;
Electrical &amp;amp; Computer Engg.&lt;br&gt;
Iowa State University&lt;br&gt;
3218 Coover Hall&lt;br&gt;
Ames, IA 50014 USA&lt;br&gt;
Email: &lt;a href=&#34;mailto:aluru@iastate.edu&#34;&gt;aluru@iastate.edu&lt;/a&gt;&lt;br&gt;
Tel: 515-294-3539&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prof. David A. Bader&lt;/strong&gt;&lt;br&gt;
Electrical &amp;amp; Computer Engg.&lt;br&gt;
University of New Mexico&lt;br&gt;
Albuquerque, NM 87131 USA&lt;br&gt;
Email: &lt;a href=&#34;mailto:dbader@eece.unm.edu&#34;&gt;dbader@eece.unm.edu&lt;/a&gt;&lt;br&gt;
Tel: 505-277-6724&lt;/p&gt;
&lt;p&gt;Computational Biology is fast emerging as an important discipline for academic
research and industrial application. The large size of biological data sets,
inherent complexity of biological problems and the necessity to deal with
error-prone data all result in large run-time and memory requirements.
Biological sequence databases are growing at an exponential rate. Recent
invention of microarrays has facilitated high throughput studies of gene
expression, allowing researchers to simultaneously sample the expression
levels of tens of thousands of genes. All of these factors will make the
application of parallel and distributed processing increasingly important in
computational biology.&lt;/p&gt;
&lt;p&gt;The goal of this special issue is to provide a forum for the publication of
important research contributions in developing high-performance computing
solutions to problems arising from molecular biology. We are especially
interested in parallel algorithms for solving fundamental as well as applied
problems, memory-efficient algorithms, large scale data mining techniques, and
design of high-performance software.&lt;/p&gt;
&lt;p&gt;Manuscripts submitted for this special issue should describe significant
original research in the field of computational biology and be relevant to
aims and scope of the Journal of Parallel and Distributed Computing. Topics of
interest include but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bioinformatic databases&lt;/li&gt;
&lt;li&gt;Computational genomics&lt;/li&gt;
&lt;li&gt;Computational proteomics&lt;/li&gt;
&lt;li&gt;DNA assembly, clustering, and mapping&lt;/li&gt;
&lt;li&gt;Gene expression and microarrays&lt;/li&gt;
&lt;li&gt;Gene identification and annotation&lt;/li&gt;
&lt;li&gt;Parallel algorithms for biological analysis&lt;/li&gt;
&lt;li&gt;Parallel architectures for biological applications&lt;/li&gt;
&lt;li&gt;Molecular evolution&lt;/li&gt;
&lt;li&gt;Molecular sequence analysis&lt;/li&gt;
&lt;li&gt;Phylogeny reconstruction algorithms&lt;/li&gt;
&lt;li&gt;Protein structure&lt;/li&gt;
&lt;li&gt;String data structures and algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Submission Guidelines:&lt;/p&gt;
&lt;p&gt;Interested authors should submit an electronic version of the manuscript
either in postscript or PDF format as an email attachment to one of the guest
editors. If you are unable to submit electronically, please submit a hardcopy.
The manuscript should be prepared according to the guidelines for the Journal
of Parallel and Distributed Computing, available at
&lt;a href=&#34;http://www.academicpress.com/www/journal/pc/pcifa.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.academicpress.com/www/journal/pc/pcifa.htm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Schedule:&lt;/p&gt;
&lt;p&gt;November 15, 2002&lt;br&gt;
Deadline for manuscript submission&lt;/p&gt;
&lt;p&gt;February 1, 2003&lt;br&gt;
Feedback to authors&lt;/p&gt;
&lt;p&gt;April 1, 2003&lt;br&gt;
Revised paper due&lt;/p&gt;
&lt;p&gt;May 15, 2003&lt;br&gt;
Notification of Final Decision&lt;/p&gt;
&lt;p&gt;August 2003&lt;br&gt;
Publication of Special Issue&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC2002: Discussions to Cover Computation and Controversy</title>
      <link>http://localhost:1313/blog/20021018-hpcwire/</link>
      <pubDate>Fri, 18 Oct 2002 22:29:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20021018-hpcwire/</guid>
      <description>&lt;p&gt;Attendees of SC2002 will be treated to a thought-provoking set of panel
discussions on topics from homeland security to innovations in high-end
computing to the impact of the Earth Simulator, the world&amp;rsquo;s fastest
supercomputer. This year&amp;rsquo;s conference, with the theme &amp;ldquo;From Terabytes to
Insights,&amp;rdquo; will convene Nov. 16-22 at the Baltimore Convention Center.&lt;/p&gt;
&lt;p&gt;Some of the best-known experts in the field will lead the discussions on
significant questions and major accomplishments in high performance computing,
including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;High End Information Technology Requirements for Homeland Security. &lt;br&gt;
Panelists: Chaitan Baru, University of California, San Diego; Cray Henry,
Larry Davis, DoD High Performance Computing Modernization Program; Russ
Graves, MITRE; Joe Picciano, FEMA; Ted Senator, DARPA Information Awareness
Office &lt;br&gt;
Panelists will discuss the role of HPC resources in homeland security,
including: managing large, heterogeneous databases; balancing the need for
information sharing and security; and molecular dynamics simulations as
countermeasures to biological threats.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Planning for a Homeland Security Research Agenda &lt;br&gt;
Panelists: Paul Rosenbloom, University of Southern California Information
Sciences Institute; Tom DeFanti, University of Illinois at Chicago; Stephen
Squires, Hewlett Packard; Lee Holcomb, U.S. Office of Homeland Security;
Art L. Money, consultant; Peter Freeman, National Science Foundation
Experts will examine the issues involved in developing a long-term research
and development agenda for homeland security using ideas from recent
workshops, emerging industrial technologies, defense department
experiences, and academia.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The 40 Tflop/s Earth Simulator System: Its Impact on the Future
Development of Supercomputing &lt;br&gt;
Panelists: David Bader, DOE Office of Biological and Environmental
Research; David Kahaner, Asian Technology Information Program, Tokyo;
Burton Smith, Cray, Inc.; Hisashi Nakamura, Research Organization for
Information Science &amp;amp; Technology, Tokyo. &lt;br&gt;
This panel will look beyond the initial excitement over the introduction of
the Earth Simulator System, the world&amp;rsquo;s fastest computer, to the possible
scientific advantages that could be gained from such a huge performance
leap and the possible impact of ESS on commodity-based supercomputers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HPCS: Achieving high-end computing productivity &lt;br&gt;
Panelists: Marty Deneroff, SGI; Mootaz Elnozahy, IBM; Richard Kaufmann,
Hewlett Packard; Richard Games, MITRE; Burton Smith, Cray, Inc.; Danny
Cohen, Sun Microsystems Inc. &lt;br&gt;
Participants will discuss the High Productivity Computing Systems (HPCS)
Program, initiated by DARPA to bring a broad spectrum of innovative
technologies and architectures into DoD computing systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Computational biology and high performance computing &lt;br&gt;
Panelists: Chris Johnson, University of Utah; John Reynders, Celera; &lt;strong&gt;David
Bader&lt;/strong&gt;, University of New Mexico; Debra Goldfarb, IDC; Rick Stevens,
Argonne National Laboratory/ University of Chicago &lt;br&gt;
Bioinformatics, genomics, and other biological sciences promise to more
than double the size of the high performance computing market. This panel
will discuss HPC strategies for solving biological and biomedical problems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Truth and Consequences: The Making of Discovery Channel&amp;rsquo;s &amp;ldquo;Unfolding
Universe&amp;rdquo; &lt;br&gt;
Panelists: Tom Lucas , Tom Lucas Productions, Inc.; Ed Seidel, Max Planck
Institute; Neil Tyson, American Museum of Natural History; Michael Norman,
University of California, San Diego; Robert Patterson, NCSA &lt;br&gt;
The Discovery Channel documentary &amp;ldquo;Unfolding Universe&amp;rdquo; includes extensive
astronomy, astrophysics, computational science and scientific visualizations.
This panel promises a lively discussion/debate on how to best present
science to the general public.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Are Designer Supercomputers an Endangered Species? &lt;br&gt;
Panelists: Thomas Sterling, Center for Advanced Computing Research,
California Institute of Technology; Gita Alaghband, University of Colorado,
Denver; Jamshed Mirza, IBM; Tadaski Watanabe, NEC; Candace Culhane, NSA. &lt;br&gt;
Will the emergence of Components Off-the-Shelf (COTS) clusters and
distributed computing projects lead to the end of custom-designed machines?
Or will the new Earth Simulator System result in a resurgence of
specialized vector machines?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Desktop Grids: 10,000-fold Parallelism for the Masses &lt;br&gt;
Panelists: Kim Baldridge, San Diego Supercomputer Center; David Ceperley,
University of Illinois at Urbana-Champaign; Andrew Chien, University of
California at San Diego; David Dixon, Pacific Northwest National
Laboratory; John Reynders, Celera &lt;br&gt;
Internet distributed computing projects have used hundreds of thousands of
processors to solve problems and may represent a breakthrough technology.
Panelists will explore how to accelerate the use of distributed desktop grids.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More about panels, including abstracts and a schedule, can be found by
going to  &lt;a href=&#34;http://www.sc-conference.org/sc2002/program&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sc-conference.org/sc2002/program&lt;/a&gt; and clicking on
&amp;ldquo;panels.&amp;rdquo;  To register for SC2002, see
&lt;a href=&#34;http://www.sc-conference.org/sc2002/attendees&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.sc-conference.org/sc2002/attendees&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SC2002, the annual high performance networking and computing conference,
brings together scientists, engineers, educators, visualization artists,
programmers, and business leaders to share ideas and glimpse the future of
high performance networking and computing, data analysis and management,
visualization, and computational modeling. SC2002 is sponsored by the
Institute of Electrical and Electronics Engineers Computer Society and by
the Association for Computing Machinery&amp;rsquo;s Special Interest Group on
Computer Architecture.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Large-Scale Phylogenetic Analysis</title>
      <link>http://localhost:1313/blog/20020829-utaustin/</link>
      <pubDate>Thu, 29 Aug 2002 21:20:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20020829-utaustin/</guid>
      <description>&lt;p&gt;The goal of phylogenetic analysis is to reconstruct the evolutionary
history of different taxa (e.g., species, genera). Recent
advances in molecular biology and genomics have provided
biologists with molecular data at an unprecedented
rate and scale. New approaches are necessary because the
most accurate analyses are obtained through solving (or attempting
to solve) NP-hard optimization problems. Furthermore,
any such analysis can return hundreds or thousands
of trees. Finally, some taxa evolve down networks rather
than down trees. Our research uses discrete algorithms, graph
theory, probability theory, and experimental performance
analyses. Below we describe just three of several ongoing
projects in the group.&lt;/p&gt;
&lt;h2 id=&#34;1-genome-rearrangement-phylogeny&#34;&gt;1. Genome Rearrangement Phylogeny:&lt;/h2&gt;
&lt;p&gt;Whole genomes evolve via events, such as inversions, transpositions,
deletions, and duplications, that change the order
and content of genes within genomes. Such events are relatively
rare, compared to nucleotide substitutions, and thus
contain significantly stronger phylogenetic “signal”. Our
group works on developing new methods for reconstructing
phylogenies on whole genomes. The software suite
GRAPPA, a result of collaboration with Bernard Moret and
&lt;strong&gt;David Bader&lt;/strong&gt; at the University of New Mexico, is several
orders of magnitude faster than the previous best software
for this kind of problem.&lt;/p&gt;
&lt;h2 id=&#34;2-absolute-fast-converging-methods&#34;&gt;2. Absolute Fast Converging Methods:&lt;/h2&gt;
&lt;p&gt;Statisticians are interested in the sequence lengths needed
by methods to reconstruct trees under Markov models of
evolution. Our new “absolute fast converging methods” are
methods that recover the true tree from polynomial lengths.
Our experimental performance analyses show that our new
methods greatly out perform other polynomial time methods
with respect to topological accuracy, especially on big
trees.&lt;/p&gt;
&lt;h2 id=&#34;3-visualization-and-clustering-of-sets-of-phylogenetic-trees&#34;&gt;3. Visualization and Clustering of Sets of Phylogenetic Trees:&lt;/h2&gt;
&lt;p&gt;When a phylogenetic analysis of a dataset produces thousands
of equally good trees, biologists summarize the information
in the analysis with a consensus tree. We replace the
traditional one-consensus approach with multi-consensus
methods using clustering of trees, and develop tools for visualizing
the distributions of trees in tree space.
Large-Scale Phylogenetic Analysis&lt;/p&gt;
&lt;h2 id=&#34;faculty&#34;&gt;Faculty&lt;/h2&gt;
&lt;h3 id=&#34;computer-science&#34;&gt;Computer Science&lt;/h3&gt;
&lt;p&gt;Tandy Warnow &lt;br&gt;
Nina Amenta &lt;br&gt;
Bernard Moret (U. New Mexico) &lt;br&gt;
&lt;strong&gt;David Bader&lt;/strong&gt; (U. New Mexico) &lt;br&gt;
Katherine St. John (CUNY) &lt;br&gt;
Tamara Munzner (Compaq)&lt;/p&gt;
&lt;h3 id=&#34;biology&#34;&gt;Biology&lt;/h3&gt;
&lt;p&gt;Robert Jansen &lt;br&gt;
Randy Linder &lt;br&gt;
Linda Raubeson (Central Wash. U)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cs.utexas.edu/users/phylo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.utexas.edu/users/phylo&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Post-Doc receives Sloan Foundation fellowship </title>
      <link>http://localhost:1313/blog/20020501-unm-sloan/</link>
      <pubDate>Wed, 01 May 2002 06:45:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20020501-unm-sloan/</guid>
      <description>&lt;p&gt;University of New Mexico Post-Doctoral student Tiffani Williams has been awarded an Alfred P. Sloan Foundation Post-doctoral Fellowship in Computational Molecular Biology for two years. Twenty-six past Sloan Fellows have become Nobel Laureates.&lt;/p&gt;
&lt;p&gt;Williams will work with School of Engineering Professors Bernard Moret, Computer Science, and &lt;strong&gt;David Bader&lt;/strong&gt;, Electrical and Computer Engineering. She also taught one course in computer science at UNM this semester.&lt;/p&gt;
&lt;p&gt;Williams’ research is in the area of phylogenetic reconstruction, the inference of the evolutionary history of a collection of organisms. These relationships are graphically represented by as a phylogenetic tree, where modern organisms are placed at the leaves, ancestral organisms occupy internal nodes, and the edges of the tree denote the evolutionary relationships. Such reconstructions are based on molecular data such as DNA sequences collected from present-day species and on a hypothesized model of evolution. From a computational standpoint, phylogeny reconstruction is enormously expensive. Williams will use high-performance computers to increase the speed of existing phylogeny reconstruction algorithms.&lt;/p&gt;
&lt;p&gt;“Basically my research will reconstruct evolutionary trees and find out how they all relate,” Williams said. “I will look at different techniques that build these trees.”&lt;/p&gt;
&lt;p&gt;She said that the research will lead to the development of better drugs and products. Agricultural laboratories use phylogenetic research to produce better strains of basic foods such as rice or wheat. In addition, public health researchers use phylogenies to track the spread of various strains of the HIV virus (the cause of AIDS). Pharmaceutical companies use them to identify likely drug targets.&lt;/p&gt;
&lt;p&gt;Williams received her Ph.D. in computer science from the University of Central Florida (UCF) and her bachelor’s degree in computer science from Marquette University in Milwaukee, Wis. She was a instructor/visiting lecturer at UCF from 1996-2001. From 1995-96, she was an undergraduate advisor at UCF.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20020619061841/http://www.unm.edu/news/Releases/May1williams.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/May1williams.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Judge for the New Mexico High School Supercomputing Challenge</title>
      <link>http://localhost:1313/blog/20020428-nmhighschool/</link>
      <pubDate>Sun, 28 Apr 2002 18:33:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20020428-nmhighschool/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20020428-nmhighschool/NMHighSchool_hu_fbecf44abd19599d.webp 400w,
               /blog/20020428-nmhighschool/NMHighSchool_hu_3822971f9ae65913.webp 760w,
               /blog/20020428-nmhighschool/NMHighSchool_hu_9335d8d923df11ff.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20020428-nmhighschool/NMHighSchool_hu_fbecf44abd19599d.webp&#34;
               width=&#34;584&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://supercomputingchallenge.org/01-02/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://supercomputingchallenge.org/01-02/index.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Receives 2002 Lawton-Ellis Award</title>
      <link>http://localhost:1313/blog/20020325-unm-lawton-ellis/</link>
      <pubDate>Mon, 25 Mar 2002 21:38:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20020325-unm-lawton-ellis/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20020325-unm-lawton-ellis/award_hu_9d25a87629b09c7.webp 400w,
               /blog/20020325-unm-lawton-ellis/award_hu_cdb354c0d2bc1984.webp 760w,
               /blog/20020325-unm-lawton-ellis/award_hu_b75e3d584ab5dcef.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20020325-unm-lawton-ellis/award_hu_9d25a87629b09c7.webp&#34;
               width=&#34;476&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Presented to &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt; From the Chair and Faculty of the Department of Electrical and Computing Engineering, University of New Mexico, as a recipient of the 2002 Lawton-Ellis Award&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM to join in Linux supercomputing effort</title>
      <link>http://localhost:1313/blog/20020102-cnet/</link>
      <pubDate>Wed, 02 Jan 2002 13:37:43 -0500</pubDate>
      <guid>http://localhost:1313/blog/20020102-cnet/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Stephen Shankland&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The computer, called LosLobos, will connect 256 two-processor IBM Intel-based servers with high-speed Myrinet network cards, said John Patrick, vice president of IBM Internet technology. The machine will be able to perform 375 billion calculations per second, &lt;strong&gt;UNM&lt;/strong&gt; said.&lt;/p&gt;
&lt;p&gt;Though UNM and its partners in the National Computational Science Alliance intend to use LosLobos for scientific purposes, IBM has its own, more commercial agenda. It believes LosLobos will help researchers adapt this &amp;ldquo;cluster&amp;rdquo; approach to running IBM software for business tasks such as email, database hosting, instant messaging or e-commerce, he said.&lt;/p&gt;
&lt;p&gt;Linux, a clone of the Unix operating system, has displayed remarkable versatility in its spread across the computing landscape. In addition to its most common use in low-end servers, it also is making inroads into sub-PC gadgets and supercomputers.&lt;/p&gt;
&lt;p&gt;Of the major hardware companies, Compaq has been the strongest backer of so-called &amp;ldquo;Beowulf&amp;rdquo; computers, which share a computing task across many interconnected computers, most often running Linux and special software to pass messages among the different nodes. But Compaq, with its high-performance Alpha chip, has been aiming mostly at number-crunchers.&lt;/p&gt;
&lt;p&gt;Beowulf systems have been popular with scientists who need inexpensive systems to run simulations and other mathematically intense operations. Business use has been limited to number-crunchers such as Amerada Hess, which built a 32-computer Beowulf system from Dell computers.&lt;/p&gt;
&lt;p&gt;The Beowulf technique may be a great way to gang together lots of cheap computers, but the catch is that software must be extensively rewritten to use the system&amp;ndash;and not all computing operations are amenable to being spread across a lot of independent machines.&lt;/p&gt;
&lt;p&gt;IBM hopes to hasten the day that clusters of cheap computers become more useful. &amp;ldquo;I would expect we will uncover certain things that Linux is missing that we have somewhere in IBM,&amp;rdquo; Patrick said. &amp;ldquo;Our intention is to apply all the technology and resources we can to help it grow as fast as it possibly can. We intend to contribute those technologies into the open-source community,&amp;rdquo; Patrick said.&lt;/p&gt;
&lt;p&gt;Specifically, IBM intends to work on making programs more &amp;ldquo;multithreaded,&amp;rdquo; meaning that tasks are divided into independent jobs that can be divvied up more easily across a multitude of computers, Patrick said. The company also is working on how to spread database access tasks across many machines and how best to communicate with centralized data storage systems.&lt;/p&gt;
&lt;p&gt;Linux companies see clustering as big business. TurboLinux, for example, offers enFuzion software that lets all sorts of computers&amp;ndash;even the Windows boxes in the accounting department that sit around idly all night long&amp;ndash;be harnessed to crunch numbers. EnFuzion is in use at JP Morgan and Rockefeller University.&lt;/p&gt;
&lt;p&gt;New Mexico has a host of high-profile Beowulf systems. Los Alamos National Laboratory built its Avalon system in 1998, Sandia National Laboratories in Albuquerque has its C-Plant cluster and UNM has already a 128-processor machine.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnet.com/news/ibm-to-join-in-linux-supercomputing-effort/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnet.com/news/ibm-to-join-in-linux-supercomputing-effort/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>$1.1 million grant goes to professors</title>
      <link>http://localhost:1313/blog/20011018-abqjournal/</link>
      <pubDate>Thu, 18 Oct 2001 07:01:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/20011018-abqjournal/</guid>
      <description>&lt;p&gt;University of New Mexico School of Engineering professors Bernard Moret of Computer Science and &lt;strong&gt;David Bader&lt;/strong&gt; of Electrical and Computer Engineering have received more than $1 million in grants this fall from the National Science Foundation for research in reconstructing evolutionary &amp;ldquo;trees.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The $1.1 million is broken up into three grants for different aspects of the project.&lt;/p&gt;
&lt;p&gt;The research is being conducted in collaboration with the University of Texas at Austin.&lt;/p&gt;
&lt;p&gt;Phylogeny, or an evolutionary &amp;ldquo;tree,&amp;rdquo; reconstructs the evolutionary history of modern organisms from unknown ancestral organisms through the process of bifurcation, in which an ancestral species gives rise to two new species.&lt;/p&gt;
&lt;p&gt;The reconstructions are based on molecular data such as DNA sequences collected from modern species and on a hypothesized model of evolution.&lt;/p&gt;
&lt;p&gt;Moret said the work tracks the spread of strains of the HIV virus and agricultural labs use the information to produce better strains of basic foods such as rice or wheat.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/379408617/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/379408617/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Engineering Professors receive $1.1 million in NSF Grants</title>
      <link>http://localhost:1313/blog/20011011-unm-nsfgrants/</link>
      <pubDate>Thu, 11 Oct 2001 06:37:29 -0400</pubDate>
      <guid>http://localhost:1313/blog/20011011-unm-nsfgrants/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;200202-UNM-Mirage.pdf&#34;&gt;UNM Mirage&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;University of New Mexico School of Engineering Professors Bernard Moret, Computer Science, and &lt;strong&gt;David Bader&lt;/strong&gt;, Electrical and Computer Engineering, have received more than $1.1 million in grants this fall from the National Science Foundation (NSF) to pursue research in reconstructing evolutionary trees (known as “phylogenies”).&lt;/p&gt;
&lt;p&gt;This research program is being conducted in collaboration with the University of Texas at Austin. The UT-UNM group received two other awards from the NSF, made directly to the University of Texas, bringing the total group funding for the next five years to more than $7 million. This makes the joint UT-UNM research group the largest and most active group in the world in the area of computational phylogenetics.&lt;/p&gt;
&lt;p&gt;A phylogeny, or evolutionary tree, is a reconstruction of the evolutionary history of a collection of present-day organisms from unknown ancestral organisms through the process of bifurcation, in which an ancestral species gives rise to two new species. Such reconstructions are based on molecular data such as DNA sequences collected from present-day species and on a hypothesized model of evolution.&lt;/p&gt;
&lt;p&gt;Moret said the project will create tools for biologists to conduct research on evolution. He said that phylogenetic analyses are used in biological, medical and pharmaceutical research.&lt;/p&gt;
&lt;p&gt;“For instance, public health researchers use phylogenies to track the spread of various strains of the HIV virus (the cause of AIDS). Pharmaceutical companies use them to identify likely drug targets. A notable success was the herbicide Roundup,” he said, adding that agricultural laboratories use them to produce better strains of basic foods such as rice or wheat.&lt;/p&gt;
&lt;p&gt;Moret said that the UNM team focuses on the development, implementation and testing of computer algorithms to reconstruct phylogenies, with particular emphasis on efficiency and high-performance computing—because many of the algorithms in use today can require years of computation even on problems of modest size.&lt;/p&gt;
&lt;p&gt;Bader said that high-performance computers, which are now used for time-sensitive applications such as weather forecasting, financial investment decision-making, or tracking and guidance systems, will be used for these computational biology problems.&lt;/p&gt;
&lt;p&gt;“The shortened turnaround time will speed up the pace of research, leading to more and less expensive new drugs, for instance, and also improve its quality by enabling researchers to test more alternatives and thus gain a better understanding of the problems,” Bader said.&lt;/p&gt;
&lt;p&gt;The $1.1 million is broken up into three separate grants, which deal with different aspects of the project. Two of the grants were awarded under the competitive Information Technology Research (ITR) programs.&lt;/p&gt;
&lt;p&gt;A three-year grant for $162,000 titled, “Computing Optimal Phylogenetic Trees under Genome Rearrangement Metrics,” was one in 1,295 proposals received by NSF in its ITR category—only 10 percent of these proposals were funded. Under this and the matching UT grant, Moret, with Robert Jansen, UT biology professor, and Tandy Warnow, UT computer science professor, will design, implement and evaluate new algorithms to reconstruct phylogenies from a new type of molecular data, the complete ordering of genes along the chromosomes.&lt;/p&gt;
&lt;p&gt;A five-year grant for $793,000 titled, “Reconstructing Complex Evolutionary Histories,” was one in 661 pre-proposals in its ITR category received by NSF; again only 10 percent of these pre-proposals were eventually selected for funding. Under this and the matching UT grant, Moret and Bader will collaborate with Warnow and several UT-Austin biologists to tackle complex and more realistic models of evolution and develop algorithms that can reconstruct phylogenies under these models for very large problems.&lt;/p&gt;
&lt;p&gt;A five-year grant for $193,000 is part of a larger grant titled, “Comparative Chloroplast Genomics: Integrating Computational Methods, Molecular Evolution, and Phylogeny,” awarded to UT- Austin under an NSF program on Biocomplexity in the Environment. The UNM team led by Moret will help the biologists at UT Austin and other institutions in analyzing new genomic data collected from a sampling of land plants and refined into complete gene orderings at the Department of Energy’s Joing Genome Institute in California.&lt;/p&gt;
&lt;p&gt;Moret said that funding from NSF is the “gold standard” in the scientific community. All proposals undergo rigorous peer evaluation and fierce competition, as indicated by the funding rate of 10 percent. “Receiving five separate awards in the same year from the NSF for work in the same area is nearly unheard of and a tribute to the success of the UT-UNM collaboration of computer scientists and biologists,” Moret said.&lt;/p&gt;
&lt;p&gt;At UNM, these grants will enhance the profile of the research laboratory run by Moret and Bader, which is already funded by four other NSF awards. This includes a three-year ITR award to Moret and Bader granted last year, a five-year CAREER Award to Bader, a three-year biology award in which Bader is a co-PI, and part of a research infrastructure award to the Department of Computer Science.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20011124182239/http://www.unm.edu/news/Releases/Oct11nsf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/news/Releases/Oct11nsf.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputing Strategies</title>
      <link>http://localhost:1313/blog/20010430-canden/</link>
      <pubDate>Mon, 30 Apr 2001 15:20:27 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010430-canden/</guid>
      <description>&lt;p&gt;&lt;em&gt;Elizabeth K. Wilson, C&amp;amp;EN West Coast News Bureau&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The parallel supercomputers of today come in a number of &amp;ldquo;flavors,&amp;rdquo; and their different styles of crunching data have led to as many different varieties of hardware and software.&lt;/p&gt;


















&lt;figure  id=&#34;figure-computer-prowess-the-highest-occupied-molecular-orbital-in-the-910-dimethyleneanthracene-photodimer-was-computed-by-baldridge-and-imaged-by-johnson-of-sdsc-using-volume-graphics-techniques&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;COMPUTER PROWESS. The highest occupied molecular orbital in the 9,10-dimethyleneanthracene photodimer was computed by Baldridge and imaged by Johnson of SDSC using volume graphics techniques.&#34; srcset=&#34;
               /blog/20010430-canden/7918photodimer_hu_ace1977e695c6ecb.webp 400w,
               /blog/20010430-canden/7918photodimer_hu_c9a047899b24d481.webp 760w,
               /blog/20010430-canden/7918photodimer_hu_d0de659299b6f865.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918photodimer_hu_ace1977e695c6ecb.webp&#34;
               width=&#34;249&#34;
               height=&#34;182&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      COMPUTER PROWESS. The highest occupied molecular orbital in the 9,10-dimethyleneanthracene photodimer was computed by Baldridge and imaged by Johnson of SDSC using volume graphics techniques.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Garden-variety parallel architecture and problem-solving present numerous well-known networking and programming challenges. For example, many computational chemistry codes, be they for ab initio quantum calculations or for molecular dynamics simulations, were written decades ago for machines that process data sequentially. Over the years, chemists have undertaken the difficult task of modifying the codes to run on massively parallel machines with dozens, even hundreds, of processors.&lt;/p&gt;
&lt;p&gt;But on top of that, some new architectures are complicating the picture. They include, most notably, the fantastically popular cluster-style computers&amp;ndash;groups of small machines hooked together. Because they can be put together from off-the-shelf PCs, their cost is very low. Yet they can pack as much computational punch as more expensive Crays or IBMs. Chemists have a particular interest in clusters because of the increasing prevalence of modeling and simulations in everyday research (&lt;a href=&#34;http://cen.acs.org/isubscribe/journals/cen/78/i02/html/7802scit1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C&amp;amp;EN, Jan. 10, 2000, page 27&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;BUT MUCH MORE is involved in assembling a cluster than setting a few PCs next to each other and expecting them to talk. They require special communications tools and carry a number of complications in addition to those of traditional parallel computing. And recently, scientists have been experimenting with yet another new computer structure: clusters of clusters.&lt;/p&gt;
&lt;p&gt;Surprisingly, chemists are contributing a good deal to the development of a field that one might expect to be populated exclusively by computer scientists. In addition to modifying chemistry software, they&amp;rsquo;re designing general tools to make clusters and their ilk run better, and these accomplishments were outlined at a symposium on new computer architectures at the American Chemical Society meeting in San Diego earlier this month.&lt;/p&gt;


















&lt;figure  id=&#34;figure-jackman-photos-by-elizabeth-wilson&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Jackman. *Photos by Elizabeth Wilson*&#34; srcset=&#34;
               /blog/20010430-canden/7918sci1x_hu_546657f95251e0ec.webp 400w,
               /blog/20010430-canden/7918sci1x_hu_75957df981a99243.webp 760w,
               /blog/20010430-canden/7918sci1x_hu_6ffed7e7fcec0b9a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918sci1x_hu_546657f95251e0ec.webp&#34;
               width=&#34;87&#34;
               height=&#34;123&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Jackman. &lt;em&gt;Photos by Elizabeth Wilson&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-pineda-photos-by-elizabeth-wilson&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Pineda. *Photos by Elizabeth Wilson*&#34; srcset=&#34;
               /blog/20010430-canden/7918sci4x_hu_c7a7d63232fd27a6.webp 400w,
               /blog/20010430-canden/7918sci4x_hu_21395781caa270f7.webp 760w,
               /blog/20010430-canden/7918sci4x_hu_92efe99aad8f53f2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918sci4x_hu_c7a7d63232fd27a6.webp&#34;
               width=&#34;87&#34;
               height=&#34;123&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pineda. &lt;em&gt;Photos by Elizabeth Wilson&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-baldridge-photos-by-elizabeth-wilson&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Baldridge. *Photos by Elizabeth Wilson*&#34; srcset=&#34;
               /blog/20010430-canden/7918sci3x_hu_e3e2a59cccc71a02.webp 400w,
               /blog/20010430-canden/7918sci3x_hu_8e53b1dbb8d8738b.webp 760w,
               /blog/20010430-canden/7918sci3x_hu_93c57ad83bb35976.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918sci3x_hu_e3e2a59cccc71a02.webp&#34;
               width=&#34;87&#34;
               height=&#34;123&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Baldridge. &lt;em&gt;Photos by Elizabeth Wilson&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-gordon-photos-by-elizabeth-wilson&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Gordon. *Photos by Elizabeth Wilson*&#34; srcset=&#34;
               /blog/20010430-canden/7918sci2x_hu_10c91301aed5c416.webp 400w,
               /blog/20010430-canden/7918sci2x_hu_dc1b9d885156b795.webp 760w,
               /blog/20010430-canden/7918sci2x_hu_9ff0f2f4a57553df.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918sci2x_hu_10c91301aed5c416.webp&#34;
               width=&#34;88&#34;
               height=&#34;123&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Gordon. &lt;em&gt;Photos by Elizabeth Wilson&lt;/em&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Sponsored by the &lt;a href=&#34;http://membership.acs.org/C/COMP/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Division of Computers in Chemistry&lt;/a&gt;, the symposium also highlighted new research on ways to visualize data, new hardware designed to speed up lengthy calculations, and numerous tests of parallel versions of computational chemistry software on these new architectures.&lt;/p&gt;
&lt;p&gt;Most of these complexities arise from the fundamental issue of how processors are grouped. For the past few years, supercomputers have fallen into two basic classes, explained Andrew C. Pineda, a chemist at the &lt;a href=&#34;http://oaserv1.ahpcc.unm.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;University of New Mexico&amp;rsquo;s Albuquerque High Performance Computing Center (AHPCC)&lt;/a&gt; and organizer of the symposium. The first, known as a distributed single processor, consists of individual processors with their own memory connected to other processors by a high-speed communications network.&lt;/p&gt;
&lt;p&gt;THE SECOND TYPE is the shared-memory multiprocessor, where a number of processors sit on one board, sharing a bus and internal memory. The problem with this strategy, however, is that only so many processors can be used on a single motherboard before the computational returns begin to diminish. &amp;ldquo;You can only have eight to 16 processors on a board before they&amp;rsquo;re all fighting over access to memory,&amp;rdquo; Pineda said.&lt;/p&gt;
&lt;p&gt;What&amp;rsquo;s emerging now is a hybrid of those two, a strategy exemplified by clusters. As Pineda explained, a number of processors are put on a motherboard, into a unit known as a node, which is then hooked together with other such boards via high-speed communications networks.&lt;/p&gt;
&lt;p&gt;Nodes can be hardwired into the supercomputers produced by companies like IBM and Compaq, or they can be composed of individual small PCs, as in the case of low-end clusters.&lt;/p&gt;
&lt;p&gt;The lines between the two are blurring, however, as supercomputer companies are starting to make cluster-styled computers. For example, IBM&amp;rsquo;s SP supercomputer line has now evolved from what was essentially a cluster of IBM RISC workstations into clusters of multiprocessor nodes, Pineda said.&lt;/p&gt;
&lt;p&gt;Major supercomputer centers are adding huge &amp;ldquo;superclusters&amp;rdquo; to their inventories, behemoths that rank among some of the most powerful computers in the world. &amp;ldquo;Probably the next generation of machines that everyone&amp;rsquo;s going to be buying&amp;ndash;even at very high-end supercomputing sites&amp;ndash;will be clusters, because of the huge price difference compared with big traditional supercomputers,&amp;rdquo; Pineda said.&lt;/p&gt;
&lt;p&gt;At AHPCC, scientists are working with IBM to test the capabilities of an even more exotic computer architecture, a group of clusters known as Vista Azul. It consists of a combination of an IBM SP2 cluster (Vista), a Linux-based cluster (Azul), and a graphics engine.&lt;/p&gt;
&lt;p&gt;Such versatility would be useful to chemists, who have a variety of computing needs, noted Thomas M. Jackman, a chemist and computer scientist at &lt;a href=&#34;http://www.research.ibm.com/about/ykt.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IBM T. J. Watson Research Center&lt;/a&gt; in Yorktown Heights, N.Y. Ab initio problems are solved much differently than molecular dynamics simulations, for example. A common system with different computing architectures would allow chemists to select the portion that would best handle their particular problem, Jackman said.&lt;/p&gt;
&lt;p&gt;The Vista Azul structure would also be useful for projects that are completed in stages. For example, the hope is that Vista Azul could do large calculations on one set of nodes, render the data on another cluster, then turn that data over to the graphics engine.&lt;/p&gt;
&lt;p&gt;The group is still working out bugs in the new system. &amp;ldquo;The two pieces don&amp;rsquo;t quite play together yet, but hopefully we&amp;rsquo;ll iron that out soon,&amp;rdquo; Pineda said.&lt;/p&gt;
&lt;p&gt;This type of system would also be advantageous for those who have old IBM SPs, Pineda said. &amp;ldquo;Rather than take an old SP and throw it away, you could graft new Linux clusters on to the existing platform.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Because the processors of Linux-based clusters are not wired together internally for ultrafast communication like standalone supercomputers, a critical component is the communications network that connects the nodes. Communication has to happen fast. For low-end clusters, a network known as FastEthernet usually does the job. But if a cluster is to take its place alongside enormous standard supercomputers, it needs something more sophisticated, such as Gigabit Ethernet or Myrinet.&lt;/p&gt;
&lt;p&gt;AN ALTERNATIVE to Gigabit Ethernet and Myrinet is the so-called Scalable Coherent Interface, or SCI. But there have been questions about its performance in a cluster environment, noted &lt;a href=&#34;http://www.msg.ameslab.gov/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mark S. Gordon&lt;/a&gt;, theoretical chemistry professor at Iowa State University, Ames, and director of the Scalable Computing Laboratory (SCL) at the Department of Energy&amp;rsquo;s Ames Laboratory.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010430-canden/7918scit1.ce_hu_deeef073ab81ce02.webp 400w,
               /blog/20010430-canden/7918scit1.ce_hu_38323a301cd027c6.webp 760w,
               /blog/20010430-canden/7918scit1.ce_hu_5db786ccd5b48e4f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918scit1.ce_hu_deeef073ab81ce02.webp&#34;
               width=&#34;275&#34;
               height=&#34;376&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Gordon&amp;rsquo;s colleague David Halstead put SCI to the test. His particular concern was that SCI&amp;rsquo;s communication through nodes is monodirectional. For example, to get from node 1 to node 8 requires going through several other nodes. &amp;ldquo;The question was, Does the communication from node 1 to node 8 degrade when the nodes in between talk to each other&amp;rdquo; Gordon said. Yes, but surprisingly little, was the answer.&lt;/p&gt;
&lt;p&gt;Gordon&amp;rsquo;s group has also developed a new version of crucial cluster networking software known as MPI (message-passing interface). All distributed-memory parallel computers need some message-passing software. MPI is a collection of routines that allow communication among the processors; it was developed to work on any platform and has become an industry standard.&lt;/p&gt;
&lt;p&gt;Ames scientist David Turner has written a program called MP_LITE, a much smaller, more efficient piece of software that can either replace or run on top of standard MPI. The program should be useful not only for clusters, but also for supercomputers such as the Cray T3E, Gordon said.&lt;/p&gt;
&lt;p&gt;To get the most out of a computer with multiple processors on different nodes, it&amp;rsquo;s vital to keep the processors occupied. The number of nodes involved in a calculation is constantly changing, so a piece of software known as a scheduler is called in to help manage various tasks more efficiently. Gordon&amp;rsquo;s group compared different schedulers on clusters, finding that a program known as the Maui Scheduler, developed at the &lt;a href=&#34;http://www.mhpcc.edu/mhpcc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maui High Performance Computing Center&lt;/a&gt;, performed best.&lt;/p&gt;
&lt;p&gt;Software is also coming onboard to make the task of assembling a cluster more user-friendly. In the past, putting together clusters has required a good deal of computer knowledge. But as Kim Baldridge, computational chemist at the &lt;a href=&#34;http://www.sdsc.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;San Diego Supercomputer Center (SDSC)&lt;/a&gt;, noted at the meeting, software known as Rocks put out by computer scientist Philip Papadopoulos&amp;rsquo; group at the &lt;a href=&#34;http://www.npaci.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Partnership for Advanced Computational Infrastructure&lt;/a&gt; makes the task much easier for the nonexpert.&lt;/p&gt;
&lt;p&gt;Of course, for these scientists, the ultimate purpose of all this activity is to enhance the capabilities of computational chemistry. And so they&amp;rsquo;re testing the mettle of computational chemistry software on these new varieties of supercomputers.&lt;/p&gt;
&lt;p&gt;REFORMULATING SOFTWARE to solve problems in parallel by delegating tasks to numerous processors is a particularly onerous undertaking in computational chemistry. The initial stages of many calculations, such as those that determine electronic structure, often involve integral evaluations, which don&amp;rsquo;t require as much communication throughout the computing network. But the end of the calculation is dominated by large linear algebra problems that require a great deal of cross-checking on a parallel computer&amp;rsquo;s communication network.&lt;/p&gt;
&lt;p&gt;Therefore, it&amp;rsquo;s not surprising that the performance of such codes depends on how fast processors and nodes can communicate with each other. &amp;ldquo;They&amp;rsquo;re quite sensitive to how robust the software and hardware for the communication network is,&amp;rdquo; Pineda said.&lt;/p&gt;
&lt;p&gt;At the meeting, speakers reported their results of software performance on different computer architectures using so-called benchmark tests&amp;ndash;standard calculations with known results. In particular, they focused on GAMESS, an open-source ab initio quantum chemistry software package cowritten by Gordon, Baldridge, and others. GAMESS is also one of the quantum chemistry programs that has been modified most extensively to run on parallel machines.&lt;/p&gt;
&lt;p&gt;At SDSC, Baldridge and her colleagues studied how GAMESS ran on four different types of machines: the center&amp;rsquo;s famous IBM Blue Horizon, a huge supercomputer with 144 eight-processor nodes; a shared-memory supercomputer from Sun; a large Compaq/IBM Linux-based cluster known as the Meteor cluster; and finally, a machine called Tera, possessing a unique architecture that includes structures known as threads, which make tasks within codes even more parallel.&lt;/p&gt;
&lt;p&gt;Pineda and his colleagues are running a Department of Defense-sponsored project to study software performance on a number of clusters at AHPCC. They tested GAMESS on Los Lobos, a huge 512-processor Linux-based cluster, as well as on Azul and Vista separately. The bottom line: The cluster computers performed within a factor of two of their standard supercomputing counterparts. Not bad, considering that the price of clusters is about one-tenth that of comparable supercomputers, Pineda said.&lt;/p&gt;
&lt;p&gt;Pineda notes that DOD recognizes that the future of supercomputing likely lies with clusters. &amp;ldquo;They&amp;rsquo;re seeing the handwriting on the wall, in terms of where the next generation of supercomputers will come from: They&amp;rsquo;ll probably be large Linux-based clusters,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.sdsc.edu/~johnson/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Greg S. Johnson&lt;/a&gt; and Allen Snavely, scientists at SDSC, have been exploring ways to run more than one task on the same set of resources at the same time, a strategy they call symbiosis. Not only could symbiosis allow chemists to solve more than one problem at once, it&amp;rsquo;s also a potential boon for visualizing, or rendering, data. Usually, the huge data sets produced from a lengthy calculation are saved to disk. In order to turn the data into something visually meaningful&amp;ndash;a protein structure, for instance&amp;ndash;it has to be carted to a different system and manipulated by visualization software.&lt;/p&gt;
&lt;p&gt;But with the strategy of symbiosis, graphic representations of the calculations could be churned out in real time, and scientists could watch their simulations as their calculations are running. Not only would this save time and reduce the amount of computer resources required, but errors can be spotted and corrected sooner, Johnson said.&lt;/p&gt;
&lt;p&gt;Accomplishing symbiosis will require some work because the machines are not currently designed to schedule jobs that way. &amp;ldquo;Even to do test runs to show the benefit of symbiosis, we had to get a machine specially configured,&amp;rdquo; Johnson said. Scientists will also need to change the way they view the computer resources they use.&lt;/p&gt;
&lt;p&gt;Chemistry-computing performance is also getting a boost from new advances in computer hardware. IBM&amp;rsquo;s Jackman noted a number of new devices coming down the pipeline, including the series of accelerator chips known as MD-GRAPE from IBM and the Institute of Chemical Research (RIKEN) in Tokyo. The chips operate on principles similar to graphics accelerators, except that they speed up dynamics calculations.&lt;/p&gt;


















&lt;figure  id=&#34;figure-on-the-cover-the-graphic-shown-on-the-cover-of-this-weeks-cen-is-a-three-dimensional-rendering-of-the-transition-state-between-two-carbocation-structures-of-the-triquinane-molecule-san-diego-supercomputing-center-scientist-johnson-used-code-called-mpire-to-image-data-from-gamess-structure-calculations-performed-by-sdscs-baldridge-red-and-dark-blue-show-regions-of-high-orbital-density-fading-out-to-low-density-greens-the-groups-goal-is-to-run-programs-like-these-in-tandem-to-directly-visualize-the-results-of-calculations-in-a-process-they-call-symbiosis&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ON THE COVER. The graphic shown on the cover of this week&amp;#39;s C&amp;EN is a three-dimensional rendering of the transition state between two carbocation structures of the triquinane molecule. San Diego Supercomputing Center scientist Johnson used code called MPIRE to image data from GAMESS structure calculations performed by SDSC&amp;#39;s Baldridge. Red and dark blue show regions of high orbital density, fading out to low-density greens. The group&amp;#39;s goal is to run programs like these in tandem to directly visualize the results of calculations in a process they call symbiosis.&#34; srcset=&#34;
               /blog/20010430-canden/7918triquinane_hu_d156cd49e36b3a33.webp 400w,
               /blog/20010430-canden/7918triquinane_hu_1dbdd8c646c87c1d.webp 760w,
               /blog/20010430-canden/7918triquinane_hu_89c82ceacff9310d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918triquinane_hu_d156cd49e36b3a33.webp&#34;
               width=&#34;318&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ON THE COVER. The graphic shown on the cover of this week&amp;rsquo;s C&amp;amp;EN is a three-dimensional rendering of the transition state between two carbocation structures of the triquinane molecule. San Diego Supercomputing Center scientist Johnson used code called MPIRE to image data from GAMESS structure calculations performed by SDSC&amp;rsquo;s Baldridge. Red and dark blue show regions of high orbital density, fading out to low-density greens. The group&amp;rsquo;s goal is to run programs like these in tandem to directly visualize the results of calculations in a process they call symbiosis.
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-fruitful-md-grape-chips-speed-up-calculations-ibm-watson-research-center&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;FRUITFUL. MD-GRAPE chips speed up calculations. IBM WATSON RESEARCH CENTER&#34; srcset=&#34;
               /blog/20010430-canden/7918md2_boardgood3_hu_3e76df9079f77bfc.webp 400w,
               /blog/20010430-canden/7918md2_boardgood3_hu_772150824cc692b4.webp 760w,
               /blog/20010430-canden/7918md2_boardgood3_hu_5aa470f655d13ba3.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918md2_boardgood3_hu_3e76df9079f77bfc.webp&#34;
               width=&#34;325&#34;
               height=&#34;186&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      FRUITFUL. MD-GRAPE chips speed up calculations. IBM WATSON RESEARCH CENTER
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-encounter-group-several-of-the-albuquerque-high-performance-computing-centers-high-powered-clusters&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ENCOUNTER GROUP. Several of the Albuquerque High Performance Computing Center&amp;#39;s high-powered clusters.&#34; srcset=&#34;
               /blog/20010430-canden/7918superCluster_hu_e19295ad801706d8.webp 400w,
               /blog/20010430-canden/7918superCluster_hu_58b9c8bd036f02be.webp 760w,
               /blog/20010430-canden/7918superCluster_hu_eaceec6e1ef8cf29.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010430-canden/7918superCluster_hu_e19295ad801706d8.webp&#34;
               width=&#34;325&#34;
               height=&#34;272&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ENCOUNTER GROUP. Several of the Albuquerque High Performance Computing Center&amp;rsquo;s high-powered clusters.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Nanotechnology C&amp;amp;EN Special Report, Science &amp;amp; Technology, 79(18):42, April 30, 2001.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pubsapp.acs.org/cen/coverstory/7918/7918supercomputer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://pubsapp.acs.org/cen/coverstory/7918/7918supercomputer.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives NSF CAREER Award</title>
      <link>http://localhost:1313/blog/20010401-maryland-career/</link>
      <pubDate>Sun, 01 Apr 2001 14:08:23 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010401-maryland-career/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dr. David Bader&lt;/strong&gt;, who earned his doctoral degree from the Department in May 1996, has received a National Science Foundation (NSF) Faculty Early Career Development (CAREER) Award for his work on High-Performance Algorithms for Scientific Applications.&lt;/p&gt;
&lt;p&gt;This prestigious grant emphasizes the importance NSF places on the early development of academic careers dedicated to research, inspired teaching and enthusiastic learning.&lt;/p&gt;
&lt;p&gt;Bader&amp;rsquo;s CAREER research plan will investigate new algorithms to support irregular computations, mostly tree and graph-based, along with new insights on how to leverage the theoretical research in PRAM algorithms.  Science-driven problems in genomics, bioinformatics, and computational ecology will provide the focus for his research.&lt;/p&gt;
&lt;p&gt;Bader is an assistant professor and Regents&amp;rsquo; Lecturer in the Electrical and Computer Engineering Department at the University of New Mexico.  His advisor at Maryland was Prof. Joseph JáJá.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Selected as UNM Regents&#39; Lecturer</title>
      <link>http://localhost:1313/blog/20010330-unm/</link>
      <pubDate>Fri, 30 Mar 2001 21:29:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010330-unm/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010330-unm/letter_hu_f1449f8297f09382.webp 400w,
               /blog/20010330-unm/letter_hu_b0ae64f83014f7c4.webp 760w,
               /blog/20010330-unm/letter_hu_8177986b27855b5e.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010330-unm/letter_hu_f1449f8297f09382.webp&#34;
               width=&#34;601&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;David&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;Congratulations! You have been selected to become the next School of Engineering UBM Regents&amp;rsquo; Lecturer. This is a special award that recognizes your extraordinary accomplishments and leadership in teaching, research, and service as a junior faculty member. There are only two such awards available in the School at any given time, so this award represents a singular honor. Your selection was the result of a rigorous review of the nomination packages of a number of excellent candidates by the School of Engineering Administrative Committee (AdCom). You should be justly proud of your selection.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Joseph L. Cecchi&lt;br&gt;
Interim Dean and Professor&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercomputer Math Speeds Up</title>
      <link>http://localhost:1313/blog/20010325-abqjournal/</link>
      <pubDate>Sun, 25 Mar 2001 07:01:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010325-abqjournal/</guid>
      <description>&lt;p&gt;*By John Fleck, Journal Staff Writer&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/359985517/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/359985517/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Award honors computer work</title>
      <link>http://localhost:1313/blog/20010304-abqjournal/</link>
      <pubDate>Sun, 04 Mar 2001 06:53:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010304-abqjournal/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Liz Otero Vallejos&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, an assistant professor of electrical and computer engineering at the University of New Mexico, has been awarded the &lt;strong&gt;National Science Foundation&amp;rsquo;s Faculty Early Career Development (CAREER) Award&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Parallel computing has long offered the promise of very high performance, but it has delivered only in a narrow range of applications,&amp;rdquo; Bader said. &amp;ldquo;With the advent of symmetric multiprocessors (SMPs), however, shared memory on a modest scale is becoming an available commodity. Over the next five to 10 years, clusters of SMPs will likely be the predominant architecture for scalable high-performance computing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader&amp;rsquo;s research plan will investigate new algorithms to support irregular computations, mostly tree- and graph-based, along with new insights on how to leverage the theoretical research in PRAM algorithms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/359982626/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/359982626/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Flower&#39;s Family Tree</title>
      <link>http://localhost:1313/blog/20010302-hpcwire/</link>
      <pubDate>Fri, 02 Mar 2001 22:15:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010302-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By J. William Bell, NCSA Senior Science Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Gene data allow researchers to recover the evolutionary history of plants, but
even the smallest dataset can require impossibly large computations. Using an
Alliance Linux cluster and newly designed software, a team from the University
of New Mexico and the University of Texas have increased the speed of the
process millionfold for one family of plants.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re looking for extreme diversity, consider bluebells, officially the
Campanulaceae family. The 2,000 species of these plants with bell-shaped
flowers show amazing variety. They live everywhere on Earth except the Sahara,
Antarctica, and the northern extremes of Greenland. Some are annuals, and
others are perennials. Despite their common name, the flowers can be blue,
purple, red, or yellow. The bluebells of North America typically grow close to
the ground, while Asian species can grow as tall as eight feet.&lt;/p&gt;
&lt;p&gt;As fascinating as that diversity is, it&amp;rsquo;s not the sort of thing that
computational scientists usually get excited about. Uncovering how that
diversity came to be has captured the attention of a team of researchers at
Alliance partner University of New Mexico and the University of Texas, though.
Using the 512-processor LosLobos Linux Pentium III supercomputing cluster at
the Albuquerque High Performance Computing Center, the team has created a
phylogeny reconstruction &amp;ndash; or evolutionary history &amp;ndash; of 12 bluebell species,
predicting all of the steps that take these species back to a single common
ancestor. To meet the challenge, they created a whole new piece of software
known as GRAPPA.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In our context, we can answer the question of why we do this work by saying,
&amp;lsquo;We just want to know, and we like a challenge.&amp;rsquo; But phylogeny reconstruction
has very significant implications to pharmaceutical design and in other
industries,&amp;rdquo; says Bernard Moret, a computer science professor at the
University of New Mexico.&lt;/p&gt;
&lt;p&gt;Before beginning to reconstruct the species&amp;rsquo; evolutionary history, the team
had to decide which model of evolution would inform their project. &amp;ldquo;There are
a number of models out there that talk about how evolution occurs. They
represent the different general principles that evolution might have
followed,&amp;rdquo; says &lt;strong&gt;David A. Bader&lt;/strong&gt;, an electrical and computer engineering
professor at the University of New Mexico.&lt;/p&gt;
&lt;p&gt;Some models figure the likelihood of given events within the species&amp;rsquo; history,
such as duplications, deletions, and insertions of genes, and use that
information to create possible histories. These methods are computationally
expensive, and gathering the data necessary to run them is difficult.&lt;/p&gt;
&lt;p&gt;Rather than taking a statistical approach, however, the model used by Bader
and Moret&amp;rsquo;s team was built on an idea known as parsimony. &amp;ldquo;By saying
parsimony, we&amp;rsquo;re basically saying that nature is efficient,&amp;rdquo; says Moret. &amp;ldquo;It
gives rise to new species through the least amount of change. Parsimony is
founded on the same principle as Occam&amp;rsquo;s razor: the simplest explanation is
the best. Here, the shortest evolutionary path is the best.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The team arrived at this theory by following the lead of biologists like their
colleague Robert Jansen, chairman of the University of Texas at Austin&amp;rsquo;s
Section of Integrative Biology, who studies evolution and collects the gene
data used by the team. The gene data are taken from the chloroplast of each
species of Campanulaceae. Chloroplasts provide energy for the plant cells and
do not occur independently outside of cells. They also make excellent
candidates for phylogenic reconstruction because each chloroplast has a single
chromosome. The genes on the chloroplasts&amp;rsquo; chromosomes have been sequenced,
and biologists like Jansen hypothesize that evolution occurs through a
mechanism known as inversion. Inversion contributes to evolution by changing
the order and orientation of a sequence of genes within a genome. At times
inversions may even undo themselves, returning the order of genes on the
chloroplast chromosome to its former state.&lt;/p&gt;
&lt;p&gt;Parsimony may imply efficiency, but, the efforts necessary to actually build
reconstructions, clearly leave simplicity far behind.&lt;/p&gt;
&lt;p&gt;Each phylogenetic reconstruction, called a tree, represents one possible
history of the species. The Campanulaceae project begins with 12 modern
species of bluebell and a single species of tobacco. Tobacco is used as an
outgroup, a species that is clearly very distant from the others, and is used
to identify the root of the tree. To predict the evolutionary history, almost
14 billion trees must be built and compared to one another. Bader, Moret and
their colleague Tandy Warnow of the computer science department at the
University of Texas at Austin go far beyond constructing the underlying tree
and its eventual outcome, also calculating gene order for each predicted
ancestor within the trees. That means a whopping 100 billion genomes must be
reconstructed.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We need to build as complete a picture of the ancestry as possible,&amp;rdquo; says
Moret. &amp;ldquo;Computing ancestral gene orders also enables us to put differing
evolutionary models to the test.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;For these computer scientists, the reconstructions amount to a massive
optimization problem &amp;ndash; a game of creating all the possible evolutionary
scenarios that could have occurred and then narrowing down those billions of
options to a single best solution. The process begins with the raw gene data
taken from the chloroplast of each species of Campanulaceae. The bluebell
chloroplasts&amp;rsquo; single chromosomes are each made up of 105 gene segments. The
genes within the individual genomes are always the same. That is, all 13
genomes have identical genes and identical lengths, but in different species
the genes appear along the chromosome in a different order or orientation.&lt;/p&gt;
&lt;p&gt;The team&amp;rsquo;s code generates trees one at a time. Once a tree is generated, its
internal nodes &amp;ndash; the intervening ancestors that come between the individual
species and their final common ancestor &amp;ndash; are labeled by the software.
Labels, which consist of the gene order data for a given node, are derived
through a complex optimization process based on the notion of breakpoints.&lt;/p&gt;
&lt;p&gt;A breakpoint occurs any time two genes are adjacent in one genome but are not
adjacent in a genome to which the first is compared. An internal node&amp;rsquo;s label
is derived by finding the gene order that minimizes the number of breakpoints
between a node and its three closest neighbors. &amp;ldquo;This is where the parsimony
criterion comes in,&amp;rdquo; say Moret. &amp;ldquo;We find a label that minimizes the amount of
change at this place in the tree.&amp;rdquo; A travelling salesperson problem solver &amp;ndash;
a common, if expensive, mathematical method of solving optimization problems
&amp;ndash; is used to find the median, calculating the hypothesized gene order data
for each node.&lt;/p&gt;
&lt;p&gt;In the initial labeling of the tree, nodes that represent known data are
separated by many intervening nodes. As a result, a great deal of
approximation is used in the early stages. Multiple passes at a tree refine
the approximation. Once the initial labels are assigned, each node has closer
neighbors that can be used to find the breakpoint median. The code
recalculates the nodes&amp;rsquo; labels based on these new data, repeating this process
again and again and recalculating any node that saw changes in one of its
neighbors in the previous pass, until the tree stabilizes.&lt;/p&gt;
&lt;p&gt;Labeling and refining the trees is by far the most challenging step.
&amp;ldquo;Computing a single median is intractable in itself, and we solve these over
and over. It&amp;rsquo;s a very computationally intensive procedure,&amp;rdquo; Moret says. With
this step complete, each tree is scored, using inversions as the metric. These
scores show researchers which trees are most parsimonious. Thus, the scores
also show which evolutionary history best fits the model and give a plausible
snapshot of each genome within that history.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In completing so many trees, you see very good and very bad ones. Ones that
match up very closely and ones that don&amp;rsquo;t,&amp;rdquo; says Bader. &amp;ldquo;What we look for is
commonality. A consensus tree that we can dig in deep on and that will give us
a foothold to larger phylogenies.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But mostly, the team lets biologists who use their data look for commonality,
consensus, and the like. Bader, Moret, Warnow, and their students are in it
for the computational challenge. And the Campanulaceae phylogeny
reconstruction certainly gives them that.&lt;/p&gt;
&lt;p&gt;At the beginning of the project, they wanted nothing more than to improve
BPAnalysis, a code used for breakpoint phylogeny research. BPAnalysis would
have required over 200 years to generate, label, and score the nearly 14
billion trees represented in the Campanulaceae problem.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We started with the goal of reimplementing BPAnalysis from the ground up. It
was simply much too slow. We wanted to gain efficiency and speed by improving
the algorithm and by parallelizing the code. Just focusing on one of those
wouldn&amp;rsquo;t have given us the kinds of improvements we&amp;rsquo;ve seen,&amp;rdquo; says Moret.&lt;/p&gt;
&lt;p&gt;BPAnalysis relabels every internal node each time it refines a tree; GRAPPA
recalculates labels for only those nodes that could possibly show a change.
BPAnalysis looks at identical strings of genes over and over again, even those
matching other gene fragments that have already been analyzed; GRAPPA
identifies common subsequences and condenses them, leaving fewer genes to be
considered. BPAnalysis runs on only one processor; GRAPPA scales linearly to
hundreds of processors running in parallel. GRAPPA leaves a scant memory
footprint of only 1.6 megabytes and can work almost entirely in a computer&amp;rsquo;s
cache memory thanks to a working set of less than 0.5 megabytes. GRAPPA is
also modular, allowing different methods of calculating the nodes&amp;rsquo; labels to
be swapped in and out easily.&lt;/p&gt;
&lt;p&gt;When recently tested on the Campanulaceae problem on Albuquerque&amp;rsquo;s LosLobos
computing cluster, GRAPPA showed incredible results. The massive
reconstruction was completed in one hour and 40 minutes on the machine&amp;rsquo;s 512
733-MHz Pentium III processors &amp;ndash; a 1,000,000-fold speedup over BPAnalysis.&lt;/p&gt;
&lt;p&gt;Now that&amp;rsquo;s the sort of thing a computational scientist can get excited about
&amp;ndash; an example of cluster computing in full bloom.&lt;/p&gt;
&lt;p&gt;Relevant URLs:&lt;/p&gt;
&lt;p&gt;&amp;ndash;Access story:
&lt;a href=&#34;http://access.ncsa.uiuc.edu/CoverStories/phylogeny/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://access.ncsa.uiuc.edu/CoverStories/phylogeny/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;ndash;GRAPPA homepage:
&lt;a href=&#34;http://thelma.cs.unm.edu/~moret/GRAPPA/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://thelma.cs.unm.edu/~moret/GRAPPA/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;ndash;David A. Bader&amp;rsquo;s homepage:
&lt;a href=&#34;http://www.eece.unm.edu/~dbader/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.eece.unm.edu/~dbader/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM Campus Briefs: UNM School of Engineering professor David A. Bader receives NSF CAREER Award </title>
      <link>http://localhost:1313/blog/20010228-unm/</link>
      <pubDate>Wed, 28 Feb 2001 17:21:58 -0600</pubDate>
      <guid>http://localhost:1313/blog/20010228-unm/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010228-unm/DailyLobo_hu_8e3994ebb118cf18.webp 400w,
               /blog/20010228-unm/DailyLobo_hu_412996d1a69ccfdc.webp 760w,
               /blog/20010228-unm/DailyLobo_hu_115ac0f28e0410f4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010228-unm/DailyLobo_hu_8e3994ebb118cf18.webp&#34;
               width=&#34;760&#34;
               height=&#34;576&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>UNM Engineering Professors receive NSF CAREER Awards</title>
      <link>http://localhost:1313/blog/20010220-unm-career/</link>
      <pubDate>Tue, 20 Feb 2001 21:48:04 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010220-unm-career/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;200108-UNM-Mirage.pdf&#34;&gt;UNM Mirage&lt;/a&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010220-unm-career/David_lab_hu_4716e26947965684.webp 400w,
               /blog/20010220-unm-career/David_lab_hu_4eed0c3d8a32ced.webp 760w,
               /blog/20010220-unm-career/David_lab_hu_9bdd24c607bac00b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010220-unm-career/David_lab_hu_4716e26947965684.webp&#34;
               width=&#34;565&#34;
               height=&#34;662&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;University of New Mexico School of Engineering professors &lt;strong&gt;David A. Bader&lt;/strong&gt; and Hy D. Tran recently received the National Science Foundation Faculty Early Career Development (CAREER) Awards.&lt;/p&gt;
&lt;p&gt;Bader, assistant professor in Electrical and Computer Engineering, has been awarded the grant in High-Performance Algorithms for Scientific Applications. His CAREER research plan will investigate and develop algorithms for high-performance computers that have multiple processors, advanced memory subsystems and state-of-the-art communication networks. He harnesses all of these resources concurrently to solve computional science applications. Science-driven problems in genomics, bioinformatics and computational ecology will provide the focus for this research.&lt;/p&gt;
&lt;p&gt;“Many scientific applications require the solution to computationally hard problems,” Bader said. “For instance, a simulation model may require datasets in the order of terabytes that overwhelm the capacity of storage on personal computers and workstations. Other problems are difficult in that they require time-consuming operations whereby a PC may take months, years or even centuries, to solve a problem (e.g., weather prediction) whereas the solution must be obtained in a reasonable amount of time for it to be useful.”&lt;/p&gt;
&lt;p&gt;He said a personal computer typically contains a single processor (e.g., an Intel Pentium) and applications written for this machine in general give the processor a single stream of instructions to execute one-by-one. “Imagine now using hundreds, or thousands, of processors together to solve a computational problem,” he said. “We still must give each processor a stream of instructions, but now, we must find clever ways to partition the work among a number of processors.”&lt;/p&gt;
&lt;p&gt;Tran, assistant professor in Mechanical Engineering, received his CAREER grant in micro-electro-mechanical systems (MEMS) specifically, investigating alternative means (as opposed to batteries) for providing power to microsensors and other microsystems.&lt;/p&gt;
&lt;p&gt;“I am especially interested in scavenging energy from the environment to power microsystems,” Tran said. “If you look at the environment, ambient light can provide energy, as can ambient vibrations and acoustic sound. I will be investigating the use of ambient temperature fluctuations to generate power for microsystems.”&lt;/p&gt;
&lt;p&gt;His research has environmental benefits, including reduced use of batteries with possibly hazardous materials, such as Nickel/Cadmium.&lt;/p&gt;
&lt;p&gt;Tran said the NSF CAREER grant will also support integration of research into education, and educational outreach to K-12 students. The educational benefits should include greater awareness of engineering and technology in K-12, and eventually, more students choosing science/engineering majors.&lt;/p&gt;
&lt;p&gt;The CAREER program is a NSF-wide activity that supports junior faculty within the context of their overall career development. It combines in a single program the support of research and education of the highest quality and in the broadest sense. This premier program emphasizes the importance the Foundation places on the early development of academic careers dedicated to stimulating the discovery process in which the excitement of research is enhanced by inspired teaching and enthusiastic learning.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20010228185415/http://www.unm.edu/~paaffair/Releases/Feb20nsf.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/~paaffair/Releases/Feb20nsf.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ECE Prof. David A. Bader Receives NSF Career Award</title>
      <link>http://localhost:1313/blog/20010219-career/</link>
      <pubDate>Mon, 19 Feb 2001 22:16:35 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010219-career/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010219-career/bader_outside_hu_5e4c8de649662b56.webp 400w,
               /blog/20010219-career/bader_outside_hu_25c4ad16af18edfe.webp 760w,
               /blog/20010219-career/bader_outside_hu_7dd4891234eaf446.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010219-career/bader_outside_hu_5e4c8de649662b56.webp&#34;
               width=&#34;150&#34;
               height=&#34;113&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Prof. David A. Bader&lt;/strong&gt; has been awarded the National Science Foundation&amp;rsquo;s Faculty Early Career Development (CAREER) Award. This highly prestigious grant in High-Performance Algorithms for Scientific Applications emphasizes the importance NSF places on the early development of academic careers dedicated to stimulating the discovery process in which the excitement of research is enhanced by inspired teaching and enthusiastic learning. Parallel computing has long offered the promise of very high performance, but it has delivered only in a narrow range of applications. With the advent of symmetric multiprocessors (SMPs), however, shared-memory on a modest scale is becoming an available commodity. Over the next five to ten years, clusters of SMPs will likely be the predominant architecture for scalable high-performance computing; however, little work has been done to date to support effective parallel computing on these SMP clusters. Prof. Bader&amp;rsquo;s CAREER research plan will investigate new algorithms to support irregular computations, mostly tree- and graph- based, along with new insights on how to leverage the theoretical research in PRAM algorithms. Science-driven problems in genomics,bioinformatics, and computational ecology will provide the focus for this research. Prof. Bader directs the EECE Department&amp;rsquo;s High-Performance Computing Laboratory located in Room 217.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20041010143851/http://www.ece.unm.edu/event/news/news_sublist.php?category=faculty&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ece.unm.edu/event/news/news_sublist.php?category=faculty&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader elected Senior Member of the IEEE</title>
      <link>http://localhost:1313/blog/20010120-ieee/</link>
      <pubDate>Sat, 20 Jan 2001 14:54:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/20010120-ieee/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010120-ieee/letter_hu_e194c4d8af89d362.webp 400w,
               /blog/20010120-ieee/letter_hu_a06ce98d969e999f.webp 760w,
               /blog/20010120-ieee/letter_hu_7c5a165d6a1156a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010120-ieee/letter_hu_e194c4d8af89d362.webp&#34;
               width=&#34;590&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20010120-ieee/award_hu_583a8b8a5a99e3d7.webp 400w,
               /blog/20010120-ieee/award_hu_77f68ed81716317d.webp 760w,
               /blog/20010120-ieee/award_hu_bbe69f9110451bd2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20010120-ieee/award_hu_583a8b8a5a99e3d7.webp&#34;
               width=&#34;760&#34;
               height=&#34;574&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The Institute of Electrical and Electronics Engineers, Inc., (IEEE)&lt;/p&gt;
&lt;p&gt;In recognition of professional standing the Offers and Board of Directors of the Institute certify that &lt;strong&gt;David Albert Bader&lt;/strong&gt; has been elected to the grade of Senior Member.&lt;/p&gt;
&lt;p&gt;Joel B. Snyder&lt;br&gt;
President&lt;/p&gt;
&lt;p&gt;Hugo M. Fernandez Verstegen&lt;br&gt;
Secretery&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Phylogeny Reconstruction Code, LosLobos Cluster, Mean Speedy Solution to Computational Problem</title>
      <link>http://localhost:1313/blog/20001212-ncsa/</link>
      <pubDate>Tue, 12 Dec 2000 21:26:36 -0400</pubDate>
      <guid>http://localhost:1313/blog/20001212-ncsa/</guid>
      <description>&lt;p&gt;Using the largest open, production Linux supercluster, LosLobos, researchers at The University of New Mexico&amp;rsquo;s Albuquerque High Performance Computing Center have achieved a nearly one-million-fold speedup in solving the computationally-hard phylogeny reconstruction problem for the family of twelve Bluebell species (scientific name: Campanulaceae) from the flowers&amp;rsquo; chloroplast gene order data. (The problem size includes a thirteenth plant, Tobacco, used as a distantly-related outgroup). Phylogenies derived from gene order data may prove crucial in answering some fundamental open questions in biomolecular evolution. Yet very few techniques are available for such phylogenetic reconstructions.&lt;/p&gt;
&lt;p&gt;One method is breakpoint analysis, developed by Blanchette and Sankoff for solving the &amp;ldquo;breakpoint phylogeny.&amp;rdquo; Earlier experiments with various techniques for reconstructing phylogenies from gene order data suggested that Sankoff and Blanchette&amp;rsquo;s implementation of BPAnalysis is much too slow. On a collection of Campanulaceae with 13 genomes of 105 gene segments, it is estimated that Sankoff and Blanchette&amp;rsquo;s BPAnalysis would take well over 200 years to complete—an estimate based on the average number of trees processed by the code per unit time and extended to the 13,749,310,575 tree topologies on 13 leaves.&lt;/p&gt;
&lt;p&gt;Professors &lt;strong&gt;David A. Bader&lt;/strong&gt; (EECE, UNM) and Bernard M.E. Moret (Computer Science, UNM) along with Professor Tandy Warnow (Computer Science, University of Texas - Austin) have solved the Campanulaceae problem in about one hour and forty minutes on the 512-processor UNM/Alliance LosLobos supercluster, an IBM Netfinity cluster with 512 733MHz Pentium III processors, interconnected with four 64-way Myrinet 2000 switches, and running the Linux 2.2 operating system. The parallelization uses MPI and includes techniques for concurrently evaluating candidate trees and sharing improved upper and lower bounds by the processors.&lt;/p&gt;
&lt;p&gt;The new phylogeny reconstruction code, GRAPPA, is freely available as open source from &lt;a href=&#34;http://www.cs.unm.edu/~moret/GRAPPA/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.unm.edu/~moret/GRAPPA/&lt;/a&gt; and shows the power of algorithmic engineering techniques. Run on a single processor, GRAPPA performs 2500 times faster than the BPAnalysis code, using similar algorithms, but with the implementation paying close attention to the detail of cache-sensitive and efficient data structures and algorithms. GRAPPA is able to take full advantage of parallel computers like the LosLobos supercluster, for additional speedups. Hence, the total speedup for our solution (going from an estimated 200 years down to 100 minutes) is one million—nearly 6 orders of magnitude.&lt;/p&gt;
&lt;p&gt;This research is part of Moret and Bader&amp;rsquo;s NSF Information Technology Research project &amp;ldquo;Algorithms for Irregular Discrete Computations on SMPs&amp;rdquo; based at the High Performance Computing, Educational and Research Center (HPCERC), a strategic center at The University of New Mexico.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20010124103600/http://access.ncsa.uiuc.edu/Headlines/00Headlines/001212.GRAPPA.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://access.ncsa.uiuc.edu/Headlines/00Headlines/001212.GRAPPA.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GRAPPA Runs In A Record Time</title>
      <link>http://localhost:1313/blog/20001123-hpcwire/</link>
      <pubDate>Thu, 23 Nov 2000 20:25:20 -0400</pubDate>
      <guid>http://localhost:1313/blog/20001123-hpcwire/</guid>
      <description>&lt;p&gt;Using the largest open-production Linux supercluster in
the world, LosLobos, researchers at The University of New Mexico&amp;rsquo;s Albuquerque
High Performance Computing Center ( &lt;a href=&#34;http://www.ahpcc.unm.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ahpcc.unm.edu&lt;/a&gt; ) have achieved a
nearly one-million-fold speedup in solving the computationally-hard phylogeny
reconstruction problem for the family of twelve Bluebell species (scientific
name: Campanulacae) from the flowers&amp;rsquo; chloroplast gene order data. (The
problem size includes a thirteenth plant, Tobacco, used as a distantly-related
outgroup). Phylogenies derived from gene order data may prove crucial in
answering some fundamental open questions in biomolecular evolution. Yet very
few techniques are available for such phylogenetic reconstructions.&lt;/p&gt;
&lt;p&gt;One method is breakpoint analysis, developed by Blanchette and Sankoff for
solving the &amp;ldquo;breakpoint phylogeny.&amp;rdquo; Earlier experiments with various
techniques for reconstructing phylogenies from gene order data suggested that
Sankoff and Blanchette&amp;rsquo;s implementation of BPAnalysis is much too slow. On a
collection of Campanulaceae with 13 genomes of 105 gene segments, it is
estimated that Sankoff and Blanchette&amp;rsquo;s BPAnalysis would take well over 200
years to complete - an estimate based on the average number of trees processed
by the code per unit time and extended to the 13,749,310,575 tree topologies
on 13 leaves.&lt;/p&gt;
&lt;p&gt;Professors Bernard M.E. Moret (Computer Science, University fo New Mexico)
and David A. Bader (Department of Electrical and Computer Engineering,
University of New Mexico) along with Professor Tandy Warnow (Computer Science,
University of Texas - Austin) have solved the Campanulacae problem in about
one hour and forty minutes on the 512-processor UNM/Alliance LosLobos
supercluster, an IBM Netfinity cluster with 512 733MHz Pentium III processors,
interconnected with four 64-way Myrinet 2000 switches, and running the Linux
2.2 operating system.&lt;/p&gt;
&lt;p&gt;The new phylogeny reconstruction code, GRAPPA, is freely available as open
source from &lt;a href=&#34;http://www.cs.unm.edu/~moret/GRAPPA/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.unm.edu/~moret/GRAPPA/&lt;/a&gt; and shows the power of
algorithmic engineering techniques. Run on a single processor, GRAPPA performs
2500 times faster than the BPAnalysis code, using similar algorithms, but with
the implementation paying close attention to the detail of cache-sensitive and
efficient data structures and algorithms. GRAPPA is able to take full
advantage of parallel computers like the LosLobos supercluster, for additional
speedups. Hence, the total speedup for our solution (going from an estimated
200 years down to 100 minutes) is one million - nearly 6 orders of magnitude.&lt;/p&gt;
&lt;p&gt;This research is part of Moret and Bader&amp;rsquo;s NSF Information Technology
Research project &amp;ldquo;Algorithms for Irregular Discrete Computations on SMPs&amp;rdquo;
based at the High Performance Computing, Educational and Research Center
(HPCERC), a strategic center at The University of New Mexico.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solving the mystery of life with sixfold speedup</title>
      <link>http://localhost:1313/blog/20001001-ieeeconcurrency/</link>
      <pubDate>Sun, 01 Oct 2000 22:35:05 -0400</pubDate>
      <guid>http://localhost:1313/blog/20001001-ieeeconcurrency/</guid>
      <description>&lt;p&gt;To understand the evolutionary
process from the beginning of life
itself to present-day species, we must
first determine the “tree of life.” In this
tree, called a &lt;em&gt;phylogeny&lt;/em&gt;, known species
reside at the tree’s leaves, while conjectured
ancestor (extinct) species
reside where the tree’s branches split.
We follow this process down to the
tree’s base, normally represented by
the three major limbs for plants, animals,
and single-celled organisms.&lt;/p&gt;
&lt;p&gt;In recent years, geneticists have
made wondrous progress in determining
genetic sequences from generation
to generation; they have now mapped
complete genomes for several species.
Evolutionary models are approximations
at best, but nevertheless provide
the best guidance for determining the
interrelation between species. Biologists
are often concerned with a small
portion of the phylogeny (a subtree)
containing a family of related species.
They believe that Nature follows
a path of minimizing evolutionary
processes, but even computing the
minimum evolutionary tree (called
phylogeny reconstruction) for a handful
of species is intractable on several
levels.&lt;/p&gt;
&lt;p&gt;At the University of New Mexico’s
Albuquerque High Performance
Computing Center (&lt;a href=&#34;https://www.ahpcc.unm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.ahpcc.unm&lt;/a&gt;.
edu), &lt;strong&gt;David Bader&lt;/strong&gt;, Bernard Moret,
and Tandy Warnow have achieved a
nearly one-million-fold speedup on
the UNM/Alliance LosLobos supercluster
in solving the phylogeny reconstruction
problem for the family of
twelve Bluebell species. The problem
size includes a thirteenth plant, tobacco,
used as a distantly related outgroup.
The LosLobos supercluster has
512 733-MHz Pentium III processors,
interconnected with four 64-way
Myrinet 2000 switches and running
the Linux 2.2 operating system. The
parallelization uses MPI and includes
techniques for concurrently evaluating
candidate trees and sharing
improved upper and lower bounds by
the processors.&lt;/p&gt;
&lt;p&gt;The LosLobos supercluster executed
the problem in about one hour
and 40 minutes using the new phylogeny
reconstruction code, Grappa
(freely available as open source from
&lt;a href=&#34;https://www.cs.unm.edu/~moret/GRAPPA/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.cs.unm.edu/~moret/GRAPPA/)&lt;/a&gt;.
Run on a single processor, Grappa
performs 2,500 times faster than previous
methods but takes full advantage
of parallel processing for additional
speedups. Hence, the total speedup
for the new solution is one million—
equivalent to going from an estimated
200 years down to 100 minutes. Phylogenies
derived from gene-order data
might prove crucial in answering fundamental
open questions in biomolecular
evolution&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM And UNM Open Linux Supercluster Center</title>
      <link>http://localhost:1313/blog/20000915-hpcwire/</link>
      <pubDate>Fri, 15 Sep 2000 13:48:58 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000915-hpcwire/</guid>
      <description>&lt;p&gt;Albuquerque, N.M. — IBM announced the opening of the industry’s first Linux supercluster briefing center for customers developing scalable applications on Linux systems. The briefing center will be based at the &lt;strong&gt;University of New Mexico’s (UNM) Albuquerque High Performance Computing Center (AHPCC)&lt;/strong&gt;, and complements the existing briefing facility at the Maui High Performance Computing Center (Maui HPCC).&lt;/p&gt;
&lt;p&gt;The new briefing center will assist customers in planning their Linux and Linux clusters plans with IBM, and allow them to benchmark and “test-drive” key applications. The UNM supercluster, called Los Lobos, consists of 256 IBM Netfinity servers (512 Intel IA-32 processors running at 733 MHz) that collectively provide peak performance of about 375 gigaflops. IBM will engage petroleum and manufacturing companies, the scientific and technical community, financial institutions, and higher education and government institutions who are deploying high performance computing solutions with Linux. Customers can draw on IBM and UNM’s experience, skill and resources to help meet their technical needs.&lt;/p&gt;
&lt;p&gt;“The University of New Mexico is excited to extend our ongoing relationship with IBM,” said Dr. Frank Gilfeather, executive director of the High Performance Computing, Educational and Research Center at the University of New Mexico. “This new briefing center will bring high performance computing beyond the research community.”&lt;/p&gt;
&lt;p&gt;UNM is a Doctoral/Research Universities-Extensive, one of about 145 universities. Among UNM’s outstanding research units are the Cancer Center, New Mexico Engineering Research Institute, Center for High Technology Materials, Center for Micro-Engineered Ceramics and the Center for Non-Invasive Diagnosis.&lt;/p&gt;
&lt;p&gt;IBM is the world’s largest information technology company, with 80 years of leadership in helping businesses innovate. IBM creates, develops and manufactures some of the industry’s most advanced information technologies, including computer systems, software, networking systems, storage devices and microelectronics. The fastest way to get more information about IBM is through the IBM home page at &lt;a href=&#34;http://www.ibm.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ibm.com&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2000/09/15/ibm-and-unm-open-linux-supercluster-center/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2000/09/15/ibm-and-unm-open-linux-supercluster-center/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM And University Of New Mexico Open Linux Supercluster Briefing Center</title>
      <link>http://localhost:1313/blog/20000913-linuxtoday/</link>
      <pubDate>Wed, 13 Sep 2000 13:23:14 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000913-linuxtoday/</guid>
      <description>&lt;p&gt;&amp;ldquo;IBM today announced the opening of the industry&amp;rsquo;s first Linux supercluster briefing center for customers developing scalable applications on Linux systems. The briefing center will be based at the &lt;strong&gt;University of New Mexico&amp;rsquo;s (UNM) Albuquerque High Performance Computing Center (AHPCC)&lt;/strong&gt;, and complements the existing briefing facility at the Maui High Performance Computing Center (Maui HPCC).&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The new briefing center will assist customers in planning their Linux and Linux clusters plans with IBM, and allow them to benchmark and &amp;ldquo;test-drive&amp;rdquo; key applications. The UNM supercluster, called Los Lobos, consists of 256 IBM Netfinity servers (512 Intel IA-32 processors running at 733 MHz(1)) that collectively provide peak performance of about 375 gigaflops. IBM will engage petroleum and manufacturing companies, the scientific and technical community, financial institutions, and higher education and government institutions who are deploying high performance computing solutions with Linux. Customers can draw on IBM and UNM&amp;rsquo;s experience, skill and resources to help meet their technical needs.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.linuxtoday.com/high_performance/2000091302104prhe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.linuxtoday.com/high_performance/2000091302104prhe&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM makes it easier to build clusters of Linux servers</title>
      <link>http://localhost:1313/blog/20000815-computerworld/</link>
      <pubDate>Tue, 15 Aug 2000 07:32:13 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000815-computerworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jaikumar Vijayan, Computerworld&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;IBM is making it easier for users to build clusters of Linux servers for running large computationally intensive applications.&lt;/p&gt;
&lt;p&gt;The company today announced a series of prepackaged Linux cluster offerings that it claimed will allow users to tie up to 64, two-processor Intel Corp. servers into one massive Linux cluster.&lt;/p&gt;
&lt;p&gt;Cluster configurations are used to harness the power of multiple servers to run large applications. Clusters are also often used to boost application uptime. For instance, each server in a high-availability fail-over cluster is capable of taking on the load of a failed server.&lt;/p&gt;
&lt;p&gt;IBM&amp;rsquo;s Solutions Series for Linux Clusters combines its Netfinity PC server line with Arcadia, Calif.-based Myricom Inc.&amp;rsquo;s Myrinent cluster interconnect technologies, Santa Clara, Calif.-based Extreme Network Inc.&amp;rsquo;s Ethernet switches and terminal servers from Equinox Systems Inc. in Sunrise, Fla.&lt;/p&gt;
&lt;p&gt;IBM will also bundle in supporting software and utilities for installing and managing applications, said David Gelardi, a director at IBM&amp;rsquo;s Deep Computing group.&lt;/p&gt;
&lt;p&gt;Users can buy the clusters in configurations of 8, 16, 32 and 64 nodes. Pricing for an 8-node cluster with 16-processor support starts at $115,000, and will support the Caldera, Red Hat, SuSE and Turbo Linux distributions, Gelardi added.&lt;/p&gt;
&lt;p&gt;The first Linux clusters are being targeted mainly at engineering and scientific markets because that is where the applications are, Gelardi said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Linux clusters have been very popular with the technical community,&amp;rdquo; said Rich Partridge, an analyst at D. H. Brown Associates Inc., in Port Chester, N.Y.&lt;/p&gt;
&lt;p&gt;Not only do technical applications often need the high-end scalability provided by clustering, but they can also be manipulated more easily to take advantage of clustering technologies, Partridge said.&lt;/p&gt;
&lt;p&gt;IBM plans to make similar clusters available for commercial applications such as data warehousing and Web serving in the future, Gelardi said. Also on the cards is a high-availability cluster configuration, he added.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The University of New Mexico&lt;/strong&gt;, for instance, has already installed a 256-node Netfinity-based Linux cluster with more than 1.5T bytes of storage.&lt;/p&gt;
&lt;p&gt;The system is being used by scientists and researchers around the country to run a range of biology, chemistry, physics, cosmology and other computationally intensive applications, said Dr. Frank Gilfeather, executive director of the university&amp;rsquo;s high-performance computing center.&lt;/p&gt;
&lt;p&gt;One of the biggest advantages of running Linux is the fact that &amp;ldquo;it truly is the first environment that is upwards and downwards scalable,&amp;rdquo; Gilfeather said. &amp;ldquo;The same environment that runs on the desktop runs on the super-cluster,&amp;rdquo; without any changes.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;No other computing environment offers this advantage,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;IBM&amp;rsquo;s latest Linux initiative builds on a series of similar moves during the past few months.&lt;/p&gt;
&lt;p&gt;Just last week, for instance, IBM announced plans to release a new version of its AIX Unix operating system featuring extensive Linux support (see story).&lt;/p&gt;
&lt;p&gt;In July, the company announced new hardware, software and pricing options designed to make it cheaper for users to run Linux applications on mainframes.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Whether there is a huge demand for Linux or not, all the vendors are trying to make sure they don&amp;rsquo;t get left behind,&amp;rdquo; Partridge said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The development costs are not very large, so they can say they support it without [expending] a great deal of development resources,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2596970/ibm-makes-it-easier-to-build-clusters-of-linux-servers.amp.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2596970/ibm-makes-it-easier-to-build-clusters-of-linux-servers.amp.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grand Opening of the University of New Mexico&#39;s LosLobos Supercomputer</title>
      <link>http://localhost:1313/blog/20000808-hpcerc/</link>
      <pubDate>Tue, 08 Aug 2000 15:17:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000808-hpcerc/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000808-hpcerc/letter_hu_f7fa66684010c914.webp 400w,
               /blog/20000808-hpcerc/letter_hu_8232007951bd649.webp 760w,
               /blog/20000808-hpcerc/letter_hu_3a45aaa2ab85fb9b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000808-hpcerc/letter_hu_f7fa66684010c914.webp&#34;
               width=&#34;590&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Mr. Bader&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;You are cordially invited to the grand opening of the University of New Mexico&amp;rsquo;s LosLobos Supercomputer.  The opening and reception will be held Wednesday evening, September 13, 2000 from 5:30 to 7:30 P.M. at the Albuquerque High Performance Computing Center, 1601 Central Ave, on the northwest corner of Central and University.&lt;/p&gt;
&lt;p&gt;LosLobos is the world&amp;rsquo;s first large production Linux-based IBM Supercluster. Cluster computing technology has a distinctly New Mexico flavor since it has grown from efforts at the DOE laboratories here, especially at Sandia National Laboratory [sic] where a cluster has for some time been featured as the world&amp;rsquo;s fastest computer. And just 18 months ago, UNM inaugurated Roadrunner, the first generation of production superclusters. UNM faculty, student, and center staff are continuing to make pioneering advances in this cutting edge technology.&lt;/p&gt;
&lt;p&gt;RoadRunner, like LosLobos, is funded and supported by the National Science Foundation through the National Computational Science Alliance led by the University of Illinois. LosLobos joins the Alliance computing resources to serve academic and other users as a key national asset. We are proud to have worked in conjunction with university, industry, and government communities to develop LosLobos. The supercluster is a significant step in bringing highly capable, accessible and economical supercomputing to all.&lt;/p&gt;
&lt;p&gt;UNM is extremely proud of both of its supercomputing centers, the Maui High Performance Computing Center and the Albuquerque High Performance Computing Center and the international recognition for their accomplishments. We hope that you can join us in celebrating a milestone in the growth of the Albuquerque center and the advancement of an exciting technology.  If you desire any special arrangements please contact Martina Kindilien at 505-277-8249 or &lt;a href=&#34;mailto:mkindili@ahpcc.unm.edu&#34;&gt;mkindili@ahpcc.unm.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Frank Gilfeather&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux not just for little systems</title>
      <link>http://localhost:1313/blog/20000701-librarysystems/</link>
      <pubDate>Sat, 01 Jul 2000 10:30:33 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000701-librarysystems/</guid>
      <description>&lt;p&gt;Even though the library community is focusing on Linux for small systems, IBM has introduced a supercomputer that uses the Linux operating system. The system, called Los Lobos, is a super-cluster of servers and consists of 256 IBM Netfinity PC servers, which are linked using special clustering software and high-speed networking hardware acting as one to process at a speed of 375 gigaflops. That makes it the 24th most powerful computer in the world. The National Computational Science Alliance, a consortium of 50 academic and research entities, plans to use Los Lobos to create a new computer network for research.&lt;/p&gt;
&lt;p&gt;Library Systems Newsletter (American Library Association, Chicago, IL), 20(7):55, July 2000.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://librarytechnology.org/document/7786/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://librarytechnology.org/document/7786/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Systems</title>
      <link>http://localhost:1313/blog/20000629-washingtontechnology/</link>
      <pubDate>Thu, 29 Jun 2000 16:00:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000629-washingtontechnology/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jon William Toigo&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Just three years ago, debates were raging in the technology trade press regarding the future of Linux and open-source computing. Would government and business be willing to host mission-critical applications on what amounted to a shareware operating system? Today, that question seems to have been answered decisively: Linux is big business. Shipments of open-source Unix operating system &amp;ldquo;look-alikes&amp;rdquo; accounted for one-quarter of the 5.7 million operating environment shipments made to consumers around the world in 1999, according to Dan Kusnetzky, vice president of systems software research for market research firm International Data Corp. in Framingham, Mass. The Linux share ranked a close second to Microsoft Corp., whose NT operating system made up 38 percent of operating environment shipments. The Linux market share could be much larger, according to Kusnetzky, owing to the fact that only purchased software, as opposed to software that is freely distributed, is included in IDC estimates.&lt;/p&gt;
&lt;p&gt;The Linux operating system can be downloaded free of charge from numerous Web servers and file transfer protocol sites on the Internet, in addition to being purchased as shrink-wrapped software from Red Hat Inc. of Durham, N.C., Corel Corp. of Ottawa and others.&lt;/p&gt;
&lt;p&gt;It is also included on many servers shipping today, including all platforms from Dell Computer Corp. But market share data alone is insufficient to evaluate the penetration of Linux into the mainstream of government or business computing, according to Kusnetzky.&lt;/p&gt;
&lt;p&gt;Dismissing as incorrect &amp;ldquo;a popular perception&amp;rdquo; that all operating environments are general purpose in nature, he said that end users tend to use different operating environments for very different purposes. The four uses most often cited by companies deploying Microsoft NT and Novell Netware operating environments are, in order, file and print services, electronic messaging, communications services and database support, Kusnetzky said.&lt;/p&gt;
&lt;p&gt;By contrast, companies fielding Unix servers ranked database support as their No. 1 use for the operating system, followed by electronic messaging and custom application development. &amp;ldquo;While functionally similar, operating environments fill very different application niches,&amp;rdquo; he said. Linux, while capable of supporting a broad range of applications, &amp;ldquo;is primarily used to support Web servers,&amp;rdquo; he added.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;In contrast to Unix, less than 10 percent of companies use Linux to host databases.&amp;rdquo; If Web serving tops the list of applications for which Linux typically is deployed, high-performance computing must be the second application niche for the operating system, especially within the circles of governmental scientific and technological research.&lt;/p&gt;
&lt;p&gt;For the past two years, Linux has been at the heart of numerous government-sponsored supercomputer development efforts throughout the United States.&lt;/p&gt;
&lt;p&gt;Collaborations involving the National Science Foundation, the departments of Energy and Defense, leading academic institutions and name-brand computer hardware vendors have focused on exploiting the clustering capabilities of Linux implemented on commodity hardware platforms. The objective has been to build &amp;ldquo;supercluster&amp;rdquo; platforms capable of doing the work of single-purpose supercomputers, but at a fraction of the cost.&lt;/p&gt;
&lt;p&gt;According to Frank Gilfeather, director of the High Performance Computing, Education and Research Center (HPCERC) at the University of New Mexico in Albuquerque, the short-term results of this activity will boost supercomputing capabilities for use in government and private research. And in the longer term, he said, the collaborations will set the stage for a new generation of low-cost, high-performance, business computing platforms that offer scalability and manageability well surpassing existing systems. Gilfeather and others also said forward-looking systems integrators and IT solution providers should keep an eye out on the work that is being done at the leading academic institutions in the field, including the University of New Mexico, University of Minnesota, Carnegie Mellon University in Pittsburgh and Cornell University in Ithaca, N.Y., as well as Energy Department-backed national research laboratories, including Argonne, Sandia, Los Alamos and the Pacific Northwest National Laboratory. These organizations are spearheading the technology that may one day support business applications amenable to high-speed parallel processing, such as data mining, according to Tom Morgan, program manager for Argonne National Laboratory near Chicago.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Argonne is no stranger to primary research, Morgan said. The first national laboratory, Argonne was an outgrowth of the Manhattan Project and developed the first nuclear reactors for power generation and submarine propulsion. In the early 1960s, the laboratory moved into basic scientific and mathematical research. Last fall, in an effort funded by the Energy Department&amp;rsquo;s Office of Science and supported by IBM Corp. of Armonk, N.Y., and VA Linux Systems Inc. of Sunnyvale, Calif., Argonne fielded a 512-processor Linux supercluster nicknamed Chiba City. Morgan said that the supercluster, which unites 256 IBM servers running VA Linux Systems via a combination of Fast Ethernet, Gigabit Ethernet and Myrinet interconnects, provides a scalable platform for use in &amp;ldquo;pure science research.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;He could not disclose the total cost of the implementation, but suggested that the laboratory&amp;rsquo;s long-term partnership with IBM allowed Argonne significant discounts on hardware street prices. Chiba City was assembled by Argonne team members in two days in September 1999, according to Morgan. It provides a flexible development environment for scalable, open-source software in four key categories: cluster management, high-performance systems software (file systems, schedulers and libraries), scientific visualization and distributed computing. The platform is used to &amp;ldquo;push the boundaries of high-end clustering and to design software and algorithms that can utilize it,&amp;rdquo; Morgan said. He said that Argonne-developed shared-processing software is at the heart of the National Computing Grid, a National Science Foundation-backed effort to provide a high-performance computing infrastructure accessible to government, academia and business.&lt;/p&gt;
&lt;p&gt;One of Argonne&amp;rsquo;s designers, Ian Foster, is credited with developing the Globus Project, Morgan said, &amp;ldquo;which is the basis of most Grid software.&amp;rdquo; In addition to contributing the underlying software and algorithms, Argonne also participates directly in grid management as a member of the National Computational Science Alliance. The alliance, which comprises more than 50 universities and research labs, was formed by NSF in October 1997 with the mission of prototyping an advanced computational infrastructure for the 21st century. NSF has promised to invest $170 million in the effort over five years.&lt;/p&gt;
&lt;p&gt;The alliance has been involved in numerous supercluster development projects, serving as a funding agent in some cases. It is headed by director Daniel Reed, who also chairs the Department of Computer Science at the University of Illinois in Champaign-Urbana. Morgan said three universities are the major stars in the supercomputing university: the University of Illinois, which hosts the National Center for Supercomputing Applications; the University of California at San Diego, which hosts the National Partnership for Advanced Computational Infrastructure; and the University of New Mexico, with its HPCERC. Chiba City was not developed as part of the NSF-sponsored Grid, but as a separate, Energy Department-funded project, Morgan noted. The difference is important, as it affects how the resources may be accessed and used by universities, businesses or government agencies. &amp;ldquo;We contribute software for the Grid, but we are not subject to the Grid&amp;rsquo;s peer review processes [which determine how, when and by whom the supercluster is used],&amp;rdquo; Morgan said. Argonne has collaborated since 1990 with numerous universities, businesses and government agencies on various projects, he said, but not as part of the Grid.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Scott Jackson, system administration task leader for Linux Clusters at the Pacific Northwest National Laboratory in Richland, Wash., noted that his organization&amp;rsquo;s new 96-node, dual-processor Linux supercluster is not part of the national Grid either, but has planned several Grid-type software experiments. As an Energy Department-backed lab, Pacific Northwest conducts research in the fields of environment, energy, health sciences and national security. The laboratory has been operated on behalf of the agency by Battelle Memorial Institute, Columbus, Ohio, since 1965. According to Jackson, the laboratory went looking in October 1999 for a supercomputing platform to support the growing needs of its 3,400-strong staff involved in multidisciplinary scientific research. &amp;ldquo;We looked at available technologies that would give us cost-effective operations and support for a diversity of applications, such as the molecular simulation of the effects of contamination on microorganisms,&amp;rdquo; Jackson said.&lt;/p&gt;
&lt;p&gt;Using $380,000 from internal laboratory investments and funds from two Department of Energy programs, Pacific Northwest turned to Dell Computer Corp., Round Rock, Texas, to build their supercluster solution, nicknamed Colony. &amp;ldquo;We already had a managed hardware program with Dell, and their recommended clustering solution was cost-effective,&amp;rdquo; said Jackson. Contracts were made for equipment and software in November 1999. Initially, the clustering technology selected for interconnecting the Dell PowerEdge-1300 servers was limited to 64 servers, said Jarek Nieplocha, chief scientist within Pacific Northwest&amp;rsquo;s Advanced Computing Group, who contributed his insights to the cluster acquisition. When Dell delivered the equipment in January, the cluster needed to be divided into two partitions of 32 and 64 nodes, respectively. &amp;ldquo;We upgraded the Giganet Network cLAN interconnect, operating the Virtual Interface Architecture (VIA) protocol, and merged the entire cluster into a single partition in early May,&amp;rdquo; Nieplocha said. VIA is an interface protocol that defines mechanisms for low-latency, high-bandwidth message passing between interconnected nodes. It is embraced by a number of industry-leading companies such as Compaq Computer Corp., Intel Corp. and Microsoft Corp. The Colony has become a key platform for researchers at Pacific Northwest, according to Jackson. It is not shared with the National Computational Science Alliance Grid because of the high demand of internal programs. The Energy Department, he noted, purchased the system, and agency programs get dibs on its use. Pacific Northwest is interested in upgrading and enlarging the Colony as budgets permit, Jackson added.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In contrast to the national laboratories, the University of New Mexico, with its two high-performance computing centers in Albuquerque and Maui, Hawaii, is a centerpiece of the Grid computing effort. Using a combination of Alliance and NSF funding, university money and grants from IBM, the university has fielded three clusters in as many years, according to Brian Smith and Patricia Kovatch, officials at the Albuquerque center. The latest is Los Lobos, which features 256 IBM NetFinity 4500R dual processor servers running the Red Hat Linux operating system and linked via a Myrinet interconnect.&lt;/p&gt;
&lt;p&gt;The new supercluster, which has an estimated street price of $2.25 million, is viewed as an upgrade of the university&amp;rsquo;s existing 128-node Road Runner supercluster, which was put into service last April. Equipment was delivered in early June and was expected to go live July 1, according to Kovatch, who manages the High Performance Computing Support Group at the center. Smith, the director there, said that Los Lobos would join the resources contributed by six educational institutions on the Grid. &amp;ldquo;Researchers can submit proposals for projects that will use compute cycles,&amp;rdquo; he said. &amp;ldquo;Three allocation boards, comprised of academicians at the San Diego and Illinois centers and the National Science Foundation, perform technical reviews of the reasonableness and feasibility of the proposed use, then grant time.&amp;rdquo; Kovatch said that for Los Lobos, researchers in fields ranging from astronomy and fluid dynamics to computational chemistry will have access to a computing platform capable of more than 375 million operations per second (called gigaflops). In addition to its superclustering work, New Mexico also is working with IBM on hyperclustering.&lt;/p&gt;
&lt;p&gt;A hypercluster couples two or more superclusters of different machine types. Using a hypercluster, the operations of an application can be allocated to either supercluster based on the suitability of the machine type. Gilfeather at New Mexico&amp;rsquo;s HPCERC used the example of a rendering application to illustrate the concept. &amp;ldquo;The Linux cluster can do the compute operations involved in efficient rendering,&amp;rdquo; Gilfeather said. &amp;ldquo;But current Linux clusters are weak in input/output operations and visualization, so these tasks could be passed to [a cluster of machines that perform these tasks well.]&amp;rdquo; Project Vista Azul, initiated at HPCERC in December 1999 following a grant from IBM, is expected to run through 2000 as university scientists seek to create a stable hypercluster from an IBM RS/6000 SP cluster and a Linux supercluster based on IBM NetFinity servers. Gilfeather said he expects the project to enhance existing &amp;ldquo;community codes&amp;rdquo; and produce new codes that can be reused within the community of researchers to build other hyperclusters. This will be a foundation for building scalable heterogeneous platforms to serve the broader world of business computing, he said.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Dave Turek, IBM&amp;rsquo;s vice president of deep computing, is more reluctant to endorse Linux clustering as ready for the mainstream. He referred to universities and government laboratories as classic early adopters of new technology. They have skills that enable them to capitalize on open-source code, &amp;ldquo;free labor&amp;rdquo; in the form of skilled workers and graduate students and a motivation to do as much as possible with limited resources, Turek said. And &amp;ldquo;they have strong economic reasons for acquiring free or low-cost operating systems and running them on commodity hardware,&amp;rdquo; Turek said. Once the Linux platform becomes operational, service and support is an even bigger issue, Turek said. While experience shows that support of Linux is more timely than other software products ? &amp;ldquo;if you have a problem, send it out via e-mail and you will probably find another Linux programmer somewhere in the world who can help you at any time day or night&amp;rdquo; ? he questioned whether companies will be willing to surrender their shrink-wrap software service agreements. Al Stutz, director of high-performance computing at the Ohio Supercomputer Center in Columbus, said &amp;ldquo;only time will tell the answer to that issue.&amp;rdquo; Stutz said support from Silicon Graphics Inc. of Mountain View, Calif., with respect to his center&amp;rsquo;s 128-processor supercluster has been very good to date. New Mexico&amp;rsquo;s Kovatch said the calls she fields from businesses are increasing. &amp;ldquo;We get calls from companies who are interested in Linux clusters. [They] are finding that relying on one vendor to look after their software [leads to systems] that are down more often than they are up,&amp;rdquo; she said. The ultimate determination of Linux clustering, superclustering or hyperclustering success will be the number of applications found suitable to the platform, said Morgan at Argonne. &amp;ldquo;There is a divided opinion over what is useful or appropriate for high-performance computing even in scientific research,&amp;rdquo; he said. &amp;ldquo;Being able to get support is important. But it is also important to identify applications that can take advantage of parallel processing. &amp;ldquo;Certain types of code will never run on this platform,&amp;rdquo; he said. &amp;ldquo;Data mining, however, is an example of a business app that is also a parallel app. There may be others that haven&amp;rsquo;t been invented yet.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By Jon William Toigo&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The terminology of high-performance clustering can sound more like astronomy than computer science. Here is a brief definition of terms: Connecting two or more computers in such a way that they behave like a single computer. Clustering is used for parallel processing of applications, for load balancing and for fault tolerance. Most IT professionals are familiar with failover clustering, in which a server takes over the load from a primary server if the primary server fails. Application clusters, invented by Digital Equipment Corp. nearly 20 years ago, are tight couplings of servers that represent themselves to applications as a single, virtual server system. Application operations can be distributed among the processors of an application cluster to make the best possible use of the computing resources available, resulting theoretically in the best possible application performance. A technology for coupling the servers that comprise nodes in a cluster. Interconnects range from Fast Ethernet and Gigabit Ethernet network connections to high-performance and low-cost products, such as Myrinet from Myracom Inc., to proprietary (and often extremely expensive) technologies from Compaq Computer Corp., Silicon Graphics Inc. and others. The simultaneous use of more than one central processing unit to execute an application program. Theoretically, parallel processing makes a program operate more efficiently because there are more processing engines to support it. In practice, it is often difficult to divide a program to capitalize on multiple CPUs without having program operations interfere with each other. Parallel processing is different from multitasking, in which a single processor executes several programs at once. A term coined by the University of New Mexico High Performance Computing, Education and Research Center to describe a class of clusters featuring nodes with fast processors and large memories coupled via a high bandwidth (greater than one gigabit per second), low-latency (under 20 microsecond) interconnect technology. Another University of New Mexico HPCERC-coined term describing clustering architecture in which two or more superclusters, often with different node operating systems (i.e., IBM AIX and Linux), are coupled via a common interconnect technology. Application processes may be divided between the joined superclusters based on the appropriateness of the supercluster to the application program task.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.washingtontechnology.com/2000/06/open-systems/325217/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.washingtontechnology.com/2000/06/open-systems/325217/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Albuquerque Tribune Business Almanac</title>
      <link>http://localhost:1313/blog/20000619-abqtribune/</link>
      <pubDate>Mon, 19 Jun 2000 10:45:35 -0600</pubDate>
      <guid>http://localhost:1313/blog/20000619-abqtribune/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, an assistant professor of computer engineering at UNM&amp;rsquo;s Electrical and Computer Engineering Department, received The Institute of Electrical and Electronics Engineers Outstand Young Engineering Award 2000 from the UNM chapter of the IEEE.&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_43_hu_2e29ed5ea51b920.webp 400w,
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_43_hu_9b73b917ed51219c.webp 760w,
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_43_hu_9a6939342c6b1f09.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_43_hu_2e29ed5ea51b920.webp&#34;
               width=&#34;593&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_44_hu_ac438731ab00e075.webp 400w,
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_44_hu_affc8cfdf87f5b70.webp 760w,
               /blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_44_hu_97c2131eeeb23893.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000619-abqtribune/The_Albuquerque_Tribune_2000_06_19_44_hu_ac438731ab00e075.webp&#34;
               width=&#34;345&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Bader receives IEEE &amp; Sigma Xi 2000 Young Outstanding Engineer Award</title>
      <link>http://localhost:1313/blog/20000510-sigmaxi/</link>
      <pubDate>Wed, 10 May 2000 19:03:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000510-sigmaxi/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000510-sigmaxi/letter_hu_1e56126be433c28d.webp 400w,
               /blog/20000510-sigmaxi/letter_hu_840b22dcb9c040.webp 760w,
               /blog/20000510-sigmaxi/letter_hu_2ba23b177f2ad2da.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000510-sigmaxi/letter_hu_1e56126be433c28d.webp&#34;
               width=&#34;582&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000510-sigmaxi/certificate_hu_b5a664a66c52d4ab.webp 400w,
               /blog/20000510-sigmaxi/certificate_hu_956db64d07b0fc18.webp 760w,
               /blog/20000510-sigmaxi/certificate_hu_6633d5fc824f14d7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000510-sigmaxi/certificate_hu_b5a664a66c52d4ab.webp&#34;
               width=&#34;760&#34;
               height=&#34;588&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Sigma Xi, The Scientific Research Society&lt;/p&gt;
&lt;p&gt;Devoted to the Promotion of Research in Science&lt;/p&gt;
&lt;p&gt;Certificate of Recognition&lt;/p&gt;
&lt;p&gt;Presented by&lt;/p&gt;
&lt;p&gt;The University of New Mexico Chapter&lt;/p&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;&lt;br&gt;
Young Outstanding Engineering&lt;br&gt;
2000&lt;/p&gt;
&lt;p&gt;Harjit S. Ahluwalia&lt;br&gt;
Chapter President&lt;/p&gt;
&lt;p&gt;Walter Gerstle&lt;br&gt;
Chapter Secretary&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Receives Junior Faculty Research Excellence Award, University of New Mexico</title>
      <link>http://localhost:1313/blog/20000505-unm/</link>
      <pubDate>Fri, 05 May 2000 18:35:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000505-unm/</guid>
      <description>&lt;p&gt;Congratulations to &lt;strong&gt;Professor David Bader&lt;/strong&gt; who has been selected as the recipient for the SOE 2000 Junior Research Excellence Award.  Professor Bader has been active in research on parallel algorithms and applications for high performance computers. The award will be presented by Dean Paul Fleury and ECE Department Chair Christos Christodoulou on Friday, May 5 at 11:30am at the annual SOE Awards Event.&lt;/p&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000505-unm/letter_hu_92f82e9b5fa9cec8.webp 400w,
               /blog/20000505-unm/letter_hu_f2e56952324f10f8.webp 760w,
               /blog/20000505-unm/letter_hu_66b9fd7f4c4efd47.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000505-unm/letter_hu_92f82e9b5fa9cec8.webp&#34;
               width=&#34;591&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000505-unm/award_hu_89744736ed505618.webp 400w,
               /blog/20000505-unm/award_hu_a037fbcc0a66155e.webp 760w,
               /blog/20000505-unm/award_hu_d1322db9fe6bd9a9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000505-unm/award_hu_89744736ed505618.webp&#34;
               width=&#34;661&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;University of New Mexico&lt;/p&gt;
&lt;p&gt;Junior Faculty Research Excellence Award&lt;/p&gt;
&lt;p&gt;May 5, 2000&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Electrical and Computer Engineering Dept.&lt;br&gt;
School of Engineering&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Supercomputer System</title>
      <link>http://localhost:1313/blog/20000417-esj/</link>
      <pubDate>Mon, 17 Apr 2000 10:43:04 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000417-esj/</guid>
      <description>&lt;p&gt;IBM is providing a supercomputer that uses the Linux operating system, allowing researchers and developers access to computational power they previously could not afford.&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance, comprised of 50 academic, government and research partners, will use IBM computers as a part of its effort to create a new, highly sophisticated computer network for research.&lt;/p&gt;
&lt;p&gt;The system, called Los Lobos, is a &amp;ldquo;supercluster&amp;rdquo; of servers, and consists of 256 IBM Netfinity PC servers, which are linked together using special clustering software and high-speed networking hardware, acting as one to process at a speed of 375 gigaflops, or 375 billion operations per second. That speed would place Los Lobos at number 24 on the world&amp;rsquo;s current list of the top 500 fastest supercomputers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://esj.com/articles/2000/04/17/linux-supercomputer-system.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://esj.com/articles/2000/04/17/linux-supercomputer-system.aspx&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader Awarded EECE Outstanding Researcher of the Year (First Recipient of this Award)</title>
      <link>http://localhost:1313/blog/20000412-unm/</link>
      <pubDate>Wed, 12 Apr 2000 21:13:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000412-unm/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/20000412-unm/letter_hu_8c9e6dd6b113660d.webp 400w,
               /blog/20000412-unm/letter_hu_6dcc630fb2f205c8.webp 760w,
               /blog/20000412-unm/letter_hu_4ec96be5ec071235.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/20000412-unm/letter_hu_8c9e6dd6b113660d.webp&#34;
               width=&#34;583&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Gigantic clusters: Where are they and what are they doing?</title>
      <link>http://localhost:1313/blog/20000401-ieeeconcurrency/</link>
      <pubDate>Sat, 01 Apr 2000 22:26:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000401-ieeeconcurrency/</guid>
      <description>&lt;h3 id=&#34;collosal-clusters-at-alliance-centers&#34;&gt;COLLOSAL CLUSTERS AT ALLIANCE CENTERS&lt;/h3&gt;
&lt;p&gt;The Albuquerque High Performance Computing Center at
the University of New Mexico has long been a proponent of
colossal clusters. The AHPCC and the National Computational
Science Alliance (the Alliance), comprising more than 50
academic, government, and industry research partners from
across the US, have formed a partnership that the National
Science Foundation funds. The Alliance, which wants to provide
an advanced computational infrastructure, is running a
128-processor Linux SuperCluster with Myrinet (Roadrunner)
from Alta Technologies using dual Intel 450-MHz nodes,
each with 512 Mbytes of RAM. The AHPCC is acquiring a
512-processor Linux SuperCluster known as Los Lobos,
reports &lt;strong&gt;David Bader&lt;/strong&gt; of the University of New Mexico. The
Alliance intends to make Los Lobos the largest open-production
Linux supercluster geared to the research community.&lt;/p&gt;
&lt;p&gt;Los Lobos uses dual Intel 733-MHz IA-32 processor
Netfinity 4500R nodes; 1 Gbyte of memory per node; 1 Tbyte
of SSA RAID; and 2 Tbytes of tertiary storage (tape robot) to
deliver a peak theoretical performance of 375 Gflops. The
high-performance interconnect network between the cluster
nodes is Myricom’s Myrinet, providing speeds exceeding 1
Gbits per second, which is comparable
to the fastest interconnects in today’s
traditional supercomputers. The system
maximizes the computing power per
square foot on an Intel-based platform.
This thin server is designed to deliver
the highest computing power per square
foot on Intel-based platforms and is one
of the industry’s most complete rackoptimized
product lines for Linux, Windows,
and Novell servers, according to
IBM. Patricia Kovatch, High Performance
Computing Systems Group
manager, thinks Linux clusters are compelling
for several reasons, especially for
the cost-performance ratio. Furthermore, the cost is much
less than buying a traditional supercomputer, and the performance
rivals one. Another benefit for applications folks is the
ease of porting their applications to Linux from other Unix
environments. The prospects for the near future look even
more momentous: “Multiple multiple-terascale Linux-based
superclusters will be built in the next year with a 10-terascale
or better Linux supercluster highly likely in about a year,”
says Frank Gilfeather, Executive Director of High Performance
Computing.&lt;/p&gt;
&lt;p&gt;Another Alliance partner, the National Center for Supercomputing
Applications at the University of Illinois at Urbana-
Champaign, is running a 128-processor dual-Pentium-III
Xeon, 550-MHz, 1-Gbyte RAM, and Myrinet network technology
on an NT operating system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM builds Linux supercluster</title>
      <link>http://localhost:1313/blog/20000330-cnn/</link>
      <pubDate>Thu, 30 Mar 2000 11:02:26 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000330-cnn/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Marc Songini, Network World Fusion&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;(IDG) &amp;ndash; A system of computers from IBM may soon let users run Linux applications at supercomputer speeds.&lt;/p&gt;
&lt;p&gt;Last week, IBM announced it had jointly created a cluster of 256 Netfinity two-way servers running Red Hat Linux at the &lt;strong&gt;University of New Mexico&lt;/strong&gt;. The resulting prototype system, called &amp;ldquo;Los Lobos,&amp;rdquo; can handle 375 billion operations per second - making it the 24th fastest supercomputer in the world, IBM says. The fastest machine in that class is reputed to be the ASCI Red SuperComputer, based in Sandia National Laboratories, which handles one trillion operations per second.&lt;/p&gt;
&lt;p&gt;The $1.5 million Los Lobos system, expected to go into production this summer, will run complex math, physics, chemistry and genetics applications for scientists and researchers located all over the country who will be able to tap Los Lobos via the Internet. IBM is considering rolling out a commercial version of Los Lobos software in the future, says Tom Bradicich, an IBM executive.&lt;/p&gt;
&lt;p&gt;Experts say that while Linux is popular with certain technical users and is widely used as a basis for Web sites, it is not considered to have the muscle for handling industrial-strength business tasks. Los Lobos may change that perception.&lt;/p&gt;
&lt;p&gt;Los Lobos isn&amp;rsquo;t the only example of large Linux clusters. Alta Technology claims its parallel servers will let users tie together more than a 1,000 Intel processors at a time to run Linux applications.&lt;/p&gt;
&lt;p&gt;IBM already offers a supercomputer, the RS/6000 SP, that doesn&amp;rsquo;t run Linux, but rather IBM&amp;rsquo;s AIX. Nevertheless, a Linux supercomputer would be a lot less expensive than an SP, which can cost millions of dollars.&lt;/p&gt;
&lt;p&gt;Teams of programmers from IBM and the university wrote the software to install, configure and manage the Los Lobos cluster, using open source Beowulf code and tools from third parties such as the Extreme Linux community.&lt;/p&gt;
&lt;p&gt;Within the Linux supercluster, one dedicated server acts as the master scheduler, deciding which individual machines will be assigned which tasks and when. On the network side, the lab is using Myrinet, a proprietary network technology from Myricom, to link processors and server chassis.&lt;/p&gt;
&lt;p&gt;Similar Linux clusters are going to become commonplace for shops needing a lot of computing power, according to Frank Gilfeather, the University of New Mexico&amp;rsquo;s director of high-performance computing. &amp;ldquo;This is the future,&amp;rdquo; he says.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cnn.com/2000/TECH/computing/03/30/uber.linux.idg/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cnn.com/2000/TECH/computing/03/30/uber.linux.idg/index.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Los Lobos launched</title>
      <link>http://localhost:1313/blog/20000330-globalmail/</link>
      <pubDate>Thu, 30 Mar 2000 10:37:10 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000330-globalmail/</guid>
      <description>&lt;p&gt;IBM is launching a new class of low-cost supercomputers that use the Linux operating system, allowing researchers and developers access to computational power that they previously couldn&amp;rsquo;t afford.&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance, comprised of 50 academic, government and research partners, says it will use the system as a part of its effort to create a new, highly sophisticated computer network for research.&lt;/p&gt;
&lt;p&gt;The system, called Los Lobos, is actually a supercluster of 256 PC servers (powerful computers that manage networks of other computers) that will be located at the University of New Mexico.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.theglobeandmail.com/technology/los-lobos-launched/article4162390/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.theglobeandmail.com/technology/los-lobos-launched/article4162390/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zooming in: Linux power quest</title>
      <link>http://localhost:1313/blog/20000327-bostonglobe/</link>
      <pubDate>Mon, 27 Mar 2000 15:24:15 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000327-bostonglobe/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Hiawatha Bray&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;technology--innovation-sector-report--hardware&#34;&gt;TECHNOLOGY &amp;amp; INNOVATION SECTOR REPORT / HARDWARE&lt;/h1&gt;
&lt;p&gt;When the leading maker of dial-up telephone modems, 3Com Inc., announces that it&amp;rsquo;s getting out of the business, you might think it&amp;rsquo;s a sign that old-fashioned modems are doomed.&lt;/p&gt;
&lt;p&gt;But then, you&amp;rsquo;re not Frank Manning, president of Boston-based Zoom Telephonics Inc., a major modem producer. Manning says his company, like 3Com, sees booming demand for high-speed cable and DSL connections to the Internet. But Zoom has no intention of bailing out of the market for traditional modems.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We will continue to support our Zoom and Hayes brand modem customers and will continue to develop new modem products,&amp;rdquo; Manning said last week. He added that having a strong presence in the modem business should boost demand for more advanced Zoom products.&lt;/p&gt;
&lt;p&gt;Just last year, Zoom reinforced its commitment to the modem business through its purchase of the US assets of Hayes Corp. Before falling on hard times in the 1990s, Hayes had been the dominant maker of telephone modems and had actually created the software used throughout the industry to control the way modems work.&lt;/p&gt;
&lt;p&gt;Zoom says it is investing most of its research and development budget into various broadband technologies that are likely to supersede traditional modems. Zoom already makes a line of cable modems, and is working on DSL products.&lt;/p&gt;
&lt;p&gt;But there will still be improvements in traditional modems. A new standard called V.92 is expected this year. The V.92 modems will allow for &amp;ldquo;call waiting,&amp;rdquo; so that a user can interrupt an Internet session, take a voice phone call, then hop right back onto the Net.&lt;/p&gt;
&lt;h2 id=&#34;the-skinny-on-servers&#34;&gt;The skinny on servers&lt;/h2&gt;
&lt;p&gt;For most of us, an appliance is something that keeps the beer cold. For many companies on the Internet, however, an appliance is a simple but powerful plug-and-play server computer. When an online firm needs to boost its computing capacity, many rely on server appliances from Randolph-based Network Engines Inc.&lt;/p&gt;
&lt;p&gt;Setting up an Internet server can be a daunting task, even for someone with technical skill. Network Engines offers servers preloaded with the Linux operating system or with Microsoft Corp.&amp;rsquo;s Windows 2000 operating system. Each server is about as thick as a pizza box, designed to snap easily into the mounting racks favored at heavy-duty server installations.&lt;/p&gt;
&lt;p&gt;Network Engines&amp;rsquo; skinny boxes are making a surprisingly large splash. The hot Linux computer maker VA Linux Systems is sticking its label on a box made by Network Engines, and IBM Corp.&amp;rsquo;s Netfinity 4000R server is a Network Engine in disguise.&lt;/p&gt;
&lt;h2 id=&#34;linux-power-quest&#34;&gt;Linux power quest&lt;/h2&gt;
&lt;p&gt;Speaking of IBM, that company is throwing a bunch of its server computers to the wolves in an effort to hasten commercial applications of the Linux operating system.&lt;/p&gt;
&lt;p&gt;In cooperation with the &lt;strong&gt;University of New Mexico&lt;/strong&gt;, IBM is hitching together 256 of its Netfinity server computers into a giant Linux supercomputer called LosLobos, Spanish for &amp;ldquo;wolfpack.&amp;rdquo; They&amp;rsquo;re using Netfinity. Each of the Netfinity computers will contain a pair of Intel Corp. microprocessors, for a total of 512 processors. The result, say IBM researchers, will be a supercomputer cluster capable of performing up to 357 billion calculations per second.&lt;/p&gt;
&lt;p&gt;NASA scientists first began building computing superclusters in 1994, in a project called Beowulf. Since then, universities around the world have constructed ever more powerful clusters, based on standard PC hardware and the Linux operating system. But few of these Beowulf-type systems are used for commercial applications. They&amp;rsquo;re used for ultra-sophisticated scientific research, such as analyzing the planet&amp;rsquo;s atmosphere.&lt;/p&gt;
&lt;p&gt;But IBM says it hopes to use LosLobos to develop Linux-based business software that fully exploits the power of the system. That could mean a boost for IBM&amp;rsquo;s e-business efforts and for the company&amp;rsquo;s aggressive new move into Linux software.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re about 2 to 3 percent into what the Internet is going to do for the world,&amp;rdquo; said John Patrick, IBM&amp;rsquo;s vice president of Internet technology. As merchants seek to add more and better services, they&amp;rsquo;ll need all the computing power they can muster. &amp;ldquo;Down the road,&amp;rdquo; said Patrick, &amp;ldquo;we believe that Linux clusters are going to be as important for e-business as they are for modeling the weather.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Each week, Sector Report will examine a different segment of the high-tech economy. Next week: Business-to-Business.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM builds Linux supercluster</title>
      <link>http://localhost:1313/blog/20000327-networkworld/</link>
      <pubDate>Mon, 27 Mar 2000 11:08:19 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000327-networkworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;by Marc Songini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;ALBUQUERQUE, N.M. &amp;ndash; A system of computers from IBM may soon let users run Linux applications at supercomputer speeds.&lt;/p&gt;
&lt;p&gt;Last week, IBM announced it had jointly created a cluster of 256 Netfinity two-way servers running Red Hat Linux at the &lt;strong&gt;University of New Mexico&lt;/strong&gt;. The resulting prototype system, called &amp;ldquo;Los Lobos,&amp;rdquo; can handle 375 billion operations per second - making it the 24th fastest supercomputer in the world, IBM says. The fastest machine in that class is reputed to be the ASCI Red SuperComputer, based in Sandia National Laboratories, which handles one trillion operations per second.&lt;/p&gt;
&lt;p&gt;The $1.5 million Los Lobos system, expected to go into production this summer, will run complex math, physics, chemistry and genetics applications for scientists and researchers located all over the country who will be able to tap Los Lobos via the Internet. IBM is considering rolling out a commercial version of Los Lobos software in the future, says Tom Bradicich, an IBM executive.&lt;/p&gt;
&lt;p&gt;Experts say that while Linux is popular with certain technical users and is widely used as a basis for Web sites, it is not considered to have the muscle for handling industrial-strength business tasks. Los Lobos may change that perception.&lt;/p&gt;
&lt;p&gt;Los Lobos isn&amp;rsquo;t the only example of large Linux clusters. Alta Technology claims its parallel servers will let users tie together more than a 1,000 Intel processors at a time to run Linux applications.&lt;/p&gt;
&lt;p&gt;IBM already offers a supercomputer, the RS/6000 SP, that doesn&amp;rsquo;t run Linux, but rather IBM&amp;rsquo;s AIX. Nevertheless, a Linux supercomputer would be a lot less expensive than an SP, which can cost millions of dollars.&lt;/p&gt;
&lt;p&gt;Teams of programmers from IBM and the university wrote the software to install, configure and manage the Los Lobos cluster, using open source Beowulf code and tools from third parties such as the Extreme Linux community.&lt;/p&gt;
&lt;p&gt;Within the Linux supercluster, one dedicated server acts as the master scheduler, deciding which individual machines will be assigned which tasks and when. On the network side, the lab is using Myrinet, a proprietary network technology from Myricom, to link processors and server chassis.&lt;/p&gt;
&lt;p&gt;Similar Linux clusters are going to become commonplace for shops needing a lot of computing power, according to Frank Gilfeather, the University of New Mexico&amp;rsquo;s director of high-performance computing. &amp;ldquo;This is the future,&amp;rdquo; he says.&lt;/p&gt;
&lt;p&gt;IBM: &lt;a href=&#34;https://www.ibm.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.ibm.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://books.google.com/books?id=fBcEAAAAMBAJ&amp;amp;pg=PA14&amp;amp;lpg=PA14#v=onepage&amp;amp;q&amp;amp;f=false&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://books.google.com/books?id=fBcEAAAAMBAJ&amp;pg=PA14&amp;lpg=PA14#v=onepage&amp;q&amp;f=false&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Introduces 512-Processor Linux Supercluster</title>
      <link>http://localhost:1313/blog/20000324-hpcwire/</link>
      <pubDate>Fri, 24 Mar 2000 06:50:03 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000324-hpcwire/</guid>
      <description>&lt;p&gt;San Diego, CA — The National Computational Science Alliance (Alliance) will take delivery of a 512-processor Linux supercluster within the next month – a move that will give this nationwide partnership the largest open production Linux supercluster aimed at the research community. The new supercluster, called LosLobos, will be located at the &lt;strong&gt;University of New Mexico’s (UNM) Albuquerque High Performance Computing Center (AHPCC)&lt;/strong&gt;, one of the Alliance Partners for Advanced Computational Resources sites.&lt;/p&gt;
&lt;p&gt;“The Linux movement benefits greatly from the strong support of top researchers and programmers at our nation’s leading universities and laboratories,” said Irving Wladawsky-Berger, Vice President, Technology &amp;amp; Strategy, IBM Enterprise Systems Group. “The innovation of the Alliance, supported by the research community, will lead the way for commercial Linux applications and product development.”&lt;/p&gt;
&lt;p&gt;LosLobos will consist of 256 IBM Netfinity PC Servers. The 733 MHz Intel IA-32 processor based computer system is expected to provide a peak theoretical performance of about 375 gigaflops. The high performance interconnect network between the cluster nodes will be Myricom’s Myrinet, providing speeds exceeding 1 gigabyte per second, which is comparable to the fastest interconnects in today’s traditional supercomputers.&lt;/p&gt;
&lt;p&gt;This new supercluster will increase the computing capabilities of the Alliance, a National Science Foundation (NSF) funded partnership, providing researchers with a platform for developing improved cluster management tools. It will also offer the research community the chance to gain operational experience on large-scale clusters, and to explore the scalability of different types of science and engineering applications.&lt;/p&gt;
&lt;p&gt;“The Alliance is committed to leading the effort to develop and deploy large clusters to the scientific research community,” said Dan Reed, director of the Alliance. “LosLobos represents the synthesis of two major trends in the scientific and research community: open source software and commodity high performance cluster computing. Open source software, such as Linux, decreases costs dramatically because the community of users continually augments the software base. In addition, superclusters made from off-the-shelf products are a very cost effective way to offer supercomputer performance to the user community.”&lt;/p&gt;
&lt;p&gt;Unlike previous superclusters made of commodity computers, LosLobos will integrate new advanced management tools, such as the Maui Scheduler, into its array of services. The Maui Scheduler, developed by UNM’s Maui High Performance Computing Center (MHPCC), will integrate scheduling and reservation capabilities into the cluster, decreasing queue wait times and allowing easier access to a larger percentage of computing resources. LosLobos will be a part of the Alliance’s Virtual Machine Room, which is the national infrastructure for geographically distributed computing.&lt;/p&gt;
&lt;p&gt;“Superclusters are becoming supercomputers in their own right,” noted Frank Gilfeather, executive director of the High Performance Computing, Educational and Research Center (HPCERC) at the University of New Mexico. “Many researchers are now using smaller clusters for research development, and are then turning to superclusters for large-scale computations. Develop research locally, then seamlessly run globally using open source software developed to support specific research disciplines is the model that will bring high performance computing to a broader base of researchers.”&lt;/p&gt;
&lt;p&gt;LosLobos represents a major trend in supercomputing – a moderately priced entry point into high performance computing that offers the enhanced performance required by scientific and engineering applications. The IBM supercluster system will be delivered in mid-April, and the Alliance will begin to allocate time nationally on the system this summer. The Alliance currently offers a 128-processor Linux supercluster, RoadRunner, also located at AHPCC, and a 416-processor supercluster primarily used with Windows NT at its leading edge site, the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign.&lt;/p&gt;
&lt;p&gt;The Alliance is a partnership to prototype an advanced computational infrastructure for the 21st century and includes more than 50 academic, government and industry research partners from across the United States. The Alliance is one of two partnerships funded by the NSF’s Partnerships for Advanced Computational Infrastructure (PACI) program, and receives cost sharing at partner institutions. NSF also supports the National Partnership for Advanced Computational Infrastructure (NPACI), led by the San Diego Supercomputer Center.&lt;/p&gt;
&lt;p&gt;NCSA is the leading-edge site for the Alliance. NCSA is a leader in the development and deployment of cutting-edge high-performance computing, networking, and information technologies. NSF, the state of Illinois, the University of Illinois, industrial partners, and other federal agencies fund NCSA.&lt;/p&gt;
&lt;p&gt;HPCERC, a strategic center of the University of New Mexico, is a partner of the Alliance, and its two supercomputing centers, AHPCC and the MHPCC, are SuperNodes on the PACI Grid. AHPCC and MHPCC are supported through grants and contracts from the Air Force Research Laboratory, the Department of Defense, NSF, other government agencies, and commercial contracts.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/2000/03/24/alliance-introduces-512-processor-linux-supercluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/2000/03/24/alliance-introduces-512-processor-linux-supercluster/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM develops low-cost Linux supercomputer system</title>
      <link>http://localhost:1313/blog/20000323-reuters/</link>
      <pubDate>Thu, 23 Mar 2000 14:56:41 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000323-reuters/</guid>
      <description>&lt;p&gt;IBM said on Wednesday it is providing a new class of low-cost supercomputer that uses the Linux alternative operating system, allowing researchers and developers access to computational power they previously could not afford.&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance, comprised of 50 academic, government and research partners, said it would use the system of IBM computers as a part of its effort to create a new highly sophisticated computer network for research.&lt;/p&gt;
&lt;p&gt;The system, called Los Lobos, is actually a &amp;ldquo;supercluster&amp;rdquo; of servers, which are powerful computers that manage networks of other computers. It will be located at the &lt;strong&gt;University of New Mexico&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Los Lobos will consist of 256 IBM Netfinity PC servers, which linked together will provide supercomputer-level performance at lower cost.&lt;/p&gt;
&lt;p&gt;A source close to IBM said this was the first of six such deals with universities expected this year.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Linux superclusters are the new supercomputers of the 21st Century,&amp;rdquo; said Frank Gilfeather, the University of New Mexico`s director of supercomputing. &amp;ldquo;We see them as replacing the traditional Cray, IBM, and Silicon Graphics supercomputers because of their cost benefit.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Researchers said it was the combination of industry-standard PC servers with the Linux operating system that would put supercomputer performance within reach for modestly funded research projects.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Open source software, such as Linux, decreases costs dramatically because the community of users continually augments the software base,&amp;rdquo; said Dan Reed, director of the Alliance. &amp;ldquo;In addition, superclusters made from off-the-shelf products are a very cost effective way to offer supercomputer performance to the user community.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Netfinity servers are linked together using special clustering software and high speed networking hardware, acting as one to process at a speed of 375 gigaflops, or 375 billion operations per second. That speed would place Los Lobos at No. 24 on the world`s current list of the top 500 fastest supercomputers.&lt;/p&gt;
&lt;p&gt;The effort is seen as a nod to the potential of Linux, which has grown from being a programmer`s hobby into a force in the commercial marketplace for computer operating systems.&lt;/p&gt;
&lt;p&gt;While Linux is popular with technicians and used widely at the back-end of Web sites, it is not yet considered to have the muscle for handling industrial-strength business tasks.&lt;/p&gt;
&lt;p&gt;IBM, which has embraced the fast-growing operating system by making many of its products run Linux, said that the work of the research community could help to beef up Linux for wider commercial uses.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Linux movement is benefiting greatly from the strong support of top researchers and programmers at our nation&amp;rsquo;s leading universities and laboratories,&amp;rdquo; said Irving Wladawsky-Berger, vice president of technology and strategy at IBM`s Enterprise Systems Group.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The innovation of the National Computational Science Alliance supported by the research community will lead the way for commercial Linux applications and product development,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;The IBM supercluster system will be delivered in mid-April, and the Alliance will begin to allocate time nationally on the system this summer, for use to make advances in areas such as medicine, physics, chemistry and genetics.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.itweb.co.za/content/LPwQ57ly2PoMNgkj&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.itweb.co.za/content/LPwQ57ly2PoMNgkj&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM Supercomputer to Use Linux</title>
      <link>http://localhost:1313/blog/20000323-latimes/</link>
      <pubDate>Thu, 23 Mar 2000 13:43:01 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000323-latimes/</guid>
      <description>&lt;p&gt;IBM Corp. said it’s providing a new class of low-cost supercomputer that uses the Linux alternative operating system, allowing researchers and developers access to computational power they previously could not afford. The National Computational Science Alliance, made up of 50 academic, government and research partners, said it would use the system of IBM computers as a part of its effort to create a new, highly sophisticated computer network for research. The system, called Los Lobos, is actually a “supercluster” of servers acting as one to process at a speed of 375 gigaflops, or 375 billion operations per second. It will be located at the &lt;strong&gt;University of New Mexico&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.latimes.com/archives/la-xpm-2000-mar-23-fi-11793-story.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.latimes.com/archives/la-xpm-2000-mar-23-fi-11793-story.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM Unveils Linux-Based Supercomputer</title>
      <link>http://localhost:1313/blog/20200323-ecommercetimes/</link>
      <pubDate>Thu, 23 Mar 2000 13:16:49 -0500</pubDate>
      <guid>http://localhost:1313/blog/20200323-ecommercetimes/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Matthew Beale&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;IBM (NYSE: IBM) has edged yet another step away from Microsoft by working with the &lt;strong&gt;University of New Mexico&lt;/strong&gt; to create a Linux-based supercomputer that will do more than just crunch research numbers.&lt;/p&gt;
&lt;p&gt;If Big Blue has its way, the new Linux-powered Los Lobos clustering computer will significantly accelerate the range and capabilities of e-commerce operations while reducing costs dramatically.&lt;/p&gt;
&lt;p&gt;Linux-based servers have been moving into the market for e-commerce solutions where, according to experts, a growing number of businesses are recognizing the benefits of open-source versus the proprietary model used by such operating systems as Microsoft&amp;rsquo;s Windows.&lt;/p&gt;
&lt;p&gt;Set for release later this year &amp;ndash; possibly as early as April &amp;ndash; the 512 processor-driven Los Lobos could also be deployed for database hosting and e-mail functionality.&lt;/p&gt;
&lt;h2 id=&#34;ibm-linux-and-e-commerce&#34;&gt;IBM, Linux and E-Commerce&lt;/h2&gt;
&lt;p&gt;Big Blue began seriously moving on Linux last year, administering its ServerProven Solutions program in order to test software compatibility with Netfinity servers for vendors that support the open-source Linux platform. &amp;ldquo;The validation provided by the ServerProven program helps give customers additional confidence to deploy Linux,&amp;rdquo; observed Sandy Carter, director of IBM PartnerWorld for Developers, Netfinity brand.&lt;/p&gt;
&lt;p&gt;In February, IBM issued a spate of announcements that could have a significant impact on e-commerce professionals and the open-source community, including an initiative to foster open-source development. The computer industry titan now offers versions of its software &amp;ndash; at no cost to commercial developers &amp;ndash; to initiate what it hopes will become a new class of Linux applications.&lt;/p&gt;
&lt;p&gt;Tapping into its strategic partnership with Linux vendor Caldera Systems, Inc., IBM distributes an application developer&amp;rsquo;s kit, which offers messaging, collaboration and dynamic Web application serving capabilities. The kit also includes Java technology and tools for application development, a relational database for managing information and a Web application server for tasks such as Web publishing.&lt;/p&gt;
&lt;p&gt;IBM also worked to boost the e-commerce features of Caldera&amp;rsquo;s OpenLinux eServer 2.3 package release, bundling its WebSphere Application Server and VisualAge products.&lt;/p&gt;
&lt;h2 id=&#34;clustering-benefits&#34;&gt;Clustering Benefits&lt;/h2&gt;
&lt;p&gt;IBM, which recently announced several supply chain management initiatives to assist mid-sized business-to-business (B2B) e-commerce companies, is certainly not the only player in the Linux supercomputer game.&lt;/p&gt;
&lt;p&gt;In January, IBM strategic partner TurboLinux announced the release of its enFuzion clustering technology that is designed to transform a company&amp;rsquo;s network of Linux, UNIX and Windows servers and workstations into a supercomputer.&lt;/p&gt;
&lt;p&gt;enFuzion, typically utilized for modeling complex problems, is also designed to accelerate application calculations by more than 100 times, and supports Linux, HP-UX, IBM-AIX, SGI Irix, Solaris, Tru64 and Windows NT platforms. AMP Asset Management and JP Morgan were early enFuzion adopters.&lt;/p&gt;
&lt;p&gt;Clustering utilizes multiple linked servers that operate as one machine and is user transparent. If one server fails or crashes while processing a user&amp;rsquo;s request, another in the cluster is made available within a very short period of time.&lt;/p&gt;
&lt;p&gt;According to TurboLinux, Linux-based systems are being widely used for enterprise applications such as print, file and Web servers. Current developments represent a Linux penetration for the first time into higher-end computing environments, such as high-traffic e-commerce Web sites.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ecommercetimes.com/story/2809.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ecommercetimes.com/story/2809.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM to build powerful Linux cluster</title>
      <link>http://localhost:1313/blog/20000322-computerworld/</link>
      <pubDate>Wed, 22 Mar 2000 06:52:23 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000322-computerworld/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Dominique Deckmyn&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;IBM is helping the &lt;strong&gt;University of New Mexico&lt;/strong&gt; build what it claims will be the world&amp;rsquo;s fastest Linux supercomputer. It is a cluster of 256 rack-mounted, dual-processor IBM Netfinity servers running Linux and is able to execute 375 billion floating-point operations per second.&lt;/p&gt;
&lt;p&gt;Later this year, IBM intends to market a packaged Linux cluster for commercial applications, officials said.&lt;/p&gt;
&lt;p&gt;According to IBM, the machine, which will be called Los Lobos, will be the 24th-most-powerful computer in the world. It will cost just over $1.5 million. The clustering software is based in part on the open-source Beowulf code and other open-source projects, including IBM-developed software for installing, monitoring and managing clusters. Los Lobos is scheduled to be operational by this summer.&lt;/p&gt;
&lt;p&gt;Dave Turek, vice president of Deep Computing at IBM, said Linux clusters are &amp;ldquo;still relatively immature in certain aspects&amp;rdquo; and that the company&amp;rsquo;s RS/6000 SP massively parallel servers running AIX may be better adapted to certain demanding parallel processing tasks.&lt;/p&gt;
&lt;p&gt;However, Turek said, Linux clusters will soon move well beyond the scientific market, where they have already established a strong foothold. He said that Linux clusters based on Netfinity servers are already being implemented at commercial companies by IBM&amp;rsquo;s Global Services division. A packaged Linux cluster will ship from IBM well before the end of the year, he said.&lt;/p&gt;
&lt;p&gt;Turek said that Linux clusters similar to the one being rolled out at the University of New Mexico can be used for transaction processing and electronic-business applications.&lt;/p&gt;
&lt;p&gt;IBM already markets an 8-node Windows NT clustering technology called IBM Netfinity Availability Extensions for Microsoft Cluster Service.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.computerworld.com/article/2592547/ibm-to-build-powerful-linux-cluster.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.computerworld.com/article/2592547/ibm-to-build-powerful-linux-cluster.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Supercomputer Howls</title>
      <link>http://localhost:1313/blog/20000328-wired/</link>
      <pubDate>Wed, 22 Mar 2000 06:46:33 -0500</pubDate>
      <guid>http://localhost:1313/blog/20000328-wired/</guid>
      <description>&lt;p&gt;&lt;strong&gt;THE UNIVERSITY OF New Mexico&lt;/strong&gt; and IBM are teaming up to build the world&amp;rsquo;s fastest Linux-based supercomputer.&lt;/p&gt;
&lt;p&gt;Named &amp;ldquo;LosLobos&amp;rdquo;, the new supercomputer is scheduled to be fully operational by the summer.&lt;/p&gt;
&lt;p&gt;LosLobos is a departure from the traditional supercomputer set-up. It&amp;rsquo;s built from 256 IBM Netfinity servers.&lt;/p&gt;
&lt;p&gt;The Netfinity servers are linked together using special clustering software and high-speed networking hardware, which causes the separate units to act as one computer, delivering a processing speed of 375 gigaflops, or 375 billion operations per second.&lt;/p&gt;
&lt;p&gt;Although its creators believe LosLobos is the fastest Linux supercomputer, it will only rank 24th on the list of the top 500 fastest supercomputers.&lt;/p&gt;
&lt;p&gt;Dr. Frank Gilfeather, executive director for the High Performance Computing Education and Research Center at the University of New Mexico, said that LosLobos is part of a &amp;ldquo;major supercluster movement&amp;rdquo; involving many people in the Linux and Open-System communities.&lt;/p&gt;
&lt;p&gt;He believes superclusters will revolutionize the high-end computing environment.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The evolution of large Linux superclusters emerges from the proliferation of commodity components such as PCs, the development of high-speed COTS networks, such as Myrinet, and rapid expansion of the open software movement,&amp;rdquo; Gilfeather said. &amp;ldquo;Thus, true supercomputers can be created at a extremely reasonable cost in comparison to traditional supercomputers.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;One researcher who tests distributed cluster environments said there is definitely a move away from the standard all-in-one supercomputer model.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Power is unlimited when you&amp;rsquo;re using clusters,&amp;rdquo; said Stephen Scott, a research scientist at Oak Ridge National Lab&amp;rsquo;s computer-science division. &amp;ldquo;An average ninth-grader can plug the machinery in,&amp;rdquo; Scott said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The problem is dealing with the networking. I can buy a single switch with non-blocking communications, so any machine can talk to any machine &amp;ndash; but it maxes out at 64 machines. Nobody has developed 128 machine switches yet.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Gilfeather said that the biggest problem with deploying scalable production-class superclusters is the lack of mature and tested management tools comparable to what the traditional supercomputer vendors provide.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I/O, including scalable file systems, continues to be universal problem for supercomputers,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;The LosLobos supercluster is part of the National Science Foundation&amp;rsquo;s National Computational Sciences Alliance program, which gives scientists remote access to the fast machines needed for scientific research.&lt;/p&gt;
&lt;p&gt;The foundation is developing a country-wide technology grid, which will connect researchers across the country by linking together the supercomputers located in national labs and universities.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The scientific world likes Linux because it&amp;rsquo;s close to standard Unix,&amp;rdquo; Scott said. &amp;ldquo;Most high-performance environments are Unix, but all of the free GNU tools make it much easier and cheaper to deploy Linux.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;John Patrick, vice president of Internet technology at IBM, said the introduction of superclusters at the top university and government research facilities will impact e-business down the road.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Right now, even the largest websites are tiny compared to what we will see in the near future,&amp;rdquo; Patrick said. &amp;ldquo;The superclusters of today will provide the test bed to create the e-business systems of tomorrow.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.wired.com/2000/03/linux-supercomputer-howls/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.wired.com/2000/03/linux-supercomputer-howls/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Introduces 512-Processor LosLobos Supercluster</title>
      <link>http://localhost:1313/blog/20000321-unm/</link>
      <pubDate>Tue, 21 Mar 2000 15:49:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000321-unm/</guid>
      <description>&lt;p&gt;The National Computational Science Alliance (Alliance) will take delivery of a 512-processor Linux supercluster within the next month – a move that will give this nationwide partnership the largest open production Linux supercluster aimed at the research community. The new supercluster, called LosLobos, will be located at the University of New Mexico’s (UNM) Albuquerque High Performance Computing Center (AHPCC), one of the Alliance Partners for Advanced Computational Resources sites.&lt;/p&gt;
&lt;p&gt;“The Linux movement benefits greatly from the strong support of top researchers and programmers at our nation’s leading universities and laboratories,” said Irving Wladawsky-Berger, Vice President, Technology &amp;amp; Strategy, IBM Enterprise Systems Group. “The innovation of the Alliance, supported by the research community, will lead the way for commercial Linux applications and product development.”&lt;/p&gt;
&lt;p&gt;LosLobos will consist of 256 IBM Netfinity PC Servers. The 733 MHZ Intel IA-32 processor based computer system is expected to provide a peak theoretical performance of about 375 gigaflops. The high performance interconnect network between the cluster nodes will be Myricom’s Myrinet, providing speeds exceeding 1 gigbytes per second, which is comparable to the fastest interconnects in today’s traditional supercomputers. This new supercluster will increase the computing capabilities of the Alliance, a National Science Foundation (NSF) funded partnership, providing researchers with a platform for developing improved cluster management tools, gaining operational experience on large-scale clusters, and exploring the scalability of different types of science and engineering applications.&lt;/p&gt;
&lt;p&gt;“The Alliance is committed to leading the effort to develop and deploy large clusters to the scientific research community,” said Dan Reed, director of the Alliance. “LosLobos represents the synthesis of two major trends in the scientific and research community: open source software and commodity high performance cluster computing. Open source software, such as Linux, decreases costs dramatically because the community of users continually augments the software base. In addition, superclusters made from off-the-shelf products are a very cost effective way to offer supercomputer performance to the user community.”&lt;/p&gt;
&lt;p&gt;Unlike previous superclusters made of commodity computers, LosLobos will integrate new advanced management tools, such as the Maui Scheduler, into its array of services. The Maui Scheduler, developed by UNM’s Maui High Performance Computing Center (MHPCC), will integrate scheduling and reservation capabilities into the cluster, decreasing queue wait times and allowing easier access to a larger percentage of computing resources. LosLobos will be a part of the Alliance’s Virtual Machine Room, which is the national infrastructure for geographically distributed computing.&lt;/p&gt;
&lt;p&gt;“Superclusters are becoming supercomputers in their own right,” noted Frank Gilfeather, executive director of the High Performance Computing, Educational and Research Center (HPCERC) at the University of New Mexico. “Many researchers are now using smaller clusters for research development, and are then turning to superclusters for large-scale computations. Develop research locally, then seamlessly run globally using open source software developed to support specific research disciplines is the model that will bring high performance computing to a broader base of researchers.”&lt;/p&gt;
&lt;p&gt;LosLobos represents a major trend in supercomputing – a moderately priced entry point into high performance computing that offers the enhanced performance required by scientific and engineering applications. The IBM supercluster system will be delivered in mid-April, and the Alliance will begin to allocate time nationally on the system this summer. The Alliance currently offers an 128-processor Linux supercluster, RoadRunner, also located at AHPCC, and a 320-processor supercluster that runs both Linux and Windows NT at its leading edge site, the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Champaign.&lt;/p&gt;
&lt;p&gt;The Alliance is a partnership to prototype an advanced computational infrastructure for the 21st century and includes more than 50 academic, government and industry research partners from across the United States. The Alliance is one of two partnerships funded by the NSF’s Partnerships for Advanced Computational Infrastructure (PACI) program, and receives cost sharing at partner institutions. NSF also supports the National Partnership for Advanced Computational Infrastructure (NPACI), led by the San Diego Supercomputer Center.&lt;/p&gt;
&lt;p&gt;NCSA is the leading-edge site for the Alliance. NCSA is a leader in the development and deployment of cutting-edge high-performance computing, networking, and information technologies. NSF, the state of Illinois, the University of Illinois, industrial partners, and other federal agencies fund NCSA.&lt;/p&gt;
&lt;p&gt;HPCERC, a strategic center of the University of New Mexico, is a partner of the Alliance, and its two supercomputing centers, AHPCC and the MHPCC, are SuperNodes on the Alliance National Technology Grid. AHPCC and MHPCC are supported through grants and contracts from the Air Force Research Laboratory, the Department of Defense, NSF, other government agencies, and commercial contracts.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20000819060614/http://www.unm.edu/~paaffair/news/news%20releases/Mar21hpcc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.unm.edu/~paaffair/news/news%20releases/Mar21hpcc.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM plans to buy &#39;supercluster&#39;</title>
      <link>http://localhost:1313/blog/20000318-abqjournal/</link>
      <pubDate>Sat, 18 Mar 2000 06:48:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000318-abqjournal/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Fleck, Journal Staff Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://abqjournal.newspapers.com/image/262850552/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://abqjournal.newspapers.com/image/262850552/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM&#39;s RS/6000 SP, Linux To Be Wed In Vista Azul Hypercluster Project</title>
      <link>http://localhost:1313/blog/20000101-computertechnologyreview/</link>
      <pubDate>Sat, 01 Jan 2000 21:19:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/20000101-computertechnologyreview/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Joshua Piven&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The ability of Linux to handle visual and scientific supercomputing tasks in a mixed OS environment will be put to the test in a new research effort at the &lt;strong&gt;University of New Mexico&lt;/strong&gt; called Project Vista Azul (Vista Blue). The project is a joint effort of IBM&amp;rsquo;s RS/6000 group and UNM&amp;rsquo;s High Performance Computing, Education, and Research Center (HPCERC).&lt;/p&gt;
&lt;p&gt;The project&amp;rsquo;s goal is to integrate RS/6000 SP supercomputing systems (running AIX) with Linux superclusters (A Linux supercluster is composed of off-the-shelf PCs or workstations running the Linux OS joined via high-speed interconnects.). Vista Azul will create a so-called &amp;ldquo;hypercluster&amp;rdquo; environment that will allow researchers (including UNM students) to explore the optimal use of Linux for scientific applications, as well as to examine management strategies for hybrid clusters.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our goal is to explore the boundaries of high-performance computing by connecting cutting-edge IBM deep computing technology with Linux clusters,&amp;rdquo; said Rod Adkins, general manager of IBM RS/6000 unit. &amp;ldquo;We expect the hypercluster will enable researchers at UNM to pursue the solution of difficult problems in scientific and visual computing while also creating insight into interesting issues of interoperability between Linux and AIX clusters.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The hypercluster hardware and software will include an AIX-based, SMP RS/6000 SP system and a Linux-based, SMP Netfinity cluster, as well as an advanced networking infrastructure, parallel data storage, and a prototype Scalable Graphics Engine from IBM Research for use in visualization research. IBM officials say that building Vista Azul will require the development of advanced networking technology, new programming techniques for hybrid computational systems, and integrated concurrent visualization software.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We believe heterogeneous supercomputer clusters constitute an important trend in high-end technical computing,&amp;rdquo; said Frank Gilfeather, director of the HPCERC at UNM. &amp;ldquo;The joint IBM and UNM effort is a critical step to this future.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;COPYRIGHT 2000 West World Productions, Inc.&lt;br&gt;
No portion of this article can be reproduced without the express written permission from the copyright holder.&lt;br&gt;
Copyright 2000, Gale Group. All rights reserved. Gale Group is a Thomson Corporation Company.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.thefreelibrary.com/IBM%27s&amp;#43;RS%2F6000&amp;#43;SP%2C&amp;#43;Linux&amp;#43;To&amp;#43;Be&amp;#43;Wed&amp;#43;In&amp;#43;Vista&amp;#43;Azul&amp;#43;Hypercluster&amp;#43;Project-a059628917&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.thefreelibrary.com/IBM%27s+RS%2F6000+SP%2C+Linux+To+Be+Wed+In+Vista+Azul+Hypercluster+Project-a059628917&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM RS/6000 SP &amp; Linux Clusters Getting Hitched in New Mexico, IBM and University of New Mexico Researchers to Build &#39;Vista Azul&#39; Hypercluster</title>
      <link>http://localhost:1313/blog/19991213-ibm-sur/</link>
      <pubDate>Mon, 13 Dec 1999 22:07:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/19991213-ibm-sur/</guid>
      <description>&lt;p&gt;IBM and the University of New Mexico (UNM) today announced a joint research project to integrate leading-edge IBM RS/6000 SP supercomputing technology running AIX, IBM&amp;rsquo;s UNIX operating system, with the fast-developing world of Linux superclusters. A Linux supercluster is made up of off-the-shelf PCs or workstations interconnected with high-speed networking technologies, having large storage capabilities and running under the Linux operating system.&lt;/p&gt;
&lt;p&gt;The system, called Vista Azul (Blue Vista in Spanish), will create a unique &amp;ldquo;hypercluster&amp;rdquo; environment composed of IBM SP and Linux technologies, that will allow researchers to explore the optimal use of Linux for scientific applications as well as management strategies for hybrid clusters. This multi-technology platform will serve as a test bed for UNM and IBM researchers to integrate heterogeneous information systems into a solution that could be replicated at other science and technology sites.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our goal is to explore the boundaries of high-performance computing by connecting cutting-edge IBM deep computing technology with Linux clusters,&amp;rdquo; said Rod Adkins, general manager, IBM RS/6000. &amp;ldquo;We expect the hypercluster will enable researchers at UNM to pursue the solution of difficult problems in scientific and visual computing while also creating insight into interesting issues of interoperability between Linux and AIX clusters.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;An IBM Shared University Research (SUR) grant awarded to the University of New Mexico&amp;rsquo;s High Performance Computing, Education, and Research Center (HPCERC) will provide the hypercluster hardware and software. This will include an AIX-based symmetric multi-processor IBM RS/6000 SP system and a Linux-based symmetric multi-processor IBM Netfinity cluster, as well as an advanced networking infrastructure, parallel data storage, and a prototype Scalable Graphics Engine from IBM Research for use in visualization research.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;UNM hosts a Linux supercluster for the National Science Foundation&amp;rsquo;s National Computational Science Alliance (Alliance) at the Albuquerque High Performance Computing Center (AHPCC) and a large, DoD-funded IBM SP configuration at the Maui High Performance Computing Center (MHPCC),&amp;rdquo; said Dr. William Gordon, president of UNM. &amp;ldquo;Vista Azul will capitalize on our expertise with these technologies and user groups to investigate hybrid technologies to solve a wide array of computational applications problems.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Building Vista Azul will require the development of advanced networking technology, new programming techniques for hybrid computational systems, and integrated concurrent visualization software. The Vista Azul project promises a close collaboration among IBM researchers and UNM faculty, students, and staff. Members of the scientific research teams will work to move a number of computationally intensive applications to the hypercluster, including software devoted to the quantum mechanics of materials engineering, the dynamics of atom-ion collisions, and the physics of multiphase flows. Vista Azul will also include the hardware and software necessary to effectively support data-intensive applications, such as the data mining of very large scientific and commercial databases. Lastly, UNM&amp;rsquo;s existing Alliance Roadrunner Linux Supercluster will be integrated with Vista Azul to demonstrate complete interoperability.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We believe heterogeneous supercomputer clusters, such an IBM RS/6000 SP and commodity PCs running Linux, constitute an important trend in high-end technical computing,&amp;rdquo; said Frank Gilfeather, director of the HPCERC at UNM. &amp;ldquo;The joint IBM and UNM effort is a critical step to this future. The Vista Azul Project will allow researchers across many disciplines to investigate hybrid technology and experimental visualization equipment to achieve new and powerful results.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;IBM Research is the world&amp;rsquo;s largest research organization dedicated to information technology, with eight labs around the world, including Austin, Beijing, Delhi, Haifa, Tokyo, San Jose, Yorktown Heights (New York), and Zurich.&lt;/p&gt;
&lt;p&gt;UNM is a Carnegie Research I university, one of 88 in the country. This high distinction is carried by 59 public and 29 private institutions. Among UNM&amp;rsquo;s outstanding research units are the Cancer Center, New Mexico Engineering Research Institute, Center for High Technology Materials, Center for Micro-Engineered Ceramics and the Center for Non-Invasive Diagnosis.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20041013122904/http://www.ece.unm.edu/event/news/single.php?id=19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ece.unm.edu/event/news/single.php?id=19&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IBM RS/6000 SP and Linux Clusters Getting Hitched in New Mexico</title>
      <link>http://localhost:1313/blog/19991213-ibm/</link>
      <pubDate>Mon, 13 Dec 1999 20:58:53 -0400</pubDate>
      <guid>http://localhost:1313/blog/19991213-ibm/</guid>
      <description>&lt;p&gt;ALBUQUERQUE, NM&amp;ndash;Dec. 13, 1999&amp;ndash;IBM and the University of New Mexico (UNM) today announced a joint research project to integrate leading-edge IBM RS/6000 SP supercomputing technology running AIX, IBM&amp;rsquo;s UNIX operating system, with the fast-developing world of Linux superclusters. A Linux supercluster is made up of off-the-shelf PCs or workstations interconnected with high-speed networking technologies, having large storage capabilities and running under the Linux operating system.&lt;/p&gt;
&lt;p&gt;The system, called Vista Azul (Blue Vista in Spanish), will create a unique &amp;ldquo;hypercluster&amp;rdquo; environment composed of IBM SP and Linux technologies, that will allow researchers to explore the optimal use of Linux for scientific applications as well as management strategies for hybrid clusters. This multi-technology platform will serve as a test bed for UNM and IBM researchers to integrate heterogeneous information systems into a solution that could be replicated at other science and technology sites.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Our goal is to explore the boundaries of high-performance computing by connecting cutting-edge IBM deep computing technology with Linux clusters,&amp;rdquo; said Rod Adkins, general manager, IBM RS/6000. &amp;ldquo;We expect the hypercluster will enable researchers at UNM to pursue the solution of difficult problems in scientific and visual computing while also creating insight into interesting issues of interoperability between Linux and AIX clusters.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;An IBM Shared University Research (SUR) grant awarded to the University of New Mexico&amp;rsquo;s High Performance Computing, Education, and Research Center (HPCERC) will provide the hypercluster hardware and software. This will include an AIX-based symmetric multi-processor IBM RS/6000 SP system and a Linux-based symmetric multi-processor IBM Netfinity cluster, as well as an advanced networking infrastructure, parallel data storage, and a prototype Scalable Graphics Engine from IBM Research for use in visualization research.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;UNM hosts a Linux supercluster for the National Science Foundation&amp;rsquo;s National Computational Science Alliance (Alliance) at the Albuquerque High Performance Computing Center (AHPCC) and a large, DoD-funded IBM SP configuration at the Maui High Performance Computing Center (MHPCC),&amp;rdquo; said Dr. William Gordon, president of UNM. &amp;ldquo;Vista Azul will capitalize on our expertise with these technologies and user groups to investigate hybrid technologies to solve a wide array of computational applications problems.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Building Vista Azul will require the development of advanced networking technology, new programming techniques for hybrid computational systems, and integrated concurrent visualization software. The Vista Azul project promises a close collaboration among IBM researchers and UNM faculty, students, and staff. Members of the scientific research teams will work to move a number of computationally intensive applications to the hypercluster, including software devoted to the quantum mechanics of materials engineering, the dynamics of atom-ion collisions, and the physics of multiphase flows. Vista Azul will also include the hardware and software necessary to effectively support data-intensive applications, such as the data mining of very large scientific and commercial databases. Lastly, UNM&amp;rsquo;s existing Alliance Roadrunner Linux Supercluster will be integrated with Vista Azul to demonstrate complete interoperability.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We believe heterogeneous supercomputer clusters, such an IBM RS/6000 SP and commodity PCs running Linux, constitute an important trend in high-end technical computing,&amp;rdquo; said Frank Gilfeather, director of the HPCERC at UNM. &amp;ldquo;The joint IBM
and UNM effort is a critical step to this future. The Vista Azul Project will allow researchers across many disciplines to investigate hybrid technology and experimental visualization equipment to achieve new and powerful results.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;IBM Research is the world&amp;rsquo;s largest research organization dedicated to information technology, with eight labs around the world, including Austin, Beijing, Delhi, Haifa, Tokyo, San Jose, Yorktown Heights (New York), and Zurich.&lt;/p&gt;
&lt;p&gt;UNM is a Carnegie Research I university, one of 88 in the country. This high distinction is carried by 59 public and 29 private institutions. Among UNM&amp;rsquo;s outstanding research units are the Cancer Center, New Mexico Engineering Research Institute, Center for High Technology Materials, Center for Micro-Engineered Ceramics and the Center for Non-Invasive Diagnosis.&lt;/p&gt;
&lt;p&gt;IBM, RS/6000, and AIX are registered trademarks or trademarks of the International Business Machines Corporation in the United States, other countries, or both. UNIX is a registered trademark in the United States and other countries, licensed exclusively through X/Open Company, Limited. Other company, product and service names, which may be denoted by a double asterisk&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20010119111200/http://ibm.com/Press/prnews.nsf/jan/3F4E88B102477AA5852568460067A52A&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.archive.org/web/20010119111200/http://ibm.com/Press/prnews.nsf/jan/3F4E88B102477AA5852568460067A52A&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Partners Showcase Progress at SC99</title>
      <link>http://localhost:1313/blog/19991105-hpcwire/</link>
      <pubDate>Fri, 05 Nov 1999 20:21:58 -0400</pubDate>
      <guid>http://localhost:1313/blog/19991105-hpcwire/</guid>
      <description>&lt;p&gt;Participants at the upcoming SC99 conference in Portland,
OR, will have ample opportunity to experience the progress made by the
National Computational Science Alliance in prototyping the next century&amp;rsquo;s
advanced computational and information infrastructure. Alliance partners
will host seven research exhibits and give numerous speeches, presentations,
and tutorials. SC99, the annual high-performance networking and computing
conference, will be held Nov. 13-19 at the Oregon Convention Center.&lt;/p&gt;
&lt;p&gt;For the first time at an SC conference, researchers will show how the
Alliance is developing the Access Grid, a system that links people in
virtual spaces for collaborative science, workshops, and distance education
sessions. In fact, the Alliance, along with at least three other Alliance
partner sites - the Albuquerque High Performance Computing Center/Maui High
Performance Computing Center, Boston University, and Argonne National
Laboratory (ANL) - will feature constant, real-time links to the Access
Grid in their exhibitor booths. These connections will allow people to tap
into the Alliance&amp;rsquo;s SC99 demos and presentations from around the country at
Access Grid nodes at UIUC, ANL, and the Alliance Center for Collaboration,
Education, Science and Software in Arlington, VA. Access Grid nodes are the
entry points to the Access Grid&amp;rsquo;s virtual workspace. They can be as simple
as a desktop computer or as sophisticated as a large format multimedia
display system used in an interactive meeting room.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;As usual a great number of Alliance researcher, partner sites, users of
Alliance resources, and our federal partners are contributing a great deal
of their time and expertise to the upcoming SC conference.&amp;rdquo; said Larry
Smarr, director of the Alliance and NCSA. &amp;ldquo;And for those who won&amp;rsquo;t be able
to come to Portland to hear about our work, the live Access Grid nodes on
the exhibit hall floor will give them a chance to see and hear about
important research efforts from a remote location.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Daniel Reed, of the University of Illinois at Urbana-Champaign, head of
the Alliance Data and Collaboration team, and member of the Alliance
Executive Committee, will present a State of the Field talk called
&amp;ldquo;Performance: Myth, Hype and Reality.&amp;rdquo; The talk will present an overview of
the continuing challenge of getting hardware and software to work together
to perform calculations in the shortest time possible.&lt;/p&gt;
&lt;p&gt;Chris Johnson, of the University of Utah and the Alliance Distributed
Computing team, will present a talk during the invited speakers sessions
titled &amp;ldquo;The ABC&amp;rsquo;s of Large-scale Computing and Visualization: ASCI, Brains,
Cardiology and Combustion.&amp;rdquo; Andrew Chien, a member of the Alliance Parallel
Computing team, will present an invited speakers talk titled &amp;ldquo;Supercomputing
on Windows NT Clusters: Experience and Future Directions.&amp;rdquo; Reed, Michael
Heath and Josep Torrellas of the University of Illinois at Urbana-Champaign,
ANL&amp;rsquo;s Foster and Lori Freitag, and George Karniadakis, a major user of
Alliance resources and a professor at Brown University, will each present
technical papers during SC99 as well.&lt;/p&gt;
&lt;p&gt;Alliance team members will also be active in the SC99 tutorials,
presenting six of the 20 conference tutorials on Nov. 14 and 15. Charlie
Catlett, a senior associate director of the Alliance and an Alliance
Executive Committee member, will talk about computer and network security
issues in a tutorial titled &amp;ldquo;Introduction to Cryptography, Security and
Privacy Technologies.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Jim Ferguson, Mark Gates, Jason Novotny, Meghan Thornton, Mitch Kutzko,
and Kai Chen, all of NCSA and the National Laboratory for Applied Network
Research distributed applications support team, will present &amp;ldquo;Running
Applications on High-speed Networks - Theory, Practice, and Case Study.&amp;rdquo;
Ian Foster, Gregor von Laszewski, and Steve Tuecke of ANL will lead &amp;ldquo;The
Globus Grid Programming Toolkit,&amp;rdquo; the second straight year they will offer a
Globus tutorial. Globus is a software infrastructure for scientific problem
solving over computational grids. The Globus Programming Toolkit helps
application developers and tool builders overcome the obstacles of remote
computing and distributed supercomputing to construct &amp;ldquo;grid enabled&amp;rdquo;
applications.&lt;/p&gt;
&lt;p&gt;Creating supercomputer clusters using off-the-shelf workstations will be
addressed by Robert Pennington of NCSA and &lt;strong&gt;David Bader&lt;/strong&gt; and Barney Maccabe of
the University of New Mexico in a tutorial called &amp;ldquo;Design and Analysis of NT
and Linux Superclusters for Computational Grids.&amp;rdquo; Pennington is head of the
Alliance&amp;rsquo;s NT Supercluster development team while Bader and Maccabe lead the
effort to develop the Alliance&amp;rsquo;s Linux Supercluster called Roadrunner. Other
tutorials involving Alliance members are &amp;ldquo;Tuning MPI Applications for Peak
Performance,&amp;rdquo; presented by William Gropp, Ewing Lusk, and Rajeev Thakur of
ANL, and &amp;ldquo;Production Linux Clusters: Architecture and System Software for
Manageability and Multi-user Access,&amp;rdquo; presented by Remy Evard of ANL and
Peter Beckman of Los Alamos National Laboratory.&lt;/p&gt;
&lt;p&gt;Alliance partners with research exhibits at SC99 are: Argonne National
Laboratory, Boston University, University of Utah, Los Alamos National
Laboratory, Maui High Performance Computing Center/Albuquerque High
Performance Computing Center, Ohio Supercomputer Center (OSC), and the
National Computational Science Alliance itself.&lt;/p&gt;
&lt;p&gt;Paul Woodward, of the University of Minnesota and member of the Alliance
Executive Committee, will demonstrate the InTENsity PowerWall in the
Department of Energy Accelerated High Performance Computing Initiative
(ASCI) research booth. The InTENsity PowerWall&amp;rsquo;s room-sized immersive
virtual reality system - the development of which was supported in part by
the Alliance - will be used to demonstrate the integration of the ASCI&amp;rsquo;s
efforts in the study of turbulence and transport, engineering, materials
science, and distance and distributed computing and communications.&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance is a partnership to prototype
an advanced computational infrastructure for the 21st century and includes
more than 50 academic, government and industry research partners from across
the United States. The Alliance is one of two partnerships funded by the
National Science Foundation&amp;rsquo;s Partnerships for Advanced Computational
Infrastructure (PACI) program, and receives cost-sharing at partner
institutions. NSF also supports the National Partnership for Advanced
Computational Infrastructure (NPACI), led by the San Diego Supercomputer
Center.&lt;/p&gt;
&lt;p&gt;The National Center for Supercomputing Applications is the leading-edge
site for the National Computational Science Alliance. NCSA is a leader in
the development and deployment of cutting-edge high-performance computing,
networking, and information technologies. The National Science Foundation,
the state of Illinois, the University of Illinois, industrial partners, and
other federal agencies fund NCSA.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1999/11/05/alliance-partners-showcase-progress-at-sc99/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1999/11/05/alliance-partners-showcase-progress-at-sc99/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Clustering Extends Trend</title>
      <link>http://localhost:1313/blog/19990920-internetweek/</link>
      <pubDate>Mon, 20 Sep 1999 18:21:30 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990920-internetweek/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Lenny Liebmann&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Linux delivers lots of computing power on commodity lntel processors, and it&amp;rsquo;s especially popular with Net devotees. So could Linux turn out to be the OS of choice for dot.com server clustering?&lt;/p&gt;
&lt;p&gt;Plenty of vendors think so. Network Engines recently started to ship its XEngine Linux cluster. And, Linux systems leaders VA Research lnc. and TurboLinux lnc. are also shipping clustering solutions based on Linux.&lt;/p&gt;
&lt;p&gt;Some prominent users are buying in: The University of New Mexico built a 128-server cluster using technology from Alta Technology Corp., which runs the Red Hat Linux OS on Pentium II processors.
Nicknamed &amp;ldquo;Roadrunner,&amp;rdquo; the National Science Foundation-sponsored project gives users all over the world access to major computing power via the
internet.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re interested in using clusters to solve problems that would overwhelm an individual workstation,&amp;rdquo; says &lt;strong&gt;David Bader&lt;/strong&gt;, an assistant professor of electrical and computer engineering who heads up the project.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;lt may be that you need more capacity, or that time is a critical factor in what you&amp;rsquo;re doing,&amp;rdquo; Bader adds.
Such applications include highly complex particle physics equations and visualization of huge amounts of data generated by space observation equipment.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;With a Linux cluster, you can write an application on your own workstation and then easily transfer it over to our machine,&amp;rdquo; Bader explains. &amp;ldquo;With some other
large-scale computing platform, you&amp;rsquo;d either have to have access to it for development, or you&amp;rsquo;d have to worry about porting it and modifying it after you developed it on your own machine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Linux clustering is not only for the rarified world of academia. Building clusters with Linux servers offers a powerful bottom line case for business users:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The two machines I&amp;rsquo;m using in my TurboLinux cluster cost me $1,100 apiece,&amp;rdquo; boasts Michael Kuehl, president of Digital Facilities Management lnc., a Web hosting and lnternet development firm. &amp;ldquo;To achieve the same results with a commercial Unix product could cost as much as $70,000.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Workshop on Ethical Challenges and Practical Solutions for Managers in Research</title>
      <link>http://localhost:1313/blog/19990910-sigmaxi/</link>
      <pubDate>Fri, 10 Sep 1999 19:17:55 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990910-sigmaxi/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19990910-sigmaxi/letter_hu_f5b185b53e3d8560.webp 400w,
               /blog/19990910-sigmaxi/letter_hu_f7ca4a81cf1859c8.webp 760w,
               /blog/19990910-sigmaxi/letter_hu_327308a04e562301.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19990910-sigmaxi/letter_hu_f5b185b53e3d8560.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;On behalf of ORI and Sigma Xi, we welcome you to this workshop on
&amp;ldquo;Ethical Challenges and Practical Solutions for Managers in Research.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Science and engienering undergird the economic success of this country
and will be drivers in the next century.  However, to maintain the
quality of the necessary research and the confidence of the public and
others who fund research, it will be essential for managers to instill
standards of high integrity. This workshop will examine some of the
challenges to accomplishing this task.&lt;/p&gt;
&lt;p&gt;You active participation is an essential element for making this
workshop a success. We invite you to question the speakers and
participate fully in the breakout session to offer your own insights
into the problems and potential solutions.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Chris B. Pascal, J.D.&lt;br&gt;
Acting Director&lt;br&gt;
Office of Research Integrity&lt;/p&gt;
&lt;p&gt;John F. Ahearne&lt;br&gt;
Director&lt;br&gt;
Sigma Xi Center&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SC99 Tutorials Address Latest Issues in HPC</title>
      <link>http://localhost:1313/blog/19990903-hpcwire/</link>
      <pubDate>Fri, 03 Sep 1999 20:16:09 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990903-hpcwire/</guid>
      <description>&lt;p&gt;From cutting-edge research projects, to the most talked
about trends in networking, programming, performance analysis, and computer
and network security, the SC99 Tutorials Program offers something for
everyone with an interest in high performance computing and networking.&lt;/p&gt;
&lt;p&gt;SC99, the annual high performance computing and networking conference,
takes place Nov. 13-19 at the Oregon Convention Center. The conference&amp;rsquo;s 12
full-day and eight half-day tutorials will be offered Sunday, Nov. 14, and
Monday, Nov. 15.&lt;/p&gt;
&lt;p&gt;New tutorial topics this year include a full-day session on computer and
network security issues, sessions on Linux and NT superclusters and an
overview of file systems. The session on security issues, called
&amp;ldquo;Introduction to Cryptography, Security and Privacy Technologies,&amp;rdquo; will
provide an overview on the often-discussed issues of computer security and
privacy. Topics covered will include the building blocks of computer
security and privacy, cryptographic technologies, and protocols used to
construct secure and private services and systems. Charlie Catlett, senior
associate director for science and technology at the National Center for
Supercomputing Applications (NCSA) will conduct this tutorial.&lt;/p&gt;
&lt;p&gt;The hot topic of clustering off-the-shelf workstations into machines with
supercomputing performance will be discussed in two tutorials. On Sunday
William Saphir and Patrick Boseman of Lawrence Berkley National
Laboratory/National Energy Research Scientific Computing Center, Remy Evard
of Argonne National Laboratory, and Pete Beckman of Los Alamos National
Laboratory, will guide participants through the issues involved in setting
up and running a Linux cluster. They will present an overview of cluster
architectures, system software, and application-level software, with the
goal of pointing out software and architectures that work. On Monday
afternoon Rob Pennington, of NCSA, and &lt;strong&gt;David Bader&lt;/strong&gt; and Barney Maccabe, of
the University of New Mexico, will talk about the NT and Linux superclusters
constructed by the National Computational Science Alliance. The NT
supercluster, located at NCSA, runs on Windows NT and uses commercial
desktop computers from Hewlett-Packard. The New Mexico supercluster, called
Roadrunner, runs on Linux and was integrated by Alta Technology. The
tutorial will provide details on the construction, configuration and
management of these systems as well as details on porting applications to
the superclusters.&lt;/p&gt;
&lt;p&gt;Another full-day tutorial, called &amp;ldquo;From Physics to File Systems,&amp;rdquo; will be
presented by Rodney Van Meter and Paul Massiglia of Quantum Corp. on Sunday.
This tutorial will start with an overview of storage and I/O systems,
including buses, networks, disks, and tapes, and will conclude with the
latest information on hierarchical storage management systems and
distributed file systems. The tutorial will be organized around three
themes: data movement, data storage and distributed file systems.&lt;/p&gt;
&lt;p&gt;Several popular tutorials will be back this year with updated information.
An introduction to high-performance data mining will be presented by Robert
Grossman, of the University of Illinois at Chicago, and Vipin Kumar, of the
University of Minnesota. Several tutorials on performance analysis will be
offered, as will sessions on visualization of large datasets and the Globus
Grid Programming Toolkit. Globus is a software infrastructure for scientific
problem solving over computational grids. The Globus Grid Programming
Toolkit is designed to help application developers and tool builders
overcome the obstacles of remote computing and distributed supercomputing to
construct &amp;ldquo;Grid Enabled&amp;rdquo; scientific and engineering applications. The
tutorial will be presented by Steven Fitzgerald and Carl Kesselman of the
University of Southern California&amp;rsquo;s Information Sciences Institute, and Ian
Foster, Gregor von Laszewski and Steve Tuecke of Argonne.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The SC tutorials are always one of the prime events of the conference,&amp;rdquo;
said Paul Shahady, chair of the SC99 Tutorials Committee. &amp;ldquo;This year, the
tutorials will offer an incredible range of topics and a huge amount of
information that participants can take back with them and use in their own
work.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Separate registration is required for tutorials. A one or two-day Tutorial
Passport gives attendees the flexibility to attend several tutorials. More
information on SC99 tutorials, including a schedule, is available at
&lt;a href=&#34;http://www.SC99.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.SC99.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1999/09/03/sc99-tutorials-address-latest-issues-in-hpc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1999/09/03/sc99-tutorials-address-latest-issues-in-hpc/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artists Bring Ancient World Alive With Cutting Edge Technology at 21st Century Tech Road Show</title>
      <link>http://localhost:1313/blog/19990811-newswise/</link>
      <pubDate>Wed, 11 Aug 1999 12:00:00 -0500</pubDate>
      <guid>http://localhost:1313/blog/19990811-newswise/</guid>
      <description>&lt;p&gt;(Boston, Mass.) - This September, tourists will be able to explore the ruins of an imaginary ancient palace, bringing to light the artifacts of a long-lost civilization, interacting with kinetic sculptures and chatting with other visitors to this virtual world. The magical 3-D world of Spirited Ruins is the latest creation of the Boston University-based consortium, High Performance Computing in the Arts (HiPArt), where researchers and artists work in close collaboration and visitors connect and interact through the Internet with other virtual laboratories around the country.&lt;/p&gt;
&lt;p&gt;The virtual tour is only one of the highlights of a four-day conference previewing how a new computer backbone, the Alliance Grid, being built by National Computational Science Alliance (Alliance), will change the way business, education, and research are conducted in the 21st century. Sponsored by the Alliance, the conference, or Chautauqua, will be held at Boston University September 13 - 16.&lt;/p&gt;
&lt;p&gt;Chatauqua is a Seneca Indian word for meeting or gathering adopted during the Industrial Revolution to describe the traveling educational meetings that crossed the country promoting new technology. The Chautauquas, with demonstrations and seminars originating at sites across the country linked via the Alliance Grid, will bring together researchers, teachers, students, journalists, and entrepreneurs for a preview of how this dynamic new interactive environment will work.&lt;/p&gt;
&lt;p&gt;The Grid integrates multiple sites and multiple computer capabilities, including streaming audio, video, PowerPoint presentations, shared whiteboards, chat rooms - anything that can be done on a single computer can be done collaboratively from multiple sites via the Grid. The new technology supports activities such as distributed meetings, remote visualization, and distance education. It also facilitates tele-immersion - where people at different sites work together in virtual environments - a group of surgeons at sites across the country collaborating on a new surgical procedure or a group of artists developing a new film project, would be able to work together in ways never before possible.&lt;/p&gt;
&lt;p&gt;While this interactive participation is currently available through high-priced, proprietary telecommunications technologies, the Chautauquas will showcase emerging technologies that offer readily accessible and affordable alternatives.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Chautauquas will give new audiences the chance to experience the possibilities of the Grid, including remote collaborative tutorials and seminars that allow for group interactions,&amp;rdquo; said Larry Smarr, director of the Alliance and the National Center for Supercomputer Applications at the University of Illinois at Urbana-Champaign. &amp;ldquo;The Grid is really a preview of the work and educational environment of the 21st century.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The Alliance&amp;rsquo;s three Chautauqua 99 sites (the University of New Mexico, August 9-10; the University of Kentucky, August 22-23; and Boston University, September 13-15) are among the early nodes on the Access Grid, giving researchers, educators, students and the general public entry points into this new system of online collaborative work environments.&lt;/p&gt;
&lt;p&gt;A schedule of conference highlights is attached. Further details about the Boston event can be found at &lt;a href=&#34;http://chautauqua.bu.edu/chautauqua/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://chautauqua.bu.edu/chautauqua/&lt;/a&gt;. For information about the other Chautauquas, visit the Alliance web site at &lt;a href=&#34;http://www.alliance.ncsa.uiuc.edu/chautauqua/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.alliance.ncsa.uiuc.edu/chautauqua/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We expect the Chautauqua to stimulate a new level of creativity in the nationwide digital research community,&amp;rdquo; says Glenn Bresnahan, director of Scientific Computing and Visualization at Boston University. &amp;ldquo;Our meeting in Boston is an opportunity to involve larger groups of researchers, teachers, students and company representatives in the development of future technology that will soon be as accessible as the Internet is today.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Please contact Joan Schwartz, 617/353-4626, &lt;a href=&#34;mailto:joschwar@bu.edu&#34;&gt;joschwar@bu.edu&lt;/a&gt; for complimentary press registration.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Selected Highlights of&lt;br&gt;
CHAUTAUQUA 99 AT BOSTON UNIVERSITY&lt;br&gt;
September 13 - 15, 1999&lt;br&gt;
Innovations in Science, Computing and Grid Technology&lt;/p&gt;
&lt;p&gt;Monday, September 13&lt;br&gt;
9 AM - noon Regional Networking: A framework for understanding high-performance networking initiatives in the US, with a focus on Internet 2.&lt;br&gt;
4 - 6 PM Technology Demonstrations&lt;/p&gt;
&lt;p&gt;Tuesday, September 14&lt;br&gt;
9 AM - 5 PM: Main Conference, Alliance Chautauqua 99: Day One&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keynote: A preview of the work and educational environment of the 21st century: Larry Smarr, Director, National Computational Science Alliance and the National Center for Supercomputing Applications&lt;/li&gt;
&lt;li&gt;&amp;ldquo;How to get on the Grid: &amp;quot; Glenn Bresnahan, Boston University&lt;/li&gt;
&lt;li&gt;Clusters - The Most Rapidly Growing Architecture of High-End Computing: &lt;strong&gt;David A. Bader&lt;/strong&gt; of University of New Mexico, lead for the UNM Roadrunner Linux-based Supercluster, on the &amp;ldquo;next wave&amp;rdquo; in high performance computing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Noon - 2 PM Technology Demonstrations&lt;br&gt;
6 - 9 PM Dinner at Boston University School of Management&lt;/p&gt;
&lt;p&gt;Wednesday, September 15&lt;br&gt;
9 AM - 5 PM: Main Conference, Alliance Chautauqua 99: Day Two&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keynote: Ruzena Bajcsy, Assistant Director of the Computer and Information Science and Engineering Directorate at the National Science Foundation, remote from the Access Center in Arlington, VA&lt;/li&gt;
&lt;li&gt;Education, Outreach, &amp;amp; Training Partnership for Advanced Computational Infrastructure
Roscoe Giles, Boston University, on reaching out via education, outreach, and training to make the Alliance Grid accessible to all.&lt;/li&gt;
&lt;li&gt;Applying Computational Resources to Research: Greg Bryan, Massachusetts Institute of Technology on recent advances in computational resources for research.&lt;/li&gt;
&lt;li&gt;Visualization - Adding a New Dimension to Research: Donna Cox, National Center for Supercomputing Applications and School of Art and Design at the University of Illinois.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5 PM - 9 PM Opening Reception for Boston University&amp;rsquo;s New Virtual Reality Environment, &amp;ldquo;Spirited Ruins.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Thursday, September 16&lt;br&gt;
9 AM - 5 PM High Performance Computation and the Arts, hosted by HiPArt, panel discussions and talks which explore the relevance of high-end technologies for artists, the importance of engaging artists in the process of technology development, how art and technology influence each other, and current work by artists and technologists actively engaged in this field.&lt;br&gt;
9 AM - noon Supercomputing Clusters Tutorial: clusters provide easy-to-use high performance computing systems at reasonable prices by connecting PCs via high performance systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newswise.com/articles/ancient-world-alive-at-21st-century-tech-road-show&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newswise.com/articles/ancient-world-alive-at-21st-century-tech-road-show&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chautauquas Revive an American Forum for a New Era</title>
      <link>http://localhost:1313/blog/19990804-chautauqua/</link>
      <pubDate>Wed, 04 Aug 1999 15:23:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990804-chautauqua/</guid>
      <description>&lt;p&gt;CHAMPAIGN, IL &amp;ndash; The National Computational Science
Alliance (Alliance) will host three technology road
shows this summer designed to demonstrate to
researchers, educators and students how emerging
technologies and the developing National Technology Grid
will change the way people communicate, learn, and
conduct research and business in the 21st century.&lt;/p&gt;
&lt;p&gt;The three events are scheduled for&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Aug. 9 and 10 at the University of New Mexico
    in Albuquerque (co-hosted by the University of Kansas),
    -&amp;gt; http://chautauqua.ahpcc.unm.edu/

    Aug. 23 and 24 at the University of Kentucky
    in Lexington, and
    -&amp;gt; http://www.ccs.uky.edu/~chautauqua

    Sept. 13 at 14 at Boston University.
    -&amp;gt; http://chautauqua.bu.edu/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are being called Chautauquas, a Seneca Indian word
meaning meeting or gathering. The original Chautauqua
movement started in the late 19th century, when
traveling educational meetings were used to introduce
new concepts and cultural realities of the industrial
revolution to an increasingly diverse American
population.&lt;/p&gt;
&lt;p&gt;Today the information revolution has created a similar
need to disseminate information about new technologies
and concepts. Alliance Chautauquas 99 are designed to
introduce new audiences to the National Technology Grid,
the prototype of the next century&amp;rsquo;s information
infrastructure which is being developed by the Alliance,
and demonstrate how technological innovations can be
used on the Grid in ways that will impact science,
education, business and government.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;For the last two years the Alliance has been in the
business of developing and experimenting with a new
digital community, which five or 10 years down the road
will be the norm for everyone,&amp;rdquo; said Larry Smarr,
director of the Alliance and the National Center for
Supercomputing Applications (NCSA) at the University of
Illinois at Urbana-Champaign, the leading-edge site for
the Alliance. &amp;ldquo;The Chautauquas give us a chance to share
these developments with a wider group of university
researchers and educators. We hope that these meetings
will stimulate the growth of the nationwide digital
research community that will drive scientific research
and technology development in the 21st century.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Each Chautauqua site will serve as an access point to
the Grid, offering several presentations that will be
multicast from remote sites. The remote presentations
are part of the Alliance&amp;rsquo;s comprehensive plan to deploy
sites as Grid access points and to coordinate research
and training activities at these sites with the new
Alliance Center for Collaboration, Education, Science
and Software (ACCESS) in Arlington, VA.&lt;/p&gt;
&lt;p&gt;Each Chautauqua will target key groups located in
specific regions of the country. The University of New
Mexico Chautauqua will target Native American colleges,
researchers from government laboratories and regional
institutions that are part of the Experimental Program
to Stimulate Competitive Research (EPSCoR), a National
Science Foundation program to stimulate research in
states that have traditionally received few research
dollars. The University of Kentucky will focus on EPSCoR
institutions as well as the Southeastern Universities
Research Association (SURA), a consortium of 41
universities in 13 southeastern states, and institutions
with the Committee on Institutional Cooperation (CIC), a
consortium of major Midwestern and Big 10 universities.
Boston University will also target regional EPSCoR and
CIC institutions, as well as other East Coast academic
institutions.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The Chautauquas will highlight research and educational
initiatives specific to each region,&amp;rdquo; said Frank
Gilfeather, director of the University of New Mexico&amp;rsquo;s
Albuquerque High Performance Computing Center and team
lead of the Alliance&amp;rsquo;s Partners for Advanced
Computational Services (PACS), which is sponsoring the
events. &amp;ldquo;But the Chautauquas will also be national in
scope because they will highlight the Alliance&amp;rsquo;s vision
to link researchers with each other and with tools and
technologies via the National Technology Grid. &amp;quot;&lt;/p&gt;
&lt;p&gt;Events at all three Chautauquas will focus on the
Alliance&amp;rsquo;s three key initiatives, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;capability computing, or computing &amp;ldquo;superjobs&amp;rdquo; that
require dedicated use of supercomputing power;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the evolution of networks into a ubiquitous Grid
complete with software and middleware that allows people
to interact in collaborative virtual spaces; and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the emergence of a scientific common portal
architecture, which will give researchers access to each
other and all the tools they need for their work through
simple mouse clicks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each Chautauqua will feature a keynote address from
Smarr and an address by an NSF representative. The
Chautauquas will also feature targeted tutorials and
events including a tutorial on scientific computing on a
Linux cluster and a seminar on high-performance
computing and the arts.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://list.uvm.edu/cgi-bin/wa?A2=ind9908&amp;amp;L=I2-TEAM&amp;amp;P=63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://list.uvm.edu/cgi-bin/wa?A2=ind9908&amp;L=I2-TEAM&amp;P=63&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;chautauqua-agenda-august-9-10-1999-albuquerque-nm&#34;&gt;Chautauqua Agenda, August 9-10, 1999, Albuquerque, NM&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20000203101440/http://chautauqua.ahpcc.unm.edu/agenda.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agenda&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;1:30PM - CLUSTERS - The Most Rapidly Growing Architecture of High-End Computing&lt;br&gt;
&lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, University of New Mexico, and lead for the UNM Roadrunner Linux-based Supercluster, will talk about the &amp;ldquo;next wave&amp;rdquo; in high performance computing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Roadrunner Supercluster Now on the Grid</title>
      <link>http://localhost:1313/blog/19990723-hpcwire/</link>
      <pubDate>Fri, 23 Jul 1999 20:09:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990723-hpcwire/</guid>
      <description>&lt;p&gt;The National Computational Science Alliance&amp;rsquo;s
(Alliance) Linux Roadrunner Supercluster at the University of New Mexico
(UNM) is officially open for business as a node on the Alliance Grid.&lt;/p&gt;
&lt;p&gt;A prototype of the national information infrastructure of the 21st
century, the Alliance Grid is an emerging integrated computational and
collaborative environment that links people, resources, and services over
high speed networks. Joining the Alliance&amp;rsquo;s arsenal of parallel computing
systems located at facilities from Boston to Maui, Roadrunner is a 64-node
AltaCluster by Alta Technology Corporation. Each node has two Intel 450 MHz
Pentium II processors  interconnected via a 1.2 Gbyte bandwidth Myrinet
network for high speed communications. The system runs on the Linux
operating system. The Roadrunner Supercluster has also been fully integrated
with the Globus Infrastructure, a high performance, distributed computing
toolkit that allows ready access to geographically distributed resources
such as superclusters and supercomputers, data repositories, scientific
instruments and visualization suites.&lt;/p&gt;
&lt;p&gt;The Roadrunner Supercluster was unveiled at an April 8 dedication at UNM&amp;rsquo;s
Albuquerque High Performance Computing Center (AHPCC) and has been in a
&amp;ldquo;friendly-user&amp;rdquo; mode since June 1,operating as a production research
platform with user support services 24 hours a day. More than 50 Alliance
users have been test driving this speedy yet cost-effective system since
June. Early results from scientists reveal Roadrunner to be a promising
option for high-performance computing, especially relative to its price.&lt;/p&gt;
&lt;p&gt;UNM Professors &lt;strong&gt;David A. Bader&lt;/strong&gt; and Barney Maccabe are in charge of the
efforts to evaluate the performance of scientific applications on
Roadrunner, and so far, they have been successful in making codes scale on
the nodes of the Supercluster.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Using the Cactus benchmark in scaling, the Roadrunner Supercluster
outperforms all other kinds of clusters,&amp;rdquo; noted Bader. The Cactus code is a
modular manageable high-performance 3D tool for Numerical Relativity.
Details can be found at
&lt;a href=&#34;http://www.aei-potsdam.mpg.de/~wehrens/cactus/cluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.aei-potsdam.mpg.de/~wehrens/cactus/cluster/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To further evaluate Roadrunner, MILC, a conjugate gradient algorithm for
Kogut-Susskind quarks, was used as a benchmark. &amp;ldquo;We are pleased to see that
the MILC benchmark achieved greater than 60 Mflops per processor on
Roadrunner, said Bader. He pointed out that this performance is significant
in terms of cost per flop when compared to the 76 Mflop performance per
processor of the Cray T3E 900, and the 120 Mflop performance on the SGI
Origin2000 using 250 MHz R10000 processors. For more information on MILC,
see &lt;a href=&#34;http://www.physics.indiana.edu/~sg/milc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.physics.indiana.edu/~sg/milc.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Other advantages of the Roadrunner Supercluster include ease of system
administration and software availability, often from the public domain. In
fact almost all software, including the operating system, schedulers,
compilers, and applications, is constantly being rewritten and improved as
part of the open system development. Usability and portability are other
features of the Linux Supercluster.  Dr. Dan Weber, Research Scientist from
the Center for the Analysis and Prediction of Storms at the University of
Oklahoma, discovered that &amp;ldquo;Porting code to the Supercluster is transparent.
I had models running in less than 30 minutes.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Bader and Maccabe, and Rob Pennington, technical program manager for the
Alliance NT Supercluster team, will present a Supercluster tutorial and
forum at the upcoming Alliance Chautauquas 99. The Chautauqua program begins
August 9 - 10 at UNM, and continues with programs August 23-24 at the
University of Kentucky, and September 14-15 at Boston University. For more
on the Chautauquas, see &lt;a href=&#34;http://www.ncsa.uiuc.edu/alliance/chautauqua/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ncsa.uiuc.edu/alliance/chautauqua/&lt;/a&gt; Bader,
Maccabe, and Pennington will also present a workshop on Superclusters at
SC99 in Portland OR, November 13-19.&lt;/p&gt;
&lt;p&gt;Roadrunner Project Engineer Patricia Kovatch says that academic users may
request allocations of up to 4,000 CPU hours at &lt;a href=&#34;http://www.alliance.unm.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.alliance.unm.edu&lt;/a&gt;
Larger allocations can be requested from the Alliance Allocation Board at
&lt;a href=&#34;http://www.ncsa.uiuc.edu/alliance/applying/Overview.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ncsa.uiuc.edu/alliance/applying/Overview.html&lt;/a&gt; &amp;ldquo;We&amp;rsquo;re open for
business,&amp;rdquo; Kovatch noted, &amp;ldquo;and we&amp;rsquo;re planning enhancements in the near
future, including implementing a high performance, scalable storage
subsystem. We sent a survey to the initial users asking for feedback and
ideas to improve the supercluster.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance is a partnership to prototype
an advanced computational infrastructure for the 21st century and includes
more than 50 academic, government and industry research partners from across
the United States. The National Center for Supercomputing Applications at
the University of Illinois at Urbana-Champaign is the leading-edge site for
the Alliance. The Alliance is one of two partnerships funded by the National
Science Foundation&amp;rsquo;s Partnerships for Advanced Computational Infrastructure
(PACI) program and receives cost-sharing at partner institutions. NSF also
supports the National Partnership for Advanced Computational Infrastructure
(NPACI), led by the San Diego Supercomputer Center.&lt;/p&gt;
&lt;p&gt;More information about Roadrunner can be found at
&lt;a href=&#34;http://www.alliance.unm.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.alliance.unm.edu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1999/07/23/alliance-roadrunner-supercluster-now-on-the-grid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1999/07/23/alliance-roadrunner-supercluster-now-on-the-grid/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustered Systems Compete With Supercomputers</title>
      <link>http://localhost:1313/blog/19990503-electronicnews/</link>
      <pubDate>Mon, 03 May 1999 12:12:25 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990503-electronicnews/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Arik Hesseldahl&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;New York&amp;ndash; How do you put the power of a supercomputer into a design that can be built with standard off-the-shelf personal computer components?&lt;/p&gt;
&lt;p&gt;The latest addition to the National Technology Grid, the nationwide arsenal of supercomputing sites overseen by the National Computational Science Alliance, is a 128-processor supercomputing cluster known as Roadrunner based at the Albuquerque High Performance Computing Center at the &lt;strong&gt;University of New Mexico&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The cluster machine came not from a major OEM like SGI or Compaq, but from a small Utah-based firm called Alta Technology.&lt;/p&gt;
&lt;p&gt;Using off-the-shelf Intel Pentium II processors running at 450 megahertz, Alta custom built the cluster for the alliance, and plans are afoot to grow the cluster to 512 processors by the end of the year.&lt;/p&gt;
&lt;p&gt;Founded in 1989, Alta started out building board-level components for the embedded market centered on the Alpha Processor architecture, said Clark Roundy, Alta&amp;rsquo;s director of marketing.&lt;/p&gt;
&lt;p&gt;About two years ago, the company met with some researchers at a Linux trade show.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We basically asked them what they wanted in a clustered computing system, and they told us they wanted a system that put as much power in as small a space as possible, but using off-the-shelf component,&amp;rdquo; Roundy said.&lt;/p&gt;
&lt;p&gt;The result was the Alta cluster, which comes in standard eight- and 16-processor models that range in price from $15,000 to $35,000, and run on the Linux operating system.&lt;/p&gt;
&lt;p&gt;So far, most of Alta&amp;rsquo;s clients have been academic institutions and laboratories, but commercial and government clients are beginning to show an interest, Roundy said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We sold some systems to Boeing, which they are using for some modeling applications,&amp;rdquo; Round said. Another client is Japan&amp;rsquo;s Ministry of Industry and Trade, which just bought a system containing 256 Alpha 21264 processors running at 500MHz, he said.&lt;/p&gt;
&lt;p&gt;However designing the box was not without serious engineering challenges. The more processors you put in a single box, the more heat you have to dissipate.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We have had problems with the temperature in the past, but we solved them,&amp;rdquo; Roundy said. &amp;ldquo;We put fans on the front and the back,&amp;rdquo; he said. The company has also developed a software package that allows the user to remotely monitor temperatures, control power sequencing within the cluster and remotely shut down single processors as needed.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;If one node is shut down or fails, the system automatically reconfigures the node out of the system and continues on with the nodes that remain,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Larry Smarr, director of the National Computational Science Alliance, said that researchers, both in the academic and private sectors, are increasingly looking for inexpensive ways to do the complex calculations that require a supercomputer.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The industry is constantly driving down the costs and up the performance of the PC in the consumer market, as opposed to supercomputers which are typically built of very specialized components and are therefore typically more expensive,&amp;rdquo; Smarr said.&lt;/p&gt;
&lt;p&gt;Smarr had previously been involved with research efforts to connect 64 two-processor PCs together for supercomputer applications.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;What we wanted to do was build something that was a cross between a supercomputer and a stack of PCs,&amp;rdquo; Smarr said.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20010420191146/http://www.electronicnews.com/enews/Issue/1999/05031999/18altaah.asp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.electronicnews.com/enews/Issue/1999/05031999/18altaah.asp&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ceremony Dedicates Linux Supercluster</title>
      <link>http://localhost:1313/blog/19990420-ncsaaccess/</link>
      <pubDate>Tue, 20 Apr 1999 18:05:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990420-ncsaaccess/</guid>
      <description>&lt;p&gt;The National Computational Science Alliance (Alliance) introduced its first 128-processor workstation supercluster running the Linux operating system at an April 8 dedication at the Supercluster Computing Facility of the Albuquerque High Performance Computing Center (AHPCC), located on the University of New Mexico (UNM) campus.&lt;/p&gt;
&lt;p&gt;The supercluster, called Roadrunner, is a 64-node AltaClusterTM by Alta Technology Corporation containing 128 Intel 450 MHz Pentium II processors. The supercluster runs the Linux operating system and the processors are interconnected via a Myrinet network for high-speed communications. Roadrunner will provide the scientific community with a shared, cost-effective production environment for solving computational tasks too large for individual workstations. Roadrunner is designed to support traditional high-performance computing applications and emerging national information infrastructure applications, such as scalable Web serving, interactive visualization and data exploration, information serving, and data mining.&lt;/p&gt;
&lt;p&gt;The supercluster will also be used for computer science projects that compare software performance with other Alliance machines, such as the Windows NT Supercluster and the Silicon Graphics Origin2000TM array at NCSA, and the IBM RS/6000 SP at the Maui High Performance Computing Center (MHPCC). UNM, the Alliance, Alta Technology and Intel are working together and with others in the industry to further evolve Linux-based cluster technology. Plans are to grow the Roadrunner Supercluster to 512 processors over the next 12 months, subject to the availability of necessary resources.&lt;/p&gt;
&lt;p&gt;About 150 people attended the dedication event, including U.S. Sen. Pete Domenici, University of New Mexico President William C. Gordon, Alliance and NCSA Director Larry Smarr, and VIP guests from Los Alamos and Sandia National Laboratories, the Air Force Research Lab, IBM, Silicon Graphics, Inc., Sun Microsystems, Alta Technology Corporation and Intel.&lt;/p&gt;
&lt;p&gt;Sen. Domenici expressed his pride in the role New Mexico, UNM and AHPCC are playing in high-performance computing and research. Smarr stressed that the Roadrunner Supercluster is part of the overall vision of a nationwide computational infrastructure called the National Technology Grid, which is being prototyped by the Alliance. The Grid will be a distributed computing environment&amp;ndash;accessible anywhere and at any time&amp;ndash;that integrates high-performance computers, advanced visualization environments, mass storage devices, and massive databases via high-speed networks.&lt;/p&gt;
&lt;p&gt;For more on the Linux Roadrunner Supercluster dedication see our April 5 article.&lt;/p&gt;
&lt;p&gt;&lt;a name=&#34;gal_Access19990420&#34;&gt;&lt;/a&gt;
&lt;a href=&#34;files/LinuxSupercomputer/Access19990420.pdf&#34;&gt;NCSA/Alliance Access, Ceremony Dedicates Linux Supercluster. Bader is pictured in the top-left photo. (20 April 1999):&lt;/a&gt; 

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19990420-ncsaaccess/Access19990420_hu_3f577f50a6535ed2.webp 400w,
               /blog/19990420-ncsaaccess/Access19990420_hu_32d635dc9417eb7c.webp 760w,
               /blog/19990420-ncsaaccess/Access19990420_hu_2b16fbf25d54e462.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19990420-ncsaaccess/Access19990420_hu_3f577f50a6535ed2.webp&#34;
               width=&#34;300&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/19990508225855/http://access.ncsa.uiuc.edu/Briefs/990420.Linux.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://access.ncsa.uiuc.edu/Briefs/990420.Linux.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Plugged In: Linux Showing Up In Supercomputers</title>
      <link>http://localhost:1313/blog/19990419-linux/</link>
      <pubDate>Mon, 19 Apr 1999 16:06:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990419-linux/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Therese Poletti&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Linux &amp;ndash; the renegade operating system that is among the hottest topics in
Silicon Valley &amp;ndash; is also making its way into the most serious bastion of computing, the supercomputing world.&lt;/p&gt;
&lt;p&gt;Linux, developed by Finnish programmer Linus Torvalds in 1991, is given away over the Internet and managed by a far-flung
group of programmers, part of what is known as the open source movement. Linux has been catching on among some
corporations and Internet service providers as a reliable system to run Web servers or e-mail servers.&lt;/p&gt;
&lt;p&gt;Several high-performance computing centers, universities and government laboratories are also looking at Linux, inspired by
its low cost, its development model of sharing software code and its closeness to Unix, the operating system typically
preferred by engineers and serious computer designers.&lt;/p&gt;
&lt;p&gt;‘‘Some of the supercomputing research community would like to start moving to Linux,’’ said Irving Wladawsky-Berger, the
general manager of IBM’s Internet division and former head of the computer giant’s supercomputing business. ‘‘In the
high-end supercomputing world, everyone is a small community and that model (of open source software) is very appealing.’’&lt;/p&gt;
&lt;p&gt;Supercomputing represents a slow-growing $2.2 billion segment of the computer industry, where massive systems are now
achieving speeds in excess of one teraflop: one trillion operations per second. They are used for scientific ‘‘grand challenges,’’
such as weather forecasting, nuclear simulations, molecular modeling, and many other number-crunching intensive
applications where machines can work on a problem for a week.&lt;/p&gt;
&lt;p&gt;While Linux is not yet running any of the ultrafast, teraflop-level machines, it is now being used by a few supercomputing
centers in so-called clusters or superclusters.&lt;/p&gt;
&lt;p&gt;Scalable clustered systems are more powerful than a desktop workstation, but not quite as hefty as the multimillion-dollar
supercomputers, the fastest computers in the world. Scalable means that they can add more processors, to improve
performance or to add additional users.&lt;/p&gt;
&lt;p&gt;In 1994, the National Aeronautics and Space Administration (NASA) pioneered the use of Linux for building extremely
cheap clusters with a project called the Beowulf project, building very low-cost clusters with off-the-shelf computer parts. But
these sprawling systems took up a lot of floor space and there was no computer maker to support the patched-together
systems.&lt;/p&gt;
&lt;p&gt;So as funding is obtained, some of the high-performance computing centers are now buying cluster computers running Intel
Pentium II processors &amp;ndash; the brains of a PC &amp;ndash; and the Linux operating system.&lt;/p&gt;
&lt;p&gt;Just last week, the Albuquerque High Performance Computing Center, located on the University of New Mexico campus,
turned on a workstation supercluster system it calls Roadrunner, which basically consists of stacks of personal computer
technology running multiple Intel Corp. (Nasdaq:INTC - news) Pentium II processors and Linux.&lt;/p&gt;
&lt;p&gt;Albuquerque bought its $400,000 system from a small, privately held company called Alta Technology Corp., based in
Sandy, Utah, which develops clustered computer systems starting at $15,000, with either Intel processors or Digital’s Alpha
processor.&lt;/p&gt;
&lt;p&gt;Albuquerque’s Roadrunner has 128 Intel Pentium II processors, running at speeds of 450 megahertz, similar to the massively
parallel supercomputing systems which gang together multiple processors and distribute the work among the chips.
‘‘We are not trying to reinvent the supercomputer,’’ said &lt;strong&gt;David Bader&lt;/strong&gt;, an assistant professor of computer engineering at the
University of New Mexico. ‘‘We hope to get maybe half the performance at 10 percent of the price.’’ Albuquerque will be
looking at environmental modeling, such as computing the climate in the Rio Grande corridor, and simulations on nuclear
stockpiles under certain conditions and of accidents involving trucks carrying nuclear waste.&lt;/p&gt;
&lt;p&gt;And with Linux, Albuquerque’s engineers will be able to share their work with other colleagues at other supercomputing
centers, because Linux runs on Compaq Computer Corp. (NYSE:CPQ - news)’s Digital Alpha processor, Sun Microsystems
Inc. (Nasdaq:SUNW - news)’s Sparc technology, IBM’s PowerPC processor architecture and others.&lt;/p&gt;
&lt;p&gt;This will be especially useful for Roadrunner, which is the latest system to be connected to what is called the National
Technology Grid, an emerging network that will link a broad range of supercomputers from Boston to Maui, so that scientists
around the United States, far from the centers, can have access to vast computing power without having to leave their own
desks.&lt;/p&gt;
&lt;p&gt;‘‘Linux has already been ported to machines made by most of the major vendors, unifying the marketplace instead of
fragmenting it,’’ said Pete Beckman, a senior computer scientist at the Advanced Computing Laboratory at the Los Alamos
National Laboratory in Los Alamos, N.M. ‘‘Laboratories from around the world can collaborate more easily, sharing and
testing extensions and improvements made to or for Linux &amp;hellip; without being hampered by non-disclosure agreements and
licensing restrictions (for vendor-controlled software).’’&lt;/p&gt;
&lt;p&gt;Los Alamos is experimenting on several applications with its own Linux cluster system from Alta Tech, which it calls the
Little Blue Penguin, installed about eight months ago.&lt;/p&gt;
&lt;p&gt;Some applications at Los Alamos include a computational accelerator, which models a linear accelerator with 200 million
particles, and an ocean modeling code that is part of a global climate modeling project.&lt;/p&gt;
&lt;p&gt;Linux is competitive in many areas of high-performance computing, but there are several areas where it falls short, with
missing components. For example, the Linux kernel &amp;ndash; the core of the operating system &amp;ndash; has not been optimized to run on
large shared memory machines with eight to 128 processors.&lt;/p&gt;
&lt;p&gt;Beckman said, however, that there are either commercial or open source software development projects addressing Linux’s
shortfalls in high-performance computing.&lt;/p&gt;
&lt;p&gt;‘‘What we are seeing here across all the national labs is really an unprecedented cooperation with Linux clustering,’’ said
Remy Evard, manager of advanced computing at Argonne National Laboratory, operated by the University of Chicago for the
U.S. Department of Energy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Catching Up with the Roadrunner</title>
      <link>http://localhost:1313/blog/19990409-unm-dailylobo/</link>
      <pubDate>Fri, 09 Apr 1999 07:28:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990409-unm-dailylobo/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Jessica Schneider&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;University of New Mexico, Daily Lobo&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM To Crank Up $400,000 Supercomputer Today</title>
      <link>http://localhost:1313/blog/19990408-abqjournal/</link>
      <pubDate>Thu, 08 Apr 1999 06:39:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990408-abqjournal/</guid>
      <description>&lt;p&gt;&lt;em&gt;By John Fleck, Journal Staff Writer&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newspapers.com/image/319289210/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.newspapers.com/image/319289210/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UNM to Dedicate Supercluster Computing Facility</title>
      <link>http://localhost:1313/blog/19990405-unm/</link>
      <pubDate>Mon, 05 Apr 1999 20:47:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990405-unm/</guid>
      <description>&lt;p&gt;United States Senator Pete Domenici and University of New Mexico (UNM) President William C. Gordon will be among those participating in the dedication of a new Supercluster Computing Facility at 4 p.m. Thursday, April 8, at the Albuquerque High Performance Computing Center (AHPCC), located on the UNM campus at 1601 Central Ave. NE.&lt;/p&gt;
&lt;p&gt;Joining them will be Larry Smarr, director of the National Computational Science Alliance (Alliance) and the National Center for Supercomputing Applications (NCSA), and Anand Chandrasekhar, general manager of the Intel Workstation Products Group.&lt;/p&gt;
&lt;p&gt;The Alliance, a National Science Foundation funded partnership, will also introduce its first 128-processor Linux-based workstation supercluster, which will be housed at UNM’s Supercluster Computing Facility. The supercluster, called &amp;ldquo;Roadrunner,&amp;rdquo; is the latest addition to the National Technology Grid, the Alliance’s arsenal of powerful computational resources. The Grid connects a broad range of parallel computing systems, located at facilities from Boston to Maui, into a single virtual machine room.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Superclusters and supercomputers have made UNM a national leader in this strategic area,&amp;rdquo; said UNM President William C. Gordon. &amp;ldquo;We now have two supernodes on the National Technology Grid - the Roadrunner cluster at AHPCC and our nationally recognized supercomputing resources at the Maui High Performance Computing Center.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;UNM stands committed to continued collaborations with the Alliance, Sandia National Laboratory, Los Alamos National Laboratory, and the Department of Defense to deliver production-level, scalable computing resources to the scientific community,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Based on the latest off-the-shelf technology, Roadrunner is the first large Linux-based workstation cluster to be added to the Grid. Roadrunner will provide the scientific community with a shared, cost-effective production environment for solving computational tasks too large for individual workstations. Roadrunner is designed to support traditional high-performance computing applications and emerging national information infrastructure applications, such as scalable Web serving, interactive visualization/data exploration, information serving, and data mining.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Scalable clusters represent the most rapidly growing architecture of high-end computing,&amp;rdquo; Smarr said. &amp;ldquo;By filling the gap between desktop workstations and teraflop-scale supercomputing systems, clusters provide a very cost-effective source of computing power close to the user.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The focus of the Linux Supercluster activity is to provide a shared cost-effective parallel-computing environment for researchers across the nation,&amp;rdquo; noted Frank Gilfeather, executive director of UNM’s High Performance Computing, Education and Research Center, which manages AHPCC and MHPCC. &amp;ldquo;Roadrunner provides the computing power of a comparable conventional supercomputer at about one-tenth the cost.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Roadrunner is an AltaCluster by Alta Technology Corporation containing 128 Intel 450 MHz Pentium II processors. The Supercluster runs the Linux operating system and the processors are interconnected via a Myrinet network. UNM, Intel, and Alta Technology Corporation are working together and with others in the industry to develop the Roadrunner Supercluster as a national shared computing resource for academic research. Plans are to develop the Roadrunner to 512 processors over the next year if resources are available.&lt;/p&gt;
&lt;p&gt;Anand Chandrasekhar, general manager of the Workstation Products Group at Intel, stated, &amp;ldquo;We are excited to see Intel Architecture based workstation technology as the foundation for the Alliance’s National Linux Supercluster being deployed at UNM. Intel is focused on driving the performance available with Intel-based workstations, which is resulting in rapid adoption of our systems in commercial markets and increasingly, for the highest-end technical and research applications.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;UNM will also collaborate with the Alliance, Sandia National Laboratory, and Los Alamos National Laboratory to build and deploy the software technology necessary for scalable Linux clusters.&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance is a partnership formed to prototype an advanced computational infrastructure for the 21st century and includes more than 50 academic, government, and industry research partners from across the United States. The Alliance is one of two partnerships funded by the National Science Foundation&amp;rsquo;s Partnerships for Advanced Computational Infrastructure (PACI) program and receives cost-sharing at partner institutions. NSF also supports the National Partnership for Advanced Computational Infrastructure (NPACI), led by the San Diego Supercomputer Center.&lt;/p&gt;
&lt;p&gt;The National Center for Supercomputing Applications is the leading-edge site for the National Computational Science Alliance. NCSA is a leader in the development and deployment of cutting-edge high-performance computing, networking and information technologies. The National Science Foundation, the state of Illinois, the University of Illinois, industrial partners, and other federal agencies fund NCSA.&lt;/p&gt;
&lt;p&gt;UNM is a partner in the Alliance and serves as a regional gateway to the Alliance’s National Technology Grid for academic researchers. UNM currently provides two supernodes on the Grid, the Roadrunner Linux Supercluster at AHPCC, and the IBM RS/6000 SP supercomputing resources located at MHPCC in Kihei, Hawaii. UNM participates in these two supercomputing activities through a Strategic University Center, the High Performance Computing, Education and Research Center.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/19990916095140/http://www.unm.edu/~paaffair/news/news%20releases/Apl5ahpcc.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.archive.org/web/19990916095140/http://www.unm.edu/~paaffair/news/news%20releases/Apl5ahpcc.htm&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alliance Unveils Roadrunner Linux-Based Workstation Supercluster</title>
      <link>http://localhost:1313/blog/19990405-ncsaaccess/</link>
      <pubDate>Mon, 05 Apr 1999 19:29:40 -0400</pubDate>
      <guid>http://localhost:1313/blog/19990405-ncsaaccess/</guid>
      <description>&lt;p&gt;ALBUQUERQUE, NM&amp;ndash;The National Computational Science Alliance will introduce its first 128 processor workstation supercluster running the Linux operating system as the latest addition to the National Technology Grid, its arsenal of powerful computational resources. The supercluster, called Roadrunner, will be unveiled at an April 8 dedication of the Supercluster Computing Facility of the Albuquerque High Performance Computing Center (AHPCC), located on the University of New Mexico (UNM) campus.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Scalable clusters represent the most rapidly growing architecture of high-end computing,&amp;rdquo; said Larry Smarr, director of the Alliance and the National Center for Supercomputing Applications (NCSA), located at the University of Illinois at Urbana-Champaign. &amp;ldquo;By filling the gap between desktop workstations and teraflop-scale supercomputing systems, clusters provide a very cost-effective source of computing power close to the user.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The emerging National Technology Grid connects a broad range of parallel computing systems, located at facilities from Boston to Maui, into a single virtual machine room. Based on the latest off-the-shelf technology, Roadrunner is the first large Linux-based workstation cluster to be added to the Grid. Roadrunner will provide the scientific community with a shared, cost-effective production environment for solving computational tasks too large for individual workstations. Roadrunner is designed to support traditional high-performance computing applications and emerging national information infrastructure applications, such as scalable Web serving, interactive visualization and data exploration, information serving, and data mining.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Superclusters and supercomputers have made UNM a national leader in this strategic area,&amp;rdquo; said William Gordon, president of UNM. &amp;ldquo;We now have two supernodes on the National Technology Grid &amp;ndash; the Roadrunner cluster at AHPCC and our nationally recognized supercomputing resources at the Maui High Performance Computing Center (MHPCC). UNM stands committed to continued collaborations with the Alliance, Sandia National Laboratory, Los Alamos National Laboratory, and the Department of Defense to deliver production-level, scalable computing resources to the scientific community.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Roadrunner is a 64-node AltaCluster by Alta Technology Corporation containing 128 Intel® 450 MHz Pentium® II processors. The supercluster runs the Linux operating system and the processors are interconnected via a Myrinet network for high-speed communications. The supercluster will also be used for computer science projects that compare software performance with other Alliance machines, such as the Windows NT Supercluster and the Silicon Graphics® Origin2000TM array at NCSA and the IBM RS/6000 SP at MHPCC. UNM, the Alliance, Alta Technology and Intel are working together and with others in the industry to further evolve Linux-based cluster technology. Plans are to grow the Roadrunner Supercluster to 512 processors over the next 12 months, subject to the availability of necessary resources.&lt;/p&gt;
&lt;p&gt;Anand Chanrasekher, general manager of the Workstation Products Group at Intel, stated: &amp;ldquo;We are excited to see Intel Architecture based workstation technology as the foundation for the Alliance&amp;rsquo;s National Linux Supercluster being deployed at UNM. Intel is focused on driving the performance available with Intel-based workstations, which is resulting in rapid adoption of our systems in commercial markets and increasingly, for the highest-end technical and research applications.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We&amp;rsquo;re pleased to see an AltaCluster being placed into production for general scientific computing,&amp;rdquo; said Glen Lowry, president and CEO of Alta Technology Corporation. &amp;ldquo;Linux-based AltaClusters have been used in many scientific and research environments in the past &amp;ndash; this production cluster represents a great continuation of UNM&amp;rsquo;s computation offering.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The National Computational Science Alliance is a partnership to prototype an advanced computational infrastructure for the 21st century and includes more than 50 academic, government and industry research partners from across the United States. The Alliance is one of two partnerships funded by the National Science Foundation&amp;rsquo;s Partnerships for Advanced Computational Infrastructure (PACI) program and receives cost-sharing at partner institutions. NSF also supports the National Partnership for Advanced Computational Infrastructure (NPACI), led by the San Diego Supercomputer Center.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The National Center for Supercomputing Applications is the leading-edge site for the National Computational Science Alliance. NCSA is a leader in the development and deployment of cutting-edge high-performance computing, networking and information technologies. The National Science Foundation, the state of Illinois, the University of Illinois, industrial partners, and other federal agencies fund NCSA.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/20000301172018/http://access.ncsa.uiuc.edu/Headlines/990405.Roadrunner.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://access.ncsa.uiuc.edu/Headlines/990405.Roadrunner.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NCSA Unveils Roadrunner Linux-based Workstation Supercluster</title>
      <link>http://localhost:1313/blog/19990402-hpcwire/</link>
      <pubDate>Fri, 02 Apr 1999 13:20:13 -0500</pubDate>
      <guid>http://localhost:1313/blog/19990402-hpcwire/</guid>
      <description>&lt;p&gt;Albuquerque, NM — The National Computational Science Alliance will introduce its first 128 processor workstation supercluster running the Linux operating system as the latest addition to the National Technology Grid, its arsenal of powerful computational resources. The supercluster, called Roadrunner, will be unveiled at an April 8 dedication of the Supercluster Computing Facility of the Albuquerque High Performance Computing Center (AHPCC), located on the University of New Mexico (UNM) campus.&lt;/p&gt;
&lt;p&gt;“Scalable clusters represent the most rapidly growing architecture of high-end computing,” said Larry Smarr, director of the Alliance and the National Center for Supercomputing Applications (NCSA), located at the University of Illinois at Urbana-Champaign. “By filling the gap between desktop workstations and teraflop-scale supercomputing systems, clusters provide a very cost-effective source of computing power close to the user.”&lt;/p&gt;
&lt;p&gt;The emerging National Technology Grid connects a broad range of parallel computing systems, located at facilities from Boston to Maui, into a single virtual machine room. Based on the latest off-the-shelf technology, Roadrunner is the first large Linux-based workstation cluster to be added to the Grid. Roadrunner will provide the scientific community with a shared, cost-effective production environment for solving computational tasks too large for individual workstations. Roadrunner is designed to support traditional high-performance computing applications and emerging national information infrastructure applications, such as scalable Web serving, interactive visualization and data exploration, information serving, and data mining.&lt;/p&gt;
&lt;p&gt;“Superclusters and supercomputers have made UNM a national leader in this strategic area,” said William Gordon, president of UNM. “We now have two supernodes on the National Technology Grid — the Roadrunner cluster at AHPCC and our nationally recognized supercomputing resources at the Maui High Performance Computing Center (MHPCC). UNM stands committed to continued collaborations with the Alliance, Sandia National Laboratory, Los Alamos National Laboratory, and the Department of Defense to deliver production-level, scalable computing resources to the scientific community.”&lt;/p&gt;
&lt;p&gt;Roadrunner is a 64-node AltaCluster by Alta Technology Corporation containing 128 Intel 450 MHz Pentium II processors. The supercluster runs the Linux operating system and the processors are interconnected via a Myrinet network for high-speed communications. The supercluster will also be used for computer science projects that compare software performance with other Alliance machines, such as the Windows NT Supercluster and the Silicon Graphics Origin2000 array at NCSA and the IBM RS/6000 SP at MHPCC. UNM, the Alliance, Alta Technology and Intel are working together and with others in the industry to further evolve Linux-based cluster technology. Plans are to grow the Roadrunner Supercluster to 512 processors over the next 12 months, subject to the availability of necessary resources.&lt;/p&gt;
&lt;p&gt;Anand Chandrasekhar, general manager of the Workstation Products Group at Intel stated: “We are excited to see Intel Architecture based workstation technology as the foundation for the Alliance’s National Linux Supercluster being deployed at UNM. Intel is focused on driving the performance available with Intel-based workstations, which is resulting in rapid adoption of our systems in commercial markets and increasingly, for the highest-end technical and research applications.”&lt;/p&gt;
&lt;p&gt;“We’re pleased to see an AltaCluster being placed into production for general scientific computing,” said Glen Lowry, president and CEO of Alta Technology Corporation. “Linux-based AltaClusters have been used in many scientific and research environments in the past — this production cluster represents a great continuation of UNM’s computation offering.”&lt;/p&gt;
&lt;p&gt;The National Computational Science Alliance is a partnership to prototype an advanced computational infrastructure for the 21st century and includes more than 50 academic, government and industry research partners from across the United States. The Alliance is one of two partnerships funded by the National Science Foundation’s Partnerships for Advanced Computational Infrastructure (PACI) program and receives cost-sharing at partner institutions. NSF also supports the National Partnership for Advanced Computational Infrastructure (NPACI), led by the San Diego Supercomputer Center.&lt;/p&gt;
&lt;p&gt;The National Center for Supercomputing Applications is the leading-edge site for the National Computational Science Alliance. NCSA is a leader in the development and deployment of cutting-edge high-performance computing, networking and information technologies. The National Science Foundation, the state of Illinois, the University of Illinois, industrial partners, and other federal agencies fund NCSA.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1999/04/02/ncsa-unveils-roadrunner-linux-based-workstation-supercluster/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1999/04/02/ncsa-unveils-roadrunner-linux-based-workstation-supercluster/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader chosen for 1998 NSF Engineering Education Scholars Program</title>
      <link>http://localhost:1313/blog/19980505-eesp/</link>
      <pubDate>Tue, 05 May 1998 18:53:03 -0400</pubDate>
      <guid>http://localhost:1313/blog/19980505-eesp/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19980505-eesp/letter_hu_b9776101f2ba7aca.webp 400w,
               /blog/19980505-eesp/letter_hu_f8dd4802d4a5bb7b.webp 760w,
               /blog/19980505-eesp/letter_hu_2658b812a2abecab.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19980505-eesp/letter_hu_b9776101f2ba7aca.webp&#34;
               width=&#34;579&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19980505-eesp/letter2_hu_c41fcc5a8a5d0532.webp 400w,
               /blog/19980505-eesp/letter2_hu_7d945aaed884c22c.webp 760w,
               /blog/19980505-eesp/letter2_hu_7585926c4cdb1ba6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19980505-eesp/letter2_hu_c41fcc5a8a5d0532.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19980505-eesp/photo_hu_b999d456c9dd82b1.webp 400w,
               /blog/19980505-eesp/photo_hu_ce49d75a2b08f87e.webp 760w,
               /blog/19980505-eesp/photo_hu_9defad57899ff2f9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19980505-eesp/photo_hu_b999d456c9dd82b1.webp&#34;
               width=&#34;597&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;We are happy to announce that you have been chosen to participate in the 1998 EESP program at the UW-Madison!  The program will take place on July 12-18 on the Engineering campus at the University of Wisconsin-Madison.&lt;/p&gt;
&lt;p&gt;Congratulations on your acceptance &amp;amp; we will be in touch soon !&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;br&gt;
Marina Y. Kelly&lt;br&gt;
Program Assistant&lt;br&gt;
Engineering Learning Center&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Access ParaScope from Concurrency’s home page</title>
      <link>http://localhost:1313/blog/19980101-ieeeconcurrency/</link>
      <pubDate>Thu, 01 Jan 1998 22:39:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/19980101-ieeeconcurrency/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19980101-ieeeconcurrency/magazine_hu_6294a57c9c6cc9c9.webp 400w,
               /blog/19980101-ieeeconcurrency/magazine_hu_e90eec1ab01911fb.webp 760w,
               /blog/19980101-ieeeconcurrency/magazine_hu_83d17e983ded96c7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19980101-ieeeconcurrency/magazine_hu_6294a57c9c6cc9c9.webp&#34;
               width=&#34;595&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;IEEE Concurrency&lt;/em&gt; ’s home page
(&lt;a href=&#34;http://computer.org/concurrency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://computer.org/concurrency/&lt;/a&gt;) now
includes a link to ParaScope, a comprehensive
listing of parallel computing
sites on the Internet. The list
is maintained by &lt;strong&gt;David A. Bader&lt;/strong&gt;,
assistant professor in the University
of New Mexico’s Department of
Electrical and Computer Engineering.
You can also go directly to the
links at &lt;a href=&#34;http://computer.org/parascope/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://computer.org/parascope/&lt;/a&gt;
#parallel.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Methodology for HPC Programming on SMPs Released</title>
      <link>http://localhost:1313/blog/19970523-hpcwire/</link>
      <pubDate>Fri, 23 May 1997 19:49:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/19970523-hpcwire/</guid>
      <description>&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; and Joseph Ja&amp;rsquo;Ja&amp;rsquo; have released a
technical report entitled &amp;ldquo;SIMPLE: A Methodology for Programming High
Performance Algorithms on Clusters of Symmetric Multiprocessors (SMPs),&amp;rdquo;
Technical Report Number: CS-TR-3798 and UMIACS-TR-97-48. Institute for
Advanced Computer Studies (UMIACS), University of Maryland, College
Park, May 1997.&lt;/p&gt;
&lt;p&gt;The report describes a methodology for developing high performance programs
running on clusters of SMP nodes. The methodology is based on a small kernel
(SIMPLE) of collective communication primitives that make efficient use of
the hybrid shared and message passing environment. The power of the
methodology is illustrated by presentation of experimental results for
sorting integers, two-dimensional fast Fourier transforms (FFT), and
constraint-satisfied searching. The testbed is a cluster of DEC AlphaServer
2100 4 / 275 nodes interconnected by an ATM switch.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1997/05/23/methodology-for-hpc-programming-on-smps-released/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1997/05/23/methodology-for-hpc-programming-on-smps-released/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CuraGen Assembles Most Complete Mouse EST Database to Date</title>
      <link>http://localhost:1313/blog/19970518-curagen/</link>
      <pubDate>Sun, 18 May 1997 12:43:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/19970518-curagen/</guid>
      <description>&lt;p&gt;Scientists at CuraGen Corporation have coordinated the assembly of the most complete mouse EST database to date.&lt;/p&gt;
&lt;p&gt;The assembly took as input 45,683 clusters of public mouse ESTs. The CAP2 program was used to perform the assembly, producing 49,228 assembled sequences (some clusters produced multiple assemblies) with 3-fold coverage on average.&lt;/p&gt;
&lt;p&gt;The database assembly was computationally intensive, taking approximately 2 minutes per cluster on a Sun workstation. The assembly was distributed   across computer resources worldwide for rapid assembly of the entire data set in a weekend.&lt;/p&gt;
&lt;p&gt;Participating in the assembly process were Dr. Greg Schuler, NCBI, who provided the initial clusters; Dr. Xiaoqiu Huang, Michigan Technological University, author of the CAP2 program; Dr. James Knight, CuraGen Corporation; Dr. Moshe Eisenberg, SUNY Stony Brook; &lt;strong&gt;Dr. David A. Bader&lt;/strong&gt;, University of New Mexico; and Dr. H.-W. Mewes, Max Planck Institute for Biochemistry.&lt;/p&gt;
&lt;p&gt;CuraGen Corporation (ticker: CRGN, exchange: NASDAQ) News Release - New Haven, CT, May 18, 1997&lt;/p&gt;
&lt;h4 id=&#34;contact&#34;&gt;Contact:&lt;/h4&gt;
&lt;p&gt;Mark R. Vincent&lt;br&gt;
Director, Corporate Communications
&amp;amp; Investor Relations
&lt;a href=&#34;mailto:mvincent@curagen.com&#34;&gt;mvincent@curagen.com&lt;/a&gt;&lt;br&gt;
CuraGen Corporation 1-888-GENOMICS
&lt;a href=&#34;https://www.curagen.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.curagen.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/19971015234540/http://www.curagen.com/releases/est_31897.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.curagen.com/releases/est_31897.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HPC Web Sites of Interest</title>
      <link>http://localhost:1313/blog/19960704-hpcwire/</link>
      <pubDate>Thu, 04 Jul 1996 07:31:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/19960704-hpcwire/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Alan Beck, managing editor&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Contrary to strident popular opinion, the enervating quality of the Web is
generated more through an abundance of information rather than an
accumulation of moral deficits. Nowhere is this more evident than in the
enormous collection of sites dealing with HPC. This new column, slated to
appear quarterly, is not meant to provide a thorough compendium of
HPC-related material on the Web; several resources already perform that
function quite admirably, e.g. &lt;strong&gt;David A. Bader&lt;/strong&gt;&amp;rsquo;s page,
&lt;a href=&#34;http://www.umiacs.umd.edu/~dbader/sites.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.umiacs.umd.edu/~dbader/sites.html&lt;/a&gt;, CalTech&amp;rsquo;s list,
&lt;a href=&#34;http://www.ccsf.caltech.edu/other_sites.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.ccsf.caltech.edu/other_sites.html&lt;/a&gt;, Jonathan Hardwick&amp;rsquo;s page,
&lt;a href=&#34;http://www.cs.cmu.edu/~scandal/resources.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.cmu.edu/~scandal/resources.html&lt;/a&gt; and others. Nor can it hope to
review sites of inordinate significance; while journalists may lay legitimate
claim to insight, they cannot do the same for prescience.&lt;/p&gt;
&lt;p&gt;Ideally, what the column can accomplish is simply this: to pique the
curiosity of a few interested readers &amp;ndash; perhaps motivating them to
investigate promising areas they may have inadvertendly neglected &amp;ndash; and to
provide some notion, however inadequate, of the breadth and depth of HPC
resources currently residing in cyberspace. Only time &amp;ndash; and reader comments
&amp;ndash; will tell if these goals are being met, or, indeed, if they are worth
pursuing at all.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Bulk Synchronous Parallel (BSP) Primer&lt;br&gt;
&lt;a href=&#34;http://www.scs.carleton.ca/~palepu/bsp_primer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.scs.carleton.ca/~palepu/bsp_primer.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The handiwork of Ravi Palepu at Carlton University&amp;rsquo;s School of Computer
Science, this site offers a thorough grounding in elements of the BSP model
of parallel computing. Palepu covers BSP&amp;rsquo;s evolution, components, algorithms,
languages and extensions. He notes: &amp;ldquo;the BSP model directs the programmer to
perform many local referenced memory operations before making a non-local
reference. After a sequence of local memory reference operations and at most
only one nonlocal memory reference, a global barrier synchronization is
performed. At this time, all processors are blocked until nonlocal memory
references can be carried out. This sequence of steps is called a superstep.
A series of supersteps would encompass the entire computation.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;For those committed to exploring all avenues for more efficient utilization
of parallel environments, Palepu&amp;rsquo;s presentation is certainly worth a look &amp;ndash;
or even more: Harvard and Oxford are actively doing work in this area.&lt;/p&gt;
&lt;p&gt;Fermilab&amp;rsquo;s Computation Division&lt;br&gt;
&lt;a href=&#34;http://www.fnal.gov/cd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.fnal.gov/cd/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A good antidote for those inclined toward cynicism about nationally-funded
HPC efforts, Fermilab provides a wealth of useful resources for those with
interests extending into areas beyond its specialty of particle physics
proper, including UNIX and standard resources, CAD, parallel and distributed
computing, and databases.&lt;/p&gt;
&lt;p&gt;Visualization of Parallel and Distributed Programs&lt;br&gt;
&lt;a href=&#34;http://www.cc.gatech.edu/gvu/softviz/parviz/parviz.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cc.gatech.edu/gvu/softviz/parviz/parviz.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This remarkable Georgia Tech site provides numerous striking images
generated through PARADE, an environment designed to enable visualization of
the dynamics of parallel and distributed programs. As visualization is
proving to be an invaluable tool for understanding physical, chemical,
economic and other complex processes, this site has taken the matter a step
further, turning the mirror of illuminating graphics upon advanced
computation itself. The results must be seen to be appreciated. Devotees of
20th century art (like this reviewer) will find the experience as impressive
aesthetically as it is analytically revealing.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.hpcwire.com/1996/07/04/hpc-web-sites-of-interest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hpcwire.com/1996/07/04/hpc-web-sites-of-interest/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader awarded NASA Graduate Student Researchers Program Fellowship</title>
      <link>http://localhost:1313/blog/19950929-nasa/</link>
      <pubDate>Fri, 29 Sep 1995 19:27:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/19950929-nasa/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950929-nasa/certificate_hu_505d6a274180ba49.webp 400w,
               /blog/19950929-nasa/certificate_hu_5456741b9068cc70.webp 760w,
               /blog/19950929-nasa/certificate_hu_981fccd876d4a3d2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950929-nasa/certificate_hu_505d6a274180ba49.webp&#34;
               width=&#34;760&#34;
               height=&#34;514&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;National Aeronautics and Space Administration (NASA)&lt;/p&gt;
&lt;h2 id=&#34;goddard-space-flict-center&#34;&gt;Goddard Space Flict Center&lt;/h2&gt;
&lt;h2 id=&#34;graduate-student-researchers-program-gsrp&#34;&gt;Graduate Student Researchers Program (GSRP)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, University of Maryland, is awarded a NASA Fellowship in recognition of outstanding research potential in Earth, space science and engineering.&lt;/p&gt;
&lt;p&gt;Joseph H. Rothenberg&lt;br&gt;
Director, Goddard Space Flight Center&lt;/p&gt;
&lt;p&gt;Gerald A. Soffen&lt;br&gt;
Director, Office of University Programs&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader gains membership in Omicron Delta Kappa, The National Leadership Honor Society</title>
      <link>http://localhost:1313/blog/19950404-odk/</link>
      <pubDate>Tue, 04 Apr 1995 21:49:47 -0400</pubDate>
      <guid>http://localhost:1313/blog/19950404-odk/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950404-odk/19950404-ODK_hu_29a02d7d23fe70aa.webp 400w,
               /blog/19950404-odk/19950404-ODK_hu_392adb2a8c5b8023.webp 760w,
               /blog/19950404-odk/19950404-ODK_hu_aef83d395f4e8a45.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950404-odk/19950404-ODK_hu_29a02d7d23fe70aa.webp&#34;
               width=&#34;592&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;omicron-delta-kappa&#34;&gt;Omicron Delta Kappa&lt;/h2&gt;
&lt;h2 id=&#34;the-national-leadership-honor-society&#34;&gt;The National Leadership Honor Society&lt;/h2&gt;
&lt;p&gt;This certifies that in recognition to conspicuous attainments and service in collegiate activities the Omicron Delta Kappa Society has this day conferred membership on &lt;strong&gt;David Bader&lt;/strong&gt; Through its Circle at University of Maryland, April 4, 1995&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader inducted into the Omicron Delta Kappa National Leadership Honor Society</title>
      <link>http://localhost:1313/blog/19950227-odk/</link>
      <pubDate>Mon, 27 Feb 1995 17:21:02 -0400</pubDate>
      <guid>http://localhost:1313/blog/19950227-odk/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950227-odk/letter1_hu_a0bb03d199a7f23e.webp 400w,
               /blog/19950227-odk/letter1_hu_6b72a4c92646e63b.webp 760w,
               /blog/19950227-odk/letter1_hu_cc8ac96fadf1bea5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950227-odk/letter1_hu_a0bb03d199a7f23e.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950227-odk/letter2_hu_6b127a25c08ef7b1.webp 400w,
               /blog/19950227-odk/letter2_hu_a706cd1eb7ec2c10.webp 760w,
               /blog/19950227-odk/letter2_hu_fb37aaa4ee3e1ede.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950227-odk/letter2_hu_6b127a25c08ef7b1.webp&#34;
               width=&#34;601&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dear &lt;strong&gt;David&lt;/strong&gt;,&lt;/p&gt;
&lt;p&gt;Congratulations! You have been selected for membership in the Sigma Circle of the Omicron Delta Kappa National Leadership Honor Society.  This is one of the most presitgious honors that a student can receive, and you are certainly deserving of it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Founder of the University of Maryland EE Graduate Student Association</title>
      <link>http://localhost:1313/blog/19950201-farvardin/</link>
      <pubDate>Wed, 01 Feb 1995 17:08:42 -0400</pubDate>
      <guid>http://localhost:1313/blog/19950201-farvardin/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950201-farvardin/certificate_hu_ae37f57315cb7cfd.webp 400w,
               /blog/19950201-farvardin/certificate_hu_f48cb29c9fd8a3b.webp 760w,
               /blog/19950201-farvardin/certificate_hu_2c2beac153b66205.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950201-farvardin/certificate_hu_ae37f57315cb7cfd.webp&#34;
               width=&#34;760&#34;
               height=&#34;586&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950201-farvardin/letter1_hu_2c33e1ceb893a316.webp 400w,
               /blog/19950201-farvardin/letter1_hu_79074c02a1d7ccb1.webp 760w,
               /blog/19950201-farvardin/letter1_hu_4aad040cfbd59944.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950201-farvardin/letter1_hu_2c33e1ceb893a316.webp&#34;
               width=&#34;626&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19950201-farvardin/letter2_hu_9fc422b0e286f362.webp 400w,
               /blog/19950201-farvardin/letter2_hu_502c07372020e5fa.webp 760w,
               /blog/19950201-farvardin/letter2_hu_7fbaea94fd041eb7.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19950201-farvardin/letter2_hu_9fc422b0e286f362.webp&#34;
               width=&#34;599&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In his letter of recommendation for &lt;strong&gt;David Bader&lt;/strong&gt; to the Omicron Delta Kappa National Honor Society, University of Maryland Electrical Engineering Department Chair Nariman Farvardin states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mr. David Bader&lt;/strong&gt; is the founder and currently the president of the Graduate Student Association in the Department of Electrical Engineering.  He has been working relentlessly over the past six months to create a sense of identity for this organization and to establish a closer tie between the graduate students and the faculty and staff.  He has formed a cluster of students who care about advancing the graduate student life and has led the way in a number of new initiatives such as: (i) the graduate student coffee hour, (ii) the graduate student electronic newsgroup, (iii) the graudate student fellowship book, (iv) the graduate student job bank and (v) the graduate student handbook.&lt;/p&gt;&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Bader awarded NASA Graduate Student Researchers Program Fellowship</title>
      <link>http://localhost:1313/blog/19940923-nasa-gsrp/</link>
      <pubDate>Fri, 23 Sep 1994 17:07:14 -0400</pubDate>
      <guid>http://localhost:1313/blog/19940923-nasa-gsrp/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19940923-nasa-gsrp/certificate_hu_a6580aa37eb6e5ff.webp 400w,
               /blog/19940923-nasa-gsrp/certificate_hu_26f88253630d32f3.webp 760w,
               /blog/19940923-nasa-gsrp/certificate_hu_d6aa5519b5c8f887.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19940923-nasa-gsrp/certificate_hu_a6580aa37eb6e5ff.webp&#34;
               width=&#34;760&#34;
               height=&#34;645&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;National Aeronautics and Space Administration (NASA)&lt;/p&gt;
&lt;h2 id=&#34;graduate-student-researchers-program-gsrp&#34;&gt;Graduate Student Researchers Program (GSRP)&lt;/h2&gt;
&lt;p&gt;Goddard Space Flict Center&lt;br&gt;
Greenbelt, Maryland&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, University of Maryland, is awarded a NASA Graduate Student Researchers Program Fellowship in recognition of his outstanding research potential in space science and aerospace technology.&lt;/p&gt;
&lt;p&gt;Dr. John M. Klineberg&lt;br&gt;
Director, Goddard Space Flight Center&lt;/p&gt;
&lt;p&gt;Dr. Gerald A. Soffen&lt;br&gt;
Director, Office of University Programs&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader awarded NASA Graduate Student Researchers Program Fellowship</title>
      <link>http://localhost:1313/blog/19930917-nasa-gsrp/</link>
      <pubDate>Fri, 17 Sep 1993 15:10:43 -0400</pubDate>
      <guid>http://localhost:1313/blog/19930917-nasa-gsrp/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19930917-nasa-gsrp/certificate_hu_4a24aa9050ff33be.webp 400w,
               /blog/19930917-nasa-gsrp/certificate_hu_f7a5b72b83af00c8.webp 760w,
               /blog/19930917-nasa-gsrp/certificate_hu_63b350dfa23c4fbd.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19930917-nasa-gsrp/certificate_hu_4a24aa9050ff33be.webp&#34;
               width=&#34;760&#34;
               height=&#34;608&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;National Aeronautics and Space Administration (NASA)&lt;/p&gt;
&lt;h2 id=&#34;graduate-student-researchers-program-gsrp&#34;&gt;Graduate Student Researchers Program (GSRP)&lt;/h2&gt;
&lt;p&gt;Goddard Space Flict Center&lt;br&gt;
Greenbelt, Maryland&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, University of Maryland, College Park, is awarded a NASA Graduate Student Researchers Program Fellowship in recognition of his outstanding research potential in space science and aerospace technology.&lt;/p&gt;
&lt;p&gt;John M. Klineberg&lt;br&gt;
Director, Goddard Space Flight Center&lt;/p&gt;
&lt;p&gt;Gerald A. Soffen&lt;br&gt;
Director, University Programs&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader receives NASA Graduate Student Researchers Program (GSRP) Fellowship</title>
      <link>http://localhost:1313/blog/19920330-nasa/</link>
      <pubDate>Mon, 30 Mar 1992 22:10:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/19920330-nasa/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19920330-nasa/19920330-NASA-1_hu_b7ff678371d6ac08.webp 400w,
               /blog/19920330-nasa/19920330-NASA-1_hu_3c8edb3af0190e78.webp 760w,
               /blog/19920330-nasa/19920330-NASA-1_hu_7a684e4ec983b38c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19920330-nasa/19920330-NASA-1_hu_b7ff678371d6ac08.webp&#34;
               width=&#34;592&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19920330-nasa/19920330-NASA-2_hu_ff1b7dd1c4b604f1.webp 400w,
               /blog/19920330-nasa/19920330-NASA-2_hu_1adb6a8fd581fd67.webp 760w,
               /blog/19920330-nasa/19920330-NASA-2_hu_a14a60203c3a28da.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19920330-nasa/19920330-NASA-2_hu_ff1b7dd1c4b604f1.webp&#34;
               width=&#34;587&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Dear &lt;strong&gt;Mr. Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Your proposal &amp;ldquo;Parallel Algorithms for Massively Parallel Architectures,&amp;rdquo; for a NASA Graduate Researchers Program (GSRP) Fellowship was rated very highly by our review panel.  Consequently, it is my pleasure to offer you a 1992 GSRP Fellowship award. Congratulations!&lt;/p&gt;
&lt;p&gt;This year, I received more than 75 proposals for GSRP Fellowships at Goddard.  Most of them were quite impressive, but only a few could be funded. The fact that yours is among them speaks highly of your academic record and of the research project which you are proposing. The funding of your fellowship will come from the new fellowship allotment I received from NASA Headquarters.  Yours is one of only seven such fellowships which are being funded this year.  Dr. John E. Dorband has expressed considerable interest in your work and will serve as the technical advisor for your fellowship.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Gerald A. Soffen&lt;br&gt;
Director of University Programs&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lehigh Honors Surgeon General</title>
      <link>http://localhost:1313/blog/19920113-morningcall/</link>
      <pubDate>Mon, 13 Jan 1992 16:05:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/19920113-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19920113-morningcall/clip_82931801_hu_3de69c3882457b77.webp 400w,
               /blog/19920113-morningcall/clip_82931801_hu_ff3586c21af84e8b.webp 760w,
               /blog/19920113-morningcall/clip_82931801_hu_456ff0004676984c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19920113-morningcall/clip_82931801_hu_3de69c3882457b77.webp&#34;
               width=&#34;760&#34;
               height=&#34;497&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Finding a cure for AIDS is the greatest medical challenge of our age, Antonia C. Novello, surgeon general of the U.S. Public Health Service, said yesterday.&lt;/p&gt;
&lt;p&gt;Novello, the first woman and first Hispanic to hold the position of surgeon general, received an honorary doctor of science degree during Lehigh University&amp;rsquo;s sixth annual winter commencement.&lt;/p&gt;
&lt;p&gt;Lehigh conferred degrees upon 90 area residents yesterday in Packer Memorial Church. The students are among 244 men and women who completed their degree requirements during the fall semester and will receive bachelor&amp;rsquo;s, master&amp;rsquo;s or doctoral degrees.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Never for a moment think you will pass unaffected by AIDS,&amp;rdquo; she told the capacity audience in the church. &amp;ldquo;No other disease has had an impact like AIDS.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;With increased heterosexual transmissions, she predicted that, by the year 2000, 40 million people worldwide will be infected with AIDS. By the end of the decade the disease will orphan 10 million children.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Behind every statistic is a human being,&amp;rdquo; Novello said.&lt;/p&gt;
&lt;p&gt;Beyond the AIDS epidemic, she identified violence as another crisis to overcome, saying, &amp;ldquo;This country must stop accepting violence as a way of life.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Novello also encouraged graduates to be sensitive to all ethnic and age groups and to avoid complacency, mediocrity and indifference. &amp;ldquo;Develop a healthy skepticism,&amp;rdquo; Novello said, because in five years much of what was learned in the classroom will be obsolete.&lt;/p&gt;
&lt;p&gt;Test and challenge new ideas, she said. &amp;ldquo;The harder you work, the luckier you will be. The world owes you nothing.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;In a brief meeting with the media following the exercises, Novello flatly said &amp;ldquo;no&amp;rdquo; when asked if a national health-care plan would evolve under her stewardship.&lt;/p&gt;
&lt;p&gt;To be affordable and of high quality, health coverage must combine federal, local, public and private sources, all under one roof, she said. She said she hopes the president will present a plan when he comes before Congress this month.&lt;/p&gt;
&lt;p&gt;In the coming years, medical philosophy will advocate prevention first, rather than searching for cures.&lt;/p&gt;
&lt;p&gt;In 1988, $90 million dollars was spent on prevention, compared to today&amp;rsquo;s $297 million. For every dollar spent on prevention, $14 is saved in medical costs. In her address, she said heart disease could be reduced by 45 percent through prevention.&lt;/p&gt;
&lt;p&gt;An honorary degree was also awarded to Morris Tanenbaum, retired vice chairman of the board of AT&amp;amp;T.; John Brooks Slaughter, president of Occidental College in Los Angeles, Calif., who was to receive an honorary degree, but was forced to decline because of a last-minute change in travel plans.&lt;/p&gt;
&lt;p&gt;Tanenbaum, who developed the first silicon-diffused base transistors and led the group that discovered practical materials for superconducting magnets, received an honorary doctor of science degree.&lt;/p&gt;
&lt;p&gt;Degrees were awarded individually to the students by Peter Likins, president of the university.&lt;/p&gt;
&lt;p&gt;Candidates for degrees were presented by James D. Gunton, dean of the College of Arts and Science; Richard W. Barsness, dean of the College of Business and Economics; Sunder H. Advani, new dean of the College of Engineering and Applied Science; and Alden J. Moe, dean of the College of Education.&lt;/p&gt;
&lt;p&gt;Alan W. Pense, university provost and vice president for academic affairs, presented the candidates for honorary degrees. Roy C. Herrenkohl, vice provost for research and dean of graduate studies, placed appropriate hoods on the shoulders of the candidates.&lt;/p&gt;
&lt;p&gt;The Rev. John S. Mraz, Catholic chaplain and director of the campus Newman Center, gave the invocation and asked the benediction.&lt;/p&gt;
&lt;p&gt;Raymond Bell, professor of education and social relations, served as university marshal. Fazil Erdogan, professor of mechanical engineering and mechanics, carried the mace in the processional and recessional. Fred P. Stein, professor of chemical engineering, was the chief faculty usher.&lt;/p&gt;
&lt;p&gt;Following the ceremony, a reception for the new graduates and their guests was held in the Asa Packer Room of the University Center.&lt;/p&gt;
&lt;p&gt;Seven Army and Air Force cadets who have completed the ROTC program at Lehigh were commissioned as second lieutenants in their respective services at 2:30 p.m. on Saturday, in Upper Grace Hall.&lt;/p&gt;
&lt;p&gt;Those being commissioned included Cadet Christina M. Balum of Hellertown, a Moravian College student, Army.&lt;/p&gt;
&lt;p&gt;Honor recipients include two Lehigh Valley area students. Shon Les Harker of Allentown, graduating with highest honors and Paul G. Beidler of Easton, graduating with honors.&lt;/p&gt;
&lt;p&gt;Students from the Lehigh Valley area receiving doctoral degrees are:&lt;/p&gt;
&lt;p&gt;From Bethlehem: David L. Angst Jr., chemistry; Mahmoud Abdelwahid Hamaad, industrial engineering; Bryan Clair Hoke Jr., chemical engineering; Dyllan Jye-Lun Hong, materials science and engineering; and Diane Patricia Tromans, school psychology.&lt;/p&gt;
&lt;p&gt;Patrick Clinton Wernett of Easton, chemistry, ; Priscilla Edwards Howard of Emmaus, reading; Gregory E. Buzan of Hellertown, chemical engineering; Debra Lubowicki of Quakertown, special education; Vincent G. Grassi of Schnecksville, chemical engineering; Jonathan Kendell Kramer of Slatington, business and economics, and David L. Angst Jr. of Tamaqua, chemistry. Students from the Lehigh Valley area receiving master&amp;rsquo;s degrees include:&lt;/p&gt;
&lt;p&gt;From Allentown: Pravin Bhagat, M.B.A.; Donald William Breisch, M.S., computer science; Krystina J. Butler, M.Ed., secondary education; Jeannette Ruth Byala, M.B.A.; John E. Davis, M.S., computer science; Terrence Scott Hahn, M.Eng., materials science and engineering; Norman John Marttila, M.B.A.; Robert C. Potts, M.B.A.; Robert N. Sage, M.S., manufacturing systems engineering; and Scott N. Walck, M.S., physics.&lt;/p&gt;
&lt;p&gt;From Bethlehem: Atul Arora, M.S., polymer science and engineering; &lt;strong&gt;David A. Bader&lt;/strong&gt;, M.S., electrical engineering; Eric Karl Baisch, M.S., applied mechanics; Sidney S. Blake, M.S., chemistry; Ana Patricia Chaves, M.A., secondary education; Roberta Jill Deily, M.S., educational technology; Mona A. Koury, M.Ed., reading; Lana Karen McClung, M.A., secondary education; Suzanne Elise McKenna, M.S., psychology, Thomas Michael Ossman, M.S., manufacturing systems engineering; Shahida M. Parvez, M.S., computer science; Craig F. Sanders, M.B.A.; Joan Delia Stanescu, M.S., materials science and engineering; Maryse Stanley, M.A., secondary education; Robert Cantwell Stolz, M.S., mathematics; Diane P. Tromans, M.Ed., human development; Sue Varley, M.B.A.; Paul E. Winters, M.A., English; and Richard Hilton Yam, M.A., secondary education.&lt;/p&gt;
&lt;p&gt;Brian David Clifford, M.B.A.; Glenn Thompson Eksaa, M.S., electrical engineering, and Barbara Ann Koegler, M.Ed., elementary education, all of Easton.&lt;/p&gt;
&lt;p&gt;Richard Ray Kern II, M.S., mathematics, Emmaus; Gregory E. Buzan, M.S., chemical engineering, Hellertown; Karen J. Fuls, M.Ed., special education, Kunkletown; James George Backer, M.B.A.; Catherine A. Hilliard, M.S., educational technology, and Jamie L. Mitchell, M.S., computer science, all of Macungie; Terry Edward Bennett, M.Eng., electrical engineering, and David A. Inglis, M.S., electrical engineering, both of Orefield; Kathleen M. Czupich, M.B.A., Quakertown; Roger J. Burna, M.B.A., Schnecksville; Lisa Laverty Sittler, M.B.A., Walnutport; Michelle L. Harm, M.B.A., Randa Jean Jabbour, M.B.A., and Susan B. Troyan, M.B.A., all of Whitehall.&lt;/p&gt;
&lt;p&gt;From other areas: Donald Alan Werkema, M.B.A., of Blandon; Alyssa S. Degler, M.S., chemistry, Boyertown; Theodore Silar, M.A., English, Kutztown; Thomas Francis Strelchun, M.S., electrical engineering, Mertztown; William L. Gelatka, M.S., electrical engineering, Perkasie; Virginia Anne Polinski, M.B.A., Effort; Patricia Ann Jarvis, M.S., educational technology, Stroudsburg; Christine Marie Peloghitis, M.B.A., Red Hill; Paula Ann Stanwick, M.A., English, Belvidere; and Maureen Gail Vetrecin, M.Ed., human development, Stewartsville.&lt;/p&gt;
&lt;p&gt;Students from the Lehigh Valley area receiving bachelor&amp;rsquo;s degrees are:&lt;/p&gt;
&lt;p&gt;John Paul Early, Shon Les Harker, Bryan D. O&amp;rsquo;Connell and Mayank Shyam, all of Allentown; David V. Bader, Richard Thomas Destremps and Christine Marie Narzisi, all of Bethlehem; Paul G. Beidler and Dai Quang Nguyen, both of Easton; David Brannan Teufel of Emmaus; Mark Richard Miller of Nazareth; Kevin Michael Madaya of Northampton; Corey George Blake of Cresco and John Paul Early of Lansdale.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF awards Bader an Honorable Mention</title>
      <link>http://localhost:1313/blog/19910320-nsf/</link>
      <pubDate>Wed, 20 Mar 1991 16:54:15 -0400</pubDate>
      <guid>http://localhost:1313/blog/19910320-nsf/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19910320-nsf/letter_hu_b45453d619e2fce9.webp 400w,
               /blog/19910320-nsf/letter_hu_a92be850c5307103.webp 760w,
               /blog/19910320-nsf/letter_hu_48f8d7aac99b2898.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19910320-nsf/letter_hu_b45453d619e2fce9.webp&#34;
               width=&#34;591&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Mr. Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Your application for a National Science Foundation Graduate Fellowship for the 1991-1992 award period has been carefully evaluated. Although the Foundation is unable to offer you a fellowship in this competition, it is a pleasure to inform you that on the basis of your application, which was highly meritorious, you have been accorded Honorable Mention.  Unless you have previously requested otherwise, your name will appear on the Honorable Mention List, which is scheduled for public release today.&lt;/p&gt;
&lt;p&gt;We are pleased to remind you that, as an Honorable Mention Recipient, you will be eligible to request through this office up to 10 CPU hours of supercomputer use during your graduate work, if appropriate.&lt;/p&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Sincerely yours,&lt;br&gt;
Terence L. Porter&lt;br&gt;
Director, Division of Research Career Development&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Professor Sees Prosperity Ahead</title>
      <link>http://localhost:1313/blog/19901015-morningcall/</link>
      <pubDate>Mon, 15 Oct 1990 15:26:39 -0400</pubDate>
      <guid>http://localhost:1313/blog/19901015-morningcall/</guid>
      <description>&lt;p&gt;The worldwide triumph of the free market system foreshadows prosperous economic conditions in 20 years, a Lehigh University professor told a Lehigh commencement audience yesterday.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I think it&amp;rsquo;s going to be great,&amp;rdquo; said J. Richard Aronson, professor of economics during the university&amp;rsquo;s 112th annual Founder&amp;rsquo;s Day ceremonies in Packer Memorial Church.&lt;/p&gt;
&lt;p&gt;Judging by the worldwide growth and power of the market system, average annual family incomes in 2010 could reach $95,000, he said. And college graduates could earn as much as $150,000 &amp;ndash; &amp;ldquo;so you can pay your child&amp;rsquo;s $40,000 tuition bill.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But today&amp;rsquo;s world conditions could also sour, Aronson said. The break-up of the Soviet Union into nationalist revolutions and the insecurity of the Middle East could tumble the world into deep depression, he said.&lt;/p&gt;
&lt;p&gt;America&amp;rsquo;s infrastructure, including everything from deteriorating highways to a lost will to study and take risks, could decay as well and cause economic ruin, he said.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;I get depressed just thinking of these things,&amp;rdquo; Aronson said.&lt;/p&gt;
&lt;p&gt;He advised the graduates to take advantage of the future and all its inherent risks. &amp;ldquo;Use your imaginations,&amp;rdquo; he said, &amp;ldquo;and you must find work that you love. If you love your work you&amp;rsquo;ll prosper. Prosperity runs deeper than economic success.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;After Aronson&amp;rsquo;s remarks, 95 Lehigh Valley residents received undergraduate and advanced degrees. The Lehigh Valley residents are among 206 students who completed their degree requirements during the past summer.&lt;/p&gt;
&lt;p&gt;Two honorary degrees were also awarded during the ceremony. Bernard L. Cohen The 1936 Lehigh graduate is well known for his civic leadership in the Lehigh Valley. Cohen is retired chairman of the board and chief executive operator of Piercing Pagoda, the national jewelry chain that he and his wife Berte founded.&lt;/p&gt;
&lt;p&gt;Luther W. Brady, one of the world&amp;rsquo;s foremost cancer specialists, received an honorary doctor of science degree. He is the Hylda Cohn/ American Cancer Society professor of clinical oncology and chairman of the department of radiation oncology and nuclear medicine at Hahnemann University, Philadelphia.&lt;/p&gt;
&lt;p&gt;Two Lehigh faculty members who have been named to endowed professorships in the past year were honored during the ceremony. They are Mark H. Bickhard, the Henry L. Luce Professor of Cognitive Robotics and Philosophy of Knowledge, and D. Raymond Bainbridge, the Coopers Lybrand Professor in Accounting.&lt;/p&gt;
&lt;p&gt;During the ceremony, 50 students were honored for outstanding achievements. Nine local students were among those honored. They include:&lt;/p&gt;
&lt;p&gt;Jake Fotopoulos, and Geraldine Kay Micek, both of Allentown, will receive the Elizabeth Major Nevius Award to fifth-year students on the basis of leadership, citizenship and scholarship.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, Jan Philip Falkin and James M. Schray, all of Bethlehem, will receive the Elizabeth Major Nevius Award to fifth-year students on the basis of leadership, citizenship and scholarship. Jeffrey Alan Hittinger of Bethlehem will receive the Pi Tau Sigma Prize to the highest-ranking sophomore in mechanical engineering.&lt;/p&gt;
&lt;p&gt;Janet Temos of Catasauqua will receive the Alumni Association Junior Prize to the highest ranking junior in the College of Arts and Science. Sharon Nevah of Emmaus will receive the Williams Sophomore Prize in Composition. Craig Joseph Constantine of Orefield will receive the Malcolm J. Gordon, Jr., Physics Prize to the top-ranking sophomore majoring in physics with some extracurricular activity.&lt;/p&gt;
&lt;p&gt;Local students receiving doctoral degrees are:&lt;/p&gt;
&lt;p&gt;Luan Van Tran of Allentown, Ph.D. with a major in electrical engineering. Jennifer Anne Bruce, D.Ed. with a major in school psychology; Kyungsun Hong Cho, D.Ed. with a major in educational technology; Thomas M. Colbert, Ph.D. in physics; Tommy Wayne Hawkins, Ph.D. in polymer science and engineering; Mark F. Masters, Ph.D. in physics; Elizabeth P. Patch, Ph.D. in economics; Linda Light Ravelle, Ph.D. in business and economics; John Anthony Stamatakos, Ph.D. in geology, and Binghua Wu, Ph.D. in applied mechanics, all of Bethlehem.&lt;/p&gt;
&lt;p&gt;Alois M. Himsl, of Easton, Ph.D. in chemistry. Kenneth William Tiedge of Kempton, Ph.D. in chemical engineering. Joseph James Fay of Doylestown, Ph.D. in polymer science and engineering. John A. Gilly of Cresco, Ph.D. in molecular biology. Kim Lauren McKay of Stroudsburg, Ph.D. in English. Beth Ann Mallon of Harleysville, D.Ed. with a major in school psychology.&lt;/p&gt;
&lt;p&gt;Local students receiving master&amp;rsquo;s degrees are:&lt;/p&gt;
&lt;p&gt;David G. Reinert of Alburtis, M.S. with a major in manufacturing systems engineering. Jeffrey Frederick Andrew, M.B.A.; Stephen Leon Campbell, M.Ed. with a major in special education; Deborah Jean Gross, M.B.A.; Theodore John Jeske, M.B.A.; Anthony Joseph Mauro, M.B.A.; Robert George Mentz, M.B.A.; Michael Kevin Messer, M.B.A.; Steven David Miller, M.Ed. with a major in educational administration; Thomas Earl Mutchler, M.B.A., and Eliezer Daniel Skaist, M.Ed. with a major in elementary education, all of Allentown.&lt;/p&gt;
&lt;p&gt;Enver Aksu, M.S. with a major in manufacturing systems; Rhoda Katherine Alpaugh, Ph.D. in molecular biology; Rosemary Coyle Azzalina, M.Ed. with a major in elementary education; Nanci Ellen Bain, M.Ed. with a major in elementary education; Naomi S. Bechtold, M.B.A.; Lauren Turnbach Brennan, M.Ed. with a major in elementary education; Anthony W. Favinger, M.S. with a major in educational technology; Susan Marie Gormley, M.Ed. with a major in special education; Felicia Tonya Herring, M.S. with a major in electrical engineering; Judith Lynn Hoke, M.Ed. with a major in reading; Ann Marie Koons, M.Ed. with a major in reading; Ta-Wei Lou, M.S. with a major in mechanical engineering; Susan Del Pilar Lozada, M.A. with a major in secondary education; Bonnie Eileen McDonald, M.P.A.; Leslie Fay Merz, M.Ed. with a major in elementary education; and Douglas George Muha, M.Ed. with a major in community counseling, all of Bethlehem.&lt;/p&gt;
&lt;p&gt;Ann C. Munley, M.Ed. with a major in elementary education; Karen Joanne Reichard, M.Ed. with a major in social restoration; Richard F. Schmieg, M.B.A.; Laura Christine Stearns, M.S. with a major in materials science and engineering; Susan Masters Stoltz, M.Ed. with a major in special education; James E. Swain, M.S. with a major in mechanical engineering; Clayton Paul Wagner, M.B.A.; Yi Zhou, M.S. with a major in civil engineering; and Lisa D. Carril, M.Ed. with a major in educational administration, all of Bethlehem.&lt;/p&gt;
&lt;p&gt;Sarah Clair Cloutier, M.Ed. with a major in special education; Robert Charles Helfrich, M.B.A.; Kristine Ann Lendvay, M.Ed. with a major in reading; Rosemary G. McFee, M.Ed. with a major in social restoration; and Gail Ann Polzer, M.S. in educational technology, all of Coopersburg. Melanie Sue Kenney, M.B.A.; Martha Elaine Kuder, M.Ed. with a major in elementary education; Jui Ling Liu, M.B.A.; and Carol Lynne McGrogan, M.A. with a major in secondary education, all of Easton. Daniel D. Albright, M.Ed. with a major in secondary school counseling, and Dorothy Fisher Zeppenfelt, M.Ed. with a major in social restoration, both of Emmaus. Phyllis Lathrop Parkhurst of Hellertown, M.Ed. with a major in special education. James D. Gardner, M.B.A.; Dwayne L. Hansen, M.B.A.; Carol J. Stender, M.A. with a major in English; and Alan Fong-I Yen, M.B.A., all of Macungie. Anne H. Fessler, M.Ed. with a major in reading. Elizabeth B. Rosa, M.B.A., and Danette Marie Sekerak, M.Ed. with a major in secondary education, both of Nazareth. H. Scott Fetterman of New Tripoli, M.S. with a major in electrical engineering. Dennis Joseph Rose of Slatington, M.Ed. with a major in educational administration. Brenda Lyn Truhe, M.B.A., and William Robert Truhe, M.B.A., both of Wescosville.&lt;/p&gt;
&lt;p&gt;Stephen R. Pomraning of Blandon, M.B.A. Patricia Ann Wolfe of Quakertown, M.S. with a major in industrial engineering. Lisa Jane Holczman of Palmerton, M.B.A. Jane Anne H. Swiderski of Kunkletown, M.S. with a major in materials science and engineering. Jay Marian A. Pike of Pennsburg, M.Ed. with a major in special education. Frances M. Brailo of Alpha, M.Ed. with a major in secondary education.&lt;/p&gt;
&lt;p&gt;Local students receiving bachelor&amp;rsquo;s degrees are:&lt;/p&gt;
&lt;p&gt;Eric John Abel, B.A. with a major in German; Stacia E. Horvath, B.A. with a major in international relations; Susan Kay Knoblauh, B.A. with a major in history; Patricia Louise Larkin, B.A. with a major in economics, and Davin L. Yuknis, B.S. in electrical engineering, all of Bethlehem. Scott Allan Robertson of Easton, B.S. in chemical engineering. Amy S. Urwiler of Mertztown, B.S. in civil engineering. Todd Duane Ramaly of Aquashicola, B.S. in chemical engineering.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader wins Lehigh University&#39;s Elizabeth Major Nevius Award </title>
      <link>http://localhost:1313/blog/19901004-lehigh/</link>
      <pubDate>Thu, 04 Oct 1990 16:46:06 -0400</pubDate>
      <guid>http://localhost:1313/blog/19901004-lehigh/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19901004-lehigh/letter_hu_43c5011d26bf39cf.webp 400w,
               /blog/19901004-lehigh/letter_hu_78db3a79337bb63b.webp 760w,
               /blog/19901004-lehigh/letter_hu_6021e598dc1a560f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19901004-lehigh/letter_hu_43c5011d26bf39cf.webp&#34;
               width=&#34;588&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;David&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;On behalf of the Committee on Awards and Prizes, I am pleased to inform you that you have been selected as a winner of the Elizabeth Major Nevius Award given to individuals who have entered their fifth year of work at Lehigh, whether it be a second undergraduate degree or graduate degree, on the basis of leadership, citizenship and scholarship.  With this distinguished award comes a monetary gift which will be sent to you under separate cover.&lt;/p&gt;
&lt;p&gt;Congratuations and best wishes for continued success at Lehigh University.&lt;/p&gt;
&lt;p&gt;Sincerely,&lt;/p&gt;
&lt;p&gt;Mark H. Erickson&lt;br&gt;
Acting Dean of Students&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19901004-lehigh/pic1_hu_5b9a7884fd9b81f0.webp 400w,
               /blog/19901004-lehigh/pic1_hu_1b67a4fb01f59133.webp 760w,
               /blog/19901004-lehigh/pic1_hu_dee0a8963e48304f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19901004-lehigh/pic1_hu_5b9a7884fd9b81f0.webp&#34;
               width=&#34;578&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19901004-lehigh/pic2_hu_72d3171ad0f87161.webp 400w,
               /blog/19901004-lehigh/pic2_hu_eedc82b922ee84c6.webp 760w,
               /blog/19901004-lehigh/pic2_hu_4fd65737d00148c9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19901004-lehigh/pic2_hu_72d3171ad0f87161.webp&#34;
               width=&#34;665&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Elizabeth Major Nevius Award&lt;/strong&gt; &amp;ndash; to individuals who have entered their fifth year of work at Lehigh, whether it be a second undergraduate degree or graduate degree, on the basis of leadership, citizenship and scholarshup&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NSF ERC Center Scholar, Lehigh University</title>
      <link>http://localhost:1313/blog/19900503-lehigh/</link>
      <pubDate>Thu, 03 May 1990 19:40:48 -0400</pubDate>
      <guid>http://localhost:1313/blog/19900503-lehigh/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19900503-lehigh/letter_hu_2261b3209f101e8b.webp 400w,
               /blog/19900503-lehigh/letter_hu_ab7bccca1c3de5a8.webp 760w,
               /blog/19900503-lehigh/letter_hu_7a04a9af92e144c5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19900503-lehigh/letter_hu_2261b3209f101e8b.webp&#34;
               width=&#34;589&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Dear &lt;strong&gt;Mr. Bader&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We are pleased to offer you financial aid to study computer science at Lehigh University.  In particular, you are offered support as a Center Research Scholar in our National Science Foundation Engineering Research Center, Advanced Technology for Large Structural Systems (ATLSS).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader inducted into Tau Beta Pi, The Engineering Honor Society</title>
      <link>http://localhost:1313/blog/19891204-taubetapi/</link>
      <pubDate>Mon, 04 Dec 1989 17:55:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/19891204-taubetapi/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19891204-taubetapi/19891204-TauBetaPi_hu_802fde7640d408cc.webp 400w,
               /blog/19891204-taubetapi/19891204-TauBetaPi_hu_34989da7a735fc25.webp 760w,
               /blog/19891204-taubetapi/19891204-TauBetaPi_hu_3a38f6d4ab6c5bd9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19891204-taubetapi/19891204-TauBetaPi_hu_802fde7640d408cc.webp&#34;
               width=&#34;760&#34;
               height=&#34;601&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;tau-beta-pi&#34;&gt;Tau Beta Pi&lt;/h2&gt;
&lt;h2 id=&#34;alpha-of-pennsylvania&#34;&gt;Alpha of Pennsylvania&lt;/h2&gt;
&lt;p&gt;Be it known, That in recognition of exhibiting distinguished scholarship and exemplary character while a student at Lehigh University and having been duly elected by the above Chapter &lt;strong&gt;David A. Bader&lt;/strong&gt; of the Class of 1990 is hereby declared and certified a member of Tau Beta Pi and is granted all the honors, insignia and privileges of this Association.&lt;/p&gt;
&lt;p&gt;Given this 4th day of December 1989&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David A. Bader Admitted into Membership in Eta Kappa Nu</title>
      <link>http://localhost:1313/blog/19890410-hkn/</link>
      <pubDate>Mon, 10 Apr 1989 06:11:50 -0400</pubDate>
      <guid>http://localhost:1313/blog/19890410-hkn/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19890410-hkn/HKN_hu_5fbbba2bc7eb26ed.webp 400w,
               /blog/19890410-hkn/HKN_hu_4d460597a0396c13.webp 760w,
               /blog/19890410-hkn/HKN_hu_847e057d7704cae9.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19890410-hkn/HKN_hu_5fbbba2bc7eb26ed.webp&#34;
               width=&#34;760&#34;
               height=&#34;569&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt; has this day been admitted into Membership in IEEE-Eta Kappa Nu in recognition of excellent scholarship and other attainments by which interest and ability in the profession of Electrical and Computer Engineering have been manifested.&lt;/p&gt;
&lt;p&gt;by Chi Chapter&lt;/p&gt;
&lt;p&gt;James M. Comad, President&lt;br&gt;
10 April 1989&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bader receives Geltman Scholarship</title>
      <link>http://localhost:1313/blog/19870901-lehigh/</link>
      <pubDate>Tue, 01 Sep 1987 18:54:22 -0400</pubDate>
      <guid>http://localhost:1313/blog/19870901-lehigh/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19870901-lehigh/Geltman_hu_a4f90556b39eaa96.webp 400w,
               /blog/19870901-lehigh/Geltman_hu_999ea3c71a78228f.webp 760w,
               /blog/19870901-lehigh/Geltman_hu_599e81d59f97fe67.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19870901-lehigh/Geltman_hu_a4f90556b39eaa96.webp&#34;
               width=&#34;559&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;marcella-and-samuel-geltman-1991h&#34;&gt;Marcella and Samuel Geltman 1991H&lt;/h2&gt;
&lt;p&gt;Samuel Geltman is founder of Samuel Geltman and Company Inc., a New Jersey-based real estate management company whose portfolio of apartment communities is located in the states of New Jersey, Texas, Indiana and Arizona. Geltman started the business in 1954 and has been purchasing and operating apartment properties for more than 50 years. In 1983, Geltman established the Marcella S. and Samuel Geltman Scholarship, in honor of his wife, Marcella, and to support top Lehigh students studying engineering and physical science. The students are graduates of public high schools from communities where Geltman and Company manages properties.&lt;/p&gt;
&lt;p&gt;Mrs. Geltman passed away in October 1987. Since then, his daughter, Nancy, president of Samuel Geltman &amp;amp; Company, has been attending the annual Geltman Scholars dinner at Lehigh, along with her father. Being able to establish a lasting tribute to my dear wife while at the same time assisting deserving, talented young people is a source of great personal satisfaction to me, says Geltman. He was awarded an honorary doctor of laws degree from Lehigh in 1991.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://giving.lehigh.edu/thanks/leadershipplaza/geltman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://giving.lehigh.edu/thanks/leadershipplaza/geltman&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 Carriers Receive Scholarships</title>
      <link>http://localhost:1313/blog/19870816-morningcall/</link>
      <pubDate>Sun, 16 Aug 1987 15:58:57 -0400</pubDate>
      <guid>http://localhost:1313/blog/19870816-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19870816-morningcall/award_hu_a797d7e222614058.webp 400w,
               /blog/19870816-morningcall/award_hu_59c2a8cc74871387.webp 760w,
               /blog/19870816-morningcall/award_hu_626849a73477d4c.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19870816-morningcall/award_hu_a797d7e222614058.webp&#34;
               width=&#34;586&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19870816-morningcall/clip_82931488_hu_2206f15255f02160.webp 400w,
               /blog/19870816-morningcall/clip_82931488_hu_ff13cd7faaa10ae0.webp 760w,
               /blog/19870816-morningcall/clip_82931488_hu_d06dc254529cbc0d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19870816-morningcall/clip_82931488_hu_2206f15255f02160.webp&#34;
               width=&#34;760&#34;
               height=&#34;700&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David A. Bader&lt;/strong&gt;, son of Morris and Karen Bader, of 1402 Lorain Ave., Bethlehem. Liberty High School graduate Bader has been a carrier for five years. He will attend Lehigh University and major in computer engineering.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>389 Liberty Grads Begin &#39;2nd Mile&#39; of Life</title>
      <link>http://localhost:1313/blog/19870611-morningcall/</link>
      <pubDate>Thu, 11 Jun 1987 07:23:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/19870611-morningcall/</guid>
      <description>&lt;p&gt;&amp;ldquo;The Miles of Life&amp;rdquo; was the theme as 389 Liberty High School seniors became graduates last night in Lehigh University&amp;rsquo;s Stabler Arena.&lt;/p&gt;
&lt;p&gt;Samuel Rodriguez Jr., the commencement speaker chosen by members of the graduating class, told the audience, &amp;ldquo;Graduation from Liberty represents the conclusion of the first mile and the commencement of the second.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;He explained how in the 13 years of their school careers his fellow graduates &amp;ldquo;have acquired academic fundamentals&amp;rdquo; as well as other &amp;ldquo;essentials of success in life.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;But in addition, he said, &amp;ldquo;We have acquired discipline and the ability to grow further toward wisdom. We have acquired a special pride and an intimate sense of unity.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Above all is our recognition that in this life we do not want to merely survive, but we want to succeed.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;We must not be satisfied with just reaching the reachable and conquering the conquerable. We must use all of the essentials which we now have within our grasp to reach the unreachable, to conquer the unconquerable.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;David Minor, the commencement speaker chosen by the faculty, pointed out, &amp;ldquo;Throughout our years of education, our purpose has not been to learn the right answers, but instead to see how diverse subjects as literature, mathematics, history and science are all related. We have been presented with the searches which people who have preceded us have made.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Now our turn comes, our chance to take the opportunity and initiative and search for ourselves.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Look around you and notice the events which touch your lives and why they affect you. Do not let the last 13 years be a waste. Continue your learning throughout your lives.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Liberty&amp;rsquo;s commencement was moved from the school&amp;rsquo;s campus to the arena this year to provide more space and to eliminate concerns about heat in the gymnasium and rain when attempts were made to hold it in the stadium that have made ceremonies uncomfortable in the past.&lt;/p&gt;
&lt;p&gt;Students were recognized for academic achievement by Principal Anthony Ruggiero and for school and community service by Assistant Principal Edwin E. Abel Jr. and acting Assistant Principal Roberta J. Whitcomb. Jean Trend, guidance counselor, recognized those students receiving scholarships.&lt;/p&gt;
&lt;p&gt;Bethlehem Area School District Superintendent Thomas J. Doluisio presented the class, and John F. Spirk, president of the Bethlehem Area School Board, awarded diplomas.&lt;/p&gt;
&lt;p&gt;School and community awards were received by James Cho, altruism; Carol Ann Dittbrenner and Angela Quartuch, both American Legion Grabar Lucas Post for citizenship; Michael S. Brown and Kevin L. Mingora, both Art Gallery for excellence in art; Rebecca E. Little, Bausch and Lomb science; Michelle Ann Germuga, Bethlehem Education Association, citizenship; Kristine M. Berback, Morris Black Award for citizenship; Keith J. Facchiano, Renee N. King and Raymond Reed, all Cauldron for work on yearbook; Amy Hagemann and Rodriguez, both Class of 1920 award for citizenship and service; Joanne M. Allegra, Class of 1942 award for citizenship and service; Delton J. Costan Jr., Arthur Black Award to outstanding senior wrestler; Michael Dwinal, Class of 1980 for efforts on behalf of colleagues; Lisa M. Langensiepen, congressional citizenship for contribution to school and community; Irene I. Kim, Daughters of the American Revolution for citizenship; Thalia E. Kiapokos, Daughters of Penelope for contribution to school community; Jason C. Reed and Mary E. Harvey, both dramatics; Kimberly Enstrom, Kim,Marci S. Simon, all French; Debra S. Bader, German; Dittbrenner and John B. Williamson, Spanish; Joseph F. Brandon, Joseph P. Frey Award to outstanding lineman on football team; Robert M. Simons, Harold Groman Award of American Legion Band for instrumental music; Maria del Carmen Rosada, Jean Hafner Award to student with appreciation for quality of life and achievement of educational goals, and Little, Daughters of the American Revolution history.&lt;/p&gt;
&lt;p&gt;Also, Buss and Minor, both Raymond R. Hoffert Award for music; Rodriguez, human relations award for development and improvement among students of different cultural backgrounds; Christine K. Bonney and Kimberly Enstrom, both Interact Club; Little, Jamesway Corp. for highest academic rank; David C. Nonnemaker, Mark John Kelly Memorial Award of achievement in graphic arts; John B. Callahan, John F. Kennedy Award for contribution to school at large; Denise A. Beidelman and Gregg J. Albus, both Keystone Savings for high academic attainment in business program; Brian Kroelich, Donald R. Kilpatrick Award for outstanding performance in soccer; James A. Austin, Charles A. Klein Award for service to fellow men; Stacy A. Wescoe, Knights of Columbus Trinity Council for strong moral influence; Peter Chaikowsky and Little, both booster club for school citizenship; Claudine Celebuski, Robert Galle, Thomas J. Husser III, Christopher W. Shade and Wescoe, all Liberty Life for work on school newspaper; Margaret E. Farrell, Ida C. Marakovits Award of Bethlehem Teachers&amp;rsquo; Federal Credit Union for contributions to school community; Renee E. Gonzalez, marketing and distributive education; Alissa S. Witiak, outstanding student in instrumental music; Buss, Musicians Association Local 411; Jessica R. Thompson, Pennsylvania Playhouse; &lt;strong&gt;David A. Bader&lt;/strong&gt; and Chaikowsky, both perfect attendance in Grades 9-12; Tricia K. Glancy and Kevin R. Oliver, both perfect attendance in senior year; Gwen Kay and Scott E. Snyder, both Phillip F. Phillippi Award to athletes for contributions to school; Christina M. Jackson, Dawn M. Palhofski, Robin L. Shoemaker, Simon, Beverly A. Smith and Joanna Villani, all Margaret Randt Library Club for outstanding service to school library; Earl J. Kinsley Jr., Joseph J. Risbon Award for outstanding leadership; Little, Debra Bader and Allegra, all Society of Women Engineers for accomplishments in science and mathematics; Beth Horninger, Sons of American Revolution for scholarship and character; Nancy Romig and Warren West, both outstanding athletes; Buss and Minor, both theater music; Crim and Melissa W. Tiers, both Carlton S. Weaver Award for music; Neil Szanyi, William Shakespeare Award for commitment and contribution to classic literature; Deborah Ann Scheetz, Woman&amp;rsquo;s Club Award for art; Buss, Woman&amp;rsquo;s Club Award for music; Jennifer L. Peischl, Bethlehem Area School District Woman&amp;rsquo;s Club Award for creativity, scholarship and enthusiasm; Tamara A. Bloss, Dorris R. Young Memorial Award in home economics; Chaikowsky, Kraig Eric Yurchak Memorial Award for high academic average and participation in athletics, and West, Jerry Zerfass Award for outstanding member of football team backfield.&lt;/p&gt;
&lt;p&gt;Band and orchestra awards were presented to Karen S. Becker, Joshua L. Bees, Facchiano, John S. Hayes III, Jay J. Holzinger II, Smith and Szanyi.&lt;/p&gt;
&lt;p&gt;Choraliers awards went to Bees, Buss, Crim, Lynn A. Faraldo, Jennifer L. Fischel, Sean Grove, Heather Heslin, Tammy Jo Hittinger, Holzinger, David C. Kolman, Jacqueline Podhany, Paul N. Ritter, Kasey M. Rogers, Brian K. Sharer, Szanyi and Melissa W. Tiers.&lt;/p&gt;
&lt;p&gt;Stage arts awards were presented to Fischel, Carla L. Forschner, Julie C. Frankenfield, Kay, Renee N. King, Julie A. Meckley, Podhany, Christifer F. Portner Jr., Kimberly A. Schmoyer, Shade, Sharer and Sara C. Steiner.&lt;/p&gt;
&lt;p&gt;Student government awards for contributions to the school were received by David J. Danner, LeeAnn R. Dipeppe, Erin M. Hart, Kim, Langensiepen, Little, Simon, Villani and Witiak.&lt;/p&gt;
&lt;p&gt;Students receiving the technical theater arts award were Matthew J. Boyer, Douglas W. Caldwell Jr., Jeffrey H. Demerjian, Hart, Davis C. Kolman, Witiak and Ronald C. Woodley.&lt;/p&gt;
&lt;p&gt;Students recognized for receiving scholarships and awards were Kristine M. Berback, Buss, Celebuski, Enstrom, Frankenfield, Hittinger, Beth Horninger and Peischl, all American Association of University Women; Beidelman, American Business Women&amp;rsquo;s Association Bethlehem Chapter; Richard DeJesus, Bethlehem Area School District Freedom-Liberty Run Scholarship; Horninger, Truman L. Frey of Bethlehem Area Foundation Scholarship; Nora Jane Kubat, Bethlehem High School Alumni Award; Colin D. Benert, Bethlehem Lions Club Edward C. Kunow Education Fund Award; Chaikowsky, Jeffrey Stephen Capuano Memorial Scholarship; Peischl, Class of 1960; David Paul Brown and Germuga, both Fountain Hill Exchange Club Scholarship; Peischl, GFWC Woman&amp;rsquo;s Club of Bethlehem; Eric G. Williams, J.F. Goodwin Scholarship Fund; Jason C. Reed, International Paper Company Chairman Fellowship Scholarship; Lealan Zaccone, Johnson and Wales College DECA Scholarship; Deborah Ann Scheetz, Charles A. Klein Scholarship; Hagemann, Ladies Auxiliary of Muhlenberg Hospital Center Volunteer Services; Rodriguez, Larrazza and McDonald Corporations Scholarship; Yuzhang Duan, Merit Scholarship to Northampton County Area Community College; William K. Sassaman, Comenius Scholarship of Moravian College; Szanyi, National Merit Scholarship sponsored by Richard King Mellon Foundation; John B. Callahan, National Merit Scholarshipsponsored by UPS Foundation; Horninger, Pennsylvania Motor Truck Association; Ronald C. Woodley, Pennsylvania State University both Navy ROTC and Scholars Program; Jason C. Reed, Joseph B. Preletz Memorial; Brian K. Sharer, Warren R. and Sadie Roberts Estate; Dwinal, Ross-Hulman Institute of Technology; Deborah Ann Scheetz, Spring Garden College Scholarships, both competition award and trustees scholarship; Stacey Colleen Sullivan, St. Joseph&amp;rsquo;s University Board of Directors Scholarship; Benert, Walter A. Schrempel Memorial Scholarship presented by Rotary Club of Bethlehem; Betty S. Parente, UNICO Bethlehem Chapter; Renee N. King, Washington College, both 1782 Society Scholarship and George Washington Scholar; Todd A. Burkhardt, Chaikowsky, Matthew J. Hosfeld and Michael Issa, all Michael Glenn Memorial Award, and Jason C. Reed, Kraig Eric Yurchak Memorial Scholarship.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Commencement Exercises of the Class of 1987, Liberty High School, Bethlehem, PA</title>
      <link>http://localhost:1313/blog/19870610-lhs/</link>
      <pubDate>Wed, 10 Jun 1987 12:50:38 -0400</pubDate>
      <guid>http://localhost:1313/blog/19870610-lhs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>It All Adds Up For Salisbury Team</title>
      <link>http://localhost:1313/blog/19870322-morningcall/</link>
      <pubDate>Sun, 22 Mar 1987 07:11:44 -0400</pubDate>
      <guid>http://localhost:1313/blog/19870322-morningcall/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bader awarded Vigil Honor, Order of the Arrow&#39;s highest honor</title>
      <link>http://localhost:1313/blog/19860901-vigilhonor/</link>
      <pubDate>Mon, 01 Sep 1986 09:51:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860901-vigilhonor/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860901-vigilhonor/VigilHonor_hu_8edbaf3b4453db36.webp 400w,
               /blog/19860901-vigilhonor/VigilHonor_hu_7f8bd1399d731c8a.webp 760w,
               /blog/19860901-vigilhonor/VigilHonor_hu_830ab723d721926f.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860901-vigilhonor/VigilHonor_hu_8edbaf3b4453db36.webp&#34;
               width=&#34;760&#34;
               height=&#34;485&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;h2 id=&#34;vigil-honor-order-of-the-arrow&#34;&gt;Vigil Honor Order of the Arrow&lt;/h2&gt;
&lt;h2 id=&#34;boy-scounts-of-america&#34;&gt;Boy Scounts of America&lt;/h2&gt;
&lt;p&gt;This certifies that &lt;strong&gt;David Bader&lt;/strong&gt; is a Vigil Honor member.&lt;/p&gt;
&lt;p&gt;In recognition of exceptional service, personal effort, and unselfish interest in the welfare of others. He has made distinguished contributions above and beyond his immediate responsibilities in the Order of the Arrow as a member of Lodge 44 on September 1986.&lt;/p&gt;
&lt;p&gt;E. Urner Goodman&lt;br&gt;
For Vigil Honor&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bethlehem Student Has Top Score in Pa. for Math Contest</title>
      <link>http://localhost:1313/blog/19860527-morningcall/</link>
      <pubDate>Tue, 27 May 1986 16:33:21 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860527-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860527-morningcall/clip_82933726_hu_529c21b74eae25ec.webp 400w,
               /blog/19860527-morningcall/clip_82933726_hu_c12f7ec3f685efab.webp 760w,
               /blog/19860527-morningcall/clip_82933726_hu_2fca3a40a845382a.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860527-morningcall/clip_82933726_hu_529c21b74eae25ec.webp&#34;
               width=&#34;760&#34;
               height=&#34;141&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;William Brewster, a senior at Liberty High School in Bethlehem, won the Pennsylvania Invitational Mathematics Contest at Shippensburg University last week.&lt;/p&gt;
&lt;p&gt;He had the top score among 28 high school students from across Pennsylvania who were there because of their unusually high results in the annual national math competition.&lt;/p&gt;
&lt;p&gt;This marked a sweep for Brewster in seven math contests in Pennsylvania where there was individual competition. &amp;ldquo;That&amp;rsquo;s pretty remarkable,&amp;rdquo; said Doris Helms, coach of the Liberty math team.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;That&amp;rsquo;s the first time a student from the local area won the individual part,&amp;rdquo; Helms said. &amp;ldquo;He&amp;rsquo;s captain of my team. He&amp;rsquo;s been on the team three years. He&amp;rsquo;s extremely outstanding.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Three other local students were in the Shippensburg event - &lt;strong&gt;David Bader&lt;/strong&gt; of Liberty and Kenneth Schultz and Susan Steinberger of Freedom High School in Bethlehem.&lt;/p&gt;
&lt;p&gt;In two other recent competitions:&lt;/p&gt;
&lt;p&gt;-&amp;gt; Brewster was tops among 200 high school math students and the Liberty team was best among 47 teams at an eastern Pennsylvania competition at Millersville University.&lt;/p&gt;
&lt;p&gt;Schultz of Freedom was second; Bradley Mann of Salisbury, fourth, and Haiwen Ma of Liberty, honorable mention, among the individuals.&lt;/p&gt;
&lt;p&gt;That winning Liberty team included Brewster, Ma, Christopher Rothrock and Matthew Reynolds, all seniors.&lt;/p&gt;
&lt;p&gt;-&amp;gt; Liberty won the annual oral math contest among 16 high schools for the eighth time in 11 years at East Stroudsburg University. It defeated Freedom 75-40 in the final round.&lt;/p&gt;
&lt;p&gt;The Liberty team there was Brewster, &lt;strong&gt;Bader&lt;/strong&gt; and Neil Szanyi. The runner-up Freedom team comprised Schultz as captain, Charles Esther and Charrissa Lin.&lt;/p&gt;
&lt;p&gt;Besides Liberty and Freedom, those surviving to the semifinal round were Bethlehem Catholic and Emmaus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Liberty High students get annual recognition awards </title>
      <link>http://localhost:1313/blog/19860522-morningcall/</link>
      <pubDate>Thu, 22 May 1986 10:58:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860522-morningcall/</guid>
      <description>&lt;p&gt;Awards and certificates were presented to Liberty High School students at a special student recognition program last week. Recipients were:
&amp;hellip;
&lt;strong&gt;David Bader&lt;/strong&gt; and Edward Berger, computer contest; William Brewster, Neil Szanyi, Christopher Rothrock, &lt;strong&gt;Bader&lt;/strong&gt;, Haiwen Ma, Matthew Reynolds, mathematics team; &amp;hellip; Also, Brewster, &lt;strong&gt;Bader&lt;/strong&gt; and David Rader, American High School Mathematics Examination; &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Liberty Team Places First in State Math Competition</title>
      <link>http://localhost:1313/blog/19860515-morningcall/</link>
      <pubDate>Thu, 15 May 1986 16:41:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860515-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860515-morningcall/clip_82934374_hu_1ef774ed6fcebd56.webp 400w,
               /blog/19860515-morningcall/clip_82934374_hu_a213224d40a47094.webp 760w,
               /blog/19860515-morningcall/clip_82934374_hu_8fa992a2e1f9af25.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860515-morningcall/clip_82934374_hu_1ef774ed6fcebd56.webp&#34;
               width=&#34;338&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;The Liberty High School team placed first in the 14th Professor John Steiner Gold Mathematical Competition held recently at Bucknell University.&lt;/p&gt;
&lt;p&gt;Team and individual winners were determined through a written examination given at Bucknell in Lewisburg. &lt;strong&gt;David Bader&lt;/strong&gt;, William Brewster and Neil Szanyi comprised the Liberty team from the Bethlehem Area School District. Second place went to Lewisburg Area High School and third to Williamsport Area High.&lt;/p&gt;
&lt;p&gt;Among the individual winners were Brewster, first place, and Szanyi, third. &lt;strong&gt;Bader&lt;/strong&gt; earned an honorable mention.  Brewster also received a book and certificate of recognition given to the highest ranking individual in each school.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Winners in Programming Contest are Announced</title>
      <link>http://localhost:1313/blog/19860510-morningcall/</link>
      <pubDate>Sat, 10 May 1986 10:46:31 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860510-morningcall/</guid>
      <description>&lt;p&gt;Grades 10-12: &lt;strong&gt;David Bader&lt;/strong&gt;, Liberty High School, I-U 20.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First Place, CNIU20 Microcomputer Contest</title>
      <link>http://localhost:1313/blog/19860418-award/</link>
      <pubDate>Fri, 18 Apr 1986 20:49:57 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860418-award/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860418-award/19860418-Award_hu_fd0353faebb1dd22.webp 400w,
               /blog/19860418-award/19860418-Award_hu_c00b9dceebfd87c.webp 760w,
               /blog/19860418-award/19860418-Award_hu_81f7c2a9dadf6e9d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860418-award/19860418-Award_hu_fd0353faebb1dd22.webp&#34;
               width=&#34;335&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&#34;https://www.ciu20.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colonial Intermediate Unit 20 (CNIU20)&lt;/a&gt; Microcomputer Contest&lt;br&gt;
1st Place  4/18/86&lt;br&gt;
&lt;strong&gt;David Bader&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Fun-Filled Stay in Londontown</title>
      <link>http://localhost:1313/blog/19860405-morningcall/</link>
      <pubDate>Sat, 05 Apr 1986 20:38:12 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860405-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860405-morningcall/clip_83138938_hu_c5429c913e8db950.webp 400w,
               /blog/19860405-morningcall/clip_83138938_hu_9821c0f1da3881c6.webp 760w,
               /blog/19860405-morningcall/clip_83138938_hu_5b68cb7115943347.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860405-morningcall/clip_83138938_hu_c5429c913e8db950.webp&#34;
               width=&#34;760&#34;
               height=&#34;445&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Lined up to photograph Royal Albert Hall, where the Grenadiers performed on March 23 are (from left): Tom Negron, Jim Reposh, Frank Kozero, Chris Narzissi, Tom Ladue, &lt;strong&gt;David Bader&lt;/strong&gt;, Debra Bader and Renee King.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hard Work Sends Grenadier Band Off To England</title>
      <link>http://localhost:1313/blog/19860319-morningcall/</link>
      <pubDate>Wed, 19 Mar 1986 20:48:05 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860319-morningcall/</guid>
      <description>&lt;p&gt;After nearly a year of hard work and preparation by parents, teachers and students alike, the Liberty High School Grenadier Band is finally on its way to England for 10 days of musical and educational events and some good, old- fashioned relaxation.&lt;/p&gt;
&lt;p&gt;To raise the more than $200,000 needed to finance the journey, band members sold 90,000 hoagies, 36,000 containers of pretzels and chips, 49,000 candy bars, 2,800 pounds of fruitcake and 1,300 calendar towels, according to Pat Palhofski, past band parents&amp;rsquo; president and current hoagie chairman.&lt;/p&gt;
&lt;p&gt;Plans call for the 260 band members and 44 chaperons to leave Bethlehem at 5 a.m. today for John F. Kennedy International Airport, where they will board British Airways and arrive at London&amp;rsquo;s Heathrow Airport at 9:40 p.m. London time. Band members will remain in London, travel to Bristol next Wednesday, and return to Bethlehem during the early morning hours of March 29.&lt;/p&gt;
&lt;p&gt;In addition, a group of some 200 local residents, who are fans of the Grenadiers and known as &amp;ldquo;tag-alongs,&amp;rdquo; also will travel to Britain. Their itinerary closely parallels that of the Grenadiers, although this group will leave Bethlehem tomorrow and return March 28.&lt;/p&gt;
&lt;p&gt;Band instruments and other bulky items were transported to Kennedy Airport last night in a tractor-trailer, which was donated by the Banko Beverage Co. The services of a driver also were donated by Teamsters Local 773.&lt;/p&gt;
&lt;p&gt;The highlight of the Grenadiers&amp;rsquo; trip, according to Ron Sherry, band director, will be a &lt;strong&gt;joint concert with the Coldstream Guards on Sunday in Royal Albert Hall&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Grenadiers, whose theme is &amp;ldquo;Rule Britannia,&amp;rdquo; are modeled after the Coldstream Guards, and their uniforms are exact copies of those worn by this British military group. The belts, buttons and other accessories worn by the Grenadiers are even imported from England.&lt;/p&gt;
&lt;p&gt;Sherry called the Coldstream Guards &amp;ldquo;certainly one of the world&amp;rsquo;s finest bands&amp;rdquo; and said this is the &amp;ldquo;first time for a U.S. band to play with a British military band in a jointly sponsored concert.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;According to the trip&amp;rsquo;s itinerary, among activities scheduled for band members are tours of London and its landmarks tomorrow through Friday and a parade and performance in Trafalgar Square on Saturday, which Sherry said is also a high honor for any American band.&lt;/p&gt;
&lt;p&gt;On Monday, the Grenadiers will present a field display performance in Twickenham Stadium and later that same day will visit &amp;ldquo;Shakespeare&amp;rsquo;s Feast&amp;rdquo; for a three-hour dinner show. Band members will also have time on their own in London.&lt;/p&gt;
&lt;p&gt;Next Wednesday,after arrival in Bristol, the band will be greeted at an official reception by Bristol&amp;rsquo;s lord mayor. The remainder of the Grenadiers&amp;rsquo; visit in England will be spent as guests in the homes of Bristol residents.&lt;/p&gt;
&lt;p&gt;On the band&amp;rsquo;s final day in England, it will parade with the Unicorn Band, a local Bristol band made up of students and community residents between the ages of 8 and 21. In the evening, the Grenadiers will play a benefit concert for the Unicorn Band to help the British band with its fund-raising efforts.&lt;/p&gt;
&lt;p&gt;Sherry called the trip &amp;ldquo;the thrill of a lifetime&amp;rdquo; and said it will afford the Grenadiers opportunities to do things &amp;ldquo;no other American high school band has ever done.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Thomas J. Doluisio, who was Liberty&amp;rsquo;s principal and became acting superintendent Feb. 1, had hoped to accompany the band to England, but he said his present work schedule would not allow this.&lt;/p&gt;
&lt;p&gt;Doluisio noted how pleased he was with the band&amp;rsquo;s trip itinerary. They have &amp;ldquo;a good composition of activities,&amp;rdquo; he said, adding it shows &amp;ldquo;there was lots of care in planning.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;John C. Saunders Jr., Liberty&amp;rsquo;s acting principal, is happy to be accompanying the band. &amp;ldquo;I certainly look forward to the experience. I think it is wonderful for everybody,&amp;rdquo; he said.&lt;/p&gt;
&lt;p&gt;Carl Langkamer, school district curriculum specialist for music, and his wife, Doris, a teachers&amp;rsquo; aide at Liberty, also are traveling with the band.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;One of the highlights of my job is that I get to accompany these bands,&amp;rdquo; Langkamer said, referring not only to the Liberty visit, but also to the trip by the Freedom band to Hawaii last December to play at the Aloha Bowl.&lt;/p&gt;
&lt;p&gt;He added, &amp;ldquo;These trips, in 10 days, give the kids more educationally than they can get in a classroom in one year.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;They will make everyone in Bethlehem very proud of them,&amp;rdquo; he said of the Grenadiers last week.&lt;/p&gt;
&lt;p&gt;He also pointed out how the people of Bethlehem deserve credit for the support they give all the school bands in the community. He said in a little over one year the combined bands of Liberty and Freedom had to raise some $500,000 and were successful because of the support they got from the people of Bethlehem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>&#39;Berg Math Test: New Rules, Old Titlist</title>
      <link>http://localhost:1313/blog/19860316-morningcall/</link>
      <pubDate>Sun, 16 Mar 1986 15:39:37 -0400</pubDate>
      <guid>http://localhost:1313/blog/19860316-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19860316-morningcall/clip_82930132_hu_b36d47ae3872bcd3.webp 400w,
               /blog/19860316-morningcall/clip_82930132_hu_36a09b7b5e3ba849.webp 760w,
               /blog/19860316-morningcall/clip_82930132_hu_1a28b7308b336471.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19860316-morningcall/clip_82930132_hu_b36d47ae3872bcd3.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Bethlehem Scout Becomes An Eagle</title>
      <link>http://localhost:1313/blog/19850725-eaglescout/</link>
      <pubDate>Thu, 25 Jul 1985 21:56:18 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850725-eaglescout/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850725-eaglescout/clip_82931138_hu_eaaa8872ce239cb4.webp 400w,
               /blog/19850725-eaglescout/clip_82931138_hu_c666115e2a3aa3d5.webp 760w,
               /blog/19850725-eaglescout/clip_82931138_hu_7b3de1d6e7759e08.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850725-eaglescout/clip_82931138_hu_eaaa8872ce239cb4.webp&#34;
               width=&#34;262&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;David Albert Bader&lt;/strong&gt;, son of Dr. and Mrs. Morris Bader of Bethlehem, was elevated to the rank of &lt;strong&gt;Eagle Scout&lt;/strong&gt; during ceremonies held recently in the Haupert Union Building at Moravian College. David is a member of Boy Scout Troop 346 of St. Mark&amp;rsquo;s Lutheran Church, Bethlehem.&lt;/p&gt;
&lt;p&gt;His Eagle project consisted of making a complete inventory of the Brith Sholom Community Center library, to be used by center members and the general public. He made a check of the shelf and catalog file listings and made sure all library materials were in their proper places. Cards were made for books on the shelves, not already in the files, and missing books were noted. As each shelf was completed, the books were boxed and the contents labeled in preparation for Brith Sholom&amp;rsquo;s future move to its newly constructed building. The project took 128 hours.&lt;/p&gt;
&lt;p&gt;David joined Cub Scouts in 1977 and became a Boy Scout in 1980. Among his many scounting accomplishments, he has received 27 merit badges and has served as assistant patrol leader, patrol leader, instructor and assist senior patrol leader. He also has received the Minsi Trails Council Historic Trails Award and attending the council&amp;rsquo;s leadership camp for two years, in addition to several other scouting achievements and honors.&lt;/p&gt;
&lt;p&gt;In September, David will enter 11th grade at Liberty High School, where he is a member of the Grenadier Band, chess team, German club and computer club.&lt;/p&gt;
&lt;p&gt;David also serves as a carrier for the Call-Chronicle Newspapers and is a member of the B&amp;rsquo;nai Brith Youth Organization.&lt;/p&gt;
&lt;h3 id=&#34;scouting-biography&#34;&gt;Scouting Biography&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Council Number&lt;/td&gt;
          &lt;td&gt;502&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Board of Review Date&lt;/td&gt;
          &lt;td&gt;07-MAR-85&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Eagle Program&lt;/td&gt;
          &lt;td&gt;Troop&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Eagle Unit&lt;/td&gt;
          &lt;td&gt;0346&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Eagle City&lt;/td&gt;
          &lt;td&gt;Bethlehem&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Eagle State&lt;/td&gt;
          &lt;td&gt;PA&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;BSA Region&lt;/td&gt;
          &lt;td&gt;Northeast&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;a href=&#34;https://mcall.newspapers.com/image/276074883/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mcall.newspapers.com/image/276074883/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BASD Honors Achievements</title>
      <link>http://localhost:1313/blog/19850613-morningcall/</link>
      <pubDate>Thu, 13 Jun 1985 10:39:24 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850613-morningcall/</guid>
      <description>&lt;p&gt;Among the many students singled out by the Bethlehem district in its annual awards ceremony were mathematics team members: Kevin Burns, Thomas Foster, Randy Freeman, Mary Beth Guro, William Lee, Charrissa Lin, Brooke Unger and Hoppe, all of Freedom High; Michael Angelo, &lt;strong&gt;David Bader&lt;/strong&gt;, Debra Bader, Dean Batten, William Brewster, Stephen Fahringer, Neil Szanyi and Parke Wilde, all of Liberty.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eagle Scout Court of Honor</title>
      <link>http://localhost:1313/blog/19850519-eaglescout/</link>
      <pubDate>Sun, 19 May 1985 18:24:45 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850519-eaglescout/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/NESA1_hu_6371e7b864a1d3bd.webp 400w,
               /blog/19850519-eaglescout/NESA1_hu_fe6cde65d337895d.webp 760w,
               /blog/19850519-eaglescout/NESA1_hu_723b316be774191b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/NESA1_hu_6371e7b864a1d3bd.webp&#34;
               width=&#34;760&#34;
               height=&#34;474&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/NESA2_hu_a09dbd5feecdfc3.webp 400w,
               /blog/19850519-eaglescout/NESA2_hu_acd5965c65ed77f2.webp 760w,
               /blog/19850519-eaglescout/NESA2_hu_9cbb7b1196478b71.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/NESA2_hu_a09dbd5feecdfc3.webp&#34;
               width=&#34;578&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout1_hu_fdc5f971483daa1c.webp 400w,
               /blog/19850519-eaglescout/EagleScout1_hu_ebaa25a5603426c9.webp 760w,
               /blog/19850519-eaglescout/EagleScout1_hu_9ea1da7c604439c5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout1_hu_fdc5f971483daa1c.webp&#34;
               width=&#34;493&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout2_hu_117b1bdc6ee5f30c.webp 400w,
               /blog/19850519-eaglescout/EagleScout2_hu_d3418fa746c1589c.webp 760w,
               /blog/19850519-eaglescout/EagleScout2_hu_e7542027a4f16f3b.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout2_hu_117b1bdc6ee5f30c.webp&#34;
               width=&#34;495&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout3_hu_4eb4860cbbbe5cfe.webp 400w,
               /blog/19850519-eaglescout/EagleScout3_hu_97101b3f68b9ede3.webp 760w,
               /blog/19850519-eaglescout/EagleScout3_hu_ea8efc9aecec1b97.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout3_hu_4eb4860cbbbe5cfe.webp&#34;
               width=&#34;760&#34;
               height=&#34;595&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout4_hu_101a9455d1b39d8a.webp 400w,
               /blog/19850519-eaglescout/EagleScout4_hu_87e8ef19d595149c.webp 760w,
               /blog/19850519-eaglescout/EagleScout4_hu_7583bf9ed89e36d.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout4_hu_101a9455d1b39d8a.webp&#34;
               width=&#34;760&#34;
               height=&#34;590&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout5_hu_a18d3d244ea3e733.webp 400w,
               /blog/19850519-eaglescout/EagleScout5_hu_6f7e8ea5684757db.webp 760w,
               /blog/19850519-eaglescout/EagleScout5_hu_ba80c312605a2cf5.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout5_hu_a18d3d244ea3e733.webp&#34;
               width=&#34;760&#34;
               height=&#34;586&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850519-eaglescout/EagleScout6_hu_ce4b952539c581e7.webp 400w,
               /blog/19850519-eaglescout/EagleScout6_hu_857a8079dbbb9534.webp 760w,
               /blog/19850519-eaglescout/EagleScout6_hu_dccec4e5166b13e2.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850519-eaglescout/EagleScout6_hu_ce4b952539c581e7.webp&#34;
               width=&#34;488&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Students at Liberty Receive Awards</title>
      <link>http://localhost:1313/blog/19850516-morningcall/</link>
      <pubDate>Thu, 16 May 1985 15:47:13 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850516-morningcall/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850516-morningcall/clip_82930867_hu_532a823dfbf795b8.webp 400w,
               /blog/19850516-morningcall/clip_82930867_hu_59efa06868021023.webp 760w,
               /blog/19850516-morningcall/clip_82930867_hu_5c60ce5c5649db6.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850516-morningcall/clip_82930867_hu_532a823dfbf795b8.webp&#34;
               width=&#34;760&#34;
               height=&#34;469&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Winners of awrads were:&lt;/p&gt;
&lt;p&gt;Physics Olympics competition: &lt;strong&gt;David Bader&lt;/strong&gt;, first place in launcher event&lt;/p&gt;
&lt;p&gt;Lehigh University annual written mathematics conents; Liberty won top and individual honors. Team members were Michael Angelo, William Brewster, Dean Batten and Parke Wilde. Michael Angelo won top indivudual award. First-place winners of the sophomore team competition were &lt;strong&gt;David Bader&lt;/strong&gt;, Stephen Fahringer, Debra Bader and Neil Szanyi.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>David Bader Earns Rank of Eagle Scout, Boy Scouts of America</title>
      <link>http://localhost:1313/blog/19850307-eaglescout/</link>
      <pubDate>Thu, 07 Mar 1985 13:05:28 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850307-eaglescout/</guid>
      <description>&lt;p&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850307-eaglescout/19850307-EagleCertificate_hu_c8432f721c10130e.webp 400w,
               /blog/19850307-eaglescout/19850307-EagleCertificate_hu_42347e186d4ed725.webp 760w,
               /blog/19850307-eaglescout/19850307-EagleCertificate_hu_92ef945c38f87256.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850307-eaglescout/19850307-EagleCertificate_hu_c8432f721c10130e.webp&#34;
               width=&#34;760&#34;
               height=&#34;599&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;



















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850307-eaglescout/eaglecard_hu_d09a20a873dee993.webp 400w,
               /blog/19850307-eaglescout/eaglecard_hu_125a8b67d5836965.webp 760w,
               /blog/19850307-eaglescout/eaglecard_hu_44d1a1c050fcec92.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850307-eaglescout/eaglecard_hu_d09a20a873dee993.webp&#34;
               width=&#34;760&#34;
               height=&#34;476&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Math whizzes sum it up at LU contest</title>
      <link>http://localhost:1313/blog/19850217-globetimes/</link>
      <pubDate>Sun, 17 Feb 1985 16:59:16 -0400</pubDate>
      <guid>http://localhost:1313/blog/19850217-globetimes/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19850217-globetimes/PXL_20240405_210610335_hu_4e38a7ab5b9c15ed.webp 400w,
               /blog/19850217-globetimes/PXL_20240405_210610335_hu_741629d255659471.webp 760w,
               /blog/19850217-globetimes/PXL_20240405_210610335_hu_f5edfe27e38bb4b4.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19850217-globetimes/PXL_20240405_210610335_hu_4e38a7ab5b9c15ed.webp&#34;
               width=&#34;744&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Colonial connoisseurs</title>
      <link>http://localhost:1313/blog/19830525-morningcall/</link>
      <pubDate>Wed, 25 May 1983 10:58:26 -0400</pubDate>
      <guid>http://localhost:1313/blog/19830525-morningcall/</guid>
      <description>

















&lt;figure  id=&#34;figure-morning-call-photos----don-uhrich&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Morning Call photos -- Don Uhrich&#34; srcset=&#34;
               /blog/19830525-morningcall/19830525-MorningCall-clip_hu_3eba9deb965ae005.webp 400w,
               /blog/19830525-morningcall/19830525-MorningCall-clip_hu_dc38acf8dbd03b31.webp 760w,
               /blog/19830525-morningcall/19830525-MorningCall-clip_hu_a079519c34aa74ed.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19830525-morningcall/19830525-MorningCall-clip_hu_3eba9deb965ae005.webp&#34;
               width=&#34;760&#34;
               height=&#34;514&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Morning Call photos &amp;ndash; Don Uhrich
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Fourth-graders of the Bethlehem Area School District and Moravian Academy learn of Colonial ways by doing and viewing in the annual Junior Craftsmen Along the Monocacy program, which opened yesterday in the 18th Century Industrial Area near downtown Bethlehem, framed by window in left photo. More than 85 pupils in the 8th grade of the district&amp;rsquo;s middle schools, who have researched, studied and in some cases built their crafts, demonstrated for the 700 pupils in the 4th grade.  Above, Marifel Estrada of Spring Garden Elementary School seems to enjoy her &amp;lsquo;punishment&amp;rsquo; in a pillory, where she was placed by &lt;strong&gt;David Bader&lt;/strong&gt; (left) and David Rader (right). Activities are to continue 10 a.m. - 2 p.m. today.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nursery School Diploma</title>
      <link>http://localhost:1313/blog/19730510-nurseryschool/</link>
      <pubDate>Thu, 10 May 1973 19:03:46 -0400</pubDate>
      <guid>http://localhost:1313/blog/19730510-nurseryschool/</guid>
      <description>

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19730510-nurseryschool/diploma_hu_8e77a483d4b1d470.webp 400w,
               /blog/19730510-nurseryschool/diploma_hu_35518b2030149377.webp 760w,
               /blog/19730510-nurseryschool/diploma_hu_4575b96432cfb368.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19730510-nurseryschool/diploma_hu_8e77a483d4b1d470.webp&#34;
               width=&#34;585&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Diploma&lt;/p&gt;
&lt;p&gt;This is to certify &lt;strong&gt;David Bader&lt;/strong&gt;, Summa Quite Loudly, has successfully completed one year of intensive study and has been awarded the title of Bachelor of Arts and Crafts by The Brith Shalom Community Center Nursery School on this day of May 10, 1973.&lt;/p&gt;
&lt;p&gt;Herewith attested to by Elaine C. Weiner, Nursery School Instructor&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Snake Produced 4 &#39;Beans&#39;</title>
      <link>http://localhost:1313/blog/19720907-morningcall/</link>
      <pubDate>Thu, 07 Sep 1972 23:59:07 -0400</pubDate>
      <guid>http://localhost:1313/blog/19720907-morningcall/</guid>
      <description>&lt;p&gt;&lt;em&gt;By Ann Pongracz&lt;/em&gt;&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/19720907-morningcall/The_Morning_Call_Thu__Sep_7__1972_hu_281b20ce5fc2a3ee.webp 400w,
               /blog/19720907-morningcall/The_Morning_Call_Thu__Sep_7__1972_hu_ee8480ba978f8d73.webp 760w,
               /blog/19720907-morningcall/The_Morning_Call_Thu__Sep_7__1972_hu_2c360814eb829fcf.webp 1200w&#34;
               src=&#34;http://localhost:1313/blog/19720907-morningcall/The_Morning_Call_Thu__Sep_7__1972_hu_281b20ce5fc2a3ee.webp&#34;
               width=&#34;491&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Who&#39;s New: St. Luke&#39;s</title>
      <link>http://localhost:1313/blog/19690505-morningcall/</link>
      <pubDate>Mon, 05 May 1969 10:58:59 -0400</pubDate>
      <guid>http://localhost:1313/blog/19690505-morningcall/</guid>
      <description>&lt;p&gt;Twins, a boy and a girl, were born yesterday in St. Luke&amp;rsquo;s Hospital to Mr. and Mrs. Morris Bader of 1402 Lorain Ave., Bethlehem.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
